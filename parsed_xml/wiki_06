<doc id="3628628" url="https://en.wikipedia.org/wiki?curid=3628628" title="Dual mandate">
Dual mandate

A dual mandate is the practice in which elected officials serve in more than one elected or other public position simultaneously. This practice is sometimes known as double jobbing in Britain and cumul des mandats in France; not to be confused with "double dipping" in the United States, i.e. being employed by and receiving a retirement pension from the same public authority at the same time. Thus, if someone who is already mayor of a town or city councillor becomes elected as MP or senator at the national or state legislature and retains both positions, this is a dual mandate.

Dual mandates are sometimes prohibited by law. For example, in federal states, federal office holders are often not permitted to hold state office. In states with a presidential system of government, membership of the executive, the legislature, or the judiciary generally disqualifies a person from simultaneously holding office in either of the other two bodies. In states with bicameral legislatures, one usually cannot simultaneously be a member of both houses. The holder of one office who wins election or appointment to another where a dual mandate is prohibited must either resign the former office or refuse the new one.

A member of the European Parliament (MEP) may not be a member of the legislature of a member state. This dates from a 2002 European Union decision, which came into effect at the 2004 European elections in most member states, at the 2007 national election in the Republic of Ireland, and at the 2009 European elections in the United Kingdom.

Originally, MEPs were nominated by national parliamentarians from among their own membership. Prior to the first direct elections in 1979, the dual mandate was discussed. Some advocated banning it, arguing that MEPs who were national MPs were often absent from one assembly in order to attend the other (indeed, the early death of Peter Michael Kirk was blamed by his election agent on overwork resulting from his dual mandate). Others claimed that members with a dual mandate enhanced communication between national and European assemblies. There was a particular interest in the dual mandate question in Denmark: Eurosceptic Danish Social Democrats supported a compulsory dual mandate, to ensure that the state's MEPs expressed the same views as the national legislature, and the government of Denmark supported a compulsory dual mandate when the other eight member states supported an optional dual mandate. However, a 1976 European Parliament law preparing for the 1979 elections expressly permitted a dual mandate. In 1978 the German politician Willy Brandt suggested that one third of MEPs should be national MPs.

Dual mandates are rare in Australia. It is illegal to be a member of any state parliament and the Australian parliament simultaneously. A member of a state parliament seeking federal office must resign before seeking election to the Federal Parliament. It is possible but unusual to be a member of a local government and another parliament.

In 2004 Clover Moore became the independent member for Sydney in the NSW Parliament without resigning as Lord Mayor of Sydney. The issue of Moore holding both positions had brought the issue to the forefront in Australia and led the premier of New South Wales in 2012 to propose a new law, dubbed in the media as the "Get Clover bill", which banned this dual mandate. The proposed law was adopted and in September 2012 Moore resigned her NSW seat soon after she was reelected as mayor.

As in neighboring France, the culture of dual mandates is very strong in Belgium and that country currently has one of the highest percentage of dual mandate holders (MPs, aldermen, municipal councilors) in the world. During the 2003–2009 period, 87.3% of members of the Walloon (French-speaking) Parliament held dual mandates, followed by 86.5% in the Flemish (Dutch-speaking) Parliament, 82 0% in the Chamber of Representatives (the Federal lower house) and 68.9% in the Senate. During that same period, 76.5% of all European Parliament MPs from Belgium held dual mandates.

More than one-fifth of all Belgian MPs were mayor at the same time with, by far, by the highest proportion (40%) to be found in the Walloon Parliament.

In Canada dual mandates are rare and are frequently barred by legislation at the federal, provincial, or territorial level. At the federal level, section 39 of the Constitution Act, 1867 prevents a Senator from being elected as a Member of Parliament; similarly, s. 65(c) of the Canada Elections Act makes members of provincial or territorial legislatures ineligible to be candidates to the House of Commons. At the provincial level, the situation varies from one province to another.

In other circumstances, an elected official almost always resigns their first post when elected to another. Dual representation has occurred occasionally when the member was elected to a second office shortly before their other term of office was due to expire anyway and whereby the short time frame would not merit the cost of a special by-election. In 1996, for example, Jenny Kwan continued to be a Vancouver city councillor after being elected to the provincial legislature. The British Columbia legislature had debated a "Dual Office Prohibition Act" which failed to pass second reading.

In the first few years after Confederation in 1867, however, double mandates were common. In the first House of Commons, there were fifteen Members of Parliament from Quebec who simultaneously held seats in the National Assembly of Quebec, including the Premier Pierre-Joseph-Olivier Chauveau. There were also four members of Parliament from Ontario who also held seats in the Legislative Assembly of Ontario, including the first two Premiers, John Sandfield Macdonald and Edward Blake. Other prominent federal politicians with double mandates included George-Étienne Cartier, Christopher Dunkin, Hector Langevin, the second Premier of British Columbia Amor de Cosmos, and two members from Manitoba, Donald Smith and Pierre Delorme. Another famous example is that of the "de facto" leader of the Liberals, George Brown, who ran for both federal and provincial seats in 1867. Brown lost both elections, and soon thereafter began campaigning for the prohibition of double mandates.

The double mandate was prohibited from the start in Nova Scotia and New Brunswick; it was abolished in Ontario in 1872, in Manitoba in 1873, and in 1873 the federal parliament passed a law against it; Quebec passed its own law abolishing it in 1874.

However, dual mandates within a province remained legal. From 1867 to 1985, 305 mayors were also members of the Quebec legislative assembly (MLA). The two best-known cases were those of S.N. Parent who was simultaneously mayor of Quebec City (1894-1906), MLA and Premier of Quebec (1900-1905). Longtime Montreal Mayor Camilien Houde (1928–32, 1938–40) was also simultaneously MLA for a total of 2 /1/2 years during his mandates as mayor. However that type of dual mandate had virtually ceased when laws adopted in 1978 and 1980 prohibited MLAs from holding any local mandate.

It is common for the MPs of the Finnish Parliament to hold a mandate as a member of their local municipal council as well. 79 percent of MPs elected to parliament in 2011 were also municipal council members.

The "cumul des mandats" (, "accumulation of mandates") is a common practice in the French Fifth Republic (1958–present). It consists of simultaneously holding two or more elective offices at different levels of government — local, regional, national and European — as mayors, MPs, senators, Members of the European Parliament, and President of the General Council in their home regions. Sometimes, officials hold as many as four positions. While officials may not be elected to more than one office at the same level (such as being both an MP and a senator), they may hold offices in any combination at the municipal, departmental, regional, national and European levels. The "cumul des mandats" is controversial in France, being accused of fostering absenteeism and cronyism.

Several laws to limit the practice have been introduced in recent decades. By far the most coveted local mandate is that of mayor, traditionally a highly prestigious function in France.

A hotly debated law to prohibit all dual mandates, to take effect in 2017, was adopted in July 2013. Following the adoption of the law, former President Sarkozy and other members of the opposition UMP party have declared that if elected in 2017, their party would revise or even revoke that law. Many Socialist Party MPs and senators have also expressed their unease with the law imposed by President Hollande and might welcome a review of the law. In the meantime, the ubiquitous 'député-maire' (MP and mayor) and 'sénateur-maire' are still familiar figures of the French political scene.


Multiple mandates at the legislative level

Parliamentary mandates are incompatible with each other:


A member from one of the above assemblies can not combine its mandate with more than one of the following mandates :


Exceptions: They can hold a third office in a town of less than 3,500 inhabitants.

They may also hold a third office as a councillor, vice-president or president of an Urban community, an Agglomeration community or a "Communauté de communes", as these terms are elected by indirect universal suffrage, by municipal councils from among the councillors.

For example, a member of the National Assembly has the right to be general/regional councillor or President of a regional/general council. They cannot hold a third office unless they are the mayor, deputy mayor or municipal councillor of a city of less than 3,500 inhabitants.

In 2008, 85% of members of parliament held multiple posts Following the June 2012 legislative elections, it was still the case that 85% of all National Assembly members (438 deputies out of 577) held a double mandate (often as mayor of a mid- to large-size city) and 33 have four mandates. Currently, out of 348 senators, 152 are also mayors.

The accumulation of local mandates

They cannot have more than two local mandates.

The following mandates are incompatible each other:


For example, an elected official cannot be mayor and President of the Regional Council. However, all other local mandates are cumulative. A mayor can also be a general councillor and a president of a Regional Council can also be deputy-mayor of a city.

Exceptions are the same as those for parliamentarians (Cities of less than 3,500 inhabitants and the intercommunalities)

The accumulation of mandates and governmental functions 

A member of the French government cannot be a member of any assembly. However, he may retain any local mandate he or she holds. A cabinet minister can exercise a maximum of two local mandates in addition to his or her government function.

For example, the Prime Minister, a Minister or Secretary of State can be mayor, or President of a general, regional or intercommunal council or sit in one of these assemblies.

Currently, over two-thirds of the members of the French government are engaged in one or two more local mandates.

The rationales for holding multiple offices are varied. Holding a seat in the Senate, National Assembly, or European Parliament gives local mayors a valuable method of tapping funds to develop their home cities and regions. It also can give opportunities to curry favor with other important officials, with opportunities at each level. Salaries for positions can be combined (to a point) as well. For politicians with national ambitions, retaining a position in a local town can give them a down-to-earth aura that can appeal to voters. These advantages have made politicians very wary of reducing the practice of the "cumul" with legislation despite other moves to end perceptions of favoritism and corruption among politicians.

It has been common practice in France since the French Third Republic (1870). But there are also many cases of "cumul" before this period, for example, the writer Alexis de Tocqueville was a member from 1839 to 1851. In 1849 he was appointed Foreign Affairs minister, and at the same time he was elected President of the General Council of Manche from 1849 to 1851 (councillor from 1842 to 1852).

There are several reasons for this phenomenon, and one of them is that France has a long tradition of centralization, compared to countries such as Germany, Italy, and Spain. Local governments have less power and skills than that the "Länder" of Germany, or "Autonomous Communities" of Spain. The local mandates in France are less important than in other countries, and therefore politicians have more time to devote to a parliamentary mandate.

The "cumul" is a widespread practice and has grown much more prevalent in modern France. In 1946, 36 percent of deputies in the National Assembly held an additional office. By 1956, this number had already increased to 42 percent and by 1970, 70 percent of deputies held an additional elected office; in 1988, 96 percent did.

Many of the most prominent politicians in France make use or have made use of the "cumul". Jacques Chirac served as Mayor of Paris between 1977 and 1995. During this same time, Chirac also served as a deputy in the National Assembly from Corrèze, briefly as Member of the European Parliament, and even as Prime Minister between 1986 and 1988. Former Prime Minister Pierre Bérégovoy served concurrently as mayor of Nevers and deputy of Nièvre in the mid-1980s.

According to French law against accumulation of electoral mandates, Yves Jégo should have resigned from one of the following mandates before the 21st of April 2010 (one month after the Regional elections) :
But giving as a pretext a legal complaint from the Front National's candidates, he hold the three of them during more than one year, plus his local mandate of president of the « communauté de communes des deux fleuves » (CC2F).

Lionel Jospin (Prime Minister from 1997 to 2002) imposed on his government ministers an unwritten rule of having no local office. For example, Catherine Trautmann stepped down as Mayor of Strasbourg (while remaining a member of the city council) to become Minister of Culture; conversely, Martine Aubry stepped down from the Ministry of Labour when elected Mayor of Lille in 2001. This rule was more or less upheld by Jacques Chirac during the governments of Jean-Pierre Raffarin and Dominique de Villepin for the 2002-2007 term, with a few notable exceptions (Jean-François Copé was mayor of Meaux, Nicolas Sarkozy was President of the Hauts-de-Seine General Council); for instance, Philippe Douste-Blazy had to step down from the Toulouse mayorship upon joining the government.

, no such rule was stated for the François Fillon government: Alain Juppé, former Minister for Development was mayor of Bordeaux, and was defeated in his National Assembly constituency (a third cumulative mandate) by 50.9% to 49.1% of the votes by the Socialist candidate. Additionally, Hervé Morin, the Minister of Defense, is mayor of Épaignes, and Éric Besson, Minister of Immigration and National Identity, is the mayor of Donzère.

In Hong Kong, dual mandate is common for members of the territory's Legislative Council, who serve concurrently as members of one of the territory's eighteen district councils. Before the abolition of the two municipal councils in the territory in 1999, it was common for politicians to serve concurrently at all three levels.

The instability caused by the close result of the 1981 general election was exacerbated by the number of government TDs who also served as MEPs and for whom the opposition refused pairing when they were abroad. This led to further elections in February 1982 and again in November.

In 1991, cabinet ministers and junior ministers were prohibited from serving as local councillors. The prohibition was extended to other Oireachtas members by the Local Government (No. 2) Act 2003, an amendment to the Local Government Act 2001. Attempts to include it in the 2001 Act failed after a rebellion by Fianna Fáil backbenchers; the 2003 Act passed after a compensation package was agreed for those losing out.

The 2001 Act prohibited being a member of multiple county or city councils, or multiple town councils, or both a town and city council. Brian O'Shea was a member of both Waterford City Council and Waterford County Council until 1993. County councillors were allowed to sit on a town council, and many did so. The 2003 Act provided that a candidate elected simultaneously to a forbidden combination of local councils has three days to choose which seat to take up, with the other or others then being considered vacant. The Local Government Reform Act 2014 abolished town councils and instead subdivided most counties into municipal districts; the county council's members are the district councillors for all districts within the county.
Per the Spanish Constitution, legislators in the regional assemblies of the Autonomous Communities are barred from being elected to a seat in the Congress of Deputies, the lower house of the Cortes Generales. More precisely, regional legislators can run for the seat, but if elected they must choose between the regional and national parliaments. Nevertheless, members of lower tiers of the Spanish decentralized structure, such as provincial councillors or members of local councils, including mayors, can and have held seats in the Congress of Deputies. The rule barring regional legislators does not apply to the upper house of the Cortes, the Senate: in fact, regional legislatures are entitled to appoint a varying number of members from their ranks to the Senate, according to the population of the region. Currently, the Autonomous Communities appoint 56 Senators, the other 208 being directly elected in general elections.

At the EU level, prior to the 2009 European Parliament elections, there were a small number of members of the European Parliament who were also members of the House of Lords. However, it is now European law that a member of the European Parliament (MEP) may not be a member of the legislature of a member state. This, with regard to the United Kingdom, therefore applies to the House of Commons and the House of Lords, as the constituent bodies forming that member state's legislature. As it is impossible to disclaim a life peerage, it has been ruled that peers (who sit as members of the House of Lords) must take a "leave of absence" from the Lords in order to be an MEP; this is also the procedure for when a peer is the UK's European Commissioner, which has in recent times usually been the case.

There have been members of the House of Commons also holding seats in the devolved bodies in Scotland, Wales and Northern Ireland. The November 2009 report by the Committee on Standards in Public Life into the controversy surrounding MPs' expenses noted that "double jobbing" was "unusually ingrained in the political culture" of Northern Ireland, where 16 of 18 MPs were MLAs, compared to one Scottish MP being an MSP (First Minister Alex Salmond), and no Welsh MPs being AMs. The Committee recommended that Westminster ban multiple mandates from the 2011 assembly elections. Parties in Northern Ireland agreed to a ban from the 2015 elections. The ability to dual mandate between the Assembly of Northern Ireland and the House of Commons (or the Irish Dáil Éireann) ended as from the Northern Ireland Assembly election, 2016 following the Northern Ireland (Miscellaneous Provisions) Act 2014. The Wales Act 2014 also applied a similar restriction on the National Assembly for Wales (in that its members cannot also be members of the House of Commons) as from the assembly election in 2016. As of 2019, it remains possible for members of the Scottish Parliament to be members of the UK Parliament, though at present none are.

In circumstances other than the Greater Manchester mayoralty, UK law does not prohibit a member of the House of Commons or the House of Lords from being simultaneously a mayor or council leader. They are also not allowed to serve as a Councillor for a constituent council if elected as a directly elected Mayor. Thus Ken Livingstone remained MP for Brent East until the dissolution of Parliament despite his election as Mayor of London a year before. Boris Johnson resigned his seat as MP for Henley on being elected mayor in 2008, but became an MP again in 2015, a year prior to the end of his second term as mayor (he did not seek a third term). Sadiq Khan, elected as the Labour mayor in the 2016 election, resigned his seat as MP for Tooting soon after his election to the mayoralty. Numerous members of the House of Lords however hold positions in local government.

At a lower level, it is common for people to hold seats on both a district council and a county council. Several MPs have also retained their council seats, most often until the expiration of their terms; Mike Hancock simultaneously held a council seat and a seat in Parliament between his election to Parliament in 1997 and his defeat in the local elections in 2014.

Some members of the Irish republican party Sinn Féin held the office as a Member of The Northern Ireland Assembly and Member of Parliament within the respective houses. Martin McGuinness, former Deputy First Minister of Northern Ireland held the office as Member of the Northern Ireland Assembly and simultaneously was a MP in the House of Commons. However, in 2012 Sinn Féin committed to end dual jobbing; this resulted in McGuinness' resignation from the House of Commons in 2013.
"The term dual mandate is also applied to the twin objectives of the Federal Reserve Bank: to control inflation and promote employment."

The United States Constitution prohibits members of the Senate or House from holding positions within the Executive Branch (Art. I, Sec. 6, cl. 2), and limits the president to his salary as chief executive, saying he may not "receive... any other Emolument from the United States, or any of them" (Art. II, Sec. 1, cl. 7). However, the Constitution places no restrictions that would prevent state or local office holders from simultaneously holding office in any branch of the federal government.

Historically, the U.S. inherited many basic political traditions from Great Britain, which in the eighteenth century tolerated several different forms of dual mandate. Following the establishment of the original Continental Congress and later Confederation Congress, the states possessed absolute discretion in regards to how delegates were chosen to serve, and it became common for state legislatures to appoint members from within their own ranks to Congress. At the time, this was a largely uncontroversial practise since it was widely assumed that the Congress would have relatively little to do (especially in peacetime) and that most of the consequential decision-making would take place at the state and local levels. A ban on dual mandates would therefore have been widely seen as unnecessary and unwelcome as it would have effectively barred Congressional delegates from what were perceived to be more important political posts, thus making election to the national Congress (already seen as a considerable burden due to the difficulties of eighteenth century travel) quite undesirable.

During the convention that established the present U.S. constitution, attention was primarily given to designing a federal government with branches that would be able operate independently of each other and free of undesirable foreign influence, which resulted in the aforementioned prohibitions. Barring state and local officials from federal office was not seriously debated. If it had been, it would likely have been fiercely opposed especially by the nascent anti-Federalist movement, many of whose members were keen to ensure that state officials with a vested interest in defending states' rights would be allowed to also serve simultaneously at the federal level, especially in Congress.

For the first few decades after the First United States Congress convened in 1789, Congress met infrequently and some states endeavored to accommodate dual mandates by holding their legislative sessions at times that would not conflict with Congressional sessions. Eventually, as the federal government grew in importance, Congress came to be seen as a source of great power which made the potential for conflicts of interest made increasingly difficult to justify holding mandates at different levels of government to voters. In a closely related development, Congress began meeting more frequently than originally intended, which eventually made it impractical in most states for one person to serve simultaneously in the state and federal governments. 

In time, the vast majority of states banned dual state and federal mandates. Today, the practice is forbidden by many state constitutions of many U.S. states, but as of January, 2018 it was still legal in Connecticut, only for municipal offices. Unlike many other attempts at the state level intended to place additional restrictions besides those in the U.S. Constitution regarding who can represent them in Congress, most of which have been ruled unconstitutional by the United States Supreme Court, state-originated bans on dual mandates are constitutional because their prohibitions technically restrict who is allowed to serve at the state and/or local level (i.e. they typically place some sort of "de jure" prohibition barring federal officials from simultaneously serving at the state and/or local levels, resulting in a "de facto" prohibition on the reverse arrangement occurring).

Unlike many federations, U.S. states do not generally restrict state or federal officials from seeking office at another level of government without resigning their existing offices first. For example, in the four U.S. presidential elections contested from 1988 to 2000 inclusive, three sitting state governors were nominated for the presidency, these being Michael Dukakis in 1988, Bill Clinton in 1992 and George W. Bush in 2000 (Clinton and Bush were elected president), while in 2016 sitting governor Mike Pence was elected vice president. Elsewhere, serving state officials often seek federal office, one prominent example being Illinois State Senator Barack Obama's election to the United States Senate in 2004 - Obama quickly resigned from the Illinois Senate after being elected to the U.S. Senate despite not being legally required to do so, and served as a U.S. senator until 2008 when he was elected president. Also, it is not uncommon for sitting federal officials to contest election to state offices, although in these cases the office sought is usually one of the state's highest political posts, typically governor - one such recent example being the aforementioned Mike Pence who was a sitting U.S. Representative when he was first elected governor.

Also typically permitted is for one person to seek multiple offices at the same level of government in the same election, although attempting to simultaneously seek multiple offices in the same branch of government (e.g. a sitting U.S. Representative seeking re-election to the House "and" election to the U.S. Senate) is severely frowned on and prohibited in many states (the constitutionality of these prohibitions is uncertain). Recent examples include three 2000 and 2012 presdential elections where Senator Joe Lieberman and Representative Paul Ryan respectively sought re-election and election to the vice presidency - neither was elected vice president, but both were re-elected to the offices in which they were the incumbents.

In April 1984, Governor of Florida Bob Graham received legislation that passed unanimously in both houses of the Florida Legislature that would forbid public officials from receiving retirement pay and regular pay simultaneously for the same position.

In August 2008, Governor of Illinois Rod Blagojevich proposed legislation that would prohibit dual-office holding as part of changes to the state's ethics bill, stating that "dual government employment creates the potential for a conflict of interest because a legislator's duties to his or her constituents and his or her public employer are not always consistent." Critics, such as Representative Susana Mendoza, called the actions "spite" on the part of the governor.

Fulfilling a campaign pledge that he had made when first running for the New Jersey Legislature, Jack Sinagra sponsored a bill passed by the New Jersey Senate in 1992 that would ban the practice. At the time that the legislation first passed, there were some twenty elected officials who served in the New Jersey Legislature and another elected office, including Assemblyman Bill Pascrell, who was also mayor of Paterson, New Jersey; State Senator Ronald Rice, who also served on the Newark City Council; and Assemblyman John E. Rooney, who was also mayor of Northvale. These officials protested the proposed ban as interfering with the will of voters to elect officials as they see fit. A newspaper called former State senator Wayne R. Bryant the "king of double dipping" because he was collecting salaries from as many as four public jobs he held simultaneously.

Governor of New Jersey Jon Corzine signed legislation in September 2007 that banned the practice statewide, but the 19 legislators holding multiple offices as of February 1, 2008, were grandfathered into the system and allowed to retain their positions. As of January 2013, only four of the nineteen (listed in bold) continue to hold a dual mandate.
Senators:
Assembly members:

In February 2001, Jean Schmidt introduced legislation in the Ohio House of Representatives that would forbid public officials from receiving a government pension while still serving in office.




</doc>
<doc id="16553762" url="https://en.wikipedia.org/wiki?curid=16553762" title="Legislative calendar">
Legislative calendar

A legislative calendar is used by legislatures to plan their business during the legislative session. 

Typically, one of the first items mentioned on the calendar is passing the bill enacting procedures and deadlines for the session. Time may also be allotted for considering the budget bill, which is usually the major item of business in a session. The calendar may provide scheduled committee hearings and generally includes many important deadlines. 

For instance, California has a fiscal deadline, which is the date on the legislative calendar by which all bills with fiscal effect must have been taken up in a policy Committee and referred to a fiscal Committee; any fiscal bill missing the deadline is considered "dead" unless it receives a rule waiver allowing further consideration. 

Some legislatures have a "crossover day," which is the point in the session after which each house only considers legislation sent to it by the other house. In the U.S. Congress, the phrase "placed on calendar" accompanies a bill that is pending before committees of either house; the bill is assigned a calendar number, which determines when it will be considered by that house.

On a legislative calendar, a "legislative day" is a day on which the Legislature actually meets. The Virginia General Assembly has six legislative days per week (Monday through Saturday), probably reflecting the desire to have a citizen legislature that accomplishes its business in a relatively short, intense annual session, after which the members return to their full-time employment. The Oklahoma legislature, by contrast, has four legislative days per week.

The daily version of the legislative calendar is sometimes called the daily file, agenda or calendar, which lists all the bills that will be considered on a given day. 

The term "legislative calendar" can also refer to the final published compilation of the action on each instrument during a legislative session. It can also refer to a list of legislation available to be heard by the Legislature.



</doc>
<doc id="4927404" url="https://en.wikipedia.org/wiki?curid=4927404" title="Legislative session">
Legislative session

A legislative session is the period of time in which a legislature, in both parliamentary and presidential systems, is convened for purpose of lawmaking, usually being one of two or more smaller divisions of the entire time between two elections. In each country the procedures for opening, ending, and in between sessions differs slightly. A session may last for the full term of the legislature or the term may consist of a number of sessions. These may be of fixed duration, such as a year, or may be used as a parliamentary procedural device. A session of the legislature is brought to an end by an official act of prorogation. In either event, the effect of prorogation is generally the clearing of all outstanding matters before the legislature.

Historically, each session of a parliament would last less than one year, ceasing with a prorogation during which legislators could return to their constituencies. In more recent times, development in transportation technology has permitted these individuals to journey with greater ease and frequency from the legislative capital to their respective electoral districts (sometimes called "ridings", electorate, division) for short periods, meaning that parliamentary sessions typically last for more than one year, though the length of sessions varies. Legislatures plan their business within a legislative calendar, which lays out how bills will proceed before a session ceases, although related but unofficial affairs may be conducted by legislators outside a session or during a session on days in which parliament is not meeting.

While a parliament is prorogued, between two legislative sessions, the legislature is still constitutedi.e. no general election takes place and all Members of Parliament thus retain their seats. In many legislatures, prorogation causes all orders of the bodybills, motions, etc.to be expunged. Prorogations should thus not be confused with recesses, adjournments, or holiday breaks from legislation, after which bills can resume exactly where they left off. In the United Kingdom, however, the practice of terminating all bills upon prorogation has slightly altered; public bills may be re-introduced in the next legislative session, and fast-tracked directly to the stage they reached in the prorogued legislative session.

This break takes place so as to prevent the upper house from sitting during an election campaign and to purge all upper chamber business before the start of the next legislative session. It is not uncommon for a session of parliament to be put into recess during holidays and then resumed a few weeks later exactly where it left off. Governments today end sessions whenever it is most convenient, and often, a new session will begin on the same day that the previous session ended. In most cases, when parliament reconvenes for a new legislative session, the head of state, or a representative thereof, will address the legislature in an opening ceremony.

In both parliamentary and presidential systems, sessions are referred to by the name of the body and an ordinal numberfor example, the "2nd Session of the 39th Canadian Parliament" or the "1st Session of the 109th United States Congress".

In Commonwealth realms, legislative sessions can last from a few weeks to over a year; between general elections; there are usually anywhere from one to six sessions of parliament before a dissolution by either the Crown-in-Council or the expiry of a legally mandated term limit. Each session begins with a speech from the throne, read to the members of both legislative chambers either by the reigning sovereign or a viceroy or other representative. Houses of parliament in some realms will, following this address, introduce a "pro forma" bill as a symbol of the right of parliament to give priority to matters other than the monarch's speech (always written by the cabinet of the day).

In the parliament of the United Kingdom, prorogation is immediately preceded by a speech to both legislative chambers, with procedures similar to the Throne Speech. The monarch usually approves the oration—which recalls the prior legislative session, noting major bills passed and other functions of the government—but rarely delivers it in person, Queen Victoria being the last to do so. Instead, the speech is presented by the Lords Commissioners and read by the Leader of the House of Lords. When King Charles I dissolved the Parliament of England in 1628, after the Petition of Right, he gave a prorogation speech that effectively cancelled all future meetings of the legislature, at least until he again required finances.

Prior to 1977, it was common for the federal Parliament to have up to three sessions, with Parliament being prorogued at the end of each session and recalled at the beginning of the next. This was not always the case, for instance the 10th Parliament (1926–1928) went full term without prorogation. The practice of having multiple sessions in the same parliament gradually fell into disuse, and all parliaments from 1978 to 2013 had a single session. (There were only four prorogations since 1961, twice to allow the visiting Queen to "open" Parliament, once after the death of Prime Minister Harold Holt and for political reasons in 2016.) Since 1990, it has been the practice for the parliament to be prorogued on the same day that the House is dissolved so that the Senate will not be able to sit during the election period.

However, on 21 March 2016, Prime Minister Malcolm Turnbull announced that the 44th Parliament, elected in 2013, would be prorogued on 15 April and that a second session would begin on 18 April. Prorogation is now a procedural device, the effect of which is to call the Parliament back on a particular date (especially the Senate, which the government did not control), and to wipe clean all matters before each House, without triggering an election.

In the Parliament of Canada and its provinces, the legislature is typically prorogued upon the completion of the agenda set forth in the Speech from the Throne (called the "legislative programme" in the UK). It remains in recess until the monarch, governor general, or lieutenant governor summons parliamentarians again. Historically, long prorogations allowed legislators to spend part of their year in the capital city and part in their home ridings. However, this reason has become less important with the advent of rapid transcontinental travel.

More recently, prorogations have triggered speculation that they were advised by the sitting prime minister for political purposes: for example, in the 40th Parliament, the first prorogation occurred in the midst of a parliamentary dispute, in which the opposition parties expressed intent to defeat the minority government, and the second was suspected by opposition Members of Parliament to be a way to avoid investigations into the Afghan detainees affair and triggered citizen protests. In October 2012, the provincial legislature of Ontario was prorogued under similar circumstances, allegedly to avoid scrutiny of the provincial Government on a number of issues.

Bills are numbered within each session. For example, in the federal House of Commons each session's government bills are numbered from C-2 to C-200, and the numbering returns again to C-2 following a prorogation (Bill C-1 is a "pro-forma" bill).

In the United States, some state legislatures meet only part of the year. Depending upon limitations of the state's constitution, if business arises that must be addressed before the next regular session, the governor may call a special session.

The US Congress is renewed every two years as required by the US Constitution, with all members of the House of Representatives up for reelection and one-third of the members of the US Senate up for reelection. (Senators serve a six-year term; House members serve a two-year term). Each Congress sits in two sessions lasting approximately one year. Thus, the 1st session of the 114th Congress commenced on January 3, 2015 and the 2nd session commenced on January 3, 2016, with the same members and no intervening election. All legislative business, however, is cleared at the end of each session. It is common for bills to be reintroduced in the second session that were not passed in the first session, and the restrictions on reconsideration only apply to a single session.

When the leaders of the majority party in each house have determined that no more business will be conducted by that house during that term of Congress, a motion is introduced to adjourn "sine die", effectively dissolving that house. Typically, this is done at some point after the general congressional election in November of even-numbered years. If the party in power is retained, it may happen as early as mid-November and members return to their districts for the holiday season. However, when the party in power is ousted or if important business, such as approval of appropriation bills, has not been completed, Congress will often meet in a lame-duck session, adjourning as late as December 31, before the newly elected Congress takes office on January 3.



</doc>
<doc id="5670063" url="https://en.wikipedia.org/wiki?curid=5670063" title="Favourite">
Favourite

A favourite (Australian, British, and Canadian English) or favorite (American English) was the intimate companion of a ruler or other important person. In post-classical and early-modern Europe, among other times and places, the term was used of individuals delegated significant political power by a ruler. It was especially a phenomenon of the 16th and 17th centuries, when government had become too complex for many hereditary rulers with no great interest in or talent for it, and political institutions were still evolving. From 1600 to 1660 there were particular successions of all-powerful minister-favourites in much of Europe, particularly in Spain, England, France and Sweden.

The term is also sometimes employed by writers who want to avoid terms such as "royal mistress", "friend", "companion", or "lover" (of either sex). Several favourites had sexual relations with the monarch (or the monarch's spouse), but the feelings of the monarch for the favourite ran the gamut from a simple faith in the favourite's abilities to various degrees of emotional affection and dependence, and sometimes even encompassed sexual infatuation.

The term has an inbuilt element of disapproval and is defined by the "Oxford English Dictionary" as "One who stands unduly high in the favour of a prince", citing Shakespeare: "Like favourites/ Made proud by Princes" ("Much Ado about Nothing", 3.1.9).

Favourites inevitably tended to incur the envy and loathing of the rest of the nobility, and monarchs were sometimes obliged by political pressure to dismiss or execute them; in the Middle Ages nobles often rebelled in order to seize and kill a favourite. Too close a relationship between monarch and favourite was seen as a breach of the natural order and hierarchy of society. Since many favourites had flamboyant "over-reaching" personalities, they often led the way to their own downfall with their rash behaviour. As the opinions of the gentry and bourgeoisie grew in importance, they too often strongly disliked favourites. Dislike from all classes could be especially intense in the case of favourites who were elevated from humble, or at least minor, backgrounds by royal favour. Titles and estates were usually given lavishly to favourites, who were compared to mushrooms because they sprang up suddenly overnight, from a bed of excrement. The King's favourite Piers Gaveston is a "night-grown mushrump" (mushroom) to his enemies in Christopher Marlowe's "Edward II".

Their falls could be even more sudden, but after about 1650, executions tended to give way to quiet retirement. Favourites who came from the higher nobility, such as Leicester, Lerma, Olivares, and Oxenstierna, were often less resented and lasted longer. Successful minister-favourites also usually needed networks of their own favourites and relatives to help them carry out the work of government – Richelieu had his "créatures" and Olivares his "hechuras". Oxenstierna and William Cecil, who both died in office, successfully trained their sons to succeed them.

The favourite can often not be easily distinguished from the successful royal administrator, who at the top of the tree certainly needed the favour of the monarch, but the term is generally used of those who first came into contact with the monarch through the social life of the court, rather than the business of politics or administration. Figures like William Cecil and Jean-Baptiste Colbert, whose accelerated rise through the administrative ranks owed much to their personal relations with the monarch, but who did not attempt to behave like grandees of the nobility, were also often successful. Elizabeth I had Cecil as Secretary of State and later Lord High Treasurer from the time she ascended the throne in 1558 until his death 40 years later. She had more colourful relationships with several courtiers; the most lasting and intimate one was with Robert Dudley, Earl of Leicester, who was also a leading politician. Only in her last decade was the position of the Cecils, father and son, challenged by Robert Devereux, 2nd Earl of Essex, when he fatally attempted a coup against the younger Cecil.

Cardinal Wolsey was one figure who rose through the administrative hierarchy, but then lived extremely ostentatiously, before falling suddenly from power. In the Middle Ages in particular, many royal favourites were promoted in the church, English examples including Saints Dunstan and Thomas Becket; Bishops William Waynflete, Robert Burnell and Walter Reynolds. Cardinal Granvelle, like his father, was a trusted Habsburg minister who lived grandly, but he was not really a favourite, partly because most of his career was spent away from the monarch.

Some favourites came from very humble backgrounds: Archibald Armstrong, jester to James I of England infuriated everyone else at court but managed to retire a wealthy man; unlike Robert Cochrane, a stonemason (probably a senior one, more like an architect than an artisan) who became Earl of Mar before the Scottish nobles revolted against him, and hanged him and other low-born favourites of James III of Scotland. Olivier le Daim, the barber of Louis XI, acquired a title and important military commands before he was executed on vague charges brought by nobles shortly after his master died, without the knowledge of the new king. It has been claimed that le Daim's career was the origin of the term, as "favori" (the French word) first appeared around the time of his death in 1484. "Privado" in Spanish was older, but was later partly replaced by the term "valido"; in Spanish, both terms were less derogatory than in French and English.

Such rises from menial positions became progressively harder as the centuries progressed; one of the last families able to jump the widening chasm between servants and nobility was that of Louis XIV's valet, Alexandre Bontemps, whose descendants, holding the office for a further three generations, married into many great families, even eventually including the extended royal family itself. Queen Victoria's John Brown came much too late; the devotion of the monarch and ability to terrorize her household led to hardly any rise in social or economic position.

In England, the scope for giving political power to a favourite was reduced by the growing importance of Parliament. After the "mushroom" Buckingham was assassinated by John Felton in 1628, Charles I turned to Thomas Wentworth, 1st Earl of Strafford, who had been a leader of Parliamentary opposition to Buckingham and the King, but had become his supporter after Charles made concessions. Strafford can therefore hardly be called a favourite in the usual sense even though his relationship with Charles became very close. He was also from a well-established family, with powerful relations. After several years in power, Strafford was impeached by a Parliament now very hostile to him. When that process failed, it passed a bill of attainder for his execution without trial, and it put enough pressure on Charles that to his subsequent regret, Charles signed it, and Strafford was executed in 1641. There were later minister-favourites in England, but they knew that the favour of the monarch alone was not sufficient to rule, and most also had careers in Parliament.

In France, the movement was in the opposite direction. On the death of Cardinal Mazarin in 1661, the 23-year-old Louis XIV determined that he would rule himself, and he did not allow the delegation of power to ministers that had marked the previous 40 years. The absolute monarchy pioneered by Cardinal Richelieu, Mazarin's predecessor, was to be led by the monarch himself. Louis had many powerful ministers, notably Jean-Baptiste Colbert, in finances, and François-Michel le Tellier, Marquis de Louvois, the army, but overall direction was never delegated, and no subsequent French minister ever equaled the power of the two cardinals.

The Spanish Habsburgs were not capable of so much energy, but when Olivares was succeeded by his nephew, Luis Méndez de Haro, the last real "valido", the control of government into a single pair of hands had already been weakened.

Favourites were the subject of much contemporary debate, some of it involving a certain amount of danger for the participants. There were a large number of English plays on the subject, amongst the best known to be Marlowe's "Edward II" in which Piers Gaveston is a leading character, and "Sejanus His Fall" (1603), for which Ben Jonson was called before the Privy Council, accused of "Popery and treason", as the play was claimed by his enemies to contain allusions to the contemporary court of James I of England. Sejanus, whose career under Tiberius was vividly described by Tacitus, was the subject of numerous works all around Europe. Shakespeare was more cautious, and with the exceptions of Falstaff, badly disappointed in his hopes of becoming a favourite, and Cardinal Wolsey in "Henry VIII", he gives no major parts to favourites.

Francis Bacon, almost a favourite himself, devoted much of his essay "On Friendship" to the subject, writing as a rising politician under Elizabeth I:

It is a strange thing to observe, how high a rate great kings and monarchs do set upon this fruit of friendship, whereof we speak: so great, as they purchase it, many times, at the hazard of their own safety and greatness. For princes, in regard of the distance of their fortune from that of their subjects and servants, cannot gather this fruit, except (to make themselves capable thereof) they raise some persons to be, as it were, companions and almost equals to themselves, which many times sorteth to inconvenience. The modern languages give unto such persons the name of favorites, or privadoes ... And we see plainly that this hath been done, not by weak and passionate princes only, but by the wisest and most politic that ever reigned; who have oftentimes joined to themselves some of their servants; whom both themselves have called friends, and allowed other likewise to call them in the same manner; using the word which is received between private men.

Writing of George III's old tutor, the Earl of Bute, who became Prime Minister, Lord Macaulay wrote in 1844: "He was a favourite, and favourites have always been odious in this country. No mere favourite had been at the head of the government since the dagger of Felton had reached the heart of the Duke of Buckingham".






</doc>
<doc id="1643771" url="https://en.wikipedia.org/wiki?curid=1643771" title="Delegate model of representation">
Delegate model of representation

The delegate model of representation is a model of a representative democracy. In this model, constituents elect their representatives as delegates for their constituency. These delegates act only as a mouthpiece for the wishes of their constituency/ state, and have no from the constituency only the autonomy to vote for the actual representatives of the state. This model does not provide representatives the luxury of acting in their own conscience. Essentially, the representative acts as the voice of those who are (literally) not present.

This model was contested by Edmund Burke (1729-1797), an Irish philosopher, who also created the trustee model of representation.





</doc>
<doc id="204299" url="https://en.wikipedia.org/wiki?curid=204299" title="Bicameralism">
Bicameralism

A bicameral legislature divides the legislators into two separate assemblies, chambers, or houses. Bicameralism is distinguished from unicameralism, in which all members deliberate and vote as a single group, and from some legislatures that have three or more separate assemblies, chambers, or houses. , fewer than half the world's national legislatures are bicameral.

Often, the members of the two chambers are elected or selected by different methods, which vary from country to country. This can often lead to the two chambers having very different compositions of members.

Enactment of primary legislation often requires a concurrent majority – the approval of a majority of members in each of the chambers of the legislature. When this is the case, the legislature may be called an example of perfect bicameralism. However, in many Westminster system parliaments, the house to which the executive is responsible can overrule the other house and may be regarded as an example of imperfect bicameralism. Some legislatures lie in between these two positions, with one house only able to overrule the other under certain circumstances.

The Founding Fathers of the United States favoured a bicameral legislature. The idea was to have the Senate be wealthier and wiser. Benjamin Rush saw this though, and noted that "this type of dominion is almost always connected with opulence". The Senate was created to be a stabilising force, elected not by mass electors, but selected by the State legislators. Senators would be more knowledgeable and more deliberate—a sort of republican nobility—and a counter to what Madison saw as the "fickleness and passion" that could absorb the House.

He noted further that "The use of the Senate is to consist in its proceeding with more coolness, with more system and with more wisdom, than the popular branch." Madison's argument led the Framers to grant the Senate prerogatives in foreign policy, an area where steadiness, discretion, and caution were deemed especially important. State legislators chose the Senate, and senators had to possess significant property to be deemed worthy and sensible enough for the position. In 1913, the 17th Amendment passed, which mandated choosing Senators by popular vote rather than State legislatures.

As part of the Great Compromise, the Founding Fathers invented a new rationale for bicameralism in which the Senate had an equal number of delegates per state, and the House had representatives by relative populations.

The British Parliament is often referred to as the "Mother of Parliaments" (in fact a misquotation of John Bright, who remarked in 1865 that "England is the Mother of Parliaments") because the British Parliament has been the model for most other parliamentary systems, and its Acts have created many other parliaments. Many nations with parliaments have to some degree emulated the British "three-tier" model. Most countries in Europe and the Commonwealth have similarly organised parliaments with a largely ceremonial head of state who formally opens and closes parliament, a large elected lower house, and a smaller upper house.

There have been a number of rationales put forward in favour of bicameralism. Federal states have often adopted it, and the solution remains popular when regional differences or sensitivities require more explicit representation, with the second chamber representing the constituent states. Nevertheless, the older justification for second chambers—providing opportunities for second thoughts about legislation—has survived.

The growing awareness of the complexity of the notion of representation and the multifunctional nature of modern legislatures may be affording incipient new rationales for second chambers, though these do generally remain contested institutions in ways that first chambers are not. An example of political controversy regarding a second chamber has been the debate over the powers of the Senate of Canada or the election of the Senate of France.

The relationship between the two chambers varies; in some cases, they have equal power, while in others, one chamber is clearly superior in its powers. The first tends to be the case in federal systems and those with presidential governments. The latter tends to be the case in unitary states with parliamentary systems.

There are two streams of thought: Critics believe bicameralism makes meaningful political reforms more difficult to achieve and increases the risk of gridlock—particularly in cases where both chambers have similar powers—while proponents argue the merits of the "checks and balances" provided by the bicameral model, which they believe help prevent the passage into law of ill-considered legislation.

Formal communication between houses is by various methods, including:

Some countries, such as Argentina, Australia, Austria, Belgium, Bosnia and Herzegovina, Brazil, Canada, Germany, India, Malaysia, Mexico, Pakistan, Russia, Switzerland, Nigeria, and the United States, link their bicameral systems to their federal political structure.

In the United States, Australia, Brazil, and Mexico, for example, each state is given the same number of seats in one of the houses of the legislature, despite variance in the population of each state — it is designed to ensure that smaller states are not overshadowed by larger states, which may have more representation in the other house of the legislature.

In Canada, the country as a whole is divided into a number of Senate Divisions, each with a different number of Senators, based on a number of factors. These Divisions are Quebec, Ontario, Western Provinces, and the Maritimes, each with 24 Senators, Yukon, Northwest Territories, Nunavut, each with 1 Senator, and Newfoundland and Labrador has 6 Senators, making for a total of 105 Senators.

Senators in Canada are not elected by the people but are appointed by the Governor General on the advice of the Prime Minister. The Senate does not originate most legislation (although a small fraction of government bills are introduced in the Senate and Senators may introduce private members' bills in the same way as MPs) but acts as a chamber of revision almost always passing legislation approved by the House of Commons, made up of Members of Parliament (MPs) who are elected. The Senate must pass legislation before it becomes law and can therefore act as a wise facilitator or engage in filibuster. The Senate does not have to endure the accountability and scrutiny of parliamentary elections. Therefore, the bicameral structure of Canadian parliament is more "de jure" than "de facto".

The bicameral Parliament of Australia consists of two Houses, the lower house is called the House of Representatives and the upper house is named the Senate. The lower house currently consists of 150 members, each elected from single member constituencies, known as electoral divisions (commonly referred to as "electorates" or "seats") using full-preference Instant-runoff voting. This tends to lead to the chamber being dominated by two major parties, the Liberal/National Coalition and the Labor Party. The government of the day must achieve the confidence of this House to gain and hold power.

The upper house, the Senate, is also popularly elected under the single transferable vote system of proportional representation. There are a total of 76 senators: 12 senators are elected from each of the six Australian states (regardless of population) and two from each of the two autonomous internal territories (the Australian Capital Territory and the Northern Territory).

Unlike upper houses in most Westminster parliamentary systems, the Australian Senate is vested with significant power, including the capacity to block legislation initiated by the government in the House of Representatives, making it a distinctive hybrid of British Westminster bicameralism and US-style bicameralism. As a result of proportional representation, the chamber features a multitude of parties vying for power. The governing party or coalition, which must maintain the confidence of the lower house, rarely has a majority in the Senate and usually needs to negotiate with other parties and Independents to get legislation passed.

In the German, Indian, and Pakistani systems, the upper houses (the Bundesrat, the Rajya Sabha, and the Senate respectively) are even more closely linked with the federal system, being appointed or elected directly by the governments or legislatures of each German or Indian state, or Pakistani province. This was also the case in the United States before the Seventeenth Amendment was adopted. Because of this coupling to the executive branch, German legal doctrine does not treat the "Bundesrat" as the second chamber of a bicameral system formally. Rather, it sees the "Bundesrat" and the "Bundestag" as independent constitutional bodies. Only the directly elected "Bundestag" is considered the parliament. In the German "Bundesrat", the various "Länder" have between three and six votes; thus, while the less populated states have a lower weight, they still have a stronger voting power than would be the case in a system based "purely" on population, as the most populous "Land" currently has about 27 times the population of the least populous. The Indian upper house does not have the states represented equally, but on the basis of their population.

There is also bicameralism in countries that are not federations, but have upper houses with representation on a territorial basis. For example, in South Africa, the National Council of Provinces (and before 1997, the Senate) has its members chosen by each Province's legislature.

In Spain the Senate functions as a "de facto" territorial-based upper house, and there has been some pressure from the Autonomous Communities to reform it into a strictly territorial chamber.

The European Union maintains a bicameral legislative system consisting of the European Parliament, which is elected in elections on the basis of universal suffrage, and the Council of the European Union, which consists of one representative for each Government of member countries, who are competent for a relevant field of legislation. The European Union is not considered a country nor a state, but it enjoys the power to address national Governments in many areas.

In a few countries, bicameralism involves the juxtaposition of democratic and aristocratic elements.

The best known example is the British House of Lords, which includes a number of hereditary peers. The House of Lords is a vestige of the aristocratic system that once predominated in British politics, while the other house, the House of Commons, is entirely elected. Over the years, some have proposed reforms to the House of Lords, some of which have been at least partly successful. The House of Lords Act 1999 limited the number of hereditary peers (as opposed to life peers, appointed by the Monarch on the advice of the Prime Minister) to 92, down from around 700. Of these 92, one is the Earl Marshal, a hereditary office always held by the Duke of Norfolk, one is the Lord Great Chamberlain, a hereditary office held by turns, currently by the Marquess of Cholmondeley, and the other 90 are elected by all sitting peers. Hereditary peers elected by the House to sit as representative peers sit for life; when a representative peer dies, byelections occur to fill the vacancy. The ability of the House of Lords to block legislation is curtailed by the Parliament Acts 1911 and 1949. Peers can introduce bills except Money Bills, and all legislation must be passed by both Houses of Parliament. If not passed within two sessions, the House of Commons can override the Lords delay by invoking the Parliament Act. Certain legislation, however, must be approved by both Houses and can't be forced through by the Commons under the Parliament Act. These include any bill that would extend the time length of a Parliament, private bills, bills sent to the House of Lords less than one month before the end of a session, and bills that originated in the House of Lords.

Life Peers are appointed either by recommendation of the Appointment Commission; the independent body that vets non-partisan peers, typically from academia, business or culture, or by Dissolution Honour, which takes place at the end of every Parliamentary term and leaving MPs may be offered a seat to keep their institutional memory. It is traditional to offer a peerage to every outgoing Speaker of the House of Commons.

Further reform of the Lords has been proposed; however, reform is not supported by many. Members of the House of Lords all have an aristocratic title, or are from the Clergy. 26 Archbishops and Bishops of the Church of England sit as Lords Spiritual, being the Archbishop of Canterbury, Archbishop of York, the Bishop of London, the Bishop of Durham, the Bishop of Winchester and the next 21 longest-serving Bishops. It is usual that retiring Archbishops, and certain other Bishops, are appointed to the Crossbenches and given a life peerage. Until 2009, 12 Lords of Appeal in Ordinary sat in the House as the highest court in the land; they subsequently became justices of the newly created Supreme Court of the United Kingdom. At present, 786 people sit in the House of Lords, with 92 Hereditary, 26 being Bishops or Archbishops (the Lords Spiritual) and the rest being Life Peers. Membership is not fixed and decreases only on the death of a life peer.

Another example of aristocratic bicameralism was the Japanese House of Peers, abolished after World War II and replaced with the present House of Councillors.

Many bicameral countries like the Netherlands, the Philippines, the Czech Republic, the Republic of Ireland and Romania are examples of bicameral systems existing in unitary states. In countries such as these, the upper house generally focuses on scrutinizing and possibly vetoing the decisions of the lower house.

On the other hand, in Italy the Parliament consists of two chambers that have the same role and power: the Senate (Senate of the Republic, commonly considered the "upper house") and the Chamber of Deputies (considered the "lower house").

In some of these countries, the upper house is indirectly elected. Members of France's Senate and Ireland's Seanad Éireann are chosen by electoral colleges, the latter's consisting of members of the lower house, local councillors, the Taoiseach, and graduates of selected universities, while the Netherlands' Senate is chosen by members of provincial assemblies (which, in turn, are directly elected).

Norway had a kind of semi-bicameral legislature with two chambers, or departments, within the same elected body, the Storting. These were called the Odelsting and were abolished after the general election of 2009. According to Morten Søberg, there was a related system in the 1798 constitution of the Batavian Republic.

In Hong Kong, members of the unicameral Legislative Council returned from the democratically elected geographical constituencies and partially-democratic functional constituencies are required to vote separately since 1997 on motions, bills or amendments to government bills not introduced by the government. The passage of these motions, bills or amendments to government bills requires double majority in both groups simultaneously. (Before 2004, when elections to the Legislative Council from the Election Committee was abolished, members returned through the Election Committee vote with members returned from geographical constituencies.) The double majority requirement does not apply to motions, bills and amendments introduced by the government.

In some countries with federal systems, individual states (like those of the United States, Argentina, Australia and India) may also have bicameral legislatures. A few such states as Nebraska in the U.S., Queensland in Australia, Bavaria in Germany, and Tucumán and Córdoba in Argentina have later adopted unicameral systems. (Brazilian states and Canadian provinces had all their upper houses abolished.

In Argentina, eight provinces have bicameral legislatures, with a Senate and a Chamber of Deputies: Buenos Aires, Catamarca, Corrientes, Entre Ríos, Mendoza, Salta, San Luis (since 1987) and Santa Fe. Tucumán and Córdoba changed to unicameral systems in 1990 and 2001 respectively.

In Australian states, the lower house was traditionally elected based on the one-vote-one-value principle, whereas the upper house was partially appointed and elected, with a bias towards country voters and landowners. In Queensland, the appointed upper house was abolished in 1922, while in New South Wales there were similar attempts at abolition, before the upper house was reformed in the 1970s to provide for direct election. Nowadays, the upper house both federally and in most states is elected using the Single transferable vote form of proportional representation while the lower house uses Instant-runoff voting in single member electorates. This is reversed in the state of Tasmania, where proportional representation is used for the lower house and single member electorates for the upper house.

The Legislature of the Federation of Bosnia and Herzegovina, one of the two entities of Bosnia and Herzegovina, is a bicameral legislative body. It consists of two chambers. 
The House of Representatives has 98 delegates, elected for four-year terms by proportional representation.
The House of Peoples has 58 members, 17 delegates from among each of the constituent peoples of the Federation, and 7 delegates from among the other peoples. Republika Srpska, the other entity, has a unicameral parliament, known as the National Assembly.

The German federal state of Bavaria had a bicameral legislature from 1946 to 1999, when the Senate was abolished by a referendum amending the state's constitution. The other 15 states have used a unicameral system since their founding.

Seven Indian states, Andhra Pradesh, Bihar, Karnataka, Maharashtra, Telangana, Uttar Pradesh and Jammu & Kashmir, have bicameral Legislatures, in these states the upper house is called State Legislative Council (Vidhan Parishad), one third of whom members are elected every two years. Among the members of state legislative councils:


From 1956 to 1958 the Andhra Pradesh Legislature was unicameral. In 1958, when the State Legislative Council was formed, it became bicameral until 1 June 1985 when it was abolished. This continued until March 2007 when the State Legislative Council was reestablished and elections were held for its seats. Since then the Andhra Pradesh Legislature has become once again bicameral. 
In Tamil Nadu, a resolution was passed on 14 May 1986 and the State Legislative Council was dissolved on 1 November 1986. Again on 12 April 2010, a resolution was passed to bring it back bicameral, but became unsuccessful in 2011. Similarly the states of Assam, Madhya Pradesh, Punjab and West Bengal have also dissolved the upper house of their state legislature.

Under Soviet regime regional and local Soviets were unicameral. After the adoption of 1993 Russian Constitution bicameralism was introduced in some regions. Bicameral regional legislatures are still technically allowed by federal law but this clause is dormant now. The last region to switch from bicameralism to unicameralism was Sverdlovsk Oblast in 2012.

During the 1930s, the Legislature of the State of Nebraska was reduced from bicameral to unicameral with the 43 members that once comprised that state's Senate. One of the arguments used to sell the idea at the time to Nebraska voters was that by adopting a unicameral system, the perceived evils of the "conference committee" process would be eliminated.

A conference committee is appointed when the two chambers cannot agree on the same wording of a proposal, and consists of a small number of legislators from each chamber. This tends to place much power in the hands of only a small number of legislators. Whatever legislation, if any, the conference committee finalizes must then be approved in an unamendable "take-it-or-leave-it" manner by both chambers.

During his term as Governor of the State of Minnesota, Jesse Ventura proposed converting the Minnesotan legislature to a single chamber with proportional representation, as a reform that he felt would solve many legislative difficulties and impinge upon legislative corruption. In his book on political issues, "Do I Stand Alone?", Ventura argued that bicameral legislatures for provincial and local areas were excessive and unnecessary, and discussed unicameralism as a reform that could address many legislative and budgetary problems for states.

A 2005 report on democratic reform in the Arab world by the U.S. Council on Foreign Relations co-sponsored by former Secretary of State Madeleine Albright urged Arab states to adopt bicameralism, with upper chambers appointed on a 'specialized basis'. The Council claimed that this would protect against the 'Tyranny of the majority', expressing concerns that without a system of checks and balances extremists would use the single chamber parliaments to restrict the rights of minority groups.

In 2002, Bahrain adopted a bicameral system with an elected lower chamber and an appointed upper house. This led to a boycott of parliamentary elections that year by the Al Wefaq party, who said that the government would use the upper house to veto their plans. Many secular critics of bicameralism were won around to its benefits in 2005, after many MPs in the lower house voted for the introduction of so-called morality police.

A referendum on introducing a unicameral Parliament instead of the current bicameral Parliament was held in Romania on 22 November 2009. The turnout rate was 50.95%, with 77.78% of "Yes" votes for a unicameral Parliament. This referendum had a consultative role, thus requiring a parliamentary initiative and another referendum to ratify the new proposed changes.

A referendum on a new constitution was held on 30 October 2016. The constitution draft would create a bicameral Parliament instead of the current unicameral. The Senate is expected to represent the interests of territorial collectivities and Ivoirians living abroad. Two thirds of the Senate is to be elected at the same time as the general election. The remaining one third is appointed by the president elect.



</doc>
<doc id="25202419" url="https://en.wikipedia.org/wiki?curid=25202419" title="Government spin-off">
Government spin-off

Government spin-off is civilian goods which are the collateral result of military or governmental research. One prominent example of a type of government spin-off is technology that has been commercialized through NASA funding, research, licensing, facilities, or assistance. 

NASA spin-off technologies have been produced for over forty years. The Internet is a specific example of a government spin-off resulting from DARPA funding.


</doc>
<doc id="1163448" url="https://en.wikipedia.org/wiki?curid=1163448" title="Ruling clique">
Ruling clique

A ruling clique is a group of people who jointly rule an oligarchic form of government.

Ruling cliques generally differ from another type of oligarchy: a military junta. Military juntas are always ruled by military personnel (often high-ranking like general). A ruling clique can include people from various professions. The ruling elites who comprise the leadership tend to form a council, political party, or another form of an organized group. The high-ranking members share a rough balance of power although sometimes one or more members seek to increase their power at the expense of others or some of them may attempt to transform the system into an autocracy or make it more democratic.

Some ruling cliques could be considered a form of aristocracy while others are based on a very small circle of rulers rather than a broader based organization such as a political party. In some cases, the entire ruling clique is composed of a council of leaders who are the only members of the clique.


</doc>
<doc id="859412" url="https://en.wikipedia.org/wiki?curid=859412" title="Ministry (collective executive)">
Ministry (collective executive)

In constitutional usage in Commonwealth realms and in some other systems, a ministry (usually preceded by the definite article, i.e., the ministry) is a collective body of government ministers headed by a prime minister or premier, and also referred to as the head of government. It is described by the Oxford Dictionary as "a period of government under one prime minister". Although the term "cabinet" can in some circumstances be a synonym, a ministry can be a broader concept which might include office-holders who do not participate in cabinet meetings. Other titles can include "administration" (in the United States) or "government" (in common usage among most parliamentary systems) to describe similar collectives.

The term is primarily used to describe the successive governments of the United Kingdom, Canada, Australia and New Zealand, which share a common parliamentary political heritage. In the United Kingdom, Australia and New Zealand, a new ministry begins after each election, regardless of whether the prime minister is re-elected, and whether there may have been a minor rearrangement of the ministry. For example, after winning the 1979 general election, Margaret Thatcher (as Prime Minister of the United Kingdom) formed the first Thatcher ministry. After being re-elected at the 1983 general election, she formed the second Thatcher ministry, and so on. In Canada, a new ministry is only formed if the government loses an election.

Despite the use of the term "ministry" in this sense being rare in Portugal nowadays, until the first half of the 20th century, the term was frequently used in this country to designate the collective body of government ministers. From 1911 to 1933, the official title of the Prime Minister of Portugal was even that of "President of the Ministry" (), reflecting his role as the head of the collective ministry.


</doc>
<doc id="2973918" url="https://en.wikipedia.org/wiki?curid=2973918" title="Speaker of the senate">
Speaker of the senate

Speaker of the Senate is a title given to the presiding officer of the Senate in a small number of jurisdictions and mainly amongst English-speaking countries. 




</doc>
<doc id="174363" url="https://en.wikipedia.org/wiki?curid=174363" title="Public trust">
Public trust

The concept of the public trust relates back to the origins of democratic government and its seminal idea that within the public lies the true power and future of a society; therefore, whatever "trust" the public places in its officials must be respected.
One of the reasons that bribery is regarded as a notorious evil is that it contributes to a culture of political corruption in which the public trust is eroded. Other issues related to political corruption or betrayal of public trust are lobbying, special interest groups and the public cartel.

In the Philippines, "betrayal of public trust" is one of the impeachable offenses. In "Francisco, Jr. vs. Nagmamalasakit na mga Manananggol ng mga Manggagawang Pilipino, Inc.," the Supreme Court of the Philippines ruled that the definition of "betrayal of public trust" is "a non-justiciable political question which is beyond the scope of its judicial power" under the Constitution. It did not prescribe which branch of government has the power to define it, but implies that Congress, which handles impeachment cases, has the power to do so.



</doc>
<doc id="2211969" url="https://en.wikipedia.org/wiki?curid=2211969" title="Administrative centre">
Administrative centre

An administrative centre is a seat of regional administration or local government, or a county town, or the place where the central administration of a commune is located.

In countries which have French as one of their administrative languages (such as Belgium, Luxembourg, Switzerland or many African countries) and in some other countries (such as Italy, cf. cognate "capoluogo"), a chef-lieu (, plural form "chefs-lieux" (literally "chief place" or "head place"), is a town or city that is pre-eminent from an administrative perspective. The ‘f’ in chef-lieu is pronounced, in contrast to chef-d'oeuvre where it is mute. 

The capital of an Algerian Province is called a chef-lieu. The capital of a district, the next largest division, is also called a chef-lieu. While the capital of the lowest division, the municipalities, is called agglomeration de chef-lieu (chef-lieu agglomeration) and is abbreviated as A.C.L.

The chef-lieu in Belgium is the administrative centre of each of the ten Provinces of Belgium. Three of these cities also give their name to their province (Antwerp, Liège and Namur).

Luxembourg is divided into two judicial arrondissements (Luxembourg City, Diekirch), three administrative districts (Luxembourg City, Diekirch, Grevenmacher), four electoral circonscriptions (constituencies), twelve cantons and one hundred and five communes (municipalities; Luxembourgish: "Gemengen").

Arrondissements, districts and cantons have each a chef-lieu and are named after it. The same is true for each commune which is composed of more than one town or village. Usually (with a few exceptions), the commune is named after the communal chef-lieu.

The chef-lieu of a département is known as the "préfecture". This is the town or city where the prefect of the départment (and all services under his/her control) is situated, in a building known as the prefecture. In every French region, one of the départments has pre-eminence over the others, and the prefect carries the title of "Prefect of region X…, Prefect of Department Z…" and the city where the regional prefect is found is known as "chef-lieu of the region" or, more commonly, "Regional prefecture". The services are, however, controlled by the prefecture of the départment. 

The chef-lieu of an arrondissement, commonly known as the "sous-préfecture" is the city or town where the sub-prefect of the arrondissement (and the services directly under his/her control) is situated, in a building called the sub-prefecture. The arrondissement where the département prefecture is located does not normally have a sub-prefect or sub-prefecture, the administration being devolved usually to the "Secretary-general of the departmental prefecture", who functions as sub-prefect for the arrondissement. 

The chef-lieu of a canton is usually the biggest city or town within the canton, but has only a nominal role. No specific services are controlled by it. In past decades, there was always a Gendarmerie, a treasurer and a justice of the peace. 

The chef-lieu of a commune is the principal area of the town or city that gives the commune its name, the other areas of the town being called hamlets. French typographers will use a capital for the ‘Le’ or ‘La’ preceding the name of places having ‘chef-lieu of town’ status, and lowercase ‘le’ or ‘la’ for hamlets.

In the Hashemite Kingdom of Jordan, the administrative centres are known as "chief towns" or "nahias". Nahias may be in charge of a sub-district ("qda"), a district ("liwa"), or a governorate ("muhafazah").

The chef-lieu indicates the principal city of the provinces of New Caledonia. So Nouméa is the chef-lieu of South Province. But the chef-lieu can also mean the principal area within a town. So Wé is part of the town of Lifou, but is the chef-lieu of Lifou. In the Loyalty Islands and the other islands, the name of the chef-lieu differs from that of the name of the town. For the towns of the mainland, the chef-lieu has the same name as the town. Nouméa is a town composed only of Nouméa.

Many of the West African states which gained independence from France in the mid-20th century also inherited the French administrative structure of Departments and Communes, headed by a "Chief-Lieu". States still using Chief-Lieu to identify the administrative headquarters of a government subdivision include Senegal, Burkina Faso, Benin, Mali, and Nigerl. 

Taking Niger and Mali as examples, the administrative subdivisions down to the Commune level each have a formal place of administrative headquarters, titled the chef-lieu. The larger portion of the terminology of administrative division is inherited from colonial rule as part of French West Africa, and has survived and been somewhat modified over time. In both nations there have been remarkably parallel histories. With the decentralization process begun in both nations in the 1990s, the chef-lieu has transitioned from the location of the Governor, Commandant, or Prefect and their staff, to the location of Commune, Cercles of Mali/Departments of Niger, and Regional Councils and a variety of decentralized bodies. The "chefs-lieux" of a Region, Cercle or Département, is usually also a Communal chef-lieu. Both nations collect these councils in a "High Council of Collectivites" seated at the nation's capital. Smaller sub-divisions in Mali's Communes (Villages, Tribal councils, Quarters) are administered from or identified as a "Place"/"Site" ("Site" in French), so the chef-lieu is literally the "Chief-Place" even at the lowest level.

In Russia, the term is applied to the inhabited localities, which serve as a seat of government of entities of various levels. The only exception to this rule is the republics, for which the term "capital" is used to refer to the seat of government. The capital of Russia is also an entity to which the term "administrative centre" does not apply. A similar arrangement exists in Ukraine.

The term chef-lieu is applied to the capital of each Swiss canton. In 16 of the 26 cantons, the territory is subdivided into districts. Every district also has a city nominated as chef-lieu and each has a prefect.

The term chef-lieu is used to designate the capital of each gouvernorat (department). Each of the 24 gouvernorats is subdivided into delegations (districts) which each have a central city as chef-lieu of delegation.

In the United Kingdom it is the centre of a local authority, which is distinct from a historic county with a county town.




</doc>
<doc id="25062892" url="https://en.wikipedia.org/wiki?curid=25062892" title="Soil and grain">
Soil and grain

Soil and grain (Chinese: 社稷 "sheji"; Japanese: 社稷 "shashoku") was a common political term in East Asia for the state. Altars of soil and grain were constructed alongside ancestral altars. Local kings performed ceremonies of soil and grain to affirm their sovereignty at Beijing Shejitan and Seoul Sajiktan. It has also been rendered "gods of soil and grain" in English, owing to its associations of prayer and supernatural possibilities.

During the Warring States period, ministers defied the emperor by claiming a greater loyalty to the "soil and grain".

A similar concept to "sheji" is that of Tu Di, the Earth Deity.



 


</doc>
<doc id="22163364" url="https://en.wikipedia.org/wiki?curid=22163364" title="Civic lottery">
Civic lottery

A civic lottery, a popular term for the contemporary use of sortition or allotment, is a lottery-based method for selecting citizens for public service or office. It is based on the premise that citizens in a democracy have both a duty and the desire to serve their society by participating in its governance.

Today, the most common use of the civic lottery process is found in many Anglo-Saxon judicial systems where citizen juries are summoned to hear and render verdicts in court cases. The term for this is popularly known as jury duty.

Civic lotteries are increasingly popular in Canada, where provincial Citizens' Assemblies on Electoral Reform took place in British Columbia in 2004 and in Ontario in 2006. The membership of each Assembly was determined by a civic lottery which invited citizens to volunteer as candidates.
In British Columbia, the government sent 23,034 letters to randomly identified citizens throughout the province. 1,715 replied and volunteered to serve as members of the Assembly. In Ontario, 123,489 citizens were identified during a random electronic draw from the Permanent Register of Electors. Each citizen received a letter inviting him or her to apply and 7,033 volunteered as candidates. Ultimately, during a final selection process 158 names were drawn from among the candidates to participate as members of the BC Assembly. 103 were selected as members in Ontario.

MASS LBP, a Canadian company inspired by the work of the Citizens' Assembly on Electoral Reform (Ontario) has developed an increasingly sophisticated system for running civic lotteries to randomly select citizens to participate on government advisory panels. The lotteries, which ask citizens to give up several consecutive weekends to participate on a panel, enjoy a strong positive response rate, typically exceeding five percent. This suggests that citizens are more interested public affairs than declining voter-turnout rates indicate.

Panel members are randomly selected from among the pool of candidate-respondents to create a panel that roughly matches the demographic profile of the wider population.




</doc>
<doc id="21395995" url="https://en.wikipedia.org/wiki?curid=21395995" title="Inclusive management">
Inclusive management

Inclusive management is a pattern of practices by public managers that facilitate the inclusion of public employees, experts, the public, and politicians in collaboratively addressing public problems or concerns of public interest.

In the inclusive management model, managers focus on building the capacity of the public to participate in the policy process. One way this capacity is built is through the structuring and maintenance of relationships by managers. Managers operate in a myriad of relationship structures that are used for making decisions, implementing policy, and identifying public priorities. These relationships give shape, pose constraints, or present opportunities for the way public policy is pursued.

The management component of the compound idea of inclusive management signifies that inclusion is a managed, ongoing project rather than an attainable state. The inclusion component means something different from the commonplace use of inclusion and exclusion to reference the socioeconomic diversity of the participants. The understanding of inclusion in this analysis emphasizes diversity in terms of the necessity of a diversity of perspectives to promote civic discovery in a deliberative setting. Inclusion involves active boundary spanning across differences in perspectives, institutions, issues, and time, which may or may not be founded upon work to integrate socioeconomically diverse participants.

The inclusion part of the idea is perhaps best encapsulated by the "50/50 rule", a term used by public managers in Grand Rapids, Michigan, to invoke a variety of meanings. Sometimes "50/50" means that process and outcome are equally important, in other words that the effects of a process on community building are as important as the task completion. Sometimes it means that ideally 50% of the people involved in a process have participated in prior, related processes and 50% are newcomers, such that each policy-making effort acknowledges past conversations or decisions yet remains open to new ideas that may alter the previous consensus. From the perspective of the 50/50 rule, things like process and outcome or task and community are not in a trade-off relationship, and indeed are not even separable. In this context, concluding that participation was done "for the sake of participation" rather than to effect the outcome would be a damning critique. Keeping process and outcome, newcomers and old-timers, and past and present in play are ways of creating connections across individuals, groups, interests, and issues. Tasks are accomplished, yet opportunities continue to be open for revising as well as for moving on to the other issues and tasks that emerge or are next in line. This sense of inclusion is part of an ongoing stream of issues and people involved in one process or concern rolling forward into another echoes theorizations of democracy and civic engagement as an ongoing inquiry and never-finished project.

Inclusive public management is a newly characterized form of public management following the more traditional forms of public administration, espoused by Woodrow Wilson and political scientists including Frank Goodnow and Charles A. Beard. Analyses of inclusive public management contribute to a stream of practice and research regarding New Public Management popularized by Osborne and Gaebler, particularly recent contributions on reconceptualizing members of the public as partners or coproducers of public services rather than as "customers" of government. Inclusive public management describes some practices of participatory democracy, sharing with deliberative democracy an emphasis on participants making decisions through deliberative processes rather than the mere aggregation of individual interests through voting or other mechanisms, the idea being that through deliberative processes that enable communicative rationality and civic discovery, new understandings and resources of public value or the public good may be realized. It intersects with other fields of research and practice on collaborative governance that describe collaborative processes for making and implementing public policy and urban or regional planning Inclusive management also aligns with recent writings on network governance to the extent that they focus upon cross-boundary collaborations within networks.

Inclusive management practices are one way to enact public participation and civic engagement, which may be implemented in a variety of ways. Inclusive management practices are not the same as citizen participation or as inclusion as the latter term is typically used in democratic theory to denote the involvement of ethnically or socioeconomically diverse persons or groups in a decision-making process. Instead, inclusive management theories make a distinction between inclusive practices and participatory practices, which are intersecting dimensions of any civic engagement process. Inclusion is not a term for describing participation that has been done particularly well. Indeed, participation may be done well or badly, as may inclusion. Rather, inclusion and participation are two different approaches to public engagement, with different implications for the roles of the parties involved, the kinds of decisions reached, and the kind of community fostered by engagement. A process may be characterized by one, neither, or both, along two intersecting dimensions of low to high inclusion "and" low to high participation. The following table identifies the features of high participation and high inclusion.

Quick and Feldman's distinction between inclusion and participation addresses the place of diversity in civic engagement. One potential source of confusion is that the inclusion component of the term inclusive management means something different from the commonplace use of inclusion to refer to the socioeconomic diversity of participants and adequacy of representation or the proportional representativeness of participation in an engagement process. Given the everyday use of the term, characterizing a process that has not had socioeconomically diverse participation as "inclusive" is confusing and may be misread as being dismissive of diversity. Diversity certainly does have a place in inclusive practices. Within the framework of distinguishing inclusion and participation, however, it is not an either/or proposition to decide whether a diverse process is or is not inclusive or participatory. Nor can diversity be assigned to the dimension of inclusion or participation. Instead, diversity can be engaged through participation- or inclusion-oriented strategies, but it has different meanings in these two orientations.

Inclusion involves transcending dichotomies or engaging boundaries. Dichotomies or boundaries – such as government/non-government, expert/local, internal/external, process/outcome, flexibility/accountability, participation/control, and the temporal or issue scope of a problem – are distinctions that inclusive managers often bring into play.

Broader social theory about the relationship of structure and agency, such as structuration, practice theory, and actor-network theory, or regarding the nature of boundary-work and boundary objects, communities of practice, and the narrative construction of reality, are powerful instruments for analysis in showing the interdependent relations between these dichotomies and clarifying how actions that transcend these dichotomies may be enacted.

Prior research has identified several inclusive practices, including:


To date, researchers have identified inclusive public management practices in communities that have a longstanding commitment to engaging the public, on an ongoing basis, in addressing public concerns together. This research has focused primarily on building public potential to address public problems at the local level of government in the United States. Grand Rapids, Michigan, and Charlotte, North Carolina are cities where inclusive practices have been documented. However, inclusive management practices might be found at any level of government or in any location.

Inclusive public management and leadership practices have been found to improve the quality of policy designs or the viability of policies by:



</doc>
<doc id="241905" url="https://en.wikipedia.org/wiki?curid=241905" title="Government in exile">
Government in exile

A government in exile (abbreviated as GiE) is a political group which claims to be a country or semi-sovereign state's legitimate government, but is unable to exercise legal power and instead resides in another state or foreign country. Governments in exile usually plan to one day return to their native country and regain formal power. A government in exile differs from a rump state in the sense that a rump state controls at least part of its former territory. For example, during World War I, nearly all of Belgium was occupied by Germany, but Belgium and its allies held on to a small slice in the country's west. A government in exile, in contrast, has lost all its territory.

Exiled governments tend to occur during wartime occupation, or in the aftermath of a civil war, revolution, or military coup. For example, during German expansion in World War II, some European governments sought refuge in the United Kingdom, rather than face destruction at the hands of Nazi Germany. A government in exile may also form from widespread belief in the illegitimacy of a ruling government. Due to the outbreak of the Syrian Civil War in 2011, for instance, the National Coalition for Syrian Revolutionary and Opposition Forces was formed by groups whose members sought to end the rule of the ruling Ba'ath Party. 

The effectiveness of a government in exile depends primarily on the amount of support it can receive, either from foreign governments or from the population of its own country. Some exiled governments come to develop into a formidable force, posing a serious challenge to the incumbent regime of the country, while others are maintained chiefly as a symbolic gesture.

The phenomenon of a government in exile predates the formal utilization of the term. In periods of monarchical government, exiled monarchs or dynasties sometimes set up exile courts—as the House of Stuart did when driven from their throne by Oliver Cromwell and again at the Glorious Revolution (see ). The House of Bourbon would be another example because it continued to be recognized by other countries at the time as the legitimate government of France after it was overthrown by the populace during the French Revolution. This continued to last through the rule of Napoleon Bonaparte and the Napoleonic Wars from 1803-04 to 1815. With the spread of constitutional monarchy, monarchical governments which were exiled started to include a prime minister, such as the Dutch government during World War II headed by Pieter Sjoerds Gerbrandy.

International law recognizes that governments in exile may undertake many types of actions in the conduct of their daily affairs. These actions include:

In cases where a host country holds a large expatriate population from a government in exile's home country, or an ethnic population from that country, the government in exile might come to exercise some administrative functions within such a population. For example, the WWII Provisional Government of Free India had such authority among the ethnically Indian population of British Malaya, with the consent of the then Japanese military authorities.

Governments in exile may have little or no recognition from other states. Some exiled governments have some characteristics in common with rump states. Such disputed or partially in exile cases are noted in the tables below.

These governments in exile were created by deposed governments or rulers who continue to claim legitimate authority of the state they once controlled.

These governments in exile were created by deposed governments or rulers who continue to claim legitimate authority of the state they once controlled but whose state no longer exists.

 Government of the Republic of China: The currently Taipei-based Republic of China government does not regard itself as a government-in-exile, but is claimed to be such by some participants in the debate on the political status of Taiwan. In addition to the island of Taiwan and some other islands it currently controls, the Republic of China formally maintains claims over territory now controlled by the People's Republic of China as well as some parts of Afghanistan, Bhutan, India, Japan, Myanmar, Pakistan, Russia, and Tajikistan. The usual formal reasoning on which this "government-in-exile" claim is based relies on an argument that the sovereignty of Taiwan was not legitimately handed to the Republic of China at the end of World War II, and on that basis the Republic of China is located in foreign territory, therefore effectively making it a government in exile. By contrast, this theory is not accepted by those who view the sovereignty of Taiwan as having been legitimately returned to the Republic of China at the end of the war. Both the People's Republic of China government and the Kuomintang in Republic of China (Taiwan) hold the latter view.

However, there are also some who do not accept that the sovereignty of Taiwan was legitimately returned to the Republic of China at the end of the war nor that the Republic of China is a government-in-exile, and China's territory does not include Taiwan. The current Democratic Progressive Party in Taiwan is inclined to this view, and supports Taiwan Independence.

These governments in exile claim legitimacy of autonomous territories of another state and have been created by deposed governments or rulers, who do not claim independence as a separate state.

These governments have been created in exile by political organisations and opposition parties, aspire to become actual governing authorities or claim to be legal successors to previously deposed governments, and have been created as alternatives to incumbent governments.

These governments have been created in exile by political organisations, opposition parties, and separatist movements, and desire to become the governing authorities of their territories as independent states, or claim to be the successor to previously deposed governments, and have been created as alternatives to incumbent governments.

These governments in exile are governments of non-self-governing or occupied territories. They claim legitimate authority over a territory they once controlled, or claim legitimacy of a post-decolonization authority. The claim may stem from an exiled group's election as a legitimate government.

The United Nations recognizes the right of self-determination for the population of these territories, including the possibility of establishing independent sovereign states.

From the Palestinian Declaration of Independence in 1988 in exile in Algiers by the Palestine Liberation Organization, it has effectively functioned as the government in exile of the Palestinian State. In 1994, however the PLO established the Palestinian National Authority interim territorial administration as result of the Oslo Accords signed by the PLO, Israel, the United States, and Russia. Between 1994 and 2013, the PNA functioned as an autonomy, thus while the government was seated in the West Bank it was not sovereign. In 2013, Palestine was upgraded to a non-member state status in the UN.

All of the above created an ambiguous situation, in which there are two distinct entities: The Palestinian Authority, exercising a severely limited amount of control on the ground under the tutelage of an Israeli military occupation; and the State of Palestine, recognized by the United Nations and by numerous countries as a fully sovereign and independent state, but not able to exercise such sovereignty on the ground. Both are headed by the same person—as of February 2016, President Mahmud Abbas—but are judicially distinct. For example, a dissolution of The Palestinian Authority and resumption of full rule on the ground by Israel would not in itself affect the State of Palestine, which could continue to exist as a government-in-exile diplomatically recognized by the UN and by numerous countries.

These governments have ties to the area(s) they represent, but their claimed status and/or stated aims are sufficiently ambiguous that they could fit into other categories.

The Sovereign Military Order of Malta may be considered a case of a government in exile, since it is without territory but recognised as a sovereign government by numerous sovereign countries. However, it does not claim to be a sovereign state, rather a "sovereign subject" of international law. In addition, it no longer claims jurisdiction over Malta, and recognises and maintains diplomatic relations with the independent Republic of Malta.

Many countries established a government in exile after loss of sovereignty in connection with World War II.

A large number of European governments-in-exile were set up in London.

Other exiled leaders in Britain in this time included King Zog of Albania and Emperor Haile Selassie of Ethiopia.

The Occupation of Denmark (9 April 1940) was administered mainly by the German Foreign Office, contrary to other occupied lands that were under military or civilian administration. Denmark did not establish a government in exile, although there was an Association of Free Danes established in London. King Christian X and his government remained in Denmark, and functioned comparatively independently until August 1943 when it was dissolved, placing Denmark under full German occupation. Meanwhile, Iceland, Greenland and the Faroe Islands were occupied by the Allies, and effectively separated from the Danish crown. (See British occupation of the Faroe Islands, Iceland during World War II, and History of Greenland during World War II.)

The Philippine Commonwealth (invaded 9 December 1941) established a government in exile in Australia and the United States.

While formed long before World War II, the Provisional Government of the Republic of Korea continued in exile in China until the end of the war.

At the fall of Java, and the surrender by the Dutch on behalf of Allied forces on 8 March 1942, many Dutch-Indies officials (including Dr van Mook and Dr Charles van der Plas) managed to flee to Australia in March 1942, and on 23 December 1943, the Royal Government (Dutch) decreed an official Netherlands East Indies Government-in-exile, with Dr van Mook as Acting Governor General, on Australian soil until Dutch rule was restored in the Indies.

Under the auspices of the Axis powers, Axis-aligned groups from some countries set up "governments-in-exile" in Axis territory, even though internationally recognized governments remained in place in their home countries.
Following the Ba'athist Iraqi invasion and occupation of Kuwait, during the Persian Gulf War, on August 2, 1990, Sheikh Jaber Al-Ahmad Al-Jaber Al-Sabah and senior members of his government fled to Saudi Arabia, where they set up a government-in-exile in Ta'if. The Kuwaiti government in exile was far more affluent than most other such governments, having full disposal of the very considerable Kuwaiti assets in western banks—of which it made use to conduct a massive propaganda campaign denouncing the Ba'athist Iraqi occupation and mobilizing public opinion in the Western world in favor of war with Ba'athist Iraq. In March 1991, following the defeat of Ba'athist Iraq at the hands of coalition forces in the Persian Gulf War, the Sheikh and his government were able to return to Kuwait.

Following the Turkish Invasion of Cyprus in 1974 and the displacement of many Greek Cypriots from North Cyprus, displaced inhabitants of several towns set up what are in effect Municipal Councils in Exile, headed by Mayors in Exile. The idea is the same as with a national Government in Exile – to assert a continuation of legitimate rule, even though having no control of the ground, and working towards restoration of such control. Meetings of the exiled Municipal Council of Lapithos took place in the homes of its members until the Exile Municipality was offered temporary offices at 37 Ammochostou Street, Nicosia. The current Exile Mayor of the town is Athos Eleftheriou. The same premises are shared with the Exile Municipal Council of Kythrea.

Also in the Famagusta District of Cyprus, the administration of the part retained by the Republic of Cyprus considers itself as a "District administration in exile", since the district's capital Famagusta had been under Turkish control since 1974.

Works of Alternate History as well as Science Fictional depictions of the future sometimes include fictional Fictional Governments in exile.





</doc>
<doc id="28168498" url="https://en.wikipedia.org/wiki?curid=28168498" title="Regulatory competition">
Regulatory competition

Regulatory competition, also called competitive governance or policy competition, is a phenomenon in law, economics and politics concerning the desire of law makers to compete with one another in the kinds of law offered in order to attract businesses or other actors to operate in their jurisdiction. Regulatory competition depends upon the ability of actors such as companies, workers or other kinds of people to move between two or more separate legal systems. Once this is possible, then the temptation arises for the people running those different legal systems to compete to offer better terms than their "competitors" to attract investment. Historically, regulatory competition has operated within countries having federal systems of regulation - particularly the United States, but since the mid-20th century and the intensification of economic globalisation, regulatory competition became an important issue internationally.

One opinion is that regulatory competition in fact creates a "race to the top" in standards, due to the ability of different actors to select the most efficient rules by which to be governed. The main fields of law affected by the phenomenon of regulatory competition are corporate law, labour law, tax and environmental law. Another opinion is that regulatory competition between jurisdictions creates a "race to the bottom" in standards, due to the decreased ability of any jurisdiction to enforce standards without the cost of driving investment abroad.

The concept of regulatory competition emerged from the late 19th and early 20th century experience with charter competition among US states to attract corporations to domicile in their jurisdiction. In 1890 New Jersey enacted a liberal corporation charter, which charged low fees for company registration and lower franchise taxes than other states. Delaware attempted to copy the law to attract companies to its own state. This competition ended when Woodrow Wilson as Governor tightened New Jersey's laws again through a series of seven statutes.

In academic literature the phenomenon of regulatory competition reducing standards overall was argued for by AA Berle and GC Means in "The Modern Corporation and Private Property" (1932) while the concept received formal recognition by the US Supreme Court in a decision of Justice Louis Brandeis in the 1933 case "Ligget Co. v. Lee" In 1932 Brandeis also coined the term “laboratories of democracy” in "New State Ice Company v. Liebmann", noting that the Federal government was capable of ending experiment.

American corporate law scholars have debated on the role of the regulatory competition on corporate law for more than one decade. A Comparative Bibliography
In the United States legal academia, corporate law is conventionally said to be the product of a "race" among states to attract incorporations by making their corporate laws attractive to those who choose where to incorporate. Given that it has long been possible to incorporate in one state while doing business primarily in other states, US states have rarely been able or willing to use law tied to where a firm is incorporated to regulate or constrain corporations or those who run them. However, U.S. states have long regulated corporations with other laws (e.g., environmental laws, employment laws) that are not tied to where a firm is incorporated, but are based on where a firm does business.

From the "race" to attract incorporations, Delaware has emerged as the winner, at least among publicly traded corporations. The corporate franchise tax accounts for between 15% and 20% of the state's budget.

In Europe, regulatory competition has long been prevented by the real seat doctrine prevailing in private international law of many EU and EEA member countries, which essentially required companies to be incorporated in the state where their main office was located. However, in a series of cases between 1999 and 2003 (Centros Ltd. vs. Erhvervs- og Selskabsstyrelsen, Überseering BV v Nordic Construction Company Baumanagement GmbH, Kamer van Koophandel en Fabrieken voor Amsterdam v Inspire Art Ltd.), the European Court of Justice has forced member states to recognize companies chartered in other member states, which is likely to foster regulatory competition in European company law. For instance, in 2008, Germany adopted new regulations on the GmbH (Limited Liability Company), allowing the incorporation of Limited Liability Companies [UG (haftungsbeschränkt)] without a minimum capital of EUR 25,000 (though 25% of earnings have to be retained until this threshold is reached).

Countries may, for instance, seek to attract foreign direct investment by enacting a lower minimum wage than other countries, or by making the labor market more flexible.


Legal scholars often cite environmental law as a field in which regulatory competition is particularly likely to produce a “race to the bottom” due to the externalities produced by changes in any individual state’s environmental law. Because a state is unlikely to bear all of the costs associated with any environment damage caused by industries in that state, it has an incentive to lower standards below the level that would be desirable if the state were forced to bear all of the costs. One commonly cited example of this effect is clean air laws, as states may be incentivized to lower their standards to attract business, knowing that the effects of the increased pollution will be spread across a wide area, and not simply localized within the state. Furthermore, a reduction in the standards of one state will incentivize other states to similarly lower their standards so as to not lose business.

Sometimes higher-level governing bodies institute incentives to competition among lower-level governing bodies, an example being the Race to the Top program, designed by the United States Department of Education to spur reforms in state and local district K-12 education.

The German Federal Ministry of Education and Research likewise has initiated a program called InnoRegio to reward innovative practices.

The high degree of politicization of the genetically modified organism issue made it a key battleground for competition for leadership, particularly between the European Commission and the European Council of Ministers. The result has been a protracted battle over agenda setting and issue framing and a cycle of competitive regulatory reinforcement.

The struggle between insurgents and various Afghanistan states for power, control, popular support and legitimacy in the eyes of the public has been described as competitive governance.

While during the Cold War security was provided by centralized institutions such as NATO and the Warsaw Pact, now competing profit-seeking firms provide personal, national, and international security.

Arnold Kling notes, "In democratic government, people take jurisdictions as given, and they elect leaders. In competitive government, people take leaders as given, and they select jurisdictions." Competitive governance has thus far not produced an ultra-libertarian government; although Zac Gochenour has pointed out the role of potential international migrants' switching costs in hindering consumer choice from creating greater intergovernmental competition, Bryan Caplan has stated that "[t]he bigger problem is that almost all existing governments are either non-profits (the democracies), have short time horizons (the unstable dictatorships), or reasonably worry that if they liberalize they're lose power (the stable dictatorships)." Indeed, it has been argued by Maria Brouwer that most autocracies prefer stagnation to the vagaries inherent to expansion and other forms of innovation, since the exploration of new possibilities could lead to failure, which would undermine autocratic authority. There has been some question as to whether competitive governance can be revived in Australia.

Brennan and Buchanan (1980) argue that the public sector is a 'Leviathan' which is inherently biased towards extracting money from taxpayers, but that competitive government structures can minimize such exploitation. It has also been argued that a decentralized competitive government structure allows for an experimentation of new public policies without doing too much harm if they fail.

An alternative to market-based or competitive governance is civic-based or partnership governance. Alleged disadvantages of competitive governance, compared to collaborative government, include less potential to harness the power of knowledge sharing, cooperation and collaboration within government.







</doc>
<doc id="21418402" url="https://en.wikipedia.org/wiki?curid=21418402" title="Criticism of government">
Criticism of government

Criticism of government may refer to criticism of particular government, or of the concept of government itself. In certain cases, such as in certain monarchies and authoritarian (or sometimes totalitarian or dictatorial) governments, criticizing the government is considered criminal speech and is punished in accord which the perceived severity of the claimed crime.


</doc>
<doc id="2589535" url="https://en.wikipedia.org/wiki?curid=2589535" title="Civilian control of the military">
Civilian control of the military

Civilian control of the military is a doctrine in military and political science that places ultimate responsibility for a country's strategic decision-making in the hands of the civilian political leadership, rather than professional military officers. The reverse situation, where professional military officers control national politics, is called a military dictatorship. A lack of control over the military may result in a state within a state. One author, paraphrasing Samuel P. Huntington's writings in "The Soldier and the State", has summarized the civilian control ideal as "the proper subordination of a competent, professional military to the ends of policy as determined by civilian authority".

Civilian control is often seen as a prerequisite feature of a stable liberal democracy. Use of the term in scholarly analyses tends to take place in the context of a democracy governed by elected officials, though the subordination of the military to political control is not unique to these societies. One example is the People's Republic of China. Mao Zedong stated that "Our principle is that the Party commands the gun, and the gun must never be allowed to command the Party," reflecting the primacy of the Communist Party of China (and communist parties in general) as decision-makers in Marxist–Leninist and Maoist theories of democratic centralism.

As noted by University of North Carolina at Chapel Hill professor Richard H. Kohn, "civilian control is not a fact but a process". Affirmations of respect for the values of civilian control notwithstanding, the actual level of control sought or achieved by the civilian leadership may vary greatly in practice, from a statement of broad policy goals that military commanders are expected to translate into operational plans, to the direct selection of specific targets for attack on the part of governing politicians. National Leaders with limited experience in military matters often have little choice but to rely on the advice of professional military commanders trained in the art and science of warfare to inform the limits of policy; in such cases, the military establishment may enter the bureaucratic arena to advocate for or against a particular course of action, shaping the policy-making process and blurring any clear cut lines of civilian control.

Advocates of civilian control generally take a Clausewitzian view of war, emphasizing its political character. The of Georges Clemenceau, "War is too serious a matter to entrust to military men" (also frequently rendered as "War is too important to be left to the generals"), wryly reflect this view. Given that broad strategic decisions, such as the decision to declare a war, start an invasion, or end a conflict, have a major impact on the citizens of the country, they are seen by civilian control advocates as best guided by the will of the people (as expressed by their political representatives), rather than left solely to an elite group of tactical experts. The military serves as a special government agency, which is supposed to "implement", rather than "formulate", policies that require the use of certain types of physical force. Kohn succinctly summarizes this view when he writes that:
The point of civilian control is to make security subordinate to the larger purposes of a nation, rather than the other way around. The purpose of the military is to defend society, not to define it.

A state's effective use of force is an issue of great concern for all national leaders, who must rely on the military to supply this aspect of their authority. The danger of granting military leaders full or sovereignty is that they may ignore or supplant the democratic decision-making process, and use physical force, or the threat of physical force, to achieve their preferred outcomes; in the worst cases, this may lead to a coup or military dictatorship. A related danger is the use of the military to crush domestic political opposition through intimidation or sheer physical force, interfering with the ability to have free and fair elections, a key part of the democratic process. This poses the paradox that "because we fear others we create an institution of violence to protect us, but then we fear the very institution we created for protection". Also, military personnel, because of the nature of their job, are much more willing to use force to settle disputes than civilians because they are trained military personnel that specialize strictly in warfare. The military is authoritative and hierarchical, rarely allowing discussion and prohibiting dissention. For instance, in the Empire of Japan, prime ministers and almost everyone in high positions were military people like Hideki Tojo, and advocated and basically pressured the leaders to start military conflicts against China and others because they believed that they would ultimately be victorious.

Many of the Founding Fathers of the United States were suspicious of standing militaries. As Samuel Adams wrote in 1768, "Even when there is a necessity of the military power, within a land, a wise and prudent people will always have a watchful and jealous eye over it". Even more forceful are the words of Elbridge Gerry, a delegate to the American Constitutional Convention, who wrote that "[s]tanding armies in time of peace are inconsistent with the principles of republican Governments, dangerous to the liberties of a free people, and generally converted into destructive engines for establishing despotism."

In Federalist No. 8, one of "The Federalist" papers documenting the ideas of some of the Founding Fathers, Alexander Hamilton expressed concern that maintaining a large standing army would be a dangerous and expensive undertaking. In his principal argument for the ratification of the proposed constitution, he argued that only by maintaining a strong union could the new country avoid such a pitfall. Using the European experience as a negative example and the British experience as a positive one, he presented the idea of a strong nation protected by a navy with no need of a standing army. The implication was that control of a large military force is, at best, difficult and expensive, and at worst invites war and division. He foresaw the necessity of creating a civilian government that kept the military at a distance.

James Madison, another writer of many of "The Federalist" papers, expressed his concern about a standing military in comments before the Constitutional Convention in June 1787:

In time of actual war, great discretionary powers are constantly given to the Executive Magistrate. Constant apprehension of War, has the same tendency to render the head too large for the body. A standing military force, with an overgrown Executive, will not long be safe companions to liberty. The means of defense against foreign danger, have been always the instruments of tyranny at home. Among the Romans it was a standing maxim to excite a war, whenever a revolt was apprehended. Throughout all Europe, the armies kept up under the pretext of defending, have enslaved the people.
The United States Constitution placed considerable limitations on the legislature. Coming from a tradition of legislative superiority in government, many were concerned that the proposed Constitution would place so many limitations on the legislature that it would become impossible for such a body to prevent an executive from starting a war. Hamilton argued in Federalist No. 26 that it would be equally as bad for a legislature to be unfettered by any other agency and that restraints would actually be more likely to preserve liberty. James Madison, in Federalist No. 47, continued Hamilton’s argument that distributing powers among the various branches of government would prevent any one group from gaining so much power as to become unassailable. In Federalist No. 48, however, Madison warned that while the separation of powers is important, the departments must not be so far separated as to have no ability to control the others.

Finally, in Federalist No. 51, Madison argued that to create a government that relied primarily on the good nature of the incumbent to ensure proper government was folly. Institutions must be in place to check incompetent or malevolent leaders. Most importantly, no single branch of government ought to have control over any single aspect of governing. Thus, all three branches of government must have some control over the military, and the system of checks and balances maintained among the other branches would serve to help control the military.

Hamilton and Madison thus had two major concerns: (1) the detrimental effect on liberty and democracy of a large standing army and (2) the ability of an unchecked legislature or executive to take the country to war precipitously. These concerns drove American military policy for the first century and a half of the country’s existence. While armed forces were built up during wartime, the pattern after every war up to and including World War II was to demobilize quickly and return to something approaching pre-war force levels. However, with the advent of the Cold War in the 1950s, the need to create and maintain a sizable peacetime military force "engendered new concerns" of militarism and about how such a large force would affect civil–military relations in the United States.

The United States' Posse Comitatus Act, passed in 1878, prohibits any part of the Army or the Air Force (since the U.S. Air Force evolved from the U.S. Army) from engaging in domestic law enforcement activities unless they do so pursuant to lawful authority. Similar prohibitions apply to the Navy and Marine Corps by service regulation, since the actual Posse Comitatus Act does not apply to them. The Coast Guard is exempt from Posse Comitatus since it normally operates under the Department of Homeland Security versus the Department of Defense and enforces U.S. laws, even when operating as a service with the U.S. Navy.

The act is often misunderstood to prohibit any use of federal military forces in law enforcement, but this is not the case. For example, the President has explicit authority under the Constitution and federal law to use federal forces or federalized militias to enforce the laws of the United States. The act's primary purpose is to prevent local law enforcement officials from utilizing federal forces in this way by forming a "posse" consisting of federal Soldiers or Airmen.

There are, however, practical political concerns in the United States that make the use of federal military forces less desirable for use in domestic law enforcement. Under the U.S. Constitution, law and order is primarily a matter of state concern. As a practical matter, when military forces are necessary to maintain domestic order and enforce the laws, state militia forces under state control i.e., that state's Army National Guard and/or Air National Guard are usually the force of first resort, followed by federalized state militia forces i.e., the Army National Guard and/or Air National Guard "federalized" as part of the U.S. Army and/or U.S. Air Force, with active federal forces (to include "federal" reserve component forces other than the National Guard) being the least politically palatable option.

Maoist military-political theories of people's war and democratic centralism also support the subordination of military forces to the directives of the communist party (although the guerrilla experience of many early leading Communist Party of China figures may make their status as civilians somewhat ambiguous). In a 1929 essay "On Correcting Mistaken Ideas in the Party", Mao explicitly refuted "comrades [who] regard military affairs and politics as opposed to each other and [who] refuse to recognize that military affairs are only one means of accomplishing political tasks", prescribing increased scrutiny of the People's Liberation Army by the Party and greater political training of officers and enlistees as a means of reducing military autonomy . In Mao's theory, the military—which serves both as a symbol of the revolution and an instrument of the dictatorship of the proletariat—is not merely expected to defer to the direction of the ruling non-uniformed Party members (who today exercise control in the People's Republic of China through the Central Military Commission), but also to actively participate in the revolutionary political campaigns of the Maoist era.
Civilian leaders cannot usually hope to challenge their militaries by means of force, and thus must guard against any potential usurpation of powers through a combination of policies, laws, and the inculcation of the values of civilian control in their armed services. The presence of a distinct civilian police force, militia, or other paramilitary group may mitigate to an extent the disproportionate strength that a country's military possesses; civilian gun ownership has also been justified on the grounds that it prevents potential abuses of power by authorities (military or otherwise). Opponents of gun control have cited the need for a balance of power in order to enforce the civilian control of the military.

The establishment of a civilian head of state, head of government or other government figure as the military's commander-in-chief within the chain of command is one legal construct for the propagation of civilian control.

In the United States, Article I of the Constitution gives the Congress the power to declare war (in the War Powers Clause), while Article II of the Constitution establishes the President as the commander-in-chief. Ambiguity over when the President could take military action without declaring war resulted in the War Powers Resolution of 1973.

American presidents have used the power to dismiss high-ranking officers as a means to assert policy and strategic control. Three examples include Abraham Lincoln's dismissal of George McClellan in the American Civil War when McClellan failed to pursue the Confederate Army of Northern Virginia following the Battle of Antietam, Harry S. Truman relieving Douglas MacArthur of command in the Korean War after MacArthur repeatedly contradicted the Truman administration's stated policies on the war's conduct, and Barack Obama's acceptance of Stanley McChrystal's resignation in the War in Afghanistan after a "Rolling Stone" article was published where he mocked several members of the Obama administration, including Vice President Joe Biden.

Differing opinions exist as to the desirability of distinguishing the military as a body separate from the larger society. In "The Soldier and the State", Huntington argued for what he termed "objective civilian control", "focus[ing] on a politically neutral, autonomous, and professional officer corps". This autonomous professionalism, it is argued, best inculcates an esprit de corps and sense of distinct military corporateness that prevents political interference by sworn servicemen and -women. Conversely, the tradition of the citizen-soldier holds that "civilianizing" the military is the best means of preserving the loyalty of the armed forces towards civilian authorities, by preventing the development of an independent "caste" of warriors that might see itself as existing fundamentally apart from the rest of society. In the early history of the United States, according to Michael Cairo,
[the] principle of civilian control... embodied the idea that every qualified citizen was responsible for the defense of the nation and the defense of liberty, and would go to war, if necessary. Combined with the idea that the military was to embody democratic principles and encourage citizen participation, the only military force suitable to the Founders was a citizen militia, which minimized divisions between officers and the enlisted.
In a less egalitarian practice, societies may also blur the line between "civilian" and "military" leadership by making direct appointments of non-professionals (frequently social elites benefitting from patronage or nepotism) to an officer rank. A more invasive method, most famously practiced in the Soviet Union and People's Republic of China, involves active monitoring of the officer corps through the appointment of political commissars, posted parallel to the uniformed chain of command and tasked with ensuring that national policies are carried out by the armed forces. The regular rotation of soldiers through a variety of different postings is another effective tool for reducing military autonomy, by limiting the potential for soldiers' attachment to any one particular military unit. Some governments place responsibility for approving promotions or officer candidacies with the civilian government, requiring some degree of deference on the part of officers seeking advancement through the ranks.

Historically, direct control over military forces deployed for war was hampered by the technological limits of command, control, and communications; national leaders, whether democratically elected or not, had to rely on local commanders to execute the details of a military campaign, or risk centrally-directed orders' obsolescence by the time they reached the front lines. The remoteness of government from the action allowed professional soldiers to claim military affairs as their own particular sphere of expertise and influence; upon entering a state of war, it was often expected that the generals and field marshals would dictate strategy and tactics, and the civilian leadership would defer to their informed judgments.

Improvements in information technology and its application to wartime command and control (a process sometimes labeled the "Revolution in Military Affairs") has allowed civilian leaders removed from the theater of conflict to assert greater control over the actions of distant military forces. Precision-guided munitions and real-time videoconferencing with field commanders now allow the civilian leadership to intervene even at the tactical decision-making level, designating particular targets for destruction or preservation based on political calculations or the counsel of non-uniformed advisors.

In the United States the Hatch Act of 1939 does not directly apply to the military, however, Department of Defense Directive 1344.10 (DoDD 1344.10) essentially applies the same rules to the military. This helps to ensure a non-partisan military and ensure smooth and peaceful transitions of power.

Political officers screened for appropriate ideology have been integrated into supervisory roles within militaries as a way to maintain the control by political rulers. Historically they are associated most strongly with the Soviet Union and China rather than liberal democracies.

While civilian control forms the normative standard in almost every society outside of military dictatorships, its practice has often been the subject of pointed criticism from both uniformed and non-uniformed observers, who object to what they view as the undue "politicization" of military affairs, especially when elected officials or political appointees micromanage the military, rather than giving the military general goals and objectives (like "Defeat Country X"), and letting the military decide how best to carry those orders out. By placing responsibility for military decision-making in the hands of non-professional civilians, critics argue, the dictates of military strategy are subsumed to the political, with the effect of unduly restricting the fighting capabilities of the nation's armed forces for what should be immaterial or otherwise lower priority concerns.

The "Revolt of the Admirals" that occurred in 1949 was an attempt by senior US Navy personnel, to force a change in budgets directly opposed to the directives given by the Civilian leadership.

U.S. President Bill Clinton faced frequent allegations throughout his time in office (particularly after the Battle of Mogadishu) that he was ignoring military goals out of political and media pressure—a phenomenon termed the "CNN effect". Politicians who personally lack military training and experience but who seek to engage the nation in military action may risk resistance and being labeled "chickenhawks" by those who disagree with their political goals.

In contesting these priorities, members of the professional military leadership and their non-uniformed supporters may participate in the bureaucratic bargaining process of the state's policy-making apparatus, engaging in what might be termed a form of regulatory capture as they attempt to restrict the policy options of elected officials when it comes to military matters. An example of one such set of conditions is the "Weinberger Doctrine", which sought to forestall another American intervention like that which occurred in the Vietnam War (which had proved disastrous for the morale and fighting integrity of the U.S. military) by proposing that the nation should only go to war in matters of "vital national interest", "as a last resort", and, as updated by Weinberger's disciple Colin Powell, with "overwhelming force". The process of setting military budgets forms another contentious intersection of military and non-military policy, and regularly draws active lobbying by rival military services for a share of the national budget.

Nuclear weapons in the U.S. are controlled by the civilian United States Department of Energy, not by the Department of Defense.

During the 1990s and 2000s, public controversy over LGBT policy in the U.S. military led to many military leaders and personnel being asked for their opinions on the matter and being given deference although the decision was ultimately not theirs to make.

During his tenure, Secretary of Defense Donald Rumsfeld raised the ire of the military by attempting to reform its structure away from traditional infantry and toward a lighter, faster, more technologically driven force. In April 2006, Rumsfeld was severely criticized by some retired military officers for his handling of the Iraq War, while other retired military officers came out in support of Rumsfeld. Although no active military officers have spoken out against Rumsfeld, the actions of these officers is still highly unusual. Some news accounts have attributed the actions of these generals to the Vietnam war experience, in which officers did not speak out against the administration's handling of military action. Later in the year, immediately after the November elections in which the Democrats gained control of the Congress, Rumsfeld resigned.

As of 2015, military dictatorships, where there is "no" civilian control of the military, are:

Other countries generally have civilian control of the military, to one degree or another. Strong democratic control of the military is a prerequisite for membership in NATO. Strong democracy and rule of law, implying democratic control of the military, are prerequisites for membership in the European Union.




</doc>
<doc id="2609064" url="https://en.wikipedia.org/wiki?curid=2609064" title="Municipal services">
Municipal services

Municipal services or city services refer to basic services that residents of a city expect the city government to provide in exchange for the taxes which citizens pay. Basic city services may include sanitation (both sewer and refuse), water, streets, the public library, schools, food inspection, fire department, police, ambulance, and other health department issues and transportation. City governments often operate or contract for additional utilities like electricity, gas and cable television. Mumbai even provides a lighthouse service.

The available municipal services for any individual municipality will depend on location, history, geography, statutes and tradition. Provided services may vary from country to country or even within a country. Services may be run directly by a department of the municipality or be sub-contracted to a third party.

Funding for the services provided varies with the municipality in question. Funding can include tax revenue (property tax, income tax, municipal sales tax), fees (such as building permits), Grants from other Governments, fines such as speeding or parking violations, usage fees for optional services, or other sources such as profits from municipally owned or operated utilities.
Probably the greatest influence is the country in which the municipality is located.

In the UK, a combination of local taxation based on property value and central government grants is the main means of funding core services. This is supplemented by nominal fees for services provided (e.g. leisure facilities). For some services, a competitive fee is charged compared to commercial concerns that which allows a profit to be made. For other services, full commercial rates may be charged with municipally owned utilities or commercial property, for example. For the most part, services will be part subsidized by the municipality or fully subsidized by the municipality.
In recent years, UK councils have been given some leeway in finding alternative funding which can be the simple sponsorship of flower baskets to the trading of surplus buildings and land for services from private firms. In certain notorious cases, local councils have used council funds to speculate on the money markets.

Municipalities in other countries may have other methods of funding (e.g. local income taxes or even from the profits of utilities or industrial concerns fully owned by the municipality).

In the United Kingdom, until their abolishment, Municipal Corporations were powerful organizations. In their Victorian heyday, with the growth of urbanization and industrialization, they could be responsible for the promotion, organization, funding, building and management of everything from housing to water supplies, power in the form of both gas and electricity, to the introduction of electric trams; almost any activity that the city fathers thought
was necessary to promote the economic, social and environmental well-being of their municipality.

However, special powers needed to be granted through Local Acts in Parliament (for example, the Manchester Corporation Waterworks Act needed for the construction of Manchester's reservoirs). As times changed, the Municipal Corporations continued to try to advance the cause of their municipalities. For example, in the 1930s aviation was the new technological frontier and municipalities worked to promote themselves with the development of municipal airports.

For the most part, UK municipalities lost their in-house utilities to the nationalisation and centralisation of public utilities. One notable exception is Kingston upon Hull, which still has a municipally-owned telephone company known as Kingston Communications.

One trend in the UK, (in the name of efficiency) has been the privatisation of departments, the transfer of staff and assets to the new organisation and the contracting with the new organization for services to the council. This model has been used for services from road cleaning to social housing, to leisure facilities, though no council yet seems to dare to do so for more high-profile services such as schools and social services.

In the UK, fire and police services are not under direct municipal control, even when a force can be closely identified with a specific municipal area such as Greater London. However, fire and police services are in part paid for by a surcharge to local taxation, and although they have no say in operational matters, local government appoints members to a committee to oversee the running of each force.

Where there is a substantial industrial urban population isolated from other conurbations, or when and where the growth in demand is so great that it becomes uneconomic or impractical for commercial organisations to provide, the municipalities concerned may assume functions necessary for the growth and functioning of the city.

This was the case in Victorian England also today in Mumbai, for example, the Municipal Corporation of Greater Mumbai and Brihanmumbai Electric Supply and Transport are the major municipal organizations which are needed to allow the city to function. The Los Angeles Department of Water and Power is a legacy of Los Angeles, which found itself in a similar position of rapid growth at the beginning of the last century. The similarly isolated Johannesburg has chosen to run its services as standalone and self-funding, corporate entities.




</doc>
<doc id="4514687" url="https://en.wikipedia.org/wiki?curid=4514687" title="E-participation">
E-participation

e-participation ("also written" eParticipation "and" e-Participation) is the term referring to "ICT-supported participation in processes involved in government and governance". Processes may concern administration, service delivery, decision making and policy making. E-participation is hence closely related to e-government and e-governance participation. The need for the term has emerged as citizen interests have received less attention than those of the service providers in e-government development. It also emerged as the need to distinguish between the roles of citizen and customer has become more pressing.

A more detailed definition sees e-participation as "the use of information and communication technologies to broaden and deepen political participation by enabling citizens to connect with one another and with their elected representatives". This definition includes all stakeholders in democratic decision-making processes and not only citizen related top-down government initiatives. So e-participation can be seen as part of e-democracy, the use of ICT by governments in general used by elected officials, media, political parties and interest groups, civil society organizations, international governmental organizations, or citizens/voters within any of the political processes of states/regions, nations, and local and global communities.

The complexity of e-participation processes results from the large number of different participation areas, involved stakeholders, levels of engagement, and stages in policy making. .

The term originated in the early 2000s and draws generally on three developments.


The term 'participation' means taking part in joint activities for the purpose of reaching a common goal. This encompasses both trivial situations in which participation mainly has a technical meaning, ”doing things together”. For example, a football team needs 11 players, and dancing often requires two or more people acting in a coordinated manner. But participation, even in trivial situations, also has a goal-oriented aspect which means decision making and control are involved. Participation in political science and theory of management refers to direct public participation in political, economical or management decisions. The two are not completely separated but belong on a spectrum of complexity and context. When participation becomes complicated, decision making becomes necessary. Hence, any participatory process is potentially important for the rule system governing the activities. In terms of points 2 and 3 above, this means that when service processes become complex, the implementation of them will not be in all details based on political decisions but also on what is found to be practical.

Instead of taking in and accepting knowledge as is disseminated by the media and government, by participating, one becomes an active citizen and further contributes to a democratic society. When such practical doings become implemented in government (e)service systems, they will affect decision making, as many changes will later be hard to make simply because existing procedures are implemented in ICT systems and government agencies’ procedures. There are many theories dealing with institutionalization, for example structuration theory, institutional theory, and actor-network theory. These theories all, in different ways, deal with how "ways of doing things" become established or rejected, and how those that become established increasingly affect the ways we "normally" do things.

A number of tools and models have emerged as part of Web 2.0 that can be used or inspire the design of architecture for e-participation. In particular, "the emergence of online communities oriented toward the creation of useful products suggests that it may be possible to design socially mediating technology that support public-government collaborations" .




To demonstrate e-participation at work, one can turn to one of its types - crowdsourcing. This is generally defined as the enlisting of a group of humans to solve problems via the World Wide Web. The idea is that this platform is able to collect human resources from the most unlikely and remotest places, contributing to the general store of intellectual capital. Crowdsourcing can be applied in different stages of the policy-making process and these could transpire on the information, consultation, and active participation levels. At the information level, there is a one-way relationship, wherein the participants receive information from the government. The consultation process entails a two-way interaction where citizens already provide their inputs, feedback, and reactions. Finally, active participation can refer to the deeper participatory involvement where citizens directly contribute in the formulation of policy content. This level of e-participation is increasingly being practiced through tools such as online petition, e-referendum, e-panels, citizen e-juries, and participatory GIS, among others.

eParticipation is the Preparatory Action lasts for three years (2006–2008). The EU is taking the lead in using online tools to improve the legislative process for its citizens. eParticipation which launched on January 1, 2007 will run as a series of linked projects which each contribute to a greater awareness and involvement by citizens in the legislation process from initial drafting to implementation at a regional and local level.

The individual projects will concentrate on ensuring that the legislative language and process is more transparent, understandable and accessible to citizens. In addition the projects emphasis on the communication of legislation will be used to enhance and grow citizens' involvement and contribution in the process of creating and implementing the legislation.

So far, 21 projects have been funded. The European Parliament, national parliaments and local and regional authorities are actively involved. State-of-the-art ICT tools are being tested to facilitate the writing of legal texts, including translation into different languages, and the drafting of amendments as well as making the texts easier for non-specialists to find and understand. New digital technologies are also being used to give citizens easier access to information and more opportunity to influence decisions that affect their lives. A report , which was published as a MOMENTUM white paper, highlights the major facts and figures of those projects while providing some initial policy recommendations for future use.

The European Commission has now launched a number of actions aiming at further advancing the work of supporting eParticipation.

Examples:






</doc>
<doc id="298608" url="https://en.wikipedia.org/wiki?curid=298608" title="State of emergency">
State of emergency

A state of emergency is a situation in which a government is empowered to perform actions that it would normally not be permitted to do. A government can declare such a state during a disaster, civil unrest, or armed conflict. Such declarations alert citizens to change their normal behavior and orders government agencies to implement emergency plans. "Justitium" is its equivalent in Roman law—a concept in which the senate could put forward a final decree (senatus consultum ultimum) that was not subject to dispute.

States of emergency can also be used as a rationale or pretext for suspending rights and freedoms guaranteed under a country's constitution or basic law. The procedure for and legality of doing so vary by country.

Under international law, rights and freedoms may be suspended during a state of emergency; for example, a government can detain persons and hold them without trial. All rights that can be derogated from are listed in the International Covenant for Civil and Political Rights. Non-derogable rights cannot be suspended. Non-derogable rights are listed in Article 4 of the ICCPR; they include right to life, the rights to freedom from arbitrary deprivation of liberty, slavery, torture, and ill-treatment.

Some countries have made it illegal to modify emergency law or the constitution during the emergency; other countries have the freedom to change any legislation or rights based constitutional frameworks at any time that the legislative chooses to do so. Constitutions are contracts between the government and the private individuals of that country. The International Covenant for Civil and Political Rights (ICCPR) is an international law document signed and ratified by states. Therefore, the Covenant applies to only those persons acting in an official capacity, not private individuals. However, States Parties to the Covenant are expected to integrate it into national legislation. The state of emergency (within the ICCPR framework) must be publicly declared and the Secretary-General of the United Nations and all other States Parties to the Covenant must be notified immediately, to declare the reason for the emergency, the date on which the emergency is to start, the derogations that may take place, with the timeframe of the emergency and the date in which the emergency is expected to finish. Although this is common protocol stipulated by the ICCPR, its monitoring Committee of experts has no sanction power and its recommendations are therefore not always strictly followed; enforcement is therefore better regulated by the American and European Conventions and Courts on human rights.

Though fairly uncommon in democracies, dictatorial regimes often declare a state of emergency that is prolonged indefinitely for the life of the regime, or for extended periods of time so that derogations can be used to override human rights of their citizens usually protected by the International Covenant on Civil and political rights. In some situations, martial law is also declared, allowing the military greater authority to act. In other situations, emergency is not declared and de facto measures taken or decree-law adopted by the government. Ms. Nicole Questiaux (France) and Mr. Leandro Despouy (Argentina), two consecutive United Nations Special Rapporteurs, have recommended to the international community to adopt the following "principles" to be observed during a state or de facto situation of emergency : Principles of Legality, Proclamation, Notification, Time Limitation, Exceptional Threat, Proportionality, Non-Discrimination, Compatibility, Concordance and Complementarity of the Various Norms of International Law (cf. "Question of Human Rights and State of Emergency", E/CN.4/Sub.2/1997/19, at Chapter II; see also "").

Article 4 to the International Covenant on Civil and Political Rights (ICCPR), permits states to derogate from certain rights guaranteed by the ICCPR in "time of public emergency". Any measures derogating from obligations under the Covenant, however, must be to only the extent required by the exigencies of the situation, and must be announced by the State Party to the Secretary-General of the United Nations. The European Convention on Human Rights and American Convention on Human Rights have similar derogatory provisions. No derogation is permitted to the International Labour Conventions.

Some political theorists, such as Carl Schmitt, have argued that the power to decide the initiation of the state of emergency defines sovereignty itself. In "State of Exception" (2005), Giorgio Agamben criticized this idea, arguing that the mechanism of the state of emergency deprives certain people of their civil and political rights, producing his interpretation of "homo sacer".
In many democratic states there are a selection of legal definitions for specific states of emergency, when the constitution of the State is partially in abeyance depending on the nature of the perceived threat to the general public. In order of severity these may include:
Sometimes, the state of emergency can be abused by being invoked. An example would be to allow a state to suppress internal opposition without having to respect human rights. An example was the August 1991 attempted coup in the Soviet Union (USSR) where the coup leaders invoked a state of emergency; the failure of the coup led to the dissolution of the Soviet Union.

Derogations by states having ratified or acceded to binding international agreements such as the ICCPR, the American and European Conventions on Human Rights and the International Labour Conventions are monitored by independent expert committees, regional Courts and other State Parties.

The Constitution, which has been amended several times, has always allowed for a state of emergency (literally "estado de sitio", "state of siege"), to be declared if the constitution or the authorities it creates are endangered by internal unrest or foreign attack. This provision was much abused during dictatorships, with long-lasting states of siege giving the government a free hand to suppress opposition ( a state of emergency had been declared 52 times by democratic and dictatorial governments, starting in 1854 shortly after the constitution came into force). The American Convention on Human Rights (Pacto de San José de Costa Rica), adopted in 1969 but ratified by Argentina only in 1984 immediately after the end of the National Reorganization Process, restricts abuse of the state of emergency by requiring any signatory nation declaring such a state to inform the other signatories of its circumstances and duration, and what rights are affected.

State-of-emergency legislation differs in each state of Australia.

In Victoria, the premier can declare a state of emergency if there is a threat to employment, safety or public order. The declaration expires after 30 days, and a resolution of either the upper or lower House of Parliament may revoke it earlier. Under the Public Safety Preservation Act, a declared state of emergency allows the premier to immediately make any desired regulations to secure public order and safety. However, these regulations expire if Parliament does not agree to continue them within 7 days. Also, under the Essential Services Act, the premier (or delegate) may operate or prohibit operation of, as desired, any essential service (e.g., transport, fuel, power, water, gas).

In regards to Emergency Management, regions (usually on a local government area basis) that have been affected by a natural disaster are the responsibility of the state, until that state declares a State of Emergency where access to the Federal Emergency Fund becomes available to help respond to and recover from natural disasters. A State of Emergency does not apply to the whole state, but rather districts or shires, where essential services may have been disrupted.

See also, Exceptional circumstances; a term most commonly used in Australia with regard to emergency relief payments.

Extreme act that, in Brazil ("Estado de Sítio" or "Estado de Exceção", in Portuguese), can be declared on the following circumstances:

The state of emergency could last for 30 days, being possible to extend it for more days in case of persistence of the reasons of exceptionality.

Only the President is able to declare or prorogate this State; after receiving formal authorization from National Congress and after consultation with the National Security Council or the Council of the Republic.

The federal government of Canada can use the Emergencies Act to invoke a state of emergency. A national state of emergency automatically expires after 90 days, unless extended by the Governor-in-Council. There are different levels of emergencies: Public Welfare Emergency, Public Order Emergency, International Emergency, and War Emergency.

The Emergencies Act replaced the War Measures Act in 1988. The War Measures Act was invoked three times in Canadian history, most controversially during the 1970 October Crisis, and also during World War I (from 1914 to 1920, against threat of Communism) and World War II (from 1942 to 1945, against perceived threat from Japanese Canadians following Imperial Japan's attack on Pearl Harbor).

Under the current Emergency Act a state of emergency can also be declared by provincial, territorial, and municipal governments. In addition Canada's federal government and any of its provincial governments can suspend, for five years at a time, Charter rights to fundamental freedoms in section 2, to legal rights in sections 7 through 14, and to equality rights in section 15 by legislation which invokes the notwithstanding clause, section 33, and therefore emergency powers can effectively be created even without using the Emergency Act.

The police chief in a district can impose a zone in which people can be body searched without a specific suspicion. Such an order must be issued in writing, published, and imposed for a limited period. The police law (article 6) regulates this area. The normal procedure calls for assisting the suspect to a private area and stripping them. The police can also impose a zone in where specific crimes such as violence, threats, blackmailing and vandalism can be punished with a double penalty length. The zone can only be imposed if there is an extraordinary crime development and the zone can only last up to three months unless the extraordinary crime development still applies.

If the police feel that a situation involving a crowd of people can get out of hand, they can order the assembly to be dissolved and "pass the street" in the name of the king. People that after three such warnings are still part of the crowd can then without further warning be subjugated to mass arrest. All people arrested can then be detained for 24 hours without charging them or taking them for a judge. This is called a precluding arrest.

Egyptians lived under an Emergency Law (Law No. 162 of 1958) from 1967 to 2012, except for an 18-month break in 1980 and 1981. The emergency was imposed during the 1967 Arab-Israeli War, and reimposed following the assassination of President Anwar Sadat. The law continuously extended every three years since 1981. Under the law, police powers were extended, constitutional rights suspended and censorship was legalized. The law sharply circumscribed any non-governmental political activity: street demonstrations, non-approved political organizations, and unregistered financial donations were formally banned. Some 17,000 people were detained under the law, and estimates of political prisoners run as high as 30,000. The emergency rule expired on May 31, 2012, and was put back in place in January 2013. Egypt declared a month-long national emergency on 14 August 2013.

The Egyptian presidency announced a one-month state of emergency across the country on August 14, 2013 and ordered the armed forces to help the Interior Ministry enforce security. The announcement made on state TV followed deadly countrywide clashes between supporters of deposed President Mohammed Morsi and the security forces.

Three main provisions concern various kind of "state of emergency" in France: Article 16 of the Constitution of 1958 allows, in time of crisis, "extraordinary powers" to the president. Article 36 of the same constitution regulates "state of siege" (""). Finally, the Act of 3 April 1955 allows the proclamation, by the Council of Ministers, of the "state of emergency" (""). The distinction between article 16 and the 1955 Act concerns mainly the distribution of powers: whereas in article 16, the executive power basically suspend the regular procedures of the Republic, the 1955 Act permits a twelve-day state of emergency, after which a new law extending the emergency must be voted by the Parliament. These dispositions have been used at various times, in 1955, 1958, 1961, 1988, 2005, and 2015.

The Weimar Republic constitution (1919–1933) allowed states of emergency under Article 48 to deal with rebellions. Article 48 was often invoked during the 14-year life of the Republic, sometimes for no reason other than to allow the government to act when it was unable to obtain a parliamentary majority.

After the February 27, 1933, Reichstag fire, an attack blamed on the communists, Adolf Hitler declared a state of emergency using Article 48, and then had President von Hindenburg sign the Reichstag Fire Decree, which suspended some of the basic civil liberties provided by the Weimar Constitution (such as habeas corpus, freedom of expression, freedom of the speech, the freedom to assemble or the privacy of communications) for the whole duration of the Third Reich. On March 23, the Reichstag enacted the Enabling Act of 1933 with the required two-thirds majority, which enabled Chancellor Adolf Hitler and his cabinet to enact laws without the participation of the legislative. (The Weimar Constitution was never actually repealed by Nazi Germany, but it effectively became ineffective after the passage of the Enabling Act.) These two laws implemented the "Gleichschaltung", the Nazis' institution of totalitarianism.

In the postwar Federal Republic of Germany the "Emergency Acts" state that some of the basic constitutional rights of the Basic Law may be limited in case of a state of defence, a state of tension, or an internal state of emergency or disaster (catastrophe). These amendments to the constitution were passed on May 30, 1968, despite fierce opposition by the so-called "extra-parliamentary opposition" (see German student movement for details).

The Standing Committee of the National People's Congress can declare a state of emergency and deploy troops from the People's Liberation Army Hong Kong Garrison under the Law of the People's Republic of China on the garrisoning of the Hong Kong Special Administrative Region.

The Chief Executive of Hong Kong along with the Executive Council can prohibit public gatherings, issue curfew orders, prohibit the movement of vessels or aircraft and appoint special constable all under Chapter 245 ("Public Order Ordinance") of Hong Kong Law.

Since 1997, no emergency measures have been enacted. Prior to that date, emergency measures were used for four major incidents:

According to the Hungarian Constitution, the National Assembly of Hungary can declare state of emergency in case of armed rebellion or natural or industrial disaster. It expires after 30 days, but can be extended. Most civil rights can be suspended, but basic human rights (such as the right to life, the ban of torture, and freedom of religion) cannot.

During state of emergency, the Parliament cannot be disbanded.

The Icelandic constitution provides no mechanism for state of emergency nor martial law.

The State of Emergency can be proclaimed by the President of India, when he/she perceives grave threats to the nation, albeit through the advice of the cabinet of ministers. Part XVIII of the Constitution of India gives the President the power to overrule many provisions, including the ones guaranteeing fundamental rights to the citizens of India 

In India, a state of emergency was declared twice:


The first Emergency was declared by the president, on advice of Jawaharlal Nehru, the then Prime Minister, and by President Fakhruddin Ali Ahmed on advice of the then Prime Minister, Indira Gandhi The provisions of the Constitution allows the Prime Minister to rule by decree.

In Ireland declaring a state of "national emergency" involves Article 28.3.3° of the 1937 Constitution of Ireland, which states that:
In addition, during a "war or armed rebellion", military tribunals may try civilians, and the Defence Forces are not bound by habeas corpus.

The First Amendment of the Constitution of 1939 allows an emergency to be declared during wars in which the state is a non-belligerent, subject to resolutions by the houses of the Oireachtas. By the 2nd Amendment of 1941, an emergency ends, not automatically when the war does, but only by Oireachtas resolutions. The 21st Amendment of 2002 prevents the reintroduction of capital punishment during an emergency.

The first amendment was rushed through the Oireachtas after the outbreak of the Second World War, in which the state remained neutral. Immediately after, the required resolution was passed, in turn enabling the passage of the Emergency Powers Act 1939 (EPA), which granted the government and its ministers sweeping powers to issue statutory orders termed "Emergency Powers Orders" (EPOs). (The period in Ireland was and is referred to as "The Emergency".) The EPA expired in 1946, although some EPOs were continued under the Supplies and Services (Temporary Provisions) Act 1946 until as late as 1957. Rationing continued until 1951.

The 1939 state of emergency was not formally ended until a 1976 resolution, which also declared a new state of emergency in relation to the Troubles in Northern Ireland and in particular the recent assassination of the British ambassador to Ireland, Christopher Ewart Biggs. The Emergency Powers Act 1976 was then passed to increase the Garda Síochána powers to arrest, detain, and question those suspected of offences against the state. President Cearbhall Ó Dálaigh referred the bill under Article 26 of the Constitution to the Supreme Court, which upheld its constitutionality. The referral was condemned by minister Paddy Donegan as a "thundering disgrace", causing Ó Dálaigh to resign in protest. The 1976 EPA expired after one year, but the state of emergency persisted until 1995, when as part of the Northern Ireland peace process it was rescinded as a "confidence building measure" to satisfy physical force republicans after the Provisional IRA's 1994 ceasefire.

The Offences against the State Act does not require a state of emergency under Article 28.3.3°. Part V of the Act, which provides for a non-jury Special Criminal Court (SCC), is permitted under Article 38.3.1°. Part V is activated by a declaration from the government that it is "necessary to secure the preservation of public peace and order", and it can be rescinded by vote of Dáil Éireann. Provision for internment is similarly activated and rescinded (originally by Part VI of the 1939 act, later by Part II of a 1940 amending act). Parts V and VI were both activated during the Second World War and the IRA's late 1950s Border Campaign; Part V has been continually active since 1972.

Several official reviews of the Constitution and the Offences Against the State Acts have recommended a time limit within which the operation of Article 28.3.3° or Article 38.3.1° must either be explicitly renewed by resolution or else lapse.

Israel's Emergency Defence Regulations are older than the state itself, having been passed under the British Mandate for Palestine in 1945. A repeal was briefly considered in 1967 but cancelled following the Six-Day War. The regulations allow Israel, through its military, to control movements and prosecute suspected terrorists in occupied territories, and to censor publications that are deemed prejudicial to national defense.

The Standing Committee of the National People's Congress can declare a state of emergency and deploy troops from the People's Liberation Army Macau Garrison under the Article 14 of Macau's Basic Law on the defence of the Macau Special Administrative Region.

The Chief Executive of Macau can use the Macau national security law to prohibit public gatherings, issue curfew orders, prohibit other activities perceived to be a threat against the Region or China.

Since 1999 no emergency measure have been enacted. Prior to 1999 emergency measures have been used for 1 major incident:

In Malaysia, if the Yang di-Pertuan Agong (Monarch) is satisfied that a grave emergency exists whereby the security, or the economic life, or public order in the Federation or any part thereof is threatened, he may issue a Proclamation of Emergency making therein a declaration to that effect.

In the history of Malaysia, a state of emergency was declared by the then-colonial government of Britain. The state of emergency lasted from 1948 until 1960 to deal with the communists led by Chin Peng.

States of emergency were also declared during the "Konfrontasi" in 1962, the 1966 Sarawak constitutional crisis and 1977 Kelantan Emergency.

When a race riot broke out on May 13, 1969, a state of emergency was declared.

On August 11, 2005 a state of emergency was announced for the world's 13th largest port, Port Klang and the district of Kuala Selangor after air pollution there reached dangerous levels (defined as a value greater than 500 on the Air Pollution Index or API).

Thiery Rommel, the European Commission's envoy to Malaysia, told Reuters by telephone on November 13, 2007 (the last day of his mission) that, "Today, this country still lives under (a state of) emergency." Although not officially proclaimed as a state of emergency, the Emergency Ordinance and the Internal Security Act had allowed detention for years without trial.

On June 23, 2013 a state of emergency was declared by Prime Minister Najib Abdul Razak for Muar and Ledang, Johor as smoke from land-clearing fires in Indonesia pushed air pollution index to above 750. This was the first time in years that air quality had dipped to a hazardous level with conditions worsening as dry weather persisted and fires raged in Sumatra.

On February 5, 2018, a state of emergency was declared by Maldives's President Abdulla Yameen for 15 days and ordered security forces into the supreme court and arrested a former president Maumoon Abdul Gayoom and the Chief Justice of Honorable Supreme court of Maldives. 

Namibia declared last a State of Emergency due to an ongoing drought in 2016.

The Civil Defence Emergency Management Act 2002 gives the government and local city council the power to issue a state of emergency, either over the entire country or within a specific region. This may suspend ordinary work and essential services if need be. The state of emergency in New Zealand expires on the commencement of the seventh day after the date on which it was declared, unless it is extended. However, the minister of civil defence or local mayor may lift the state of emergency after an initial review of the region's status.

In Nigeria, a state of emergency is usually declared in times of great civil unrest. In recent years, it has specifically been implemented in reaction to terrorist attacks on Nigerians by the Islamic jihadist group Boko Haram.

On 14 May 2013, Goodluck Jonathan declared a state of emergency for the entire northeastern states of Borno, Yobe and Adamawa. A more limited state of emergency had been declared on 31 December 2011 in parts of Yobe, Borno, Plateau and Niger states. This earlier declaration included the temporary shutdown of the international borders in those regions.

In Pakistan, a state of emergency was declared five times in its history:

The first three were regarded as the imposition of direct martial law.

In Romania, there are two types of states of emergency, each designed for a different type of situation.

The most well-known event in which the state of emergency has been enforced was because of 1977 Vrancea earthquake.

The last instance in which the "special zone of public safety" was enforced was in December 8, 2013-ongoing, in Pungești, Vaslui following civil unrest in Pungești from Chevron's plans to begin exploring shale-gas in the village. According to police officials, the special security zone will be maintained as long as there is conflict in the area that poses a threat to Chevron’s operations. This special security zone has faced domestic and international criticism for alleged human-rights abuses.

Sierra Leone declared, on 7 February 2019, a State of Emergency due to ongoing rape and sexual violence in the country.

States of emergency in South Africa are governed by section 37 of the Constitution and by the State of Emergency Act, 1997. The President may declare a state of emergency only when "the life of the nation is threatened by war, invasion, general insurrection, disorder, natural disaster or other public emergency" and if the ordinary laws and government powers are not sufficient to restore peace and order. The declaration is made by proclamation in the "Government Gazette" and may only apply from the time of publication, not retroactively. It can only continue for 21 days unless the National Assembly grants an extension, which may be for at most three months at a time. The High Courts have the power, subject to confirmation by the Constitutional Court, to determine the validity of the declaration of a state of emergency.

During a state of emergency the President has the power to make emergency regulations "necessary or expedient" to restore peace and order and end the emergency. This power can be delegated to other authorities. Emergency measures can violate the Bill of Rights, but only to a limited extent. Some rights are inviolable, including amongst others the rights to life and to human dignity; the prohibition of discrimination on the grounds of race, sex or religion; the prohibition of torture or inhuman punishment; and the right of accused people to a fair trial. Any violation of a constitutional right must be strictly required by the emergency. Emergency measures may not indemnify the government or individuals for illegal actions. They may impose criminal penalties, but not exceeding three years' imprisonment. They may not require military service beyond that required by the ordinary laws governing the defence force. An emergency measure may be disapproved by the National Assembly, in which case it lapses, and no emergency measure may interfere with the elections, powers or sittings of Parliament or the provincial legislatures. The courts have the power to determine the validity of any emergency measure.

The constitution places strict limits on any detention without trial during a state of emergency. A friend or family member of the detainee must be informed, and the name and place of detention must be published in the "Government Gazette". The detainee must have access to a doctor and a legal representative. He or she must be brought before a court within at most ten days, for the court to determine whether the detention is necessary, and if not released may demand repeated review every ten days. At the court review the detainee must be allowed legal representation and must be allowed to appear in person. The provisions on detention without trial do not apply to prisoners of war in an international conflict; instead they must be treated in accordance with the Geneva Conventions and other international law.

In Spain, there are three degrees of state of emergency ("estado de emergencia" in Spanish): "alarma" (alarm or alert), "excepción" (exception[al circumstance]) and "sitio" (siege). They are named by the constitution, which limits which rights may be suspended, but regulated by the "Ley Orgánica 4/1981" (Organic Law).

On December 4, 2010, the first state of alert was declared following the air traffic controllers strike. It was the first time since the Francisco Franco's regime that a state of emergency was declared.

In Sri Lanka, the President is able to proclaim emergency regulations under the "Public Security Ordinance" in the constitution in order to preserve public security and public order; suppression of mutiny, riot or civil commotion; or maintenance of supplies and services essential to the life of the community. These regulations last for one month unless confirmed otherwise by Parliament.

According to Art. 185 of the Swiss Federal Constitution The Federal Council (Bundesrat) can call up in their own competence military personnel of maximum 4000 militia for three weeks to safeguard inner or outer security (called Federal Intervention or Federal Execution, respectively). A larger number of soldiers or of a longer duration is subject to parliamentary decision. For deployments within Switzerland the principle of subsidiarity rules: as a first step, unrest has to be overcome with the aid of cantonal police units.

An emergency prevailed in Syria from 1962 to 2011. Originally predicated on the conflict with Israel, the emergency acted to centralize authority in the presidency and the national security apparatus while silencing public dissent. The emergency was terminated in response to protests that preceded the Syrian Civil War. Under the 2012 constitution, the president may pass an emergency decree with a 2/3 concurrence of his ministers, provided that he presents it to the legislature for constitutional review.

A state of emergency was declared in 1970 during the Black Power Revolution by then Prime Minister Eric Williams. During the attempted state coup by the Jamaat al Muslimeen against the NAR government of the then Prime Minister A. N. R. Robinson, a state of emergency was declared during the coup attempt and for a period after the coup.

On August 4, 1995, a state of emergency was declared to remove the Speaker of the House Occah Seepaul by Prime Minister Patrick Manning during a constitutional crisis. The government had attempted to remove the speaker via a no-confidence motion, which failed. The state of emergency was used to remove the speaker using the emergency powers granted.

The Prime Minister Kamla Persad-Bissessar announced a state of emergency on 22 August 2011 at 8:00 pm in an attempt to crack down on the trafficking of illegal drugs and firearms, in addition to gangs. The decision of the President, George Maxwell Richards, to issue the proclamation for the state of emergency was debated in the country's Parliament as required by the Constitution on September 2, 2011 and passed by the required simple majority of the House of Representatives. On September 4 the Parliament extended the state of emergency for a further 3 months. It ended in December 2011.

Since the foundation of the Republic of Turkey in 1923 the military conducted three "coups d'état" and announced martial law. Martial law between 1978 and 1983 was replaced by a state of emergency that lasted until November 2002.
The latest state of emergency was declared by President Erdoğan on 20 July 2016 following a failed coup attempt on 15 July 2016 by a faction of the country's armed forces.

In the United Kingdom, only the British Sovereign, on the advice of the Privy Council (or a Minister of the Crown in exceptional circumstances) is able to proclaim emergency regulations under the Civil Contingencies Act 2004 in case of any seriously fatal threats to their human welfare, their human society, and their environment, such as warfare or terrorism. These regulations last for a maximum of thirty days unless extended by Parliament.
A state of emergency was last invoked in 1974 by Prime Minister Edward Heath in response to increasing industrial action. 

The act grants wide-ranging powers to central and local government in the event of an emergency. It allows for the modification of primary legislation by emergency regulation, with the exception of the Human Rights Act 1998 and Part 2 of the Civil Contingencies Act 2004.

The United States Constitution explicitly provides some emergency powers:

Aside from these, many provisions of law exist in various jurisdictions, which take effect only upon an executive declaration of emergency; some 500 federal laws take effect upon a presidential declaration of emergency. The National Emergencies Act regulates this process at the federal level. It requires the President to specifically identify the provisions activated and to renew the declaration annually so as to prevent an arbitrarily broad or open-ended emergency.
Presidents have occasionally taken action justified as necessary or prudent because of a state of emergency, only to have the action struck down in court as unconstitutional.

A state governor or local mayor may declare a state of emergency within his or her jurisdiction. This is common at the state level in response to natural disasters. The Federal Emergency Management Agency maintains a system of assets, personnel and training to respond to such incidents. For example, on December 10, 2015, Washington state Governor Jay Inslee declared a state of emergency due to flooding and landslides caused by heavy rains.

The 1977 International Emergency Economic Powers Act allows the government to freeze assets, limit trade and confiscate property in response to an "unusual and extraordinary threat" to the United States that originates substantially outside of it. As of 2015 more than twenty emergencies under the IEEPA remain active regarding various subjects, the oldest of which was declared in 1979 with regard to the government of Iran. Another ongoing national emergency, declared after the September 11 attacks, authorizes the president to retain or reactivate military personnel beyond their normal term of service.








</doc>
<doc id="30507231" url="https://en.wikipedia.org/wiki?curid=30507231" title="Governmental learning spiral">
Governmental learning spiral

The governmental learning spiral is a technique used to solve specific governance challenges. It is used during prearranged educational events such as conferences, e-learning, and trainings to improve performance in democratic governance. The governmental learning spiral—a heuristic and multidisciplinary tool—has been developed and implemented at international governmental learning events throughout the past decade.

The technique consists of a nine-stage learning process divided into three phases, which include the planning and aftermath as well as the learning event itself.

A major characteristic of this type of governmental learning event is facilitation by a "learning broker" who oversees all aspects of event organization. These include logistics, content preparation, drafting and implementation of the agenda, moderation of the learning sessions, and follow-up activities. The learning broker designs the learning process according to the specific governance challenge at hand.

The event must be structured based on several factors:

The participants invited to the event must represent different substantive and organizational perspectives and play a precisely defined role as both knowledge holders and knowledge seekers. When this is achieved, participants have unlimited access to the collective wealth of the shared tacit and explicit knowledge.

The effects of applying the governmental learning spiral technique are threefold: The primary effect is that governments gain access to the latest knowledge in democratic governance, which they can then apply to specific governance challenges with concrete, practical action. A second effect is that—because of the iterative character of the learning process—the knowledge being learned is always validated and updated in real time to include the latest experiences on the subject. A third effect is that participation in the learning process evokes a sense of social belonging among the learning actors, which often leads to the creation of networks and communities of practice where governments continue to share their latest experiences and by doing so launch the next spin of the Governmental Learning Spiral.

The governmental learning spiral technique consists of a nine-stage template. The stages of the learning process are organized chronologically in the template and split into three distinct sequences for a particular learning event.

1. Before (framing phase): The "conceptualization", "triangulation", and "accommodation" stages are the preparatory stages, where the specific governance challenge is defined, existing knowledge on the topic is framed, participants are selected and inivited, and trust is established between the learning actors and the event facilitator and between participants and the learning process itself.

2. During (reflection phase): The "internalization", "externalization", "reconceptualization", and "transformation" stages represent the core of the educational process, where learning actors review and adapt new knowledge according to their personal needs. Thereafter the actors change their individual and organizational thinking and behavior in an elaborate inter- and intrapersonal procedure.

3. After (projection phase): The follow-up to the learning activity occurs in the "configuration" stage, where all the knowledge acquired during the event is made available and accessible to everyone involved in the event as well as to a wider audience. This new knowledge further serves in the final "iteration" stage as a frame for the next spin of the governmental learning spiral, as well as a feedback loop in the context of a new learning system.

Because knowledge in governance has a short half-life and has to be updated constantly, the learning process itself must also be ongoing. This iterative procedure, where knowledge is constantly reviewed, renewed, and transformed into political action in a real-time, multi-turn process, can be illustrated as a spiral. Each of the nine stages of the learning process is bound together by a "spin," which ends with the last iteration stage and restarts the next spin with its first configuration stage.







</doc>
<doc id="14031" url="https://en.wikipedia.org/wiki?curid=14031" title="Hierarchical organization">
Hierarchical organization

A hierarchical organization is an organizational structure where every entity in the organization, except one, is subordinate to a single other entity. This arrangement is a form of a hierarchy. In an organization, the hierarchy usually consists of a singular/group of power at the top with subsequent levels of power beneath them. This is the dominant mode of organization among large organizations; most corporations, governments, and organized religions are hierarchical organizations with different levels of management, power or authority. For example, the broad, top-level overview of the general organization of the Catholic Church consists of the Pope, then the Cardinals, then the Archbishops, and so on. 

Members of hierarchical organizational structures chiefly communicate with their immediate superior and with their immediate subordinates. Structuring organizations in this way is useful partly because it can reduce the communication overhead by limiting information flow.

A hierarchy is typically visualized as a pyramid, where the height of the ranking or person depicts their power status and the width of that level represents how many people or business divisions are at that level relative to the whole—the highest-ranking people are at the apex, and there are very few of them; the base may include thousands of people who have no subordinates. These hierarchies are typically depicted with a tree or triangle diagram, creating an organizational chart or organigram. Those nearest the top have more power than those nearest the bottom, and there being fewer people at the top than at the bottom. As a result, superiors in a hierarchy generally have higher status and command greater rewards than their subordinates.

All governments and most companies have similar structures. Traditionally, the monarch was the pinnacle of the state. In many countries, feudalism and manorialism provided a formal social structure that established hierarchical links at every level of society, with the monarch at the top. 

In modern post-feudal states the nominal top of the hierarchy still remains the head of state, which may be a president or a constitutional monarch, although in many modern states the powers of the head of state are delegated among different bodies. Below the head, there is commonly a senate, parliament or congress, which in turn often delegate the day-to-day running of the country to a prime minister. In many democracies, the people are considered to be the notional top of the hierarchy, over the head of state; in reality, the people's power is restricted to voting in elections.

In business, the business owner traditionally occupied the pinnacle of the organization. In most modern large companies, there is now no longer a single dominant shareholder, and the collective power of the business owners is for most purposes delegated to a board of directors, which in turn delegates the day-to-day running of the company to a managing director or CEO. Again, although the shareholders of the company are the nominal top of the hierarchy, in reality many companies are run at least in part as personal fiefdoms by their management; corporate governance rules are an attempt to mitigate this tendency.

The organizational development theorist Elliott Jacques identified a special role for hierarchy in his concept of requisite organization. 
The iron law of oligarchy, introduced by Robert Michels, describes the inevitable tendency of hierarchical organizations to become oligarchic in their decision making.

The Peter Principle is a term coined by Laurence J. Peter in which the selection of a candidate for a position in an hierarchical organization is based on the candidate's performance in their current role, rather than on abilities relevant to the intended role. Thus, employees only stop being promoted once they can no longer perform effectively, and managers in an hierarchical organization "rise to the level of their incompetence." 

Hierarchiology is another term coined by Laurence J. Peter, described in his humorous book of the same name, to refer to the study of hierarchical organizations and the behavior of their members.

The IRG Solution – hierarchical incompetence and how to overcome it argued that hierarchies were inherently incompetent, and were only able to function due to large amounts of informal lateral communication fostered by private informal networks.

In the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work "Radical Empiricism" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved. A hesitation to declare success upon the discovery of ambiguities leaves heterarchy at an artificial and subjective disadvantage in the scope of human knowledge. This bias is an artifact of an aesthetic or pedagogical preference for hierarchy, and not necessarily an expression of objective observation.

Hierarchies and hierarchical thinking has been criticized by many people, including Susan McClary and one political philosophy which is vehemently opposed to hierarchical organization: anarchism. Heterarchy is the most commonly proposed alternative to hierarchy and this has been combined with responsible autonomy by Gerard Fairtlough in his work on Triarchy theory. The most beneficial aspect of a hierarchical organization is the clear command that is established. However, hierarchy may become dismantled by abuse of power.

Amidst constant innovation in information and communication technologies, hierarchical authority structures are giving way to greater decision-making latitude for individuals and more flexible definitions of job activities and this new style of work presents a challenge to existing organizational forms, with some research studies contrasting traditional organizational forms against groups that operate as online communities that are characterized by personal motivation and the satisfaction of making one's own decisions. With all levels of an organization having access to information and communication via digital means, power structures align more as a wirearchy, enabling the flow of power and authority to be based not on hierarchical levels, but on information, trust, credibility, and a focus on results.


</doc>
<doc id="32330178" url="https://en.wikipedia.org/wiki?curid=32330178" title="False necessity">
False necessity

False necessity, or anti-necessitarian social theory, is a contemporary social theory that argues for the plasticity of social organizations and their potential to be shaped in new ways. The theory rejects the assumption that laws of change govern the history of human societies and limit human freedom. It is a critique of "necessitarian" thought in conventional social theories (like liberalism or Marxism) which hold that parts of the social order are necessary or the result of the natural flow of history. The theory rejects the idea that human societies must be organized in a certain way (for example, liberal democracy) and that human activity will adhere to certain forms (for example if people were only motivated by rational self-interest).

"False necessity" uses structural analysis to understand socio-political arrangements, but discards the tendency to assemble indivisible categories and to create law-like explanations. It aims to liberate human activity from necessary arrangements and limitations, and to open up a world without constraints where the possible becomes actual.

Modern social theory contains a tension between the realization of human freedom and the necessity of social rules. Liberal political theorists of the seventeenth century, such as Hobbes and Locke, saw the issue as one of sacrificing some individual freedoms in order to gain others. They understood social rules as enabling constraints—necessary impositions that limited activity in some spheres in order to expand it in others. (For example, traffic laws compel us to drive on one side of the road but allow us to travel more freely than if we were constantly assailed by oncoming traffic.) In the socio-political realm, these early liberal thinkers argued that we agree to surrender our freedom for political authority in order to gain greater freedom from a state of nature. The sovereign authority is a constraint, but it allows freedom from the constraints that other individuals might impose upon us. In this way, rules are always seen as a means of increasing freedom rather than rescinding it.

These early Enlightenment thinkers opposed existing religious, aristocratic, and absolutist institutions and organizations as the natural state of the world. However, they did not argue for the absolute freedom of the individual outside of any constraining rules. For them, human activity was still subject to certain types of social arrangements that followed a historical necessity. 

Inspired by Kant's thesis of human freedom, which argued that there is no evidence to disprove our absolute freedom or capacity to resist external domination, thinkers at the end of the eighteenth century addressed how human freedoms were constrained by social institutions. Thinkers like Fichte, Schiller, Schelling, and Hegel argued that those institutions that constrain human freedom and subject the individual to fear and prejudice insult human dignity and deny the individual his autonomy. But they attempted to formulate universal laws, which in turn led to deterministic social and political arrangements. Marx, for example, put humanity at the mercy of historical and institutional necessity. 

The contemporary theory of false necessity attempts to realize this idea in its entirety, and to escape the limitations of liberal and Marxist theories. It aims to realize social plasticity by decoupling human freedom from any necessary social rules or historical trajectory. The theory recognizes the need for social rules, but also affirms the human potential to transcend them. Humanity need not be constrained by any structure.

The development of the theory is credited to philosopher and politician Roberto Mangabeira Unger. His main book on the thesis, "False Necessity: Anti-necessitarian social theory in the service of radical democracy", was first published in 1987 by Cambridge University Press, and reissued in 2004 by Verso with a new 124 page introduction, and a new appendix, "Five theses on the relation of religion to politics, illustrated by allusions to Brazilian experience."

The theory of "false necessity" attempts to understand humans and human history without making ourselves the object of a law-giving fate. It rejects the assumption that certain and necessary laws of organization and change govern the social, political, and economic institutions of human activity and thereby limit human freedom. It holds that the problem with traditional deep structure theory, such as Marxism, is that it couples the distinction of deep structure and routine practice with both indivisible types of social organization, and deep seated constraints and developmental laws. The theory rejects the constraints and focuses on how human behavior is shaped by the deep structures of these institutions, and how they can be remade at will, either in whole or in part. The aim is to rescue social theory and recreate the project of self-affirmation and society.

Rather than "enabling constraint" or "universal structure", the theory advocates "structure-denying structures"—that is, structures that enable their own dissolution and remaking. Since these structures normally constrain our activities, this would increase our freedom.

The problem of false necessity arises due to the failure of transformative practice to realize its stated aim. This can take form in three different scenarios:

Unger points to mass politics as a means to counter oligarchy and group identity. However, if these forms are only disturbed and not destroyed, democracy is limited and becomes a quarrel about forms of power and seizing advantage. Likewise, enlarged economic rationality provides another source of emancipation by shifting economic and social relations in the ability to constantly innovate and renew.

The theory of "false necessity" develops the idea that the organization of society is made and can be remade—we can rebel against the worlds we have built; we can interrupt our rebellions and establish ourselves in any of those worlds. By emphasizing the disembodiment of institutional and social structures, the theory provides a basis to explain ourselves and our world without using necessitarian thought or predetermined institutional arrangements.

At the extreme, the theory of "false necessity" criticizes and thus unifies strands of the radical tradition. It frees leftist and liberal ideals from institutional fetishism, and emancipates modernist ideals from structural fetishism. The theory further detaches the radical commitment from utopian claims and provides a theoretical basis for transformative action. That transformative action, Unger believes, does not have to be a complete overhaul or total revolution, but rather is "a piecemeal but cumulative change in the organization of society". The key to the project, in the words of one critic, "is to complete the rebellion against the naturalistic fallacy (that is, the confusion of accident with essence and contingency with necessity) and to effect an irrevocable emancipation from false necessity".

Contemporary political thinkers and philosophers have developed and advocated the theory of false necessity. Roberto Mangabeira Unger has employed the theory in developing social, political, and economic alternatives, as well as in his political activism and appointments in Brazilian politics. Richard Rorty compared the theory's move towards greater liberalism with Jürgen Habermas, and called it a powerful alternative to the postmodern "School of Resentment". Other thinkers have said the theory is "a challenge that the social disciplines can ignore only at their peril". Bernard Yack wrote that it contributed to "a new left Kantian approach to the problem of realizing human freedom in our social institutions".





</doc>
<doc id="31569815" url="https://en.wikipedia.org/wiki?curid=31569815" title="Fractionalism">
Fractionalism

Fractionalism is the government system that is the closest to a confederation but differs when the market system is a central market owned mostly by the government and very little by the people, yet the people have more control over it than the control of the government. This is possible by the fact that in fractionalism, the central government, on the federal level, is made up of several levels to ensure the utmost security for the people's government.

The whole nation is broken down into several small city-states. The national government cannot intervene in any acts that the city-state chooses to enforce unless it could weaken the unity of the country as a whole. Other than that, the national government cannot intervene unless there is an almost guaranteed economic collapse in the city state, a widespread famine or biological plague, or other state of emergencies declared by that city state. However, in the case of war, the national military can inhabit any city state without the need to ask permission of any form of state or local government.


</doc>
<doc id="4239005" url="https://en.wikipedia.org/wiki?curid=4239005" title="Formative context">
Formative context

Formative contexts are the institutional and imaginative arrangements that shape a society's conflicts and resolutions. They are the structures that limit both the practice and the imaginative possibilities in a socio-political order, and in doing so shape the routines of conflict over social, political and economic resources that govern access to labor, loyalty, and social station, e.g. government power, economic capital, technological expertise, etc. In a formative context, the institutions structure conflict over government power and capital allocation, whereas the imaginative framework shapes the preconceptions about possible forms of human interaction. Through this, a formative context further creates and sustains a set of roles and ranks, which mold conflict over the mastery of resources and the shaping of the ideas of social possibilities, identities and interests. The formative context of the Western democracies, for example, include the organization of production through managers and laborers, a set of laws administering capital, a state in relation to the citizen, and a social division of labor.

Also referred to as order, framework, or structure of social life, the concept of formative context was developed by philosopher and social theorist Roberto Unger. Whereas other social and political philosophers have taken the historical context as a given, and seen one existing set of institutional arrangements as necessarily giving birth to another set, Unger rejects this naturalization of the world and moves to explain how such contexts are made and reproduced. The most forceful articulation and development of the concept is in Unger's book "False Necessity." 

The thesis of formative context is central to Unger's theory of false necessity, which rejects the idea of a closed number of institutional arrangements of human societies, e.g. feudalism and capitalism, and that these arrangements are the product of historical necessity, as theories of liberalism or Marxism claim. Rather, Unger argues that there are myriad institutional arrangements that can coalesce, and that they do so through a contingent process of struggle, reconciliation, and innovation among individuals and groups. For Unger, the concept of formative context serves to explain the basis of a certain set of institutional arrangements and their reliance upon each other. It offers an explanation of the cycles of reform and retrenchment of a socio-economic political system and how it remains undisturbed by rivalries and animosities. The theory of false necessity goes on to explain the connections of a formative context, their making and remaking, and how they maintain stability despite the contingent formation.

While a formative context of a society exerts a major influence on the course of social actions and behaviors, it is itself hard to challenge, revise, or even identify in the midst of everyday conflicts and routines. Thus there are two criteria for determining if an institution or structure belongs in a formative context, one subjective and one objective. The subjective criteria considers the perspective of the social actors themselves and the arrangements that are assumed in their speech and actions. For example, the attempts of big business and labor to protect themselves through deals with each other, and the political efforts of unorganized labor and petty bourgeoisie to undermine and circumvent these deals by pressuring the government, operate on the same institutional assumption of the distinction between economy and polity, and that victory in one can be offset by the other. The objective criteria is simply that if a substitution of the proposed structure affects the hierarchies or cyclical conflicts—if it alters the social divisions—then it can be included in the formative context. For example, a change in any one of the following conditions would completely change the formative context of a Western democratic state: if the state stopped being democratic or was democratic enough to allow collective militancy and subject private centers of power to public accountability; if business could have its way and override all regulatory controls of govt; or if no workers could unionize or all of them could and did.

The formative context of the North Atlantic democracies can be organized into four clusters of institutional arrangements: work, law, government, and occupational structure. 

The thesis of formative contexts has been heavily drawn on and used within the Social Study of Information Systems. In the field of Information systems Claudio Ciborra and Giovan Lanzara define the term "formative context" as the "set of institutional arrangements and cognitive imageries that inform actors' practical and reasoning routines in organisations". They posit that the common inability to inquire into, challenge or shape formative context can inhibit individuals and organizations from acting competently and learning what they need to know in order to make the most of situations and technological transitions as the enchaining effect of Formative Context can lead to cognitive and social inertia.




</doc>
<doc id="37072168" url="https://en.wikipedia.org/wiki?curid=37072168" title="Government district">
Government district

A government district or government area is any part of a city or town in which the primary land use by state institutions (national legislature, official residencess, foreign ministry headquarters, and so on), as opposed to a residential neighbourhood, commercial district and industrial zone, or other types of neighborhoods. In some cities, authorities use planning or zoning laws to define the boundaries of government districts.

In most national and many sub-national (state, provincial, etc.) capital cities there are distinct government districts centred on the national legislature. However, non-capitals may also have an administrative district centered on city hall or a government office building.

In the national capital, Ottawa, the main government district, centered on Parliament Hill is within Downtown Ottawa (part of Centretown), but could be extended to include small parts of Lower Town and Sandy Hill if the prime minister's and government general's residences are included, as well as the district of Hull, Quebec in the neighboring city of Gatineau where several ministries are headquartered. These areas are all linked by the ceremonial route known as "Confederation Boulevard".

In Quebec City, the provincial capital of Quebec, the "Complexe du Parlement" (including the Parliament Building) and several other government buildings, are located within the neighborhood of Parliament Hill. This is within the districts of Vieux-Québec—Cap-Blanc—colline Parlementaire and Saint-Jean-Baptiste, which are themselves in the borough of La Cité-Limoilou.

Toronto, Ontario's government district is centered on Queen's Park home of the provincial parliament.

Edmonton Alberta's main government district, is centered on Capital Boulevard (107 Street south of Jasper Avenue) which is home to the Alberta Legislature Building and the Federal Public Building. However Edmonton City Hall and Canada Place anchor another government cluster to the east.




Neighborhoods by type

</doc>
<doc id="10412265" url="https://en.wikipedia.org/wiki?curid=10412265" title="Policy studies">
Policy studies

Policy studies is a subdisicipline of political science that includes the analysis of the process of policymaking (the policy process) and the contents of policy (policy analysis). Policy analysis includes substantive area research (such as health or education policy), program evaluation and impact studies, and policy design. It "involves systematically studying the nature, causes, and effects of alternative public policies, with particular emphasis on determining the policies that will achieve given goals." It emerged in the United States in the 1960s and 1970s.

Policy Studies also examines the conflicts and conflict resolution that arise from the making of policies in civil society, the private sector, or more commonly, in the public sector (e.g. government).

It frequently focuses on the public sector but is equally applicable to other kinds of organizations (e.g., the not-for-profit sector). Some policy study experts graduate from public policy schools with public policy degrees. Alternatively, experts may have backgrounds in policy analysis, program evaluation, sociology, psychology, philosophy, economics, anthropology, geography, law, political science, social work, environmental planning and public administration.

Traditionally, the field of policy studies focused on domestic policy, with the notable exceptions of foreign and defense policies. However, the wave of economic globalization, which ensued in the late 20th and early 21st centuries, created a need for a subset of policy studies that focuses on global governance, especially as it relates to issues that transcend national borders such as climate change, terrorism, nuclear proliferation, and economic development. This subset of policy studies, which is often referred to as international policy studies, typically requires mastery of a second language and attention to cross-cultural issues in order to address national and cultural biases. For example, the Monterey Institute of International Studies at Middlebury College offers Master of Arts programs that focus exclusively on international policy through a mix of interdisciplinary and cross-cultural analysis called the "Monterey Way". Examples of academic programs in policy studies include the Harvard Kennedy School and the LBJ School of Public Affairs.






</doc>
<doc id="38430757" url="https://en.wikipedia.org/wiki?curid=38430757" title="Intelligent Governance for the 21st Century">
Intelligent Governance for the 21st Century

Intelligent Governance for the 21st Century: A Middle Way Between West and East is a book published in 2012 by the investor and philanthropist Nicolas Berggruen and the editor and writer Nathan Gardels.

It argues that Western democracies have become stymied by populism and short-term thinking, while authoritarian Eastern nations, notably China, need to bolster their meritocratic but authoritarian systems with the popular legitimacy characteristic of Western polities.

The book was first published in English and later translated into Spanish, Portuguese and other languages.


The book's conclusion, "Survival of the Wisest", argues that wisdom and long-term thinking combined to democratic legitimacy is the right political combination for success in a globalized world.




</doc>
<doc id="38566163" url="https://en.wikipedia.org/wiki?curid=38566163" title="Bureaucratic inertia">
Bureaucratic inertia

Bureaucratic inertia is the inevitable tendency of bureaucratic organizations to perpetuate the established procedures and modes, even if they are counterproductive and/or diametrically opposed to established organizational goals. This unchecked growth may continue independently of the organization's success or failure. Through bureaucratic inertia, organizations tend to take on a life of their own beyond their formal objectives.

The United States Department of Agriculture has offices in almost all U.S. counties, even though only 14% of counties have valid farms or existing agricultural relevancy.

The Boston Consulting Group has advised firms to cut down on bureaucratic inertia and advised firms to eliminate cruft, bloat, and redundancy in the aspects of the business which are not front-line for the consumer (i.e. the "face" of the company who the customer deals with and who the customer thinks is the value-provider of the company).


</doc>
<doc id="38686771" url="https://en.wikipedia.org/wiki?curid=38686771" title="Constitutional body">
Constitutional body

A Constitutional Bodies of India is created by passing a constitutional amendment bill, rather than by a regular, government or private bill.

Constitutional bodies in India are bodies or institutes that have mentioned in Indian constitution. It derives power directly from the constitution. Any type of change in mechanism of these bodies needs constitutional amendment.
Article related to the Constitutional Bodies- 


</doc>
<doc id="7198484" url="https://en.wikipedia.org/wiki?curid=7198484" title="Internal security">
Internal security

Internal security, or IS, is the act of keeping peace within the borders of a sovereign state or other self-governing territories, generally by upholding the national law and defending against internal security threats. Responsibility for internal security may range from police to paramilitary forces, and in exceptional circumstances, the military itself.

Threats to the general peace may range from low-level civil disorder,large scale violence, or even an armed insurgency. Threats to internal security may be directed at either the state's citizens, or the organs and infrastructure of the state itself, and may range from petty crime, serious organised crime, political or industrial unrest, or even domestic terrorism. Foreign powers may also act as a threat to internal security, by either committing or sponsoring terrorism or rebellion, without actually declaring war.

Governmental responsibility for internal security will generally rest with an interior ministry, as opposed to a defence ministry. Depending on the state, a state's internal security will be maintained by either the ordinary police or law enforcement agencies or more militarised police forces (known as Gendarmerie or, literally, the Internal Troops.). Other specialised internal security agencies may exist to augment these main forces, such as border guards, special police units, or aspects of the state's intelligence agencies. In some states, internal security may be the primary responsibility of a secret police force.

The level of authorised force used by agencies and forces responsible for maintaining internal security might range from unarmed police to fully armed paramilitary organisations, or employ some level of less-lethal weaponry in between. For violent situations, internal security forces may contain some element of military type equipment such as non-military armored vehicles.

Depending on the organisation of the state, internal security forces may have jurisdiction on national or federal levels. As the concept of internal security refers to the entity of the state and its citizens, persons who are threats to internal security may be designated as an enemy of the state or enemy of the people.

Persons detained by internal security forces may either be dealt with by the normal criminal justice system, or for more serious crimes against internal security such as treason, they may face special measures such as secret trials. In times of extreme unrest, internal security actions may include measures such as internment (detention without trial).

Depending on the nature of the specific state's form of government, enforcing internal security will generally not be carried out by a country's military forces, whose primary role is external defence, except in times of extreme unrest or other state of emergency, short of civil war. Often, military involvement in internal security is explicitly prohibited, or is restricted to authorised military aid to the civil power as part of the principle of civilian control of the military. Military special forces units may in some cases be put under the temporary command of civilian powers, for special internal security situations such as counter terrorism operations.



</doc>
<doc id="42299690" url="https://en.wikipedia.org/wiki?curid=42299690" title="Center of government">
Center of government

The center of government (CoG) is the institution or group of institutions that provide direct support to the chief executive (president or prime minister) in leading the management of government. Unlike line ministries and other government agencies, the CoG does not deliver services directly to the citizens, and it does not focus on a specific policy area. On the contrary, the CoG performs cross-government functions. A similar concept is "Core Executive".

Two types of CoG definition exist: by structure or by function.

In the first type, the defining criterion is the position in the structure of the Executive branch. It only includes institutions and units that directly and exclusively support the head of the government. For example, it refers to Ministries or General-Secretariats of the Presidency, Offices of the President or Prime Minister, and Cabinet Offices.

In the functional definitions, the defining criterion is that the institution performs whole-of-government functions, especially in terms of planning, coordination, monitoring, political management, and communications. Therefore, in addition to the previously mentioned institutions (such as Ministries of the Presidency), other institutions performing these tasks are also included, even if they are not part of the structure of the Presidency. For example, Ministries or Offices of Planning, , Inter-Ministerial Committees, and even Budget Offices are considered part of the CoG.

A number of authors favor functional definitions, for considering them more applicable to countries with different forms of government and institutional frameworks.

Chief Executives (Presidents and Prime Ministers) have had supporting institutions for a long time. In the United Kingdom, the dates to 1916, when the war demanded a stronger central coordination for the government. In the United States, the Executive Office of the President was established in 1939, after the stated that "the President needs help." Offices, Ministries or General Secretariats of the Presidency have also existed in Latin American for several decades now.

However, a number of factors lead to a greater importance of the CoG. The following factors have been highlighted: the cross-cutting nature of several current public problems, the need to lead with a unified orientation governments that have decentralized authority to autonomous agencies, and the growing interesting for achieving results for the citizens, beyond a purely fiscal coordination. These factors explain the interest that several governments and international organizations have on the topic, including projects by the Organisation for Cooperation and Economic Development and the Inter-American Development Bank.

Moreover, new institutional developments have generated increased interest in the CoG. The in the United Kingdom was established to ensure that the government's priorities were being implemented, through an intensive monitoring by the CoG. Several countries have replicated this model, including Malaysia and Chile.
Paraguay has established a "Center of Government" within the General-Secretariat of the Presidency, in charge of coordinating and monitoring the government action.

In OECD countries, the CoG institutions lead the functions of coordinating the preparation of Cabinet meetings, coordinating the formulation and implementation of policy, monitoring delivery, strategic planning for the whole-of-government, and communicating the government's messages. Regarding the characteristics of the CoG, there are important variations in terms of organization and staff. For example, in countries like the United Kingdom and Sweden most of the CoG staff belongs to the civil service, while in Canada, Australia and New Zealand political appointees have a larger presence. Important variations have also been identified in Latin America, not only in terms of organization but also in the capacity of CoGs to perform their functions: while some countries have more developed CoGs, others present more serious weaknesses. This heterogeneity across countries has also been detected in the Middle East and North Africa region.



</doc>
<doc id="28696566" url="https://en.wikipedia.org/wiki?curid=28696566" title="Citizen oversight">
Citizen oversight

Citizen oversight is the act of an assembly of citizens, a form of citizen participation, who review government activities. Activities may be deemed as government misconduct. Members of the group are civilians and are external to the government entity. These groups are tasked with direct involvement in the citizen complaints process and develop solutions to improve government accountability. Responsibilities of citizen oversight groups can vary significantly depending on the jurisdiction and their ability to become influential. Oversight should not criticize but improve government through citizen support for government responsiveness, accountability, transparency, and overall efficiency.

Proactive citizen oversight improves transparency and demands accountability at all levels of government. Reporting and monitoring (financial records, performance measures, and open records... etc.) are now regarded as fundamental governance responsibilities. Citizen Advisory Boards are a way for citizens to be involved in government oversight. Other forms of government oversight include citizen committees, citizen panels, citizen juries, citizen initiatives, negotiated rulemaking, and mediation Citizen oversight shares similar aspects with Demarchy and the Jury system.

An effective citizen oversight committee is structured to take on the following responsibilities: create processes for risk governance, monitoring and reporting; create clear defined duties to improve effectiveness and avoid overlapping work; recruit/retain members that are knowledgeable and engaged about policy; develop critiques that result in improved service outcomes; assign oversight responsibilities to designated individuals or groups for specific government functions; and reviews rolls regularly.

Citizen oversight committees brainstorm ideas to improve transparency and create policy proposals. Most proposals regarding citizen oversight have been with respects to police activities, healthcare, non-profit and private sector. Proposals since the 1970s about police misconduct or government corruption have universally been met with resistance from authorities and did not gained much traction.

Citizen oversight is the result of a profound change in public attitudes toward government particularly related to trust. There is a lack of trust between citizens and government/business because of historical misconduct. Misconduct included racial discrimination during the civil rights era, illegal activities during the Watergate scandal, and more recently citizen disagreement with government bailouts and financial fraud like Enron scandal. All these actions have caused an increased demand in accountability. Trust is a measured by gauging how effective citizens feel local policies and authorities are in their duties as official. A series of laws have been created indicating the growing public concern about the need for oversight of government agencies.


Benefit
Increased focus on monitoring, reporting, strategic advising, value creation, accountability, and the creation of professional standards.

Weakness or setbacks
Accountability, transparency, and reporting are important to citizen oversight. Acts like Patient Protection and Affordable Care Act have caused an increase in oversight responsibilities requiring increased reporting, extensive examination of performance, and increased accountability of internal citizen oversight. Oversight can be excessive and ultimately detrimental to desirable outcomes, and administrators spend a significant amount of time on monitoring and less on strategies. Difficulty forming citizen groups, failing to function effectively, agency role is not visible enough or influential, group is abolished altogether.

Citizen participation and accountability initiatives have become a common practice in democratic nations. Reporting and monitoring results are now regarded as fundamental governance responsibilities The growth of citizen oversight is not confined to the United States. Citizen oversight (particularly for the police) is universal and has expanded across the English-speaking world and is spreading in Latin America, Asia, and continental Europe 
International Asian countries do not look at service-oriented policing like western countries. Asian democracies focus on defense and maintenance of established rules, reviewing and monitoring government actions and policing human rights violations, police corruption,and corporate management. Research in the United Kingdom has noted the importance of oversight of state functions such as prisons to ensure the fair and humane detention of vulnerable persons such as prisoners.

Hong Kong's citizen oversight is considered to be far more transparent, independent, sufficient at holding government accountable. Possibly a result of being largely more democratic, than countries like China. Nearly, all Asian democracies have some form of oversight, but only 3 have citizen oversight.

The assembly of citizens to review government activities and misconduct first started with civilian oversight of police in the 1920s. The table below is predominantly related to police oversight between 1920 and 1980. By 1980 there were about 13 agencies, and by 2000 more than 100 such as the Independent Police Auditor (IPA) in San Jose, California and Seattle, Washington and the Office of Independent Review (OIR) in New York City, New York.



</doc>
<doc id="1939759" url="https://en.wikipedia.org/wiki?curid=1939759" title="Civil authority">
Civil authority

Civil authority or civilian authority, also known as civilian government, is the practical implementation of a State, other than its military units, that enforces law and order. It is also used to distinguish between religious authority (for example Canon law) and secular authority. The enforcement of law and order is typically the role of the police in modern states.

Among the first modern experiments in civil government took place in 1636 when Roger Williams, a Christian minister, founded the colony of Rhode Island and Providence Plantations. He sought to create a "wall of separation" between church and state to prevent corruption of the church and maintain civil order as expounded upon in his 1644 book, "Bloudy Tenent of Persecution".

Thus three forms of authority may be seen:

It can also mean the moral power of command, supported (when need be) by physical coercion, which the State exercises over its members. In this view, because man can not live in isolation without being deprived of what makes him human, and because authority is necessary for a society to hold together, the authority has not only the power but the right to command. It is natural to man to live in society, to submit to authority, and to be governed by that custom of society which crystallizes into law, and the obedience that is required is paid to the powers that be, to the authority in possession. The extent of its authority is bound by the ends it has in view, and the extent to which it provides for the government of society.

In modern states enforcement of law and order is typically the role of the police although the line between military and civil units may be hard to distinguish; especially when militias and volunteers, such as yeomanry, act in pursuance of non-military, domestic objectives.




</doc>
<doc id="99221" url="https://en.wikipedia.org/wiki?curid=99221" title="World government">
World government

World government or global government or is the notion of a common political authority for all of humanity, giving way to a global government and a single state that exercises authority over the entire world. Such a government could come into existence either through violent and compulsory world domination or through peaceful and voluntary supranational union.

There has never been a worldwide executive, legislature, judiciary, military, or constitution with global jurisdiction. The United Nations, beyond the United Nations Security Council (which has the ability to issue mandatory resolutions), is limited to a mostly advisory role, and its stated purpose is to foster cooperation between existing national governments rather than exert authority over them.

The idea and aspiration of world government has been known since the dawn of history. Bronze Age Egyptian Kings aimed to rule "All That the Sun Encircles", Mesopotamian Kings "All from the Sunrise to the Sunset", and ancient Chinese and Japanese Emperors "All under Heaven". These four civilizations developed impressive cultures of Great Unity, or Da Yitong as the Chinese put it. In 113 BC, the Han dynasty in China erected an Altar of the Great Unity.

Polybius said that the Roman achievement of imposing one government over the Mediterranean world was a "marvelous" achievement, and that the main task of future historians will be to explain how this was done.

The idea of world government outlived the fall of the Pax Romana for a millennium. Dante in the fourteenth century despairingly appealed to the human race: "But what has been the condition of the world since that day the seamless robe [of Pax Romana] first suffered mutilation by the claws of avarice, we can read—would that we could not also see! O human race! what tempests must need toss thee, what treasure be thrown into the sea, what shipwrecks must be endured, so long as thou, like a beast of many heads, strivest after diverse ends! Thou art sick in either intellect, and sick likewise in thy affection. Thou healest not thy high understanding by argument irrefutable, nor thy lower by the countenance of experience. Nor dost thou heal thy affection by the sweetness of divine persuasion, when the voice of the Holy Spirit breathes upon thee, "Behold, how good and how pleasant it is for brethren to dwell together in unity!"" ("De Monarchia", 16:1)

Early father of international law, Spanish philosopher Francisco de Vitoria (c. 1483–1546) is considered the "founder of global political philosophy". De Vitoria conceived of the "res publica totius orbis", or the "republic of the whole world". This came at a time when the University of Salamanca was engaged in unprecedented thought concerning human rights, international law, and early economics based on the experiences of the Spanish Empire.

"De jure belli ac pacis" ("On the Law of War and Peace") is a 1625 book in Latin, written by Hugo Grotius (1583–1645) and published in Paris, on the legal status of war. It is now regarded as a foundational work in international law. Grotius was a philosopher, theologian, playwright, and poet. He is known for coming up with the idea of having an international law, and is still acknowledged today by the American Society of International Law.

Immanuel Kant wrote the essay "" (1795). In his essay, Kant describes three basic requirements for organizing human affairs to permanently abolish the threat of present and future war, and, thereby, help establish a new era of lasting peace throughout the world. Specifically, Kant described his proposed peace program as containing two steps.

The "Preliminary Articles" described the steps that should be taken immediately, or with all deliberate speed:


Three Definitive Articles would provide not merely a cessation of hostilities, but a foundation on which to build a peace.


The year of the battle at Jena (1806), when Napoleon overwhelmed Prussia, Fichte in "Characteristics of the Present Age" described what he perceived to be a very deep and dominant historical trend:

In early-19th-century Mormon theology, Joseph Smith taught that a theodemocracy would guide and direct the Kingdom of God (Zion) on the earth during the end times. On March 11, 1844, Smith organized a Council of Fifty, who were to work under the direction of the Priesthood authorities of his church, along with a Council of Friends. This group of three organizations was expected to rule as a world government just prior to the Millennium.

In 1842, the English poet Alfred, Lord Tennyson, published the oft-quoted lines "Locksley Hall": "For I dipt into the future, far as human eye could see / Saw a Vision of the world, and all the wonder that would be /... / Till the war-drum throbb'd no longer / and the battle-flags were furled / In the Parliament of man, the Federation of the world. / There the common sense of most shall hold / a fretful realm in awe / And the kindly earth shall slumber / lapt in universal law".

President Ulysses S. Grant was convinced in 1873: "Transport, education and rapid development of both spiritual and material relationships by means of steam power and the telegraph, all this will make great changes. I am convinced that the Great Framer of the World will so develop it that it becomes one nation, so that armies and navies are no longer necessary."

He also commented, "I believe at some future day, the nations of the earth will agree on some sort of congress which will take cognizance of international questions of difficulty and whose decisions will be as binding as the decisions of the Supreme Court are upon us".

The first thinker to anticipate a kind of world unity ("great household of the world") under the American primacy seems to be British politician William Gladstone. In 1878, he wrote:

In 1885, Kang Youwei published his "One World Philosophy", where he based his vision on the evidence of political expansion which began in the immemorial past and went in his days on. He concludes:

No factor, he believed, in the long run could resist the "laws of empires". Kang Youwei projects the culmination of the ongoing world unification with the final confrontation between the United States and Germany: "Some day America will take in [all the states of] the American continent and Germany will take in all the [states of] Europe. This will hasten the world along the road to One World."

Friedrich Nietzsche in his "Beyond Good and Evil" (1886) envisaged: 
The French demographer, George Vacher de Lapouge, followed K'ang Yu-wei in 1899 with his "L'Aryen: Son Role Social". Similarly, he outlined the logistic growth of empires from the Bronze Age till his days, when "six states govern... three quarters of the globe", and concluded: "The moment is close when the struggle for the domination of the world is going to take place."

Vacher de Lapouge did not bet on Washington and Berlin in the final contest for world domination contrary to K'ang Yu-wei. Like his earlier compatriot, Alexis de Tocqueville, he guessed the Cold War contenders correctly but he went one step further. He estimated the chances of the United States as favorite in the final confrontation:

In the second half of the 19th century, Bahá'u'lláh founded the Bahá'í Faith, a religion which identified the establishment of world unity and a global federation of nations as a key principle. He envisioned a set of new social structures based on participation and consultation among the world's peoples, including a world legislature, an international court, and an international executive empowered to carry out the decisions of these legislative and judicial bodies. Connected principles of the Bahá'í religion include universal systems of weights and measures, currency unification, and the adoption of a global auxiliary language.

In "World Order of Bahá'u'lláh," first published in 1938, Shoghi Effendi, great-grandson of Bahá'u'lláh and the Guardian of the Bahá'í Faith from 1921 until his death in 1957, described the anticipated world government of that religion as the "world's future super-state" with the Bahá'í Faith as the "State Religion of an independent and Sovereign Power".

According to Shoghi Effendi, "The unity of the human race, as envisaged by Bahá'u'lláh, implies the establishment of a world commonwealth in which all nations, races, creeds and classes are closely and permanently united, and in which the autonomy of its state members and the personal freedom and initiative of the individuals that compose them are definitely and completely safeguarded. This commonwealth must, as far as we can visualize it, consist of a world legislature, whose members will, as the trustees of the whole of mankind, ultimately control the entire resources of all the component nations, and will enact such laws as shall be required to regulate the life, satisfy the needs and adjust the relationships of all races and peoples. A world executive, backed by an international Force, will carry out the decisions arrived at, and apply the laws enacted by, this world legislature, and will safeguard the organic unity of the whole commonwealth. A world tribunal will adjudicate and deliver its compulsory and final verdict in all and any disputes that may arise between the various elements constituting this universal system."

In his many scriptures and messages addressed to the most prominent state leaders of his time, Bahá'u'lláh called for world reconciliation, reunification, collective security and the peaceful settlement of disputes. Many of the most fundamental Bahá'í writings address the central issue of world unity, such as the following: "The earth is but one country and mankind its citizens". The World Christian Encyclopedia estimated 7.1 million Bahá'ís in the world in 2000, representing 218 countries.

Starting in 1843, International Peace Congresses were held in Europe every two years, but lost their momentum after 1853 due to the renewed outbreak of wars in Europe (Crimea) and North America (American Civil War).

International organizations started forming in the late 19th century – the International Committee of the Red Cross in 1863, the Telegraphic Union in 1865 and the Universal Postal Union in 1874. The increase in international trade at the turn of the 20th century accelerated the formation of international organizations, and, by the start of World War I in 1914, there were approximately 450 of them. Support for the idea of establishing international law grew during that period as well. The Institute of International Law was formed in 1873 by the Belgian Jurist Gustave Rolin-Jaequemyns, leading to the creation of concrete legal drafts, for example by the Swiss Johaan Bluntschli in 1866. In 1883, James Lorimer published "The Institutes of the Law of Nations" in which he explored the idea of a world government establishing the global rule of law. The first embryonic world parliament, called the Inter-Parliamentary Union, was organized in 1886 by Cremer and Passy, composed of legislators from many countries. In 1904 the Union formally proposed "an international congress which should meet periodically to discuss international questions".

H. G. Wells was a strong proponent of the creation of a world state, arguing that such a state would ensure world peace and justice. In "Anticipations" (1900), H. G. Wells envisaged that "the great urban region between Chicago and the Atlantic" will unify the English-speaking states, and this larger English-speaking unit, "a New Republic dominating the world", will by the year 2000 become the means "by which the final peace of the world may be assured forever". It will be "a new social Hercules that will strangle the serpents of war and national animosity in his cradle". Such a synthesis "of the peoples now using the English tongue, I regard not only as possible, but as a probable, thing". The New Republic "will already be consciously and pretty freely controlling the general affairs of humanity before this century closes..." Its principles and opinions "must necessarily shape and determine that still ampler future of which the coming hundred years is but the opening phase". The New Republic must ultimately become a "World-State".

The League of Nations (LoN) was an inter-governmental organization founded as a result of the Treaty of Versailles in 1919–1920. At its largest size from 28 September 1934 to 23 February 1935, it had 58 members. The League's goals included upholding the Rights of Man, such as the rights of non-whites, women, and soldiers; disarmament, preventing war through collective security, settling disputes between countries through negotiation, diplomacy, and improving global quality of life. The diplomatic philosophy behind the League represented a fundamental shift in thought from the preceding hundred years. The League lacked its own armed force and so depended on the Great Powers to enforce its resolutions and economic sanctions and provide an army, when needed. However, these powers proved reluctant to do so. Lacking many of the key elements necessary to maintain world peace, the League failed to prevent World War II. Hitler withdrew Germany from the League of Nations once he planned to take over Europe. The rest of the Axis powers soon followed him. Having failed its primary goal, the League of Nations fell apart. The League of Nations consisted of the Assembly, the Council, and the Permanent Secretariat. Below these were many agencies. The Assembly was where delegates from all member states conferred. Each country was allowed three representatives and one vote.

According to Karl Marx's theory of historical materialism, the capitalist epoch depends on the expansion of competing geopolitical markets across the planet, atomizing the global proletariat and thus sustaining economic disparity and rivalry between markets. Eventually, this will be succeeded by a Socialist epoch in which the working class throughout the world will unite to render national distinctiveness meaningless.

Although world Communism's long-term goal is a worldwide Communist society that is stateless, which would entail an absence of any government, many anti-Communists (especially during the Cold War) have considered it naive to think that the world revolution advocated by international Communists would lead to world domination by a single government or an alliance of several, yielding a de facto world government of a totalitarian nature.

The heyday of international Communism was the period from the end of World War I (the revolutions of 1917–23) through the 1950s, before the Sino-Soviet split.

Anticipating environmental movements for world unity, like Global Scenario Group, and such concepts as the Planetary phase of civilization and Spaceship Earth, British Geographer Sir Halford Mackinder wrote in 1931:
Curtis was a British official and author. He originally advocated British Empire Federalism and, later in life, a world state. He fought in the Second Boer War with the City Imperial Volunteers and served as secretary to Lord Milner (a position that had also been held by adventure-novelist John Buchan), during which time he dedicated himself to working for a united self-governing South Africa. His experience of the World War One and the rise of Hitler led him to conceptualize his version of a Federal World Government, which became his life work.

The ruling Nazi Party of 1933–1945 Germany envisaged the ultimate establishment of a world government under the complete hegemony of the Third Reich. In its move to overthrow the post-World War I Treaty of Versailles Germany had already withdrawn itself from the League of Nations, and it did not intend to join a similar internationalist organization ever again. In his desire and stated political aim of expanding the living space ("Lebensraum") of the German people by destroying or driving out "lesser-deserving races" in and from other territories dictator Adolf Hitler may have devised an ideological system of self-perpetuating expansionism, in which the expansion of a state's population would require the conquest of more territory which would, in turn, lead to a further growth in population which would then require even more conquests. In 1927, Rudolf Hess relayed to Walter Hewel Hitler's belief that world peace could only be acquired "when one power, the racially best one, has attained uncontested supremacy". When this control would be achieved, this power could then set up for itself a world police and assure itself "the necessary living space... The lower races will have to restrict themselves accordingly".

During its imperial period (1868–1947), the Japanese elaborated a worldview, "Hakkō ichiu", translated as "eight corners of the world under one roof". This was the idea behind the attempt to establish a Greater East Asia Co-Prosperity Sphere and behind the struggle for world domination.

The Atlantic Charter was a published statement agreed between the United Kingdom and the United States. It was intended as the blueprint for the postwar world after World War II, and turned out to be the foundation for many of the international agreements that currently shape the world. The General Agreement on Tariffs and Trade (GATT), the post-war independence of British and French possessions, and much more are derived from the Atlantic Charter. The Atlantic charter was made to show the goals of the allied powers during World War II. It first started with the United States and Great Britain, and later all the allies would follow the charter. Some goals include access to raw materials, reduction of trade restrictions, and freedom from fear and wants. The name, The Atlantic Charter, came from a newspaper that coined the title. However, Winston Churchill would use it, and from then on the Atlantic Charter was the official name. In retaliation, the Axis powers would raise their morale and try to work their way into Great Britain. The Atlantic Charter was a stepping stone into the creation of the United Nations.
U.S. President Harry S. Truman commented: "We must make the United Nations continue to work, and to be a going concern, to see that difficulties between nations may be settled just as we settle difficulties between States here in the United States. When Kansas and Colorado fall out over the waters in the Arkansas River, they don't go to war over it; they go to the Supreme Court of the United States, and the matter is settled in a just and honorable way. There is not a difficulty in the whole world that cannot be settled in exactly the same way in a world court". -- President Truman's remarks in Omaha, Nebraska on June 5, 1948, at the dedication of the War Memorial. The cultural moment of the late 1940s was the peak of World Federalism among Americans.

The years between the conclusion of World War II and 1950, when the Korean War started and the Cold War mindset became dominant in international politics, were the "golden age" of the world federalist movement. Wendell Willkie's book "One World", first published in 1943, sold over 2 million copies. In another, Emery Reves' book "The Anatomy of Peace" (1945) laid out the arguments for replacing the UN with a federal world government and quickly became the "bible" of world federalists. The grassroots world federalist movement in the US, led by people such as Grenville Clark, Norman Cousins, Alan Cranston and Robert Hutchins, organized itself into increasingly larger structures, finally forming, in 1947, the United World Federalists (later renamed to World Federalist Association, then Citizens for Global Solutions), claiming membership of 47,000 in 1949.

Similar movements concurrently formed in many other countries, leading to the formation, at a 1947 meeting in Montreux, Switzerland, of a global coalition, now called World Federalist Movement. By 1950, the movement claimed 56 member groups in 22 countries, with some 156,000 members.

World War II (1939–1945) resulted in an unprecedented scale of destruction of lives (over 60 million dead, most of them civilians), and the use of weapons of mass destruction. Some of the acts committed against civilians during the war were on such a massive scale of savagery, they came to be widely considered as crimes against humanity itself. As the war's conclusion drew near, many shocked voices called for the establishment of institutions able to permanently prevent deadly international conflicts. This led to the founding of the United Nations in 1945, which adopted the Universal Declaration of Human Rights in 1948. Many, however, felt that the UN, essentially a forum for discussion and coordination between sovereign governments, was insufficiently empowered for the task. A number of prominent persons, such as Albert Einstein, Winston Churchill, Bertrand Russell and Mohandas K. Gandhi, called on governments to proceed further by taking gradual steps towards forming an effectual federal world government. The United Nations main goal is to work on international law, international security, economic development, human rights, social progress, and eventually world peace. The United Nations replaced the League of Nations in 1945, after World War II. Almost every internationally recognized country is in the U.N.; as it contains 193 member states out of the 196 total nations of the world. The United Nations gather regularly in order to solve big problems throughout the world. There are six official languages: Arabic, Chinese, English, French, Russian and Spanish. The United Nations is also financed by some of the wealthiest nations. The flag shows the Earth from a map that shows all of the populated continents.

A United Nations Parliamentary Assembly (UNPA) is a proposed addition to the United Nations System that would allow for participation of member nations' legislators and, eventually, direct election of United Nations (UN) parliament members by citizens worldwide. The idea of a world parliament was raised at the founding of the League of Nations in the 1920s and again following the end of World War II in 1945, but remained dormant throughout the Cold War. In the 1990s and 2000s, the rise of global trade and the power of world organizations that govern it led to calls for a parliamentary assembly to scrutinize their activity. The Campaign for a United Nations Parliamentary Assembly was formed in 2007 by Democracy Without Borders to coordinate pro-UNPA efforts, which as of January 2019 has received the support of over 1,500 Members of Parliament from over 100 countries worldwide, in addition to numerous non-governmental organizations, Nobel and Right Livelihood laureates and heads or former heads of state or government and foreign ministers.

In France, 1948, Garry Davis began an unauthorized speech calling for a world government from the balcony of the UN General Assembly, until he was dragged away by the guards. Davis renounced his American citizenship and started a Registry of World Citizens. On September 4, 1953, Davis announced from the city hall of Ellsworth, Maine the formation of the "World Government of World Citizens" based on 3 "World Laws"—One God (or Absolute Value), One World, and One Humanity. Following this declaration, mandated, he claimed, by Article twenty one, Section three of the Universal Declaration of Human Rights, he formed the United World Service Authority in New York City as the administrative agency of the new government. Its first task was to design and begin selling "World Passports", which the organisation argues is legitimatised by on Article 13, Section 2 of the UDHR.

The World Passport is a 45-page document sold by the World Service Authority, a non-profit organization, citing Article 13, Section 2, of the Universal Declaration of Human Rights. World Passports have allegedly been accepted sporadically by some 174 countries, but no immigration authority has a de facto or de jure policy of acceptance with regards to the document. The latest edition of the World Passport, which has been on sale since January 2007, is an MRD (machine readable document) with an alphanumeric code bar enabling computer input plus an embedded "ghost" photo for security, printing overcovered with a plastic film. The document is in 7 languages: English, French, Spanish, Russian, Arabic, Simplified Chinese and Esperanto. Two covers are available: "World Passport", and "World Government Passport" (for registered World Citizens), ("passport" is in 7 languages on both covers). Other documents sold by the WSA include a World Birth Certificate, a World Political Asylum Card, a World Marriage Certificate, and a World Identity Card. Each page within the document is numbered and each page has the World Citizen logo in the background. There are two pages for affiliation with companies, organizations, and firms. There are nineteen visa pages in the document. On the back cover there are spaces for personal information such as a person's home address.

Legal anthropologist E. Adamson Hoebel concluded his treatise on broadening the legal realist tradition to include non-Western nations: "Whatever the idealist may desire, force and the threat of force are the ultimate power in the determination of international behavior, as in the law within the nation or tribe. But until force and the threat of force in international relations are brought under social control by the world community, by and for the world society, they remain the instruments of social anarchy and not the sanctions of world law. The creation in clear-cut terms of the corpus of world law cries for the doing. If world law, however, is to be realized at all, there will have to be minimum of general agreement as to the nature of the physical and ideational world and the relation of men in society to it. An important and valuable next step will be found in deep-cutting analysis of the major law systems of the contemporary world in order to lay bare their basic postulates – postulates that are too generally hidden; postulates felt, perhaps, by those who live by them, but so much taken for granted that they are rarely expressed or exposed for examination. When this is done – and it will take the efforts of many keen intellects steeped in the law of at least a dozen lands and also aware of the social nexus of the law – then mankind will be able to see clearly for the first time and clearly where the common consensus of the great living social and law systems lies. Here will be found the common postulates and values upon which the world community can build. At the same time the truly basic points of conflict that will have to be worked upon for resolution will be revealed. Law is inherently purposive".

While enthusiasm for multinational federalism in Europe incrementally led, over the following decades, to the formation of the European Union, the onset of the Cold War (1945–1992) eliminated the prospects of any progress towards federation with a more global scope. The movement quickly shrank in size to a much smaller core of activists, and the world government idea all but disappeared from wide public discourse.

Following the end of the Cold War in 1992, interest in a federal world government and, more generally, in the global protection of human rights, was renewed. The most visible achievement of the world federalism movement during the 1990s is the Rome Statute of 1998, which led to the establishment of the International Criminal Court in 2002. In Europe, progress towards forming a federal union of European states gained much momentum, starting in 1952 as a trade deal between the German and French people led, in 1992, to the Maastricht Treaty that established the name and enlarged the agreement that the European Union (EU) is based upon. The EU expanded (1995, 2004, 2007, 2013) to encompass, in 2013, over half a billion people in 28 member states. Following the EU's example, the African Union was founded in 2002 and the Union of South American Nations in 2008.

, there is no functioning global international military, executive, legislature, judiciary, or constitution with jurisdiction over the entire planet.

The world is divided geographically and demographically into mutually exclusive territories and political structures called states which are independent and sovereign in most cases. There are numerous bodies, institutions, unions, coalitions, agreements and contracts between these units of authority, but, except in cases where a nation is under military occupation by another, "all" such arrangements depend on the continued consent of the participant nations. Countries that violate or do not enforce international laws may be subject to penalty or coercion often in the form of economic limitations such as embargo by cooperating countries, even if the violating country is not part of the United Nations. In this way a countries cooperation in international affairs is voluntary, but non-cooperation still has diplomatic consequences.

Among the voluntary organizations and international arrangements are:

In addition to the formal, or semi-formal, international organizations and laws mentioned above, many other mechanisms act to regulate human activities across national borders. In particular, international trade in goods, services and currencies (the "global market") has a tremendous impact on the lives of people in almost all parts of the world, creating deep interdependency amongst nations (see globalization). Trans-national (or multi-national) corporations, some with resources exceeding those available to most governments, govern activities of people on a global scale. The rapid increase in the volume of trans-border digital communications and mass-media distribution (e.g., Internet, satellite television) has allowed information, ideas, and opinions to rapidly spread across the world, creating a complex web of international coordination and influence, mostly outside the control of any formal organizations or laws. A proactive form of globalization is emerging, spawned by international corporations that wish to loosen trade restrictions. It is the global financial firms that have been the most eager proponents of this expansion. A group of advocates from different parts of the world had been pushing for an integrated global society as envisioned in the Globalist Manifesto which is the foundation of globalism ideology.

The only union generally recognized as having achieved the status of a supranational union is the European Union.

There are a number of other regional organizations that, while not supranational unions, have adopted or intend to adopt policies that may lead to a similar sort of integration in some respects.

Other organisations that have also discussed greater integration include:

The most relevant model for the incremental establishment of a global federation may be the European Union (EU), which politically unites a large group of widely diverse (and some formerly hostile) nations spread over a large geographical area and encompassing over 500 million people. Although the EU is still evolving, it already has many attributes of a federal government such as open internal borders, a directly elected parliament, a court system, an official currency (Euro), and a centralized economic policy. A treaty change would be needed to allow for enlargement of the Union beyond the European continent.

The EU's example is being followed by the African Union, the Union of South American Nations, the Organization of Central American States, and the Association of Southeast Asian Nations. A multitude of regional associations, aggregating most nations of the world, are at different stages of development towards a growing extent of economic, and sometimes political, integration. The European Union consists of twenty-eight European states. It has developed a "single market" which allows people of different countries to travel from state to state without a passport. This also includes the same policies when it comes to trading. The European Union is said to have 26% of the world's money. Not all EU member states use the Euro; the United Kingdom (which is planning to leave the European Union in 2019) for example, retains the pound sterling. Where the Euro is in place, it allows easy access for the free circulation of trade goods. Tariffs are also the same for each country allowing no unfair practices within the union.

The North Atlantic Treaty Organization (NATO) is an intergovernmental military alliance based on the North Atlantic Treaty which was signed on 4 April 1949. The organization constitutes a system of collective defence whereby its member states agree to mutual defense in response to an attack by any external party. NATO's headquarters are in Brussels, Belgium, one of the 28 member states across North America and Europe, the newest of which, Albania and Croatia, joined in April 2009. An additional 22 countries participate in NATO's "Partnership for Peace", with 15 other countries involved in institutionalized dialogue programs. The combined military spending of all NATO members constitutes over 70% of the world's defence spending.

The Caribbean Community (CARICOM), is an organization of 15 Caribbean nations and dependencies. CARICOM's main purpose is to promote economic integration and cooperation among its members, to ensure that the benefits of integration are equitably shared and to coordinate foreign policy. Its major activities involve coordinating economic policies and development planning; devising and instituting special projects for the less-developed countries within its jurisdiction; operating as a regional single market for many of its members CARICOM Single Market and Economy (CSME); and handling regional trade disputes.

Since the establishment of CARICOM by the mainly English Creole-speaking parts of the Caribbean region CARICOM has become multilingual in practice with the addition of Dutch speaking Suriname on 4 July 1995 (although the lingua franca in Suriname is Sranan Tongo, which is an English-based Creole like the languages spoken in much of the rest of CARICOM) and Haiti, where French and Haitian Creole are spoken, on 2 July 2002. In 2001, the heads of government signed a Revised Treaty of Chaguaramas in Trinidad and Tobago, clearing the way for the transformation of the idea for a Common Market aspect of CARICOM into instead a Caribbean Single Market and Economy. Part of the revised treaty among member states includes the establishment and implementation of the Caribbean Court of Justice (CCJ).

The African Union (AU) is an organisation consisting of all the 55 African states of the continent and African waters. Established on July 9, 2002, the AU was formed as a successor to the amalgamated African Economic Community (AEC) and the Organisation of African Unity (OAU). Eventually, the AU aims to have a single currency and a single integrated defence force, as well as other institutions of state, including a cabinet for the AU Head of State. The purpose of the union is to help secure Africa's democracy, human rights, and a sustainable economy, especially by bringing an end to intra-African conflict and creating an effective common market.

Projects for improved economic and political cooperation are also happening at a regional level with the Arab Maghreb Union, the Economic Community of West African States, the Economic Community of Central African States the Southern African Development Community and the East African Community.
ASEAN ( ), the Association of Southeast Asian Nations, is a geo-political and economic organization of 10 countries located in Southeast Asia, which was formed on August 8, 1967 by Indonesia, Malaysia, the Philippines, Singapore, and Thailand as a display of solidarity against communist expansion in Vietnam and insurgency within their own borders. Its claimed aims include the acceleration of economic growth, social progress, cultural development among its members, and the promotion of regional peace. All members later founded the Asia Cooperation Dialogue, which aims to unite the entire continent.
The Shanghai Cooperation Organisation (SCO) is an intergovernmental organization which was founded on June 14, 2001 by the leaders of the People's Republic of China, Russia, Kazakhstan, Kyrgyzstan, Tajikistan and Uzbekistan. Except for Uzbekistan, these countries had been members of the Shanghai Five; after the inclusion of Uzbekistan in 2001, the members renamed the organization.
The Commonwealth of Independent States is comparable to a confederation similar to the original European Community. Although the CIS has few supranational powers, it is more than a purely symbolic organization, possessing coordinating powers in the realm of trade, finance, lawmaking, and security. It has also promoted cooperation on democratization and cross-border crime prevention. As a regional organization, CIS participates in UN peacekeeping forces. Some of the members of the CIS have established the Eurasian Economic Community with the aim of creating a full-fledged common market.
The Arab League is a regional organization of Arab states in Southwest Asia, and North and Northeast Africa. It was formed in Cairo on March 22, 1945 with six members: Egypt, Iraq, Transjordan (renamed Jordan after 1946), Lebanon, Saudi Arabia, and Syria. Yemen joined as a member on May 5, 1945. The Arab League currently has 22 members, which also include, Algeria, Bahrain, Comoros, Djibouti, Kuwait, Libya, Mauritania, Morocco, Oman, Palestine, Qatar, Somalia, Sudan, Tunisia and the United Arab Emirates.
It has also been proposed to reform the Arab League into an Arab Union. The Arab League currently is the most important organization in the region.
The Union of South American Nations, modeled on the European Union, was founded between 2006 and 2008. It incorporates all the independent states of South America. These states are Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela.

The South Asian Association for Regional Cooperation (SAARC) is an economic and political organization of eight countries in Southern Asia. In terms of population, its sphere of influence is the largest of any regional organization: almost 1.5 billion people, the combined population of its member states. It was established on December 8, 1985 by India, Pakistan, Bangladesh, Sri Lanka, Nepal, Maldives and Bhutan. In April 2007, at the Association's 14th summit, Afghanistan became its eighth member.
The Organisation of Islamic Cooperation (OIC) is an international organisation with a permanent delegation to the United Nations. It groups 57 member states, from the Middle East, Africa, Central Asia, Caucasus, Balkans, Southeast Asia and South Asia. The organization claims it represents the Global Islamic World ("ummah"). The official languages of the organisation are Arabic, English and French.

Since the 19th century, many Muslims have aspired to uniting the Muslim "ummah" to serve their common political, economic and social interests. Despite the presence of secularist, nationalist and socialist ideologies in modern Muslim states, they have cooperated to form the Organisation of Islamic Cooperation. The formation of the OIC happened in the backdrop of the loss of Muslim holy sites in Jerusalem. The final cause sufficiently compelled leaders of Muslim nations to meet in Rabat to establish the OIC on September 25, 1969.

According to its charter, the OIC aims to preserve Islamic social and economic values; promote solidarity amongst member states; increase cooperation in social, economic, cultural, scientific, and political areas; uphold international peace and security; and advance education, particularly in the fields of science and technology.

On August 5, 1990, 45 foreign ministers of the OIC adopted the Cairo Declaration on Human Rights in Islam to serve as a guidance for the member states in the matters of human rights in as much as they are compatible with the Sharia, or Quranic Law.

The Turkic Council is an international organization comprising Turkic countries. Since 1992, the "Turkic Language Speaking Countries Summit" has been organizing amongst the Turkic countries. On October 3, 2009, four of these countries signed the Nahcivan Agreement. The organizational center is İstanbul. Additionally, the Joint Administration of Turkic Arts and Culture was founded in Almaty in 1992 and the "Turkic Countries Parliamentarian Assembly" was founded in Baku in 1998. All of these organizations were coopted into the Turkic Council. The Turkic Council has an operational style similar to organization like the Arab League. The member countries are Azerbaijan, Kazakhstan, Kyrgyzstan and Turkey. The remaining two Turkic states, Turkmenistan and Uzbekistan are not currently official members of the council. However, due to their neutral stance, they participate in international relations and are strongly predicted to be future members of the council. The idea of setting up this cooperative council was first put forward by Kazakh President Nursultan Nazarbayev back in 2006.








</doc>
<doc id="46638919" url="https://en.wikipedia.org/wiki?curid=46638919" title="Diplomatic capital">
Diplomatic capital

Diplomatic capital refers to the trust, goodwill, and influence which a diplomat, or a state represented by its diplomats, has within international diplomacy. According to political scientist Rebecca Adler-Nissen, diplomatic capital is a kind of currency that can be traded in diplomatic negotiations and that is increased when positive ″social competences, reputation and personal authority" are portrayed.

Diplomatic capital can be accumulated for example by economic cooperation and by contributions to the solution of international crises, It is strengthened when in other countries the sentiment prevails that the interests of a state or the diplomats representing it are aligned with their own interests. Conversely, it can be squandered when a country engages in a confrontation, an armed conflict or a war, if that is perceived as unjust or at odds with the interests of others.

Diplomatic capital is also linked to the extent of enforcement of human rights.



</doc>
<doc id="25690925" url="https://en.wikipedia.org/wiki?curid=25690925" title="Classification of the Functions of Government">
Classification of the Functions of Government

Classification of the Functions of Government (COFOG) is a classification defined by the United Nations Statistics Division. These functions are designed to be general enough to apply to the government of different countries. The accounts of each country in the United Nations are presented under these categories. The value of this is that the accounts of different countries can be compared.



</doc>
<doc id="2951154" url="https://en.wikipedia.org/wiki?curid=2951154" title="E-governance">
E-governance

Electronic governance or e-governance is the application of information and communication technology (ICT) for delivering government services, exchange of information, communication transactions, integration of various stand-alone systems and services between government-to-citizen (G2C), government-to-business (G2B), government-to-government (G2G), government-to-employees (G2E) as well as back-office processes and interactions within the entire government framework. Through e-governance, government services are made available to citizens in a convenient, efficient, and transparent manner. The three main target groups that can be distinguished in governance concepts are government, citizens, and
businesses/interest groups. In e-governance, there are no distinct boundaries.

Although the two terms are often used interchangeably, there is a difference between e-governance and e-government. E-government refers to the use of the ICTs in public administration which, when combined with organizational change and new skills, are intended to improve public services and democratic processes and to strengthen support to the public. However, e-government has no provision for governance of ICTs. The governance of ICTs typically requires a substantial increase in regulation and policy-making capabilities, as well as additional expertise and opinion-shaping processes among various social stakeholders. The perspective of e-governance is "the use of the technologies that both help to govern and have to be governed". The central goal of e-governance is to reach the beneficiary and to ensure that their service needs are met. Ideally, the government will automatically recognize the importance of achieving this goal in order to maximize its efficiency.

Furthermore, e-government uses one-way communication protocol whereas e-governance uses two-way communication protocol. Establishing the identity of the end beneficiary is a challenge in all citizen-centric services. Statistical information published by governments and global bodies do not always reveal the facts. The best form of e-governance cuts down on the unwanted interference of too many layers while delivering governmental services. It depends on good infrastructural setup with the support of local processes and parameters for governments to reach their citizens or end beneficiaries. A budget for planning, development, and growth can be derived from well laid out e-governance systems.

The relevance of BI Analytics has brought forth a paradigm shift in assimilating and visualizing huge chunks of data in near real-time manner. The pivot of all good decision-making systems is correct, up-to-date and compliant data. Governments not only want the transformation of their own country and countrymen but also expect improved relations and healthy trade across the world. Development should be transformative and continuously evolving. Internal as well as external IT systems should work in tandem with government policies and procedures. Data Analytics has the ability to change the color and complexion of the world. E-governance should induce up-to-date information, initiate effective interaction, and engage with transparent transactions in compliance with rule of law, thus enabling a sustainable transformation model.

The public–private partnership (PPP)-based e-governance projects are hugely successful in India. Many countries implement e-government policy in an attempt to build a corruption-free government.

The goal of government-to-citizen (G2C) e-governance is to offer a variety of ICT services to citizens in an efficient and economical manner and to strengthen the relationship between government and citizens using technology.

There are several methods of government-to-customer e-governance. Two-way communication allows citizens to instant message directly with public administrators, and cast remote electronic votes (electronic voting) and instant opinion voting. Transactions such as payment of services, such as city utilities, can be completed online or over the phone. Mundane services such as name or address changes, applying for services or grants, or transferring existing services are more convenient and no longer have to be completed face to face.

G2C e-governance is unbalanced across the globe as not everyone has Internet access and computing skills, but the United States, European Union, and Asia are ranked the top three in development.

The Federal Government of the United States has a broad framework of G2C technology to enhance citizen access to Government information and services. benefits.gov is an official US government website that informs citizens of benefits they are eligible for and provides information on how to apply for assistance. US State Governments also engage in G2C interaction through the Department of Transportation, Department of Public Safety, United States Department of Health and Human Services, United States Department of Education, and others. As with e-governance on the global level, G2C services vary from state to state. The Digital States Survey ranks states on social measures, digital democracy, e-commerce, taxation, and revenue. The 2012 report shows Michigan and Utah in the lead and Florida and Idaho with the lowest scores. Municipal governments in the United States also use government-to-customer technology to complete transactions and inform the public. Much like states, cities are awarded for innovative technology. Government Technology's "Best of the Web 2012" named Louisville, KY, Arvada, CO, Raleigh, NC, Riverside, CA, and Austin, TX the top five G2C city portals.

European countries were ranked second among all geographic regions. The Single Point of Access for Citizens of Europe supports travel within Europe and Europe is a 1999 initiative supporting an online government. Main focuses are to provide public information, allow customers to have access to basic public services, simplify online procedures, and promote electronic signatures. Estonia is the first and the only country in the world with e-residency which enables anyone in the world outside Estonia to access Estonian online services.

Asia is ranked third in comparison, and there are diverse G2C programs between countries. Singapore's eCitizen Portal is an organized single access point to government information and services. South Korea's Home Tax Service (HTS) provides citizens with 24/7 online services such as tax declaration. Taiwan has top ranking G2C technology including an online motor vehicle services system, which provides 21 applications and payment services to citizens.

Government-to-Citizen is the communication link between a government and private individuals or residents. Such G2C communication most often refers to that which takes place through Information and Communication Technologies (ICTs), but can also include direct mail and media campaigns. G2C can take place at the federal, state, and local levels. 
G2C stands in contrast to G2B, or Government-to-Business networks.

One such Federal G2C network is USA.gov, the United States' official web portal, though there are many other examples from governments around the world.

A full switch to government-to-citizen e-governance will cost a large amount of money in development and implementation. In addition, government agencies do not always engage citizens in the development of their e-gov services or accept feedback. Customers identified the following barriers to government-to-customer e-governance: not everyone has Internet access, especially in rural or low-income areas, G2C technology can be problematic for citizens who lack computing skills. Some G2C sites have technology requirements (such as browser requirements and plug-ins) that won't allow access to certain services, language barriers, the necessity for an e-mail address to access certain services, and a lack of privacy.

E-governance to Employee partnership (G2E) Is one of four main primary interactions in the delivery model of E-governance. It is the relationship between online tools, sources, and articles that help employees to maintain the communication with the government and their own companies. E-governance relationship with Employees allows new learning technology in one simple place as the computer. Documents can now be stored and shared with other colleagues online.
E-governance makes it possible for employees to become paperless and makes it easy for employees to send important documents back and forth to colleagues all over the world instead of having to print out these records or fax G2E services also include software for maintaining personal information and records of employees. Some of the benefits of G2E expansion include:


Government-to-employees (abbreviated G2E) is the online interactions through instantaneous communication tools between government units and their employees. G2E is one out of the four primary delivery models of e-Government.

G2E is an effective way to provide e-learning to the employees, bring them together and to promote knowledge sharing among them. It also gives employees the possibility of accessing information in regard to compensation and benefits policies, training and learning opportunities and civil rights laws. G2E services also include software for maintaining personal information and records of employees.

G2E is adopted in many countries including the United States, Hong Kong, and New Zealand.

From the start of 1990s e-commerce and e-product, there has rampant integration of e-forms of government process. Governments have now tried to use the efficiencies of their techniques to cut down on waste. E-government is a fairly broad subject matter, but all relate to how the services and representation are now delivered and how they are now being implemented.

Many governments around the world have gradually turned to Information technologies (IT) in an effort to keep up with today's demands. Historically, many governments in this sphere have only been reactive, but recently there has been a more proactive approach in developing comparable services such things as e-commerce and e-business.

Before, the structure emulated private-like business techniques. Recently that has all changed as e-government begins to make its own plan. Not only does e-government introduce a new form of record keeping, but it also continues to become more interactive to better the process of delivering services and promoting constituency participation.

The framework of such an organization is now expected to increase more than ever by becoming efficient and reducing the time it takes to complete an objective. Some examples include paying utilities, tickets, and applying for permits. So far, the biggest concern is accessibility to Internet technologies for the average citizen. In an effort to help, administrations are now trying to aid those who do not have the skills to fully participate in this new medium of governance, especially now as e-government progressing to more e-governance terms.

An overhaul of the structure is now required as every pre-existing sub-entity must now merge under one concept of e-government. As a result, Public Policy has also seen changes due to the emerging of constituent participation and the Internet. Many governments such as Canada's have begun to invest in developing new mediums of communication of issues and information through virtual communication and participation. In practice, this has led to several responses and adaptations by interest groups, activist, and lobbying groups. This new medium has changed the way the polis interacts with government.

The purpose to include e-governance to government is to means more efficient in various aspects. Whether it means to reduce cost by reducing paper clutter, staffing cost, or communicating with private citizens or public government. E-government brings many advantages to play such as facilitating information delivery, application process/renewal between both business and private citizen, and participation with the constituency. There are both internal and external advantages to the emergence of IT in government, though not all municipalities are alike in size and participation.
In theory, there are currently 4 major levels of E-government in municipal governments:
These, along with 5 degrees of technical integration and interaction of users include: 

The adoption of e-government in municipalities evokes greater innovation in e-governance by being specialized and localized. The level success and feedback depends greatly on the city size and government type. A council-manager government municipality typically works the best with this method, as opposed to mayor-council government positions, which tend to be more political. Therefore, they have greater barriers towards its application. Council-Manager governments are also more inclined to be effective here by bringing innovation and reinvention of governance to e-governance.

The International City/County Management Association and Public Technology Inc. have done surveys over the effectiveness of this method. The results are indicating that most governments are still in either the primary stages (1 or stage 2), which revolves around public service requests. Though application of integration is now accelerating, there has been little to no instigating research to see its progression as e-governance to the government. We can only theorize it's still within the primitive stages of e-governance.

Government-to-Government (abbreviated G2G) is the online non-commercial interaction between Government organizations, departments, and authorities and other Government organizations, departments, and authorities. Its use is common in the UK, along with G2C, the online non-commercial interaction of local and central Government and private individuals, and G2B the online non-commercial interaction of local and central Government and the commercial business sector.

G2G systems generally come in one of two types: Internal-facing - joining up a single Governments departments, agencies, organizations, and authorities - examples include the integration aspect of the Government Gateway, and the UK NHS Connecting for Health Data SPINE. External facing - joining up multiple Governments IS systems - an example would include the integration aspect of the Schengen Information System (SIS), developed to meet the requirements of the Schengen Agreement.

The strategic objective of e-governance, or in this case G2G is to support and simplify governance for government, citizens, and businesses. The use of ICT can connect all parties and support processes and activities. Other objectives are to make government administration more transparent, speedy and accountable while addressing the society's needs and expectations through efficient public services and effective interaction between the people, businesses, and government.
Within every of those interaction domains, four sorts of activities take place:
Pushing data over the internet, e.g.: regulative services, general holidays, public hearing schedules, issue briefs, notifications, etc.
two-way communications between one governmental department and another, users will interact in dialogue with agencies and post issues, comments, or requests to the agency.
Conducting transactions, e.g.: Lodging tax returns, applying for services and grants.
Governance, e.g.: To alter the national transition from passive info access to individual participation by:

In the field of networking, the Government Secure Intranet (GSi) puts in place a secure link between central government departments. It is an IP-based virtual private network based on broadband technology introduced in April 1998 and further upgraded in February 2004. Among other things, it offers a variety of advanced services including file transfer and search facilities, directory services, email exchange facilities (both between network members and over the Internet) as well as voice and video services. An additional network is currently also under development: the Public Sector Network (PSN) will be the network to interconnect public authorities (including departments and agencies in England; devolved administrations and local governments) and facilitate in particular sharing of information and services among each other.

Government-to-Business (G2B) is the online non-commercial interaction between local and central government and the commercial business sector with the purpose of providing businesses information and advice on e-business best practices. G2B: Refers to the conduction through the Internet between government agencies and trading companies. B2G: Professional transactions between the company and the district, city, or federal regulatory agencies. B2G usually include recommendations to complete the measurement and evaluation of books and contracts.

The objective of G2B is to reduce difficulties for business, provide immediate information and enable digital communication by e-business (XML). In addition, the government should re-use the data in the report proper, and take advantage of commercial electronic transaction protocol. Government services are concentrated on the following groups: human services; community services; judicial services; transport services; land resources; business services; financial 
services and other. Each of the components listed above for each cluster of related services to the enterprise.

E-government reduces costs and lowers the barrier of allowing companies to interact with the government. The interaction between the government and businesses reduces the time required for businesses to conduct a transaction. For instance, there is no need to commute to a government agency's office, and transactions may be conducted online instantly with the click of a mouse. This significantly reduces transaction time for the government and businesses alike.

E-Government provides a greater amount of information that the business needed, also it makes that information more clear. A key factor in business success is the ability to planand forecast through data-driven future. The government collected a lot of economic, demographic and other trends in the data. This makes the data more accessible to companies which may increase the chance of economic prosperity.

In addition, E-Government can help businesses navigate through government regulations by providing an intuitive site organization with a wealth of useful applications. The electronic filings of applications for environmental permits give an example of it. Companies often do not know how, when, and what they must apply. Therefore, failure to comply with environmental regulations up to 70%, a staggering figure most likely due to confusion about the requirements, rather than the product of willful disregard of the law.

The government should concern that not all people are able to access the internet to gain online government services. The network reliability, as well as information on government bodies, can influence public opinion and prejudice hidden agenda. There are many considerations and implementation, designing e-government, including the potential impact of government and citizens of disintermediation, the impact on economic, social and political factors, vulnerable to cyber attacks, and disturbances to the status quo in these areas.

G2B rises the connection between government and businesses. Once the e-government began to develop, become more sophisticated, people will be forced to interact with e-government in the larger area. This may result in a lack of privacy for businesses as their government gets their more and more information. In the worst case, there is so much information in the electron transfer between the government and business, a system which is like totalitarian could be developed. As the government can access more information, the loss of privacy could be a cost.

The government site does not consider about "potential to reach many users including those who live in remote areas, are homebound, have low literacy levels, exist on poverty line incomes."


The main goal of government to business is to increase productivity by giving business more access to information in a more organize manner while lowering the cost of doing business as well as the ability to cut "red tape", save time, reduce operational cost and to create a more transparent business environment when dealing with government.


Government to business key points:


Difference between G2B and B2G


Conclusion:

The overall benefit of e-governance when dealing with business is that it enables the business to perform more efficiently.

E-governance is facing numerous challenges world over. These challenges are arising from administrative, legal, institutional and technological factors. The challenge includes security drawbacks such as spoofing, tampering, repudiation, disclosure, elevation of privilege, denial of service and other cyber crimes. Other sets of problems include implementation parts such as funding, management of change, privacy, authentication, delivery of services, standardization, technology issues and use of local languages.




</doc>
<doc id="49214523" url="https://en.wikipedia.org/wiki?curid=49214523" title="Outline of government">
Outline of government

The following outline is provided as an overview of and topical guide to government:

Government – 

Government - is a general term which can be used to refer to public bodies organizing the political life of the society. Government can also refer to the collective head of the executive branch of power in a polity.

Public policies -
Legislative power -

Executive power -

Judicial power -

Constitution -

Five characteristics of a state

Evolutionary Theory -

Social Contract Theory -

Divine Theory -

Meritocracy -

Form a More Perfect Union -

Establish Justice -

Insure Domestic Tranquility -

Provide for the Common Defense -

Promote the General Welfare -

Secure the Blessings of Liberty -

History of government

Ordered government

Limited government

Representative government

Magna Carta

Petition of Right

English Bill of Rights

Charter

Royal Colonies - New Hampshire, Massachusetts, New York, New Jersey, Virginia, North Carolina, South Carolina, and Georgia
Proprietary colonies - Maryland, Pennsylvania, Delaware
Charter colonies - Connecticut and Rhode Island

New England Confederation

Albany Plan of Union

Delegate

Boycott

Repeal

Popular sovereignty

Declaration of Independence

Articles of Confederation

Ratification

Presiding Officer

Framers of the Constitution -

Virginia Plan -

New Jersey Plan -

Connecticut Compromise -

Three-Fifths Compromise -

Slave Trade Compromise -

Federalists -

Anti-Federalists -

Quorum -

Democracy -

Dictatorship -

Unitary government -

Federal government -

Confederate government (Confederation) -

Presidential government -

Parliamentary government -

Popular sovereignty 
Limited government
Human equality

Free enterprise system -

Law of supply and demand -

Mixed economy -

Preamble

Articles

Popular Sovereignty

Limited Government
Separation of powers

Checks and balances
Judicial review
Federalism

Chambers

Parliament
Parliamentary procedure
Types

Legislator -

Committee member -

Trustee -

Delegate -

Partisan -

Politico -

Senator -

Money

Money


</doc>
<doc id="3870861" url="https://en.wikipedia.org/wiki?curid=3870861" title="Governmental accounting">
Governmental accounting

Government accounting refers to the process of recording and the management of all financial transactions incurred by the government which includes its income and expenditures.

Various governmental accounting systems are used by various public sector entities. In the United States, for instance, there are two levels of government which follow different accounting standards set forth by independent, private sector boards. At the federal level, the Federal Accounting Standards Advisory Board (FASAB) sets forth the accounting standards to follow. Similarly, there is the Governmental Accounting Standards Board (GASB) for state and local level government. 

There is an important difference between private sector accounting and governmental accounting. The main reasons for this difference is the environment of the accounting system. In the government environment, public sector entities have different goals, as opposed to the private sector entities' one main goal of gaining profit. Also, in government accounting, the entity has the responsibility of fiscal accountability which is demonstration of compliance in the use of resources in a budgetary context. In the private sector, the budget is a tool in financial planning and it isn't mandatory to comply with it.

Government accounting refers to the field of accounting that specifically finds application in the public sector or government. A special field of accounting exists because:

An exception exists on the above-mentioned differences in the case of public utility businesses (for example Electricity Services) that may be intended to produce a net income or profit, but a significant debate exists over whether there should be such an exception. Nationalisation includes, amongst others, the argument that entities should be either private or public, and that the objectives of public entities should differ significantly from that of private entities. In other words, is the generation and reticulation of electricity with the objective to generate a profit in the public interest or not? And if it is the best way, shouldn’t it then be completely private instead of having access to public funds and monopolies?

Governmental accounting standards are currently being dominated by the accounting standards (internationally sometimes referred to as IFRS) originally designed for the private sector. The so-called Generally Recognised Accounting Practices (GRAP) that are being enforced in the public sector of countries such as South Africa, one of the front-runners in this regard is based on the Generally Accepted Accounting Practices originally developed for the private sector. The above and common sense raises the question of whether this is the best solution. It is, of course, cheaper and it is alleged that the history of separate development of accounting practices for government has not been successful. Even at the onset of the current fiscal crisis in Europe and other parts of the world, it was argued authoritatively that the sometimes inapplicable accounting practices of the private sector being used, have contributed to the origination of, and belated reaction to, the fiscal crisis.(1)

Sources (not directly quoted but used in synthesis):

1. Sanderson, I. Worldwide Credit Crisis and Stimulus Packages in Accountancy SA, June 2009, p. 14.

2. Conradie, J.M. The applicability of Generally Accepted Accounting Practice in the Central Government of South Africa (English summary of a thesis written in another language) University of Pretoria, 1994.

3. Conradie, J.M. Die toepaslikheid van Algemeen Aanvaarde Rekeningkundige Praktyk in die Sentrale Owerheid van Suid-Afrika. Universiteit van Pretoria, 1993. (Original full text of the summary.)
4. Donald amcool

The governmental accounting system sometimes uses the historic system of fund accounting. A set of separate, self-balancing accounts are responsible for managing resources that are assigned to specific purposes based on regulations and limitations.

The governmental accounting system has a different focus for measuring accounting than private sector accounting. Rather than measuring the flow of economic resources, governmental accounting measures the flow of financial resources. Instead of recognizing revenue when they are earned and expenses when they are incurred, revenue is recognized when there is money available to liquidate liabilities within the current accounting period, and expenses are recognized when there is a drain on current resources.

Governmental financial statements must be accompanied by required supplementary information (RSI). The RSI is a comparison of the actual expenses compared to the original budget created at the beginning of the fiscal year for the Government's General Fund and all major Special Revenue Funds.

The unique objectives of government accounting do not preclude the use of the double entry accounting system. There can, however, be other significant differences with private sector accounting practices, especially those that are intended to arrive at a net income result. The objectives for which government entities apply accountancy that can be organized in two main categories:
- The accounting of activities for accountability purposes. In other words, the representatives of the public, and officials appointed by them, must be accountable to the public for powers and tasks delegated. The public, who have no other choice but to delegate, are in a position that differs significantly from that of shareholders and therefore need financial information, to be supplied by accounting systems, that is applicable and relevant to them and their purposes.
- Decision-making purposes. The relevant role-players, especially officials and representatives, need financial information that is accounted, organized and presented for the objectives of their decision-making. These objectives bear, in many instances, no relation to net income results but are rather about service delivery and efficiency. The taxpayer, a very significant group, simply wants to pay as little as possible taxes for the essential services for which money is being coerced by law.
Sources (not directly quoted but used in synthesis):

1. Conradie, J.M. The applicability of Generally Accepted Accounting Practice in the Central Government of South Africa (English summary of a thesis written in another language) University of Pretoria, 1993.

2. Conradie, J.M. Die toepaslikheid van Algemeen Aanvaarde Rekeningkundige Praktyk in die Sentrale Owerheid van Suid-Afrika. Universiteit van Pretoria, 1993. (Original full text of the summary.)



</doc>
<doc id="325419" url="https://en.wikipedia.org/wiki?curid=325419" title="Political authorities">
Political authorities

Political authorities hold positions of power or influence within a system of government. Although some are exclusive to one or another form of government, many exist within several types.



</doc>
<doc id="5594801" url="https://en.wikipedia.org/wiki?curid=5594801" title="Joint session">
Joint session

A joint session or joint convention is, most broadly, when two normally separate decision-making groups meet together, often in a special session or other extraordinary meeting, for a specific purpose.

Most often it refers to when both houses of a bicameral legislature sit together. A joint session typically occurs to receive foreign or domestic diplomats or leaders, or to allow both houses to consider bills together.

Some constitutions give special power to a joint session, voting by majority of all members of the legislature regardless of which house or chamber they belong to. For example, in Switzerland a joint session of the two houses elects the members of the Federal Council (cabinet). In India, disputes between houses are resolved by a joint sitting but without an intervening election.

In the Australian federal parliament, a joint sitting can be held, under certain conditions, to overcome a deadlock between the two houses. For a deadlock to be declared, a bill has to be rejected twice by the Senate at an interval of at least three months, after which a double dissolution election can be held. If, following the election, the new parliament is still unable to pass the bill, it may be considered by a joint sitting of the House of Representatives and the Senate, and must achieve an absolute majority of the total number of members and senators in order to pass. The only example of this occurring was the Joint Sitting of the Australian Parliament of 1974 under the Whitlam Labor government, at which six deadlocked bills were passed.

Because the House has twice as many members as the Senate, the former has an advantage in a joint sitting. However, the voting system used for the Senate before 1949, which might be called "multiple at-large voting", often led to landslide if not wipe-out results in each state, resulting in a winning margin over the whole of Australia of up to 36-0. That would have given the party or grouping enjoying such a large Senate majority an advantage in any joint sitting, had there been one.

The voting system now used for the Senate, quota-preferential proportional representation, almost inevitably leads to very evenly divided results. Six senators are elected from each state and two from each territory. A party or grouping has to get at least 57% of the vote in any State to obtain a four-two majority of seats in that state, whereas from 51% to 56% of the vote yields only an equality of three seats to each major party or group.

The Federal Assembly is a formal joint session of the two houses of the bicameral Austrian Parliament, to swear the elected President of Austria into office.

The Chamber of Representatives and the Senate convene as United Chambers (; ; ) to swear the King into office, as stipulated by article 91 of the Constitution.

The Canadian government procedure is called a joint address, with the members of the House of Commons attending the Senate as guests. There is no procedure in Canada for both chambers of the Parliament to sit in a true joint session.

Various government agencies and non-governmental organizations may also meet jointly to handle problems which each of the involved parties has a stake in.

The Congress of France is an assembly of both houses of the French Parliament, convened at the Palace of Versailles, which can approve certain amendments to the constitution by a three-fifth majority of all members. Since 2008, the Congress may also be convened to hear an address from the President of the Republic.

The Federal Convention elects the President of Germany. It includes members from the Bundestag and representatives of the States of Germany.

In India, if an ordinary bill has been rejected by any house of the parliament and if more than six months have elapsed, the President may summon a joint session for purpose of passing the bill. The bill is passed by a simple majority of a joint sitting. Since the lower house (Lok Sabha) has more than twice the members of the upper house (Rajya Sabha), a group commanding a majority in the lower house of the Government of India can pass such a bill even if it was previously rejected by the upper house.

So far, Joint Session of the Parliament of India has been called for only three bills - the Dowry Prohibition Act, 1961, the Banking Service Commission Repeal Bill, 1978, and the Prevention of Terrorism Act, 2002 - have been passed at joint sessions.//

In the Irish Free State, the predecessor of the Republic of Ireland, the Governor-General's Address to both houses was made to the lower house, with Senators invited to attend.

In the Philippines, Congress can convene in a joint session for the following:

While the State of the Nation address occurs annually, and presidential elections occur every six years, the only time that the other two conditions were met after the approval of the 1987 constitution was after the declaration of martial law in Maguindanao after the Maguindanao massacre.

Joint sessions are typically held at the seat of the House of Representatives, which is at the Batasang Pambansa Complex, Quezon City.

English and later British monarchs have jointly addressed the House of Commons and the House of Lords since the 16th century. Since 1939, foreign heads of state and dignitaries have been invited to address both houses of Parliament, the first to do so was French President Albert Lebrun in March 1939.

The speech from the throne upon the State Opening of Parliament is made before a joint sitting of both Houses. This occurs in the House of Lords, the upper chamber, due to the constitutional convention that the monarch never enters the House of Commons. The closing of each of parliamentary session is also marked by a speech to both Houses.

The State of the Union Address of the president of the United States is traditionally made before a "joint session" of the United States Congress. Many states refer to an analogous event as a "joint convention". Such assemblies are typically held in the chamber of the lower house as the larger body. State constitutions of U.S. states may require joint conventions for other purposes; for example Tennessee's requires such to elect the secretary of state, the state treasurer, and the comptroller of the treasury.

The first foreign dignitary to address a joint session of Congress was Ambassador André de La Boulaye of France who addressed a joint session on May 20, 1934.



</doc>
<doc id="652214" url="https://en.wikipedia.org/wiki?curid=652214" title="Hung parliament">
Hung parliament

A hung parliament is a term used in legislatures under the Westminster system to describe a situation in which no particular political party or pre-existing coalition (also known as an alliance or bloc) has an absolute majority of legislators (commonly known as members or seats) in a parliament or other legislature. This situation is also known, albeit less commonly, as a balanced parliament, or as a legislature under no overall control, and can result in a minority government. The term is not relevant in multi-party systems where it is rare for a single party to hold a majority.

In the Westminster System, in the circumstance of a hung parliament, no party or coalition has an automatic mandate to assume control of the executive – a status usually known in parliamentary systems as "forming (a) government". However, an absolute majority may still be gained through the formation of a new coalition government, or the addition of previously unaffiliated members to a pre-existing coalition. However, a minority government may instead result: that is, the party that has the most members is allowed to form government without an absolute majority, provided that it has the express, ongoing support of unaffiliated members, such as minor parties and/or independent legislators.

A normal objective of parliamentary systems – especially those requiring responsible government, such as the Westminster system – is the formation of a stable government (i.e. ideally one that lasts a full parliamentary term, until the next election would normally be due). This requires a government to be able to muster sufficient votes in parliament to pass motions of confidence and supply, especially motions of no-confidence and budget bills). If such motions fail, they normally result in the dissolution of parliament and a fresh election. In some parliamentary systems, however, a new government may be formed without recourse to an election – if, for example, a minor party holds the balance of power, it may publicly express for the opposition, thereby creating a new majority.

The term hung parliament is most often used of parliaments dominated by two major parties or coalitions. General elections in such systems usually result in one party having an absolute majority and thus quickly forming a new government. In most parliamentary systems, a hung parliament is considered exceptional and is often seen by all parties and observers as undesirable. In other contexts, a hung parliament may be seen as ideal – for example, if opinions among the voting public are polarised regarding one or more issues, a hung parliament may lead to the emergence of a compromise or consensus.

If a legislature is bicameral, the term "hung parliament" is usually used only with respect to the lower house.

In a multi-party system with legislators elected by proportional representation or a similar systems, it is usually exceptionally rare and difficult for any party to have an absolute majority. Under such situations, hung parliaments are often taken for granted and coalition governments are normal. However, the term may be used to describe an election in which no established coalition wins an outright majority (such as the German federal election of 2005 or the 2018 Italian general election).

Because Ireland uses PR-STV, it is rare for any one party to have a majority on its own. 
The last such occasion was 1977. However, one or other coalitions are known to be possible before and during the election. Therefore, a "hung Dáil" (Dáil Éireann being the lower and most dominant chamber of the Oireachtas/Parliament) in Ireland refers more to the inability of a coalition of parties who traditionally enter government together or would be expected to govern together, from doing so.

The President has no direct role in the formation of governments in the case of a hung parliament. However he retains the power to convene a meeting of either or both the Dáil and Senate which could become important if there was a government trying to use parliamentary recess to prevent confidence votes and hold onto power. The President may also refuse to dissolve Dáil Eireann and call an election if the Taoiseach loses a vote of confidence, instead giving the other parties a chance to see if they can put together a government without proceeding to another election.

In 2016, Fine Gael and Labour, who had been in government the previous five years, were unable, due to Labours collapse, to enter government again. Fianna Fáil had enough seats to put together a rainbow government with the other centre-left, hard left parties and independents but negotiations broke down. Fianna Fáil had also promised not to enter coalition with Sinn Féin due to the current leaderships Provisional IRA connections.

The press began to speculate about a Germany style "Grand Coalition" similar to the Christian Democrats and Social Democrats there. Many members of FF considered FG too right wing to enter coalition with and threatened to leave the party this came to pass. As talks continued on without a new government (the old government, constitutionally, which had just been voted out, remaining in power including ministers who had lost their seats) FF agreed to allow a government to form by abstention. The parliamentary arithmetic fell in such a way that if FF TD's abstained on confidence and supply matters, a FG minority govt could, with the support of a group of independents, form a new government. This was agreed in exchange for a number of policy concessions. 
Once the deal with FF was signed, Taoiseach Enda Kenny conducted talks with the independents and entered government for a second term.

In the United Kingdom, before World War I, a largely stable two-party system existed for generations; traditionally, only the Tories and Whigs, or from the mid-19th century the Conservative and Liberal parties, managed to deliver Members of Parliament in significant numbers. Hung parliaments were thus rare, especially during the 19th century. The possibility of change arose when, in the aftermath of the Act of Union, 1800, a number of Irish MPs took seats in the House, though initially these followed the traditional alignments. However, two Reform Acts (in 1867 and in 1884) significantly extended the franchise and redrew the constituencies, and coincided with a change in Irish politics. After the 1885 general election, neither party had an overall majority. The Irish Parliamentary Party held the balance of power and made Irish Home Rule a condition of their support. However, the Liberal Party split on the issue of Irish Home Rule, leading to another general election in 1886, in which the Conservatives won the most seats and governed with the support of the fragment of Liberalism opposed to Home Rule, the Liberal Unionist Party.

Both the election of January 1910, and that of December 1910 produced a hung parliament with an almost identical number of seats won by the governing Liberal Party and the Conservative Party. This was due both to the constitutional crisis and to the rise of the Labour Party. The elections of 1929 resulted in the last hung parliament for many years; in the meantime, Labour had replaced the Liberals as one of the two dominating parties.

Since the elections of 1929, there have been three general elections that resulted in hung parliaments in the UK. The first was the election in February 1974, and the ensuing parliament lasted only until October. The second was the May 2010 election, the result of which was a hung parliament with the Conservative party as the largest single party. The results for the 3 main parties were: Conservatives 306, Labour 258, Liberal Democrats 57. The third one resulted from the snap election held in June 2017 that had been called for by Theresa May in order to strengthen her majority heading into Brexit negotiations later in 2017. However, this election backfired on May and her Conservative Party, resulting in a hung parliament after the snap election.

The formation of the coalition resulting from the 2010 election led to the Fixed-term Parliaments Act 2011, which instituted fixed five-year Parliaments and transferred the power to call early elections from the Prime Minister to Parliament itself. This was the idea of the Deputy Prime Minister Nick Clegg, then the leader of the Liberal Democrats, who said that this would stop the Prime Minister and leader of the Conservative Party, David Cameron, from calling a snap election to end the hung parliament, as many other Conservatives had requested.

Hung parliaments can also arise when slim government majorities are eroded by by-election defeats and defection of Members of Parliament to opposition parties, as well as resignations of MPs from the House of Commons. This happened in December 1996 to the Conservative government of John Major (1990–97) and in mid-1978 to the Labour government of James Callaghan (1976–79); this latter period covers the era known as the Winter of Discontent. The minority government of Jim Callaghan came when Labour ended their 15-month Lib-Lab pact with the Liberals having lost their majority in early 1977.

According to researchers Andrew Blick and Stuart Wilks-Heeg, the phrase "hung parliament" did not enter into common usage in the UK until the mid-1970s. It was first used in the press by journalist Simon Hoggart in "The Guardian" in 1974.

Academic treatments of hung parliaments include David Butler's "Governing Without a Majority: Dilemmas for Hung Parliaments in Britain" (Sheridan House, 1986) and Vernon Bogdanor's 'Multi-Party Politics and the Constitution' (Cambridge University Press, 1983).

Three recent Canadian Parliaments (the 38th, the 39th, and the 40th Parliaments) were hung parliaments, although that term is not used in Canada. Rather, the term "minority government" is used to describe the situation in which the party with the greatest number of seats in the House of Commons (but less than a majority) forms the Government. The aftermath of all three of these elections was that the largest party ruled as a "minority government". Although minority governments have tended to be short-lived, the two successive minorities under Prime Minister Stephen Harper managed to hold on to power from February 2006 until a no confidence vote in March 2011 resulted in the dissolution of Parliament and elections held on 2 May 2011. The subsequent election saw a majority parliament elected with Harper's Conservative Party obtaining a 24-seat majority. The make-up of the 40th Canadian Parliament hung parliament resulted in the 2008–09 Canadian parliamentary dispute. The Conservative Party had a plurality of seats, however, the Liberal Party and New Democratic Party together held enough seats to have a larger plurality and reached an accord to form a minority coalition government. The Bloc Québécois agreed to provide support on confidence votes. On 4 December 2008, Governor General Michaëlle Jean granted Prime Minister Stephen Harper a prorogation on the condition that parliament reconvene early in the new year; the date was set as 26 January 2009. The first session of the 40th parliament thus ended, delaying a vote of no-confidence.

Australian parliaments are modelled on the Westminster system, with a hung parliament typically defined as a lack of a lower house parliamentary majority from either the Australian Labor Party or Liberal/National Coalition.

Hung parliaments are rare at the federal level in Australia, as a virtual two-party system, in which the Australian Labor Party competes against an alliance of the conservative parties, has existed with only brief interruptions since the early 20th century. Prior to 1910, no party had a majority in the House of Representatives. As a result, there were frequent changes of government, several of which took place during parliamentary terms. Since 1910, when the two-party system was cemented, there have been two hung parliaments, the first in 1940, and the second in 2010. At the 1940 federal election, incumbent Prime Minister Robert Menzies secured the support of the two crossbenchers and continued to govern, but in 1941 the independents switched their support to Labor, bringing John Curtin to power.

Declining support for the major parties in recent times is leading to more non-majoritarian outcomes at elections. At the 2010 federal election, which resulted in an exact 72-72 seat tie between Labor and The Liberal-National Coalition, incumbent Prime Minister Julia Gillard secured the support of four out of six Independent and Green Party crossbenchers and continued to govern.

In the 2016 federal election a hung parliament was only narrowly averted with the Liberal-National Coalition winning 76 seats, the bare minimum required to form a majority government. The Liberal-National Coalition government lost its majority government status after a by-election in 2018.

Having hung Parliaments in both Australia and the United Kingdom at the same time, and also briefly including Canada as well, is unprecedented in the history of the Commonwealth of Nations. 

Hung parliaments are rather more common at a state level. The Tasmanian House of Assembly and the unicameral Parliament of the Australian Capital Territory are both elected by Hare-Clark proportional representation, thus, elections commonly return hung parliaments. In other states and territories, candidates contest single-member seats. With far fewer seats than federal parliament, hung parliaments are more likely to be elected. Recent examples include New South Wales in 1991, Queensland in 1998 and 2015, Victoria in 1999, South Australia in 1997 and 2002, Western Australia in 2008, the Australian Capital Territory in 2008, and Tasmania in 2010.

Hung parliaments had a relatively uncommon place in New Zealand politics prior to the introduction of proportional representation in 1993. Only on five occasions since the beginnings of modern party politics in 1890 has a hung parliament occurred, in 1911, 1922, 1928, 1931 & 1993 respectively. The rarity between 1936 and 1996 was due to the regression into a two party system, alternating between the long dominating New Zealand Labour Party and New Zealand National Party. Since the first MMP election in 1996 no single party has gained an outright majority in parliament. Subsequent governments have all been formed as either coalitions or under confidence and supply agreements. 

In countries used to decisive election outcomes, a hung parliament is often viewed as an unfavourable outcome, leading to relatively weak and unstable government. A period of uncertainty after the election is common, as major party leaders negotiate with independents and minor parties to establish a working majority.

An aspiring head of government may seek to build a coalition government; in Westminster systems, this typically involves agreement on a joint legislative programme and a number of ministerial posts going to the minor coalition partners, in return for a stable majority. Alternatively, a minority government may be formed, establishing confidence and supply agreements in return for policy concessions agreed in advance, or relying on case by case support.

In the Western Australian state election of 2008 the Australian Labor Party won more seats than the Liberal Party at 28 to 24. The National Party along with three independents had the seats needed to give either party a majority. To help the Liberal Party form government, the Nationals supported the party on the condition that the Royalties for Regions policy was implemented.

In the 1999 Victorian state election, the Labor Party won 42 seats, while the incumbent Liberal National Coalition retained 43, with 3 seats falling to independents. The Labor Party formed a minority government with the 3 independents.

The 2010 Tasmanian state election resulted in a hung parliament. After a period of negotiation, the incumbent Labor government led by David Bartlett was recommissioned, but containing the Leader of the Tasmanian Greens, Nick McKim, as a minister, and the Greens' Cassy O'Connor as Cabinet Secretary.

In the 2010 federal election, neither Labor nor the Liberal coalition secured the majority of seats required to form a Government in their own right. In order to counter the potential instability of minority government involved groups may negotiate written agreements defining their terms of support. Such measures were undertaken by the Gillard Government in 2010.

In India if an election results in a 'hung assembly' in one of the state Legislative Assemblies and no party is capable of gaining confidence then fresh elections are announced to be held as soon as possible. Until this occurs President's Rule (or in the State of Jammu and Kashmir 'Governor's Rule') is applied. In India there have been many situations of hung assemblies in the state legislatures.

The first such occasion was in 1911 when the Liberal Party won fewer seats than the opposition Reform Party despite tallying the most votes. A vote of no confidence was placed by Reform and the Liberals survived by just one vote. This prompted Prime Minister Sir Joseph Ward to resign, his replacement Thomas Mackenzie was later defeated in July 1912 in a vote with several MPs and Labour crossing the floor to vote with the opposition, the last time in New Zealand history a government has changed on a confidence vote. This broke 23 years of Liberal governance and William Massey formed a new Reform Party government. Massey governed through to his death in 1925, though in 1922 the Reform Party suffered major losses and Massey was forced negotiate with several Independent MPs to retain power.

In 1928, Reform were ousted from governance and Joseph Ward once again won back power. However, the Reform and United (Liberal) parties were tied on seats with Labour holding the balance of power. Labour chose to back Ward rather than let Reform leader Gordon Coates remain in office. In the next election in 1931, there was again a three-way deadlock. On this occasion the Reform and United parties became a coalition government out of mutual fear of Labour's ever-increasing appeal as the Great Depression worsened.

1993 was the last time a hung parliament occurred in New Zealand. Governor-General Dame Catherine Tizard asked Sir David Beattie to form a committee, along with three retired appeal court judges, to decide whom to appoint as Prime Minister. 

In the February 1974 General Election, no party gained an overall parliamentary majority. Labour won the most seats (301, which was 17 seats short of an overall majority) with the Conservatives on 297 seats, although the Conservatives had a larger share of the popular vote. As the incumbent Prime Minister, Edward Heath remained in office, attempting to build a coalition with the Liberals. When these negotiations were unsuccessful Heath resigned and Labour led by Harold Wilson took over in a minority government.

In the 2010 UK General Election, another hung parliament occurred with the Conservatives as the largest party, and discussions followed to help create a stable government. This resulted in agreement on a coalition government, which was also a majority government, between the Conservative Party, which won the most votes and seats in the election, and the Liberal Democrats.

In the 2017 UK General Election, a hung parliament occurred for the second time in seven years with the Conservatives again being the largest party. The Conservatives led by Theresa May formed a minority government, supported by a confidence-and-supply agreement with the Northern Ireland's Democratic Unionist Party.

There have been occasions when, although a parliament or assembly is technically hung, the party in power has a working majority. For example, in the United Kingdom, the tradition is that the Speaker and Deputy Speakers do not vote and Sinn Féin MPs never take their seats per their policy of abstentionism, so these members can be discounted from the opposition numbers.

In 2005, this was the case in the 60-seat National Assembly for Wales, where Labour lost their majority when Peter Law was expelled for standing against the official candidate in the 2005 Westminster election in the Blaenau Gwent constituency. When the Assembly was first elected on 1 May 2003, Labour won 30 seats, Plaid Cymru won 12, the Conservatives won 11, Liberal Democrats won 6, and the John Marek Independent Party won a seat.

When Dafydd Elis-Thomas (Plaid Cymru) was reelected as the presiding officer, this reduced the number of opposition AMs who could vote to 29, as the presiding officer votes only in the event of a tie and, even then, not on party political lines but according to Speaker Denison's rule. Thus, Labour had a working majority of one seat until Law ran in Blaenau Gwent.




</doc>
<doc id="52674452" url="https://en.wikipedia.org/wiki?curid=52674452" title="Cabinet crisis">
Cabinet crisis

A cabinet crisis or government crisis is a situation when the government is challenged before the mandate period expires, because it threatens to resign over a proposal, or it is at risk at being dismissed after a motion of no confidence, a conflict between the parties in a coalition government or a coup d'état. It may also be the result of there being no clear majority willing to work together to form a government. During this period a caretaker government with a limited mandate may take care of the day-to-day affairs of the state, while waiting for a snap election.



</doc>
<doc id="862851" url="https://en.wikipedia.org/wiki?curid=862851" title="State government">
State government

A state government is the government of a country subdivision in a federal form of government, which shares political power with the federal or national government. A state government may have some level of political autonomy, or be subject to the direct control of the federal government. This relationship may be defined by a constitution.

The reference to "state" denotes country subdivisions which are officially or widely known as "states", and should not be confused with a "sovereign state". Provinces are usually divisions of unitary states. Their governments, which are also "provincial governments", are not the subject of this article.

The United States and Australia are the main examples of federal systems in which the term "state" is used for the subnational components of the federation. In addition, the Canadian provinces fulfil a similar role. The term for subnational units in non-English-speaking federal countries may also often be translated as "state", e.g. States of Germany (German "Länder").

The Commonwealth of Australia is a federal nation with six states (and two mainland territories). Section 51 of the Australian Constitution sets out the division of legislative power between the states and the Commonwealth (federal) government. The Commonwealth government is given a variety of legislative powers, including control of foreign policy, taxation (although this cannot discriminate between states or parts of states), and regulation of interstate commerce and corporations. Since the original ratification of the constitution, the High Court of Australia has settled a number of disputes concerning the extent of the Commonwealth's legislative powers, some of which have been controversial and extensively criticised; these included a dispute in 1982 over whether the Commonwealth was entitled to designate land for national heritage purposes under United Nations agreements, as well as numerous disputes over the extent of the Commonwealth's power over trade union and industrial relations legislation.

One difference between the Australian and United States models of federalism is that, in Australia, the Commonwealth Parliament has explicit constitutional power over marriage legislation; this has been a focal point for recent controversies over same-sex marriage.

Each state of Australia has a governor, who represents the Queen of Australia (currently Elizabeth II of the United Kingdom) and performs the ceremonial duties of a head of state. Every state also has a parliament; most states have a bicameral parliament, except for Queensland, where the upper chamber (the Legislative Council) was abolished in 1922. Like their Indian counterparts, Australian states have a Westminster system of parliamentary government; the head of government, known in each state as a Premier, is drawn from the state parliament.

In India, the state governments are the level of governments below the Union government. India is a Sovereign Socialist Secular Democratic Republic with a Parliamentary system of government. The Republic is governed in terms of the Constitution of India. Sovereignty is shared between the union and the state government, but the union government is given greater powers. The President is the constitutional head Executive of the State. Real executive power vests in a Union Council of Ministers with the Prime Minister as head of government. The States resembles the federal system. In the states, the Governor is the head of Executive, but real executive power vests with the Chief Minister who heads the Council of Ministers. The judicial setup of the country is headed by the Chief Justice of India at federal level, who presides over one of the largest judicial apparatus dispensing criminal, civil and all other forms of litigation, and Chief Justices of the High Courts at state level. The government head of its legal wing is the Attorney General of India at federal level and Advocate General at state level.

In Nigeria, States are constituent political entities, which shares sovereignty with the Federal Government of Nigeria. In the Nigerian Constitution all powers not granted to the States are reserved to the Federal Government.

In Pakistan, the provincial governments are the level of government below the federal government. Pakistan is a Sovereign Islamic Republic and a unitary Parliamentary Multi-party system form of a National Government.

The Provincial Government Head by the Elected Chief Minister .

South Africa is divided into nine provinces which have their own elected governments. Chapter Six of the Constitution of South Africa describes the division of power between the national government and the provincial governments, listing those "functional areas" of government that are exclusively reserved to the provincial governments and those where both levels of government have concurrent powers; the remaining areas not listed are reserved to the national government. In areas where both levels have concurrent powers there is a complex set of rules in the event of a conflict between national and provincial legislation. Generally in such a case the provincial legislation prevails, but national legislation may prescribe standards and frameworks for provinces to follow, and may prevent provinces from adversely affecting national interests or the interests of other provinces. The functional areas in which the provincial governments have powers include agriculture, arts and culture, primary and secondary education, the environment and tourism, health, housing, roads and transport, and social welfare.

The provincial governments are structured according to a parliamentary system in which the executive is dependent on and accountable to the legislature. The unicameral provincial legislature is elected by party-list proportional representation, and the legislature in turn elects one of its members as Premier to head the executive. The Premier appoints an Executive Council (a cabinet), consisting of members of the legislature, to administer the various departments of the provincial administration.

Under the 10th Amendment to the U.S. Constitution, all governmental powers not granted to the Federal government of the United States nor prohibited by it to the States, are reserved to the States respectively, or to the people.



</doc>
<doc id="55648078" url="https://en.wikipedia.org/wiki?curid=55648078" title="Direct rule">
Direct rule

Direct rule is when an imperial or central power takes direct control over the legislature, executive and civil administration of an otherwise largely self-governing territory.

In 2017, the Parliament of Catalonia unilaterally declared independence from Spain amid a constitutional crisis over the result of the independence referendum.. The Spanish Senate triggered Article 155 of the Spanish Constitution of 1978, and Prime Minister Mariano Rajoy dismissed the Executive Council of Catalonia, dissolving the Parliament of Catalonia.

In 1991, Chechnya declared independence and was named the Chechen Republic of Ichkeria. Russian army forces invaded in 1994 and again in 1999 in response to the War of Dagestan. By early 2000, Russia almost completely destroyed Grozny and put Chechnya under direct control of Moscow. The Russian government declared that the conflict ended in 2002 but operations continued until 2009.

Direct rule has occurred over India by the British Raj, and within India under the system of President's rule.

The British Raj was the rule by the British Crown in the Indian subcontinent between 1858 and 1947. The region under British control was commonly called "India" in contemporaneous usage, and included areas directly administered by the United Kingdom, which were collectively called British India, and those ruled by indigenous rulers, but under British tutelage or paramountcy, and called the princely states. The "de facto" political amalgamation was also called the "Indian Empire" and after 1876 issued passports under that name.

As India, it was a of the League of Nations, a participating nation in the Summer Olympics in 1900, 1920, 1928, 1932, and 1936, and a founding member of the United Nations in San Francisco in 1945.

This system of governance was instituted on 28 June 1858, when, after the Indian Rebellion of 1857, the rule of the British East India Company was transferred to the Crown in the person of Queen Victoria (who, in 1876, was proclaimed Empress of India). It lasted until 1947, when Britain′s Indian Empire was partitioned into two sovereign dominion states: the Dominion of India (later the Republic of India) and the Dominion of Pakistan (later the Islamic Republic of Pakistan, the eastern part of which, still later, became the People's Republic of Bangladesh). At the inception of the Raj in 1858, Lower Burma was already a part of British India; Upper Burma was added in 1886, and the resulting union, Burma, was administered as an autonomous province until 1937, when it became a separate British colony, gaining its own independence in 1948.

In the Republic of India, "President's rule" refers to the imposition of Article 356 of the Constitution of India on a state whose constitutional body has failed. In the event that a state government is unable to function, the Constitution provides for the state to come under the direct control of the central government. In other words, it is "under the President's rule". Subsequently, executive authority is exercised through the centrally appointed governor, who has the authority to appoint retired civil servants or other administrators to assist him.

When a state government is functioning correctly, it is run by an elected Council of Ministers responsible to the state's legislative assembly (Vidhan Sabha). The council is led by the Chief Minister, who is the "de facto" chief executive of the state; the Governor is only a "de jure" constitutional head. However, during President's rule, the Council of Ministers is dissolved, vacating the office of Chief Minister. Furthermore, the Vidhan Sabha is either prorogued or dissolved, necessitating a fresh election.

Similarly, in the state of Jammu and Kashmir, failure of governmental function results in "Governor's rule", imposed by invoking Section 92 of the Constitution of Jammu and Kashmir. The state’s governor issues the proclamation, after obtaining the consent of the President of India. If it is not possible to revoke Governor's rule within six months of imposition, the President's Rule under Article 356 of the Indian Constitution is imposed. There is little practical difference between the two provisions.

Following its landmark judgment in the 1994 "Bommai" case, the Supreme Court of India has restricted arbitrary impositions of President's rule. 

Chhattisgarh and Telangana are the only states where President's rule has yet to be imposed. However, while Telangana was part of Andhra Pradesh, it was under President's rule.

French administration in Indochina began on June 5, 1862. Following the defeat of Vietnamese, the Treaty of Saigon ceded Cochinchina's three eastern provinces. Later, the French forced Emperor Tự Đức to place Cambodia under French protection. On June 18, 1867, the French seized the rest of Cochinchina and conquered the Mekong Delta and later Hanoi. By 1897, France controlled all of Indochina.

Officially, each of the provinces – Cambodia, Laos, Annam, Tonkin, Cochinchina and Kouang-Tchéou-Wan – had different legal statuses. In practice, however, all were ruled directly. The French adopted a policy of assimilation rather than association. The Declaration of Rights of Man was based on the principle of egalité, liberté and fraternité for all subjects and citizens of France, and the colonies could not be an exception. French language was to be the language of administration. The whole Indochina would be “Frenchized”. Napoleonic Code was introduced in 1879 into the five provinces, sweeping away the Confucianism that has existed for centuries in Indochina.

The Parliament of the United Kingdom has granted powers to the Scottish Parliament, the National Assembly for Wales, the Northern Ireland Assembly and the London Assembly and to their associated executive bodies. This devolution may be suspended and replaced by direct rule by the Government of the United Kingdom.

Direct rule occurred in Northern Ireland from 1972 to 1998 during the Troubles, and for shorter periods between then and 2007. Major policy was determined by the British Government's Northern Ireland Office, under the direction of the Secretary of State for Northern Ireland. Legislation was introduced, amended, or repealed by means of Order in Council. Everyday matters were handled by government departments within Northern Ireland itself, and Northern Ireland continued to elect members of parliament to the Parliament of the United Kingdom.

Direct colonial rule was a form of colonialism that involves the establishment of a centralized foreign authority within a territory, which is run by colonial officials. The native population may be excluded from all but the lowest level of the colonial government.

Indirect rule is a system of government used by the British and French to control parts of their colonial empires, particularly in Africa and Asia, through pre-existing local power structures. These dependencies were often called "protectorates" or "trucial states". By this system, the day-to-day government and administration of areas both small and large was left in the hands of traditional rulers, who gained prestige and the stability and protection afforded by the Pax Britannica, at the cost of losing control of their external affairs, and often of taxation, communications, and other matters, usually with a small number of European "advisors" effectively overseeing the government of large numbers of people spread over extensive areas.


</doc>
<doc id="30875" url="https://en.wikipedia.org/wiki?curid=30875" title="Theocracy">
Theocracy

Theocracy is defined by the Webster's Encyclopedic Unabridged Dictionary of the English Language as "a form of government in which God or a deity is recognized as the supreme civil ruler. His laws being interpreted by the ecclesiastical authorities." The Merriam-Webster Dictionary defines it as a "government of a state by immediate divine guidance or by officials who are regarded as divinely guided", while the Oxford English Dictionary defines it as "a system of government in which priests rule in the name of God or a god."Therefore, states such as the Papal States or the German prince-bishoprics, while ruled by a cleric, were not theocracies since their civil government was run on the same lines as secular states and did not claim immediate divine guidance.

The word "theocracy" originates from the Greek θεοκρατία meaning "the rule of God". This in turn derives from θεός ("theos"), meaning "god", and κρατέω ("krateo"), meaning "to rule". Thus the meaning of the word in Greek was "rule by god(s)" or human incarnation(s) of god(s).

The term was initially coined by Flavius Josephus in the first century A.D. to describe the characteristic government of the Jews. Josephus argued that while mankind had developed many forms of rule, most could be subsumed under the following three types: monarchy, oligarchy, and democracy. The government of the Jews, however, was unique. Josephus offered the term "theocracy" to describe this polity, ordained by Moses, in which God is sovereign and his word is law.

Josephus' definition was widely accepted until the Enlightenment era, when the term started to collect more universalistic and negative connotations, especially in Hegel's hands. The first recorded English use was in 1622, with the meaning "sacerdotal government under divine inspiration" (as in Biblical Israel before the rise of kings); the meaning "priestly or religious body wielding political and civil power" is recorded from 1825.

In some religions, the ruler, usually a king, was regarded as the chosen favorite of God (or gods) who could not be questioned, sometimes even being the descendant of, or a god in their own right. Today, there is also a form of government where clerics have the power and the supreme leader could not be questioned in action. From the perspective of the theocratic government, "God himself is recognized as the head" of the state, hence the term "theocracy", from the Koine Greek "rule of God", a term used by Josephus for the kingdoms of Israel and Judah.
Taken literally, "theocracy" means rule by God or gods and refers primarily to an internal "rule of the heart", especially in its biblical application. The common, generic use of the term, as defined above in terms of rule by a church or analogous religious leadership, would be more accurately described as an "ecclesiocracy".

In a pure theocracy, the civil leader is believed to have a personal connection with the civilization's religion or belief. For example, Moses led the Israelites, and Muhammad led the early Muslims. There is a fine line between the tendency of appointing religious characters to run the state and having a religious-based government. According to the Holy Books, Prophet Joseph was offered an essential governmental role just because he was trustworthy, wise and knowledgeable (Quran 12: 54–55). As a result of the Prophet Joseph's knowledge and also due to his ethical and genuine efforts during a critical economic situation, the whole nation was rescued from a seven-year drought (Quran 12: 47–48).

When religions have a "holy book", it is used as a direct message from God. Law proclaimed by the ruler is also considered a divine revelation, and hence the law of God. As to the Prophet Muhammad ruling, "The first thirteen of the Prophet's twenty-three year career went on totally apolitical and non-violent. This attitude partly changed only after he had to flee from Mecca to Medina.This "hijra", or migration, would be a turning point in the Prophet's mission and would mark the very beginning of the Muslim calendar. Yet the Prophet did not establish a theocracy in Medina. Instead of a polity defined solely by Islam, he founded a territorial polity based on religious pluralism. This is evident in a document called the ’Charter of Medina’, which the Prophet signed with the leaders of the other community in the city."

According to the Quran, Prophets were not after power or material resources. For example in surah 26 verses (109, 127, 145, 164, 180), the Koran repeatedly quotes from Prophets, Noah, Hud, Salih, Lut, and Shu'aib that: "I do not ask you for it any payment; my payment is only from the Lord of the worlds." While, in theocracy many aspects of the holy book are overshadowed by material powers. Due to be considered divine, the regime entitles itself to interpret verses to its own benefit and abuse them out of the context for its political aims. An ecclesiocracy, on the other hand, is a situation where the religious leaders assume a leading role in the state, but do not claim that they are instruments of divine revelation. For example, the prince-bishops of the European Middle Ages, where the bishop was also the temporal ruler. Such a state may use the administrative hierarchy of the religion for its own administration, or it may have two "arms"—administrators and clergy—but with the state administrative hierarchy subordinate to the religious hierarchy. The papacy in the Papal States occupied a middle ground between theocracy and ecclesiocracy, since the pope did not claim he was a prophet who received revelation from God and translated it into civil law.

Religiously endorsed monarchies fall between these two poles, according to the relative strengths of the religious and political organs.

Theocracy is distinguished from other, secular forms of government that have a state religion, or are influenced by theological or moral concepts, and monarchies held "By the Grace of God". In the most common usage of the term, some civil rulers are leaders of the dominant religion (e.g., the Byzantine emperor as patron and defender of the official Church); the government proclaims it rules on behalf of God or a higher power, as specified by the local religion, and divine approval of government institutions and laws. These characteristics apply also to a caesaropapist regime. The Byzantine Empire however was not theocratic since the patriarch answered to the emperor, not vice versa; similarly in Tudor England the crown forced the church to break away from Rome so the royal (and, especially later, parliamentary) power could assume full control of the now Anglican hierarchy and confiscate most church property and income.

Secular governments can also co-exist with a state religion or delegate some aspects of civil law to religious communities. For example, in Israel marriage is governed by officially recognized religious bodies who each provide marriage services for their respected adherents, yet no form of civil marriage (free of religion, for atheists, for example) exists nor marriage by non-recognized minority religions.

Following the Capture of Rome on 20 September 1870, the Papal States including Rome with the Vatican were annexed by the Kingdom of Italy. In 1929, with the Lateran Treaty signed with the Italian Government, the new state of Vatican City (population 842) – with no connection with the former Papal States – was formally created and recognized as an independent state. The head of state of the Vatican is the pope, elected by the College of Cardinals, an assembly of Senatorial-princes of the Church. They are usually clerics, appointed as Ordinaries, but in the past have also included men who were not bishops nor clerics. A pope is elected for life, and either dies or may resign. The cardinals are appointed by the popes, who therefore choose the electors of their successors.

Voting is limited to cardinals under 80 years of age. A Secretary for Relations with States, directly responsible for international relations, is appointed by the pope. The Vatican legal system is rooted in canon law but ultimately is decided by the pope; the Bishop of Rome as the Supreme Pontiff, "has the fullness of legislative, executive and judicial powers." Although the laws of Vatican City come from the secular laws of Italy, under article 3 of the Law of the Sources of the Law, provision is made for the supplementary application of the "laws promulgated by the Kingdom of Italy". The government of the Vatican can also be considered an ecclesiocracy (ruled by the Church).

Mount Athos is a mountain peninsula in Greece which is an Eastern Orthodox autonomous region consisting of 20 monasteries under the direct jurisdiction of the Ecumenical Patriarch of Constantinople. There has been almost 1,800-years of continuous Christian presence on Mount Athos and it has a long history of monastic traditions, which dates back to at least 800 A.D. The origins of self-rule are originally from an edict by the Byzantine Emperor Ioannis Tzimisces in 972, which was later reaffirmed by the Emperor Alexios I Komnenos in 1095. After Greece's independence from the Ottoman Empire, Greece claimed mount Athos but after a diplomatic dispute with Russia the region was formally recognised as Greek after World War 1.

Mount Athos is specifically exempt from the free movement of people and goods required by Greece's membership of the European Union and entrance is only allowed with express permission from the monks. The number of daily visitors to Mount Athos is restricted, with all visitors required to obtain an entrance permit. Only men are permitted to visit and Orthodox Christians take precedence in permit issuing. Residents of Mount Athos must be men aged 18 and over who are members of the Eastern Orthodox Church and also either monks or workers.

Athos is governed jointly by a 'Holy Community' consisting of representatives from the 20 monasteries and a Civil Governor, appointed by the Greek Ministry of Foreign Affairs. The Holy Community also has a four-member executive committee called the 'Holy Administration' which is led by a Protos.

Iran has been described as a "theocratic republic" (by the CIA World Factbook), and its constitution has been described as a "hybrid" of "theocratic and democratic elements" by Francis Fukuyama. Like other Islamic states, it maintains religious laws and has religious courts to interpret all aspects of law. According to Iran's constitution, "all civil, penal, financial, economic, administrative, cultural, military, political, and other laws and regulations must be based on Islamic criteria."

In addition, Iran has a religious ruler and many religious officials in powerful government posts. The head of state, or "Supreme Leader", is a "faqih" (scholar of Islamic law),
and possesses more power than Iran's president. The Leader appoints the heads of many powerful posts: the commanders of the armed forces, the director of the national radio and television network, the heads of the powerful major religious foundations, the chief justice, the attorney general (indirectly through the chief justice), special tribunals, and members of national security councils dealing with defence and foreign affairs. He also co-appoints the 12 jurists of the Guardian Council.

The Leader is elected by the Assembly of Experts which is made up of mujtahids, who are Islamic scholars competent in interpreting "Sharia".

The Guardian Council, has the power to veto bills from majlis (parliament), approve or disapprove candidates who wish to run for high office (president, majlis, the Assembly of Experts). The council supervises elections, and can greenlight or ban investigations into the election process. Six of the Guardians (half the council) are faqih empowered to approve or veto all bills from the majlis (parliament) according to whether the faqih believe them to be in accordance with Islamic law and customs ("Sharia"). The other six members are lawyers appointed by the head of the judiciary (who is also a cleric and also appointed by the Leader).

An Islamic republic is the name given to several states that are officially ruled by Islamic laws, including the Islamic Republics of Afghanistan, Iran, Pakistan, and Mauritania. Pakistan first adopted the title under the constitution of 1956. Mauritania adopted it on 28 November 1958. Iran adopted it after the 1979 Iranian Revolution that overthrew the Pahlavi dynasty. Afghanistan adopted it in 2004 after the fall of the Taliban government. Despite having similar names the countries differ greatly in their governments and laws.

The term "Islamic republic" has come to mean several different things, some contradictory to others. To some Muslim religious leaders in the Middle East and Africa who advocate it, an Islamic republic is a state under a particular Islamic form of government. They see it as a compromise between a purely Islamic caliphate and secular nationalism and republicanism. In their conception of the Islamic republic, the penal code of the state is required to be compatible with some or all laws of Sharia, and the state may not be a monarchy, as many Middle Eastern states are presently.

The Central Tibetan Administration, colloquially known as the Tibetan government in exile, is a Tibetan exile organisation with a state-like internal structure. According to its charter, the position of head of state of the Central Tibetan Administration belongs "ex officio" to the current Dalai Lama, a religious hierarch. In this respect, it continues the traditions of the former government of Tibet, which was ruled by the Dalai Lamas and their ministers, with a specific role reserved for a class of monk officials.

On March 14, 2011, at the 14th Dalai Lama's suggestion, the parliament of the Central Tibetan Administration began considering a proposal to remove the Dalai Lama's role as head of state in favor of an elected leader.

The first directly elected Kalön Tripa was Samdhong Rinpoche, who was elected on August 20, 2001.

Before 2011, the Kalön Tripa position was subordinate to the 14th Dalai Lama who presided over the government in exile from its founding. In August of that year, Lobsang Sangay polled 55 percent votes out of 49,189, defeating his nearest rival Tethong Tenzin Namgyal by 8,646 votes, becoming the second popularly elected Kalon Tripa. The Dalai Lama announced that his political authority would be transferred to Sangay.

On September 20, 2012, the 15th Tibetan Parliament-in-Exile unanimously voted to change the title of Kalön Tripa to "Sikyong" in Article 19 of the Charter of the Tibetans in exile and relevant articles. The Dalai Lama had previously referred to the Kalon Tripa as Sikyong, and this usage was cited as the primary justification for the name change. According to "Tibetan Review", "Sikyong" translates to "political leader", as distinct from "spiritual leader". Foreign affairs Kalon Dicki Chhoyang stated that the term "Sikyong" has had a precedent dating back to the 7th Dalai Lama, and that the name change "ensures historical continuity and legitimacy of the traditional leadership from the fifth Dalai Lama". The online Dharma Dictionary translates sikyong ("srid skyong") as "secular ruler; regime, regent". The title "sikyong" had previously been used by regents who ruled Tibet during the Dalai Lama's minority.

Having a state religion is not sufficient enough to be a theocracy in the narrow sense of the term. Many countries have a state religion without the government directly deriving its powers from a divine authority or a religious authority directly exercising governmental powers. Since the narrow sense of the term has few instances in the modern world, the more common usage of it is the wider sense of an enforced state religion.

The pharaoh was the offspring of the sungod Ra.

The emperor was the offspring of the Shinto sungoddess Amaterasu.

The imperial cults in Ancient Egypt and the Roman Empire, as well as numerous other monarchies, deified the ruling monarch. The state religion was often dedicated to the worship of the ruler as a deity, or the incarnation thereof.

Early Israel was ruled by Judges before instituting a monarchy. The Judges were believed to be representatives of YHVH Yahweh (also translated as, Jehovah).

In ancient and medieval Christianity, Caesaropapism is the doctrine where a head of state is at the same time the head of the church.

Unified religious rule in Tibet began in 1642, when the Fifth Dalai Lama allied with the military power of the Mongol Gushri Khan to consolidate the political power and center control around his office as head of the Gelug school.
This form of government is known as the dual system of government. Prior to 1642, particular monasteries and monks had held considerable power throughout Tibet, but had not achieved anything approaching complete control, though power continued to be held in a diffuse, feudal system after the ascension of the Fifth Dalai Lama. Power in Tibet was held by a number of traditional elites, including members of the nobility, the heads of the major Buddhist sects (including their various tulkus), and various large and influential monastic communities.

Political power was sometimes used by monastic leaders to suppress rival religious schools through the confiscation of property and direct violence. Social mobility was somewhat possible through the attainment of a monastic education, or recognition as a reincarnated teacher, but such institutions were dominated by the traditional elites and governed by political intrigue. Non-Buddhists in Tibet were members of an outcast underclass.

The Bogd Khaanate period of Mongolia (1911–19) is also cited as a former Buddhist theocracy.

Similar to the Roman Emperor, the Chinese sovereign was historically held to be the Son of Heaven. However, from the first historical Emperor on, this was largely ceremonial and tradition quickly established it as a posthumous dignity, like the Roman institution. The situation before Qin Shi Huang Di is less clear.

The Shang dynasty essentially functioned as a theocracy, declaring the ruling family the sons of heaven and calling the chief sky god Shangdi after a word for their deceased ancestors. After their overthrow by the Zhou, the royal clan of Shang were not eliminated but instead moved to a ceremonial capital where they were charged to continue the performance of their rituals.

The titles combined by Shi Huangdi to form his new title of emperor were originally applied to god-like beings who ordered the heavens and earth and to culture heroes credited with the invention of agriculture, clothing, music, astrology, &c. Even after the fall of Qin, an emperor's words were considered sacred edicts () and his written proclamations "directives from above" ().

As a result, some Sinologists translate the title "huangdi" (usually rendered "emperor") as thearch. The term properly refers to the head of a thearchy (a kingdom of gods), but the more accurate "theocrat" carries associations of a strong priesthood that would be generally inaccurate in describing imperial China. Others reserve the use of "thearch" to describe the legendary figures of Chinese prehistory while continuing to use "emperor" to describe historical rulers.

The Heavenly Kingdom of Great Peace in 1860s Qing China was a heterodox Christian theocracy led by a person who said that he was the younger brother of Jesus Christ, Hong Xiuquan. This theocratic state fought one of the most destructive wars in history, the Taiping Rebellion, against the Qing dynasty for fifteen years before being crushed following the fall of the rebel capital Nanjing.

The Sunni branch of Islam stipulates that, as a head of state, a Caliph should be elected by Muslims or their representatives. Followers of Shia Islam, however, believe a Caliph should be an Imam chosen by God from the Ahl al-Bayt (the "Family of the House", Muhammad's direct descendants).

The Byzantine Empire ( 324–1453) operated under caesaropapism, meaning that the emperor was both the head of civil society and the ultimate authority over the ecclesiastical authorities, or patriarchates. The emperor was considered to be God's omnipotent representative on earth and he ruled as an absolute autocrat.

Jennifer Fretland VanVoorst argues, “the Byzantine Empire became a theocracy in the sense that Christian values and ideals were the foundation of the empire's political ideals and heavily entwined with its political goals". Steven Runciman says in his book on "The Byzantine Theocracy" (2004):
Historians debate the extent to which Geneva, Switzerland, in the days of John Calvin (1509–64) was a theocracy. On the one hand, Calvin's theology clearly called for separation between church and state. Other historians have stressed the enormous political power wielded on a daily basis by the clerics.

In nearby Zurich, Switzerland, Protestant reformer Huldrych Zwingli (1484-1531) built a political system that many scholars have called a theocracy, while others have denied it.

The question of theocracy has been debated at extensively by historians regarding the Mormon communities in Illinois, and especially in Utah.

Joseph Smith, mayor of Nauvoo, Illinois, and founder of the Latter Day Saint movement, ran as an independent for president in 1844. He proposed the redemption of slaves by selling public lands; reducing the size and salary of Congress; the closure of prisons; the annexation of Texas, Oregon, and parts of Canada; the securing of international rights on high seas; free trade; and the re-establishment of a national bank. His top aide Brigham Young campaigned for Smith saying, "He it is that God of Heaven designs to save this nation from destruction and preserve the Constitution." The campaign ended when Smith was killed by a mob while in the Carthage, Illinois, jail on June 27, 1844.

After severe persecution, the Mormons left the United States and resettled in a remote part of Utah, which was then part of Mexico. However the United States took control in 1848 and would not accept polygamy. The Mormon State of Deseret was short-lived. Its original borders stretched from western Colorado to the southern California coast. When the Mormons arrived in the valley of the Great Salt Lake in 1847, the Great Basin was still a part of Mexico and had no secular government. As a result, Brigham Young administered the region both spiritually and temporally through the highly organized and centralized Melchizedek Priesthood. This original organization was based upon a concept called theodemocracy, a governmental system combining Biblical theocracy with mid-19th-century American political ideals.

In 1849, the Saints organized a secular government in Utah, although many ecclesiastical leaders maintained their positions of secular power. The Mormons also petitioned Congress to have Deseret admitted into the Union as a state. However, under the Compromise of 1850, Utah Territory was created and Brigham Young was appointed governor. In this situation, Young still stood as head of The Church of Jesus Christ of Latter-day Saints (LDS Church) as well as Utah's secular government.

After the abortive Utah War of 1857–1858, the replacement of Young by an outside Federal Territorial Governor, intense federal prosecution of LDS Church leaders, and the eventual resolution of controversies regarding plural marriage, and accession by Utah to statehood, the apparent temporal aspects of LDS theodemocracy receded markedly.

During the Achaemenid Empire, Zoroastrianism was the state religion and included formalized worship. The Persian kings were known to be pious Zoroastrians and also ruled with a Zoroastrian form of law called "asha". However, Cyrus the Great, who founded the empire, avoided imposing the Zoroastrian faith on the inhabitants of conquered territory. Cyrus's kindness towards Jews has been cited as sparking Zoroastrian influence on Judaism.

Under the Seleucids, Zoroastrianism became autonomous. During the Sassanid period, the Zoroastrian calendar was reformed, image-use was banned, Fire Temples were increasingly built and intolerance towards other faiths prevailed.

The short reign (1494–1498) of Girolamo Savonarola, a Dominican priest, over the city of Florence had features of a theocracy. During his rule, "un-Christian" books, statues, poetry, and other items were burned (in the Bonfire of the Vanities), sodomy was made a capital offense, and other Christian practices became law.




</doc>
<doc id="233475" url="https://en.wikipedia.org/wiki?curid=233475" title="Multi-party system">
Multi-party system

A multi-party system is a system in which multiple political parties across the political spectrum run for national election, and all have the capacity to gain control of government offices, separately or in coalition. Apart from one-party-dominant and two-party systems, multi-party systems tend to be more common in parliamentary systems than presidential systems and far more common in countries that use proportional representation compared to countries that use first-past-the-post elections. 

First-past-the-post requires concentrated areas of support for large representation in the legislature whereas proportional representation better reflects the range of a population's views. Proportional systems may have multi-member districts with more than one representative elected from a given district to the same legislative body, and thus a greater number of viable parties. Duverger's law states that the number of viable political parties is one, plus the number of seats available in the given district. 

Argentina, Armenia, Austria, Belgium, Brazil, Croatia, Denmark, Finland, France, Germany, Iceland, India, Indonesia, Ireland, Israel, Italy, Lebanon, Maldives, Mexico, Moldova, Nepal, the Netherlands, New Zealand, Norway, Pakistan, the Philippines, Poland, Portugal, Romania, Serbia, Spain, Sri Lanka, Sweden, Switzerland, Taiwan, Tunisia and Ukraine are examples of nations that have used a multi-party system effectively in their democracies. In these countries, usually no single party has a parliamentary majority by itself. Instead, multiple political parties are compelled to form compromised coalitions for the purpose of developing power blocks and attaining legitimate mandate.

A system where only two parties have the possibility of winning an election is called two-party system. A system where only three parties have a "realistic possibility" of winning an election or forming a coalition is sometimes called a "Third-party system". But, in some cases the system is called a "Stalled Third-Party System," when there are three parties and all three parties win a large number of votes, but only two have a chance of winning an election. Usually this is because the electoral system penalizes the third party, e.g. as in Canadian or UK politics. In the 2010 elections, the Liberal Democrats gained 23% of the total vote but won less than 10% of the seats due to the first-past-the-post electoral system. Despite this, they still had enough seats (and enough public support) to form coalitions with one of the two major parties, or to make deals in order to gain their support. An example is the Conservative-Liberal Democrat coalition formed after the 2010 general election. Another is the Lib-Lab pact during Prime Minister James Callaghan's Minority Labor Government; when Labor lost its three-seat majority in 1977, the pact fell short of a full coalition. In Canada, there are three major federal political parties; the Conservative Party of Canada, the Liberal Party of Canada, and the New Democratic Party of Canada (also known as the NDP). The NDP is currently in alliance with another party, the Green Party of Canada. However, the Liberals and Conservatives have been the only two parties to form government in Canada with the New Democrats as the third party, except in the 2011 Canadian election when the New Democrats were the Official Opposition.

Unlike a one-party system (or a two-party system), a multi-party system encourages the general constituency to form multiple distinct, officially recognized groups, generally called political parties. Each party competes for votes from the enfranchised constituents (those allowed to vote). To vote in most countries, you must be at least 18 years old or older. A multi-party system prevents the leadership of a single party from controlling a single legislative chamber without challenge, as we have learned that one party should not have too much power, lest they try to take over.

If the government includes an elected Congress or Parliament, the parties may share power according to proportional representation or the first-past-the-post system. In proportional representation, each party wins a number of seats proportional to the number of votes it receives. In first-past-the-post, the electorate is divided into a number of districts, each of which selects one person to fill one seat by a plurality of the vote. First-past-the-post is not conducive to a proliferation of parties, and naturally gravitates toward a two-party system, in which only two parties have a real chance of electing their candidates to office. This gravitation is known as Duverger's law. Proportional representation, on the other hand, does not have this tendency, and allows multiple major parties to arise. But, recent coalition governments, such as that in the U.K., represent two-party systems rather than multi-party systems. This is regardless of the number of parties in government.

A two-party system requires voters to align themselves in large blocs, sometimes so large that they cannot agree on any overarching principles. Some theories argue that this allows centrists to gain control. On the other hand, if there are multiple major parties, each with less than a majority of the vote, the parties are strongly motivated to work together to form working governments. This also promotes centrism, as well as promoting coalition-building skills while discouraging polarization.



</doc>
<doc id="240683" url="https://en.wikipedia.org/wiki?curid=240683" title="Caretaker government">
Caretaker government

A caretaker government is a temporary government that rules a country for a short time until a regular government is elected.

Caretaker governments may be put in place when a government in a parliamentary system is defeated in a motion of no confidence, or in the case when the house to which the government is responsible is dissolved, to be in place for an interim period until an election is held and a new government is formed. In this sense, in some countries which use a Westminster system of government, the caretaker government is simply the incumbent government, which continues to operate in the interim period between the normal dissolution of parliament for the purpose of holding an election and the formation of a new government after the election results are known. Unlike in ordinary times, the caretaker government's activities are limited by custom and convention.

In systems where coalition governments are frequent a caretaker government may be installed temporarily while negotiations to form a new coalition take place. This usually occurs either immediately after an election in which there is no clear victor or if one coalition government collapses and a new one must be negotiated. Caretaker governments are expected to handle daily issues and prepare budgets for discussion, but are not expected to produce a government platform or introduce controversial bills.

A caretaker government is often set up following a war until stable democratic rule can be restored, or installed, in which case it is often referred to as a provisional government.

Many countries are administered by a caretaker government during election periods, such as:
Other countries that use similar mechanisms include Pakistan and New Zealand.

Heads of caretaker governments are often referred to as a "caretaker" head, for example "caretaker prime minister".

Similarly, but chiefly in the United States, caretakers are individuals who fill seats in government temporarily without ambitions to continue to hold office on their own. This is particularly true with regard to United States Senators who are appointed to office by the Governor of their state following a vacancy created by the death or resignation of a sitting senator. Sometimes governors wish to run for the seat themselves in the next election but do not want to be accused of unfairness by arranging their own appointments in the interim. Also, sometimes they do not wish to be seen as taking sides within a group of party factions or prejudicing of a primary election by picking someone who is apt to become an active candidate for the position. At one time, widows of politicians were often selected as caretakers to succeed their late husbands; this custom is rarely exercised today, as it could be viewed by some as nepotism.

In Canada and most other English-speaking countries, the more widely accepted term in this context is "interim", as in interim leader. In Italy this kind of premier is the President of Government of Experts.

The following is a list of individuals who have been considered caretaker (or provisional or interim) heads of state or heads of government:





</doc>
<doc id="334803" url="https://en.wikipedia.org/wiki?curid=334803" title="Public sector">
Public sector

The public sector (also called the state sector) is the part of the economy composed of both public services and public enterprises. 

Public services include public goods and governmental services such as the military, Law enforcement, infrastructure (public roads, bridges, tunnels, water supply, sewers, electrical grids, telecommunications, etc.), public transit, public education, along with health care and those working for the government itself, such as elected officials. The public sector might provide services that a non-payer cannot be excluded from (such as street lighting), services which benefit all of society rather than just the individual who uses the service. Public enterprises, or state-owned enterprises, are self-financing commercial enterprises that are under public ownership which provide various private goods and services for sale and usually operate on a commercial basis.

Organizations that are not part of the public sector are either a part of the private sector or voluntary sector. The private sector is composed of the economic sectors that are intended to earn a profit for the owners of the enterprise. The voluntary, civic or social sector concerns a diverse array of non-profit organizations emphasizing civil society.

The organization of the public sector can take several forms, including:


A borderline form is as follows:




</doc>
<doc id="6784" url="https://en.wikipedia.org/wiki?curid=6784" title="Citizenship">
Citizenship

Citizenship is the status of a person recognized under the custom or law as being a legal member of a sovereign state or belonging to a nation.

A person may have multiple citizenships. A person who does not have citizenship of any state is said to be stateless, while one who lives on state borders whose territorial status is uncertain is a border-lander.

Nationality is often used as a synonym for citizenship in English – notably in international law – although the term is sometimes understood as denoting a person's membership of a nation (a large ethnic group). In some countries, e.g. the United States, the United Kingdom, nationality and citizenship can have different meanings (for more information, see Nationality versus citizenship).

Each country has its own policies, regulations and criteria as to who is entitled to its citizenship. A person can be recognized or granted citizenship on a number of bases. Usually citizenship based on circumstances of birth is automatic, but in other cases an application may be required.


Many thinkers point to the concept of citizenship beginning in the early city-states of ancient Greece, although others see it as primarily a modern phenomenon dating back only a few hundred years and, for humanity, that the concept of citizenship arose with the first laws. "Polis" meant both the political assembly of the city-state as well as the entire society. Citizenship has generally been identified as a western phenomenon. There is a general view that citizenship in ancient times was a simpler relation than modern forms of citizenship, although this view has come under scrutiny. The relation of citizenship has not been a fixed or static relation, but constantly changed within each society, and that according to one view, citizenship might "really have worked" only at select periods during certain times, such as when the Athenian politician Solon made reforms in the early Athenian state.

Historian Geoffrey Hosking in his 2005 "Modern Scholar" lecture course suggested that citizenship in ancient Greece arose from an appreciation for the importance of freedom. Hosking explained:

Slavery permitted slaveowners to have substantial free time, and enabled participation in public life. Polis citizenship was marked by exclusivity. Inequality of status was widespread; citizens (πολίτης "politēs" < πόλις 'city') had a higher status than non-citizens, such as women, slaves, and resident foreigners (metics). The first form of citizenship was based on the way people lived in the ancient Greek times, in small-scale organic communities of the polis. Citizenship was not seen as a separate activity from the private life of the individual person, in the sense that there was not a distinction between public and private life. The obligations of citizenship were deeply connected into one's everyday life in the polis. These small-scale organic communities were generally seen as a new development in world history, in contrast to the established ancient civilizations of Egypt or Persia, or the hunter-gatherer bands elsewhere. From the viewpoint of the ancient Greeks, a person's public life was not separated from their private life, and Greeks did not distinguish between the two worlds according to the modern western conception. The obligations of citizenship were deeply connected with everyday life. To be truly human, one had to be an active citizen to the community, which Aristotle famously expressed: "To take no part in the running of the community's affairs is to be either a beast or a god!" This form of citizenship was based on obligations of citizens towards the community, rather than rights given to the citizens of the community. This was not a problem because they all had a strong affinity with the polis; their own destiny and the destiny of the community were strongly linked. Also, citizens of the polis saw obligations to the community as an opportunity to be virtuous, it was a source of honour and respect. In Athens, citizens were both ruler and ruled, important political and judicial offices were rotated and all citizens had the right to speak and vote in the political assembly.

In the Roman Empire, citizenship expanded from small-scale communities to the entire empire. Romans realized that granting citizenship to people from all over the empire legitimized Roman rule over conquered areas. Roman citizenship was no longer a status of political agency, as it had been reduced to a judicial safeguard and the expression of rule and law. Rome carried forth Greek ideas of citizenship such as the principles of equality under the law, civic participation in government, and notions that "no one citizen should have too much power for too long", but Rome offered relatively generous terms to its captives, including chances for lesser forms of citizenship. If Greek citizenship was an "emancipation from the world of things", the Roman sense increasingly reflected the fact that citizens could act upon material things as well as other citizens, in the sense of buying or selling property, possessions, titles, goods. One historian explained:

Roman citizenship reflected a struggle between the upper-class patrician interests against the lower-order working groups known as the plebeian class. A citizen came to be understood as a person "free to act by law, free to ask and expect the law's protection, a citizen of such and such a legal community, of such and such a legal standing in that community". Citizenship meant having rights to have possessions, immunities, expectations, which were "available in many kinds and degrees, available or unavailable to many kinds of person for many kinds of reason". The law itself was a kind of bond uniting people. Roman citizenship was more impersonal, universal, multiform, having different degrees and applications.

During the European Middle Ages, citizenship was usually associated with cities and towns, and applied mainly to middle class folk. Titles such as burgher, grand burgher (German "Großbürger") and bourgeoisie denoted political affiliation and identity in relation to a particular locality, as well as membership in a mercantile or trading class; thus, individuals of respectable means and socioeconomic status were interchangeable with citizens.

During this era, members of the nobility had a range of privileges above commoners (see aristocracy), though political upheavals and reforms, beginning most prominently with the French Revolution, abolished privileges and created an egalitarian concept of citizenship.

During the Renaissance, people transitioned from being subjects of a king or queen to being citizens of a city and later to a nation. Each city had its own law, courts, and independent administration. And being a citizen often meant being subject to the city's law in addition to having power in some instances to help choose officials. City dwellers who had fought alongside nobles in battles to defend their cities were no longer content with having a subordinate social status, but demanded a greater role in the form of citizenship. Membership in guilds was an indirect form of citizenship in that it helped their members succeed financially. The rise of citizenship was linked to the rise of republicanism, according to one account, since independent citizens meant that kings had less power. Citizenship became an idealized, almost abstract, concept, and did not signify a submissive relation with a lord or count, but rather indicated the bond between a person and the state in the rather abstract sense of having rights and duties.

The modern idea of citizenship still respects the idea of political participation, but it is usually done through "elaborate systems of political representation at a distance" such as representative democracy. Modern citizenship is much more passive; action is delegated to others; citizenship is often a constraint on acting, not an impetus to act. Nevertheless, citizens are usually aware of their obligations to authorities, and are aware that these bonds often limit what they can do.

From 1790 until the mid-twentieth century, United States law used racial criteria to establish citizenship rights and regulate who was eligible to become a naturalized citizen. The Naturalization Act of 1790, the first law in U.S. history to establish rules for citizenship and naturalization, barred citizenship to all people who were not of European descent, stating that "any alien being a free white person, who shall have resided within the limits and under the jurisdiction of the United States for the term of two years, may be admitted to become a citizen thereof."

Under early U.S. laws, African Americans were not eligible for citizenship. In 1857, these laws were upheld in the US Supreme Court case "Dred Scott v. Sandford", which ruled that "a free negro of the African race, whose ancestors were brought to this country and sold as slaves, is not a 'citizen' within the meaning of the Constitution of the United States," and that "the special rights and immunities guarantied to citizens do not apply to them."

It was not until the abolition of slavery following the American Civil War that African Americans were granted citizenship rights. The 14th Amendment to the U.S. Constitution, ratified on July 9, 1868, stated that "all persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside." Two years later, the Naturalization Act of 1870 would extend the right to become a naturalized citizen to include "aliens of African nativity and to persons of African descent".

Despite the gains made by African Americans after the Civil War, Native Americans, Asians, and others not considered "free white persons" were still denied the ability to become citizens. The 1882 Chinese Exclusion Act explicitly denied naturalization rights to all people of Chinese origin, while subsequent acts passed by the US Congress, such as laws in 1906, 1917, and 1924, would include clauses that denied immigration and naturalization rights to people based on broadly defined racial categories. Supreme Court cases such as "Ozawa v. United States" (1922) and "U.S. v. Bhagat Singh Thind" (1923), would later clarify the meaning of the phrase "free white persons," ruling that ethnically Japanese, Indian, and other non-European people were not "white persons", and were therefore ineligible for naturalization under U.S. law.

Native Americans were not granted full US citizenship until the passage of the Indian Citizenship Act in 1924. However, even well into the 1960s some state laws prevented Native Americans from exercising their full rights as citizens, such as the right to vote. In 1962, New Mexico became the last state to enfranchise Native Americans.

It was not until the passage of the Immigration and Nationality Act of 1952 that the racial and gender restrictions for naturalization were explicitly abolished. However, the act still contained restrictions regarding who was eligible for US citizenship, and retained a national quota system which limited the number of visas given to immigrants based on their national origin, to be fixed "at a rate of one-sixth of one percent of each nationality's population in the United States in 1920". It was not until the passage of the Immigration and Nationality Act of 1965 that these immigration quota systems were drastically altered in favor of a less discriminatory system.

The 1918 constitution of revolutionary Russia granted citizenship to any foreigners who were living within Russia, so long as they were "engaged in work and [belonged] to the working class." It recognized "the equal rights of all citizens, irrespective of their racial or national connections" and declared oppression of any minority group or race "to be contrary to the fundamental laws of the Republic." The 1918 constitution also established the right to vote and be elected to soviets for both men and women "irrespective of religion, nationality, domicile, etc. [...] who shall have completed their eighteenth year by the day of election." The later constitutions of the USSR would grant universal Soviet citizenship to the citizens of all member republics in concord with the principles of non-discrimination laid out in the original 1918 constitution of Russia.

National Socialism or "Nazism", the German variant of twentieth century fascism whose precepts were laid out in Adolf Hitler's Mein Kampf, classified inhabitants of the nation into three main hierarchical categories, each of which would have different rights and duties in relation to the state: citizens, subjects, and aliens. The first category, citizens, were to possess full civic rights and responsibilities. Citizenship would be conferred only on males of German (or so-called "Aryan") heritage who had completed military service, and could be revoked at any time by the state. The Reich Citizenship Law of 1935 established racial criteria for citizenship in the German Reich, and because of this law Jews and others who could not prove "German" racial heritage were stripped of their citizenship.

The second category, subjects, referred to all others who were born within the nation's boundaries who did not fit the racial criteria for citizenship. Subjects would have no voting rights, could not hold any position within the state, and possessed none of the other rights and civic responsibilities conferred on citizens. All women were to be conferred "subject" status upon birth, and could only obtain "citizen" status if they worked independently or if they married a German citizen (see women in Nazi Germany).

The final category, aliens, referred to those who were citizens of another state, who also had no rights.

Citizenship status, under social contract theory, carries with it both rights and duties. In this sense, citizenship was described as "a bundle of rights -- primarily, political participation in the life of the community, the right to vote, and the right to receive certain protection from the community, as well as obligations." Citizenship is seen by most scholars as culture-specific, in the sense that the meaning of the term varies considerably from culture to culture, and over time. In China, for example, there is a cultural politics of citizenship which could be called "peopleship". 

How citizenship is understood depends on the person making the determination. The relation of citizenship has never been fixed or static, but constantly changes within each society. While citizenship has varied considerably throughout history, and within societies over time, there are some common elements but they vary considerably as well. As a bond, citizenship extends beyond basic kinship ties to unite people of different genetic backgrounds. It usually signifies membership in a political body. It is often based on, or was a result of, some form of military service or expectation of future service. It usually involves some form of political participation, but this can vary from token acts to active service in government. 

Citizenship is a status in society. It is an ideal state as well. It generally describes a person with legal rights within a given political order. It almost always has an element of exclusion, meaning that some people are not citizens, and that this distinction can sometimes be very important, or not important, depending on a particular society. Citizenship as a concept is generally hard to isolate intellectually and compare with related political notions, since it relates to many other aspects of society such as the family, military service, the individual, freedom, religion, ideas of right and wrong, ethnicity, and patterns for how a person should behave in society. When there are many different groups within a nation, citizenship may be the only real bond which unites everybody as equals without discrimination—it is a "broad bond" linking "a person with the state" and gives people a universal identity as a legal member of a specific nation.

Modern citizenship has often been looked at as two competing underlying ideas:


Scholars suggest that the concept of citizenship contains many unresolved issues, sometimes called tensions, existing within the relation, that continue to reflect uncertainty about what citizenship is supposed to mean. Some unresolved issues regarding citizenship include questions about what is the proper balance between duties and rights. Another is a question about what is the proper balance between political citizenship versus social citizenship. Some thinkers see benefits with people being absent from public affairs, since too much participation such as revolution can be destructive, yet too little participation such as total apathy can be problematic as well. Citizenship can be seen as a special elite status, and it can also be seen as a democratizing force and something that everybody has; the concept can include both senses. According to sociologist Arthur Stinchcombe, citizenship is based on the extent that a person can control one's own destiny within the group in the sense of being able to influence the government of the group. One last distinction within citizenship is the so-called consent descent distinction, and this issue addresses whether citizenship is a fundamental matter determined by a person choosing to belong to a particular nation––by their consent––or is citizenship a matter of where a person was born––that is, by their descent.

Some intergovernmental organizations have extended the concept and terminology associated with citizenship to the international level, where it is applied to the totality of the citizens of their constituent countries combined. Citizenship at this level is a secondary concept, with rights deriving from national citizenship.

The Maastricht Treaty introduced the concept of citizenship of the European Union. Article 17 (1) of the Treaty on European Union stated that: Citizenship of the Union is hereby established. Every person holding the nationality of a Member State shall be a citizen of the Union. Citizenship of the Union shall be additional to and not replace national citizenship.

An agreement known as the amended EC Treaty established certain minimal rights for European Union citizens. Article 12 of the amended EC Treaty guaranteed a general right of non-discrimination within the scope of the Treaty. Article 18 provided a limited right to free movement and residence in Member States other than that of which the European Union citizen is a national. Articles 18-21 and 225 provide certain political rights.

Union citizens have also extensive rights to move in order to exercise economic activity in any of the Member States which predate the introduction of Union citizenship.

The concept of "Commonwealth Citizenship" has been in place ever since the establishment of the Commonwealth of Nations. As with the EU, one holds Commonwealth citizenship only by being a citizen of a Commonwealth member state. This form of citizenship offers certain privileges within some Commonwealth countries:

Although Ireland was excluded from the Commonwealth in 1949 because it declared itself a republic, Ireland is generally treated as if it were still a member. Legislation often specifically provides for equal treatment between Commonwealth countries and Ireland and refers to "Commonwealth countries and Ireland". Ireland's citizens are not classified as foreign nationals in the United Kingdom.

Canada departed from the principle of nationality being defined in terms of allegiance in 1921. In 1935 the Irish Free State was the first to introduce its own citizenship. However, Irish citizens were still treated as subjects of the Crown, and they are still not regarded as foreign, even though Ireland is not a member of the Commonwealth. The Canadian Citizenship Act of 1947 provided for a distinct Canadian Citizenship, automatically conferred upon most individuals born in Canada, with some exceptions, and defined the conditions under which one could become a naturalized citizen. The concept of Commonwealth citizenship was introduced in 1948 in the British Nationality Act 1948. Other dominions adopted this principle such as New Zealand, by way of the British Nationality and New Zealand Citizenship Act of 1948.

Citizenship most usually relates to membership of the nation state, but the term can also apply at the subnational level. Subnational entities may impose requirements, of residency or otherwise, which permit citizens to participate in the political life of that entity, or to enjoy benefits provided by the government of that entity. But in such cases, those eligible are also sometimes seen as "citizens" of the relevant state, province, or region. An example of this is how the fundamental basis of Swiss citizenship is citizenship of an individual commune, from which follows citizenship of a canton and of the Confederation. Another example is Åland where the residents enjoy a special provincial citizenship within Finland, "hembygdsrätt".

The United States has a federal system in which a person is a citizen of their specific state of residence, such as New Jersey or California, as well as a citizen of the United States. State constitutions may grant certain rights above and beyond what are granted under the United States Constitution and may impose their own obligations including the sovereign right of taxation and military service; each state maintains at least one military force subject to national militia transfer service, the state's national guard, and some states maintain a second military force not subject to nationalization.

"Active citizenship" is the philosophy that citizens should work towards the betterment of their community through economic participation, public, volunteer work, and other such efforts to improve life for all citizens. In this vein, citizenship education is taught in schools, as an academic subject in some countries. By the time children reach secondary education there is an emphasis on such unconventional subjects to be included in academic curriculum. While the diagram on citizenship to the right is rather facile and depth-less, it is simplified to explain the general model of citizenship that is taught to many secondary school pupils. The idea behind this model within education is to instill in young pupils that their actions (i.e. their vote) affect collective citizenship and thus in turn them.

It is taught in the Republic of Ireland as an exam subject for the Junior Certificate. It is known as Civic, Social and Political Education (CSPE). A new Leaving Certificate exam subject with the working title 'Politics & Society' is being developed by the National Council for Curriculum and Assessment (NCCA) and is expected to be introduced to the curriculum sometime after 2012.

Citizenship is offered as a General Certificate of Secondary Education (GCSE) course in many schools in the United Kingdom. As well as teaching knowledge about democracy, parliament, government, the justice system, human rights and the UK's relations with the wider world, students participate in active citizenship, often involving a social action or social enterprise in their local community.

There are two kinds of criticism of citizenship education in schools. Firstly, some philosophers of education argue that most governments and mainstream policies stimulate and advocate questionable approaches of citizenship education. These approaches aim to develop specific dispositions in students, dispositions conducive to political participation and solidarity. But there are radically different views on the nature of good citizenship and education should involve and develop autonomy and open-mindedness. Therefore, it requires a more critical approach than is possible when political participation and solidarity are conceived of as goals of education. Secondly, some educationalists argue that merely teaching children about the theory of citizenship is ineffective, unless schools themselves reflect democratic practices by giving children the opportunity to have a say in decision making. They suggest that schools are fundamentally undemocratic institutions, and that such a setting cannot instill in children the commitment and belief in democratic values that is necessary for citizenship education to have a proper impact. Some educationalists relate this criticism to John Dewey (see critical comments on this interpretation of Dewey: Van der Ploeg, 2016).






</doc>
<doc id="598010" url="https://en.wikipedia.org/wiki?curid=598010" title="Provisional government">
Provisional government

A provisional government, also called an interim government or transitional government, is an emergency governmental authority set up to manage a political transition generally in the cases of new nations or following the collapse of the previous governing administration. Provisional governments are generally appointed, and frequently arise, either during or after civil or foreign wars.

Provisional governments maintain power until a new government can be appointed by a regular political process, which is generally an election. They may be involved with defining the legal structure of subsequent regimes, guidelines related to human rights and political freedoms, the structure of the economy, government institutions, and international alignment. Provisional governments differ from caretaker governments, which are responsible for governing within an established parliamentary system and serve as placeholders following a motion of no confidence, or following the dissolution of the ruling coalition.

In opinion of Yossi Shain and Juan J. Linz, provisional governments can be classified to four groups:


The establishment of provisional governments is frequently tied to the implementation of transitional justice. Decisions related to transitional justice can determine who is allowed to participate in a provisional government.

The early provisional governments were created to prepare for the return of royal rule. Irregularly convened assemblies during the English Revolution, such as Confederate Ireland (1641–49), were described as "provisional". The Continental Congress, a convention of delegates from 13 British colonies on the east coast of North America became the provisional government of the United States in 1776, during the American Revolutionary War. The government shed its provisional status in 1781, following ratification of the Articles of Confederation, and continued until it was supplanted by the United States Congress in 1789.

The practice of using "provisional government" as part of a formal name can be traced to Talleyrand's government in France in 1814. In 1843, American pioneers in the Oregon Country, in the Pacific Northwest region of North America established the Provisional Government of Oregon—as the U.S. federal government had not yet extend its jurisdiction over the region—which existed until March 1849. The numerous provisional governments during the Revolutions of 1848 gave the word its modern meaning: A liberal government established to prepare for elections.

Numerous provisional governments have been established since the 1850s, including:
Provisional governments were also established throughout Europe as occupied nations were liberated from Nazi occupation by the Allies.



</doc>
<doc id="20949522" url="https://en.wikipedia.org/wiki?curid=20949522" title="Bureaucracy">
Bureaucracy

Bureaucracy () refers to both a body of non-elective government officials and an administrative policy-making group. Historically, a bureaucracy was a government administration managed by departments staffed with non-elected officials. Today, bureaucracy is the administrative system governing any large institution, whether publicly owned or privately owned. The public administration in many countries is an example of a bureaucracy, but so is the centralized hierarchical structure of a business firm.

Since being coined, the word "bureaucracy" has developed negative connotations for some. Some bureaucracies have been criticized as being inefficient, convoluted, or too inflexible to individuals. The dehumanizing effects of excessive bureaucracy became a major theme in the work of German-language writer Franz Kafka (1883–1924) and are central to his novels "The Trial" and "The Castle". The 1985 dystopian film "Brazil" by Terry Gilliam portrays a farcical macabre world in which small, otherwise insignificant errors in the bureaucratic processes of government develop into maddening and tragic consequences. The elimination of unnecessary bureaucracy is a key concept in modern managerial theory and has been an issue in some political campaigns.

Various commentators have noted the necessity of bureaucracies in modern society. The German sociologist Max Weber argued that bureaucracy constitutes the most efficient and rational way in which human activity can be organized and that systematic processes and organized hierarchies are necessary to maintain order, maximize efficiency, and eliminate favoritism. On the other hand, Weber also saw unfettered bureaucracy as a threat to individual freedom, with the potential of trapping individuals in an impersonal "iron cage" of rule-based, rational control.

The term "bureaucracy" originated in the French language: it combines the French word "bureau" – desk or office – with the Greek word κράτος ("kratos") – rule or political power. The French economist Jacques Claude Marie Vincent de Gournay (1712-1759) coined the word in the mid-18th century. Gournay never wrote the term down but a letter from a contemporary later quoted him:
The first known English-language use dates to 1818 with Irish novelist Lady Morgan referring to the apparatus used by the British to subjugate their Irish colony as "the Bureaucratie, or office tyranny, by which Ireland has so long been governed." By the mid-19th century the word appeared in a more neutral sense, referring to a system of public administration in which offices were held by unelected career officials. In this context "bureaucracy" was seen as a distinct form of management, often subservient to a monarchy. In the 1920s the German sociologist Max Weber expanded the definition to include any system of administration conducted by trained professionals according to fixed rules. Weber saw bureaucracy as a relatively positive development; however, by 1944 the Austrian economist Ludwig von Mises opined in the context of his experience in the Nazi regime that the term bureaucracy was "always applied with an opprobrious connotation," and by 1957 the American sociologist Robert Merton suggested that the term "bureaucrat" had become an "epithet, a "Schimpfwort"" in some circumstances. The word "bureaucracy" is also used in politics and government with a disapproving tone to disparage official rules that make it difficult to do things. In workplaces, the word is used very often to blame complicated rules, processes, and written work that make it hard to get something done.

Although the term "bureaucracy" first originated in the mid-18th century, organized and consistent administrative systems existed much earlier. The development of writing ( 3500 BC) and the use of documents was critical to the administration of this system, and the first definitive emergence of bureaucracy occurred in ancient Sumer, where an emergent class of scribes used clay tablets to administer the harvest and to allocate its spoils. Ancient Egypt also had a hereditary class of scribes that administered the civil-service bureaucracy.

A hierarchy of regional proconsuls and their deputies administered the Roman Empire. The reforms of Diocletian (Emperor from 284 to 305) doubled the number of administrative districts and led to a large-scale expansion of Roman bureaucracy. The early Christian author Lactantius ( 250 – 325) claimed that Diocletian's reforms led to widespread economic stagnation, since "the provinces were divided into minute portions, and many presidents and a multitude of inferior officers lay heavy on each territory." After the Empire split, the Byzantine Empire developed a notoriously complicated administrative hierarchy, and in the 20th century the term "Byzantine" came to refer to any complex bureaucratic structure.

In China, the Han dynasty (202 BC - 220 AD) established a complicated bureaucracy based on the teachings of Confucius, who emphasized the importance of ritual in a family, in relationships, and in politics. With each subsequent dynasty, the bureaucracy evolved. During the Song dynasty (960–1279) the bureaucracy became meritocratic. Following the Song reforms, competitive examinations took place to determine which candidates qualified to hold given positions.
The imperial examination system lasted until 1905, six years before the Qing dynasty collapsed, marking the end of China's traditional bureaucratic system.

Instead of the inefficient and often corrupt system of tax farming that prevailed in absolutist states such as France, the Exchequer was able to exert control over the entire system of tax revenue and government expenditure. By the late 18th century, the ratio of fiscal bureaucracy to population in Britain was approximately 1 in 1300, almost four times larger than the second most heavily bureaucratized nation, France. Thomas Taylor Meadows, Britain's consul in Guangzhou, argued in his "Desultory Notes on the Government and People of China" (1847) that "the long duration of the Chinese empire is solely and altogether owing to the good government which consists in the advancement of men of talent and merit only," and that the British must reform their civil service by making the institution meritocratic. Influenced by the ancient Chinese imperial examination, the Northcote–Trevelyan Report of 1854 recommended that recruitment should be on the basis of merit determined through competitive examination, candidates should have a solid general education to enable inter-departmental transfers, and promotion should be through achievement rather than "preferment, patronage, or purchase". This led to implementation of Her Majesty's Civil Service as a systematic, meritocratic civil service bureaucracy.

Like the British, the development of French bureaucracy was influenced by the Chinese system. Under Louis XIV of France, the old nobility had neither power nor political influence, their only privilege being exemption from taxes. The dissatisfied noblemen complained about this "unnatural" state of affairs, and discovered similarities between absolute monarchy and bureaucratic despotism. With the translation of Confucian texts during the Enlightenment, the concept of a meritocracy reached intellectuals in the West, who saw it as an alternative to the traditional "ancien regime" of Europe. Western perception of China even in the 18th century admired the Chinese bureaucratic system as favourable over European governments for its seeming meritocracy; Voltaire claimed that the Chinese had "perfected moral science" and François Quesnay advocated an economic and political system modeled after that of the Chinese.
The governments of China, Egypt, Peru and Empress Catherine II were regarded as models of Enlightened Despotism, admired by such figures as Diderot, D'Alembert and Voltaire.

Napoleonic France adopted this meritocracy system and soon saw a rapid and dramatic expansion of government, accompanied by the rise of the French civil service and its complex systems of bureaucracy. This phenomenon became known as "bureaumania". In the early 19th century, Napoleon attempted to reform the bureaucracies of France and other territories under his control by the imposition of the standardized Napoleonic Code. But paradoxically, that led to even further growth of the bureaucracy.

By the mid-19th century, bureaucratic forms of administration were firmly in place across the industrialized world. Thinkers like John Stuart Mill and Karl Marx began to theorize about the economic functions and power-structures of bureaucracy in contemporary life. Max Weber was the first to endorse bureaucracy as a necessary feature of modernity, and by the late 19th century bureaucratic forms had begun their spread from government to other large-scale institutions.

The trend toward increased bureaucratization continued in the 20th century, with the public sector employing over 5% of the workforce in many Western countries. Within capitalist systems, informal bureaucratic structures began to appear in the form of corporate power hierarchies, as detailed in mid-century works like "The Organization Man" and "The Man in the Gray Flannel Suit". Meanwhile, in the Soviet Union and Eastern Bloc nations, a powerful class of bureaucratic administrators termed "nomenklatura" governed nearly all aspects of public life.

The 1980s brought a backlash against perceptions of "big government" and the associated bureaucracy. Politicians like Margaret Thatcher and Ronald Reagan gained power by promising to eliminate government regulatory bureaucracies, which they saw as overbearing, and return economic production to a more purely capitalistic mode, which they saw as more efficient. In the business world, managers like Jack Welch gained fortune and renown by eliminating bureaucratic structures inside corporations. Still, in the modern world, most organized institutions rely on bureaucratic systems to manage information, process records, and administer complex systems, although the decline of paperwork and the widespread use of electronic databases is transforming the way bureaucracies function.

Karl Marx theorized about the role and function of bureaucracy in his "Critique of Hegel's Philosophy of Right", published in 1843. In "Philosophy of Right", Hegel had supported the role of specialized officials in public administration, although he never used the term "bureaucracy" himself. Marx, by contrast, was opposed to bureaucracy. Marx posited that while corporate and government bureaucracy seem to operate in opposition, in actuality they mutually rely on one another to exist. He wrote that "The Corporation is civil society's attempt to become state; but the bureaucracy is the state which has really made itself into civil society."

Writing in the early 1860s, political scientist John Stuart Mill theorized that successful monarchies were essentially bureaucracies, and found evidence of their existence in Imperial China, the Russian Empire, and the regimes of Europe. Mill referred to bureaucracy as a distinct form of government, separate from representative democracy. He believed bureaucracies had certain advantages, most importantly the accumulation of experience in those who actually conduct the affairs. Nevertheless, he believed this form of governance compared poorly to representative government, as it relied on appointment rather than direct election. Mill wrote that ultimately the bureaucracy stifles the mind, and that "a bureaucracy always tends to become a pedantocracy."

The German sociologist Max Weber was the first to formally study bureaucracy and his works led to the popularization of this term. In his 1922 essay "Bureaucracy", published in his magnum opus "Economy and Society", Weber described many ideal-typical forms of public administration, government, and business. His ideal-typical bureaucracy, whether public or private, is characterized by:

Weber listed several preconditions for the emergence of bureaucracy, including an increase in the amount of space and population being administered, an increase in the complexity of the administrative tasks being carried out, and the existence of a monetary economy requiring a more efficient administrative system. Development of communication and transportation technologies make more efficient administration possible, and democratization and rationalization of culture results in demands for equal treatment.

Although he was not necessarily an admirer of bureaucracy, Weber saw bureaucratization as the most efficient and rational way of organizing human activity and therefore as the key to rational-legal authority, indispensable to the modern world. Furthermore, he saw it as the key process in the ongoing rationalization of Western society. Weber also saw bureaucracy, however, as a threat to individual freedoms, and the ongoing bureaucratization as leading to a "polar night of icy darkness", in which increasing rationalization of human life traps individuals in a soulless "iron cage" of bureaucratic, rule-based, rational control. Weber's critical study of the bureaucratization of society became one of the most enduring parts of his work. Many aspects of modern public administration are based on his work, and a classic, hierarchically organized civil service of the Continental type is called "Weberian civil service".

Writing as an academic while a professor at Bryn Mawr College, Woodrow Wilson's essay "The Study of Administration" argued for bureaucracy as a professional cadre, devoid of allegiance to fleeting politics. Wilson advocated a bureaucracy that "is a part of political life only as the methods of the counting house are a part of the life of society; only as machinery is part of the manufactured product. But it is, at the same time, raised very far above the dull level of mere technical detail by the fact that through its greater principles it is directly connected with the lasting maxims of political wisdom, the permanent truths of political progress."

Wilson did not advocate a replacement of rule by the governed, he simply advised that, "Administrative questions are not political questions. Although politics sets the tasks for administration, it should not be suffered to manipulate its offices". This essay became a foundation for the study of public administration in America.

In his 1944 work "Bureaucracy", the Austrian economist Ludwig von Mises compared bureaucratic management to profit management. Profit management, he argued, is the most effective method of organization when the services rendered may be checked by economic calculation of profit and loss. When, however, the service in question can not be subjected to economic calculation, bureaucratic management is necessary. He did not oppose universally bureaucratic management; on the contrary, he argued that bureaucracy is an indispensable method for social organization, for it is the only method by which the law can be made supreme, and is the protector of the individual against despotic arbitrariness. Using the example of the Catholic Church, he pointed out that bureaucracy is only appropriate for an organization whose code of conduct is not subject to change. He then went on to argue that complaints about bureaucratization usually refer not to the criticism of the bureaucratic methods themselves, but to "the intrusion of bureaucracy into all spheres of human life." Mises saw bureaucratic processes at work in both the private and public spheres; however, he believed that bureaucratization in the private sphere could only occur as a consequence of government interference. According to him, "What must be realized is only that the strait jacket of bureaucratic organization paralyzes the individual's initiative, while within the capitalist market society an innovator still has a chance to succeed. The former makes for stagnation and preservation of inveterate methods, the latter makes for progress and improvement."

American sociologist Robert K. Merton expanded on Weber's theories of bureaucracy in his work "Social Theory and Social Structure", published in 1957. While Merton agreed with certain aspects of Weber's analysis, he also noted the dysfunctional aspects of bureaucracy, which he attributed to a "trained incapacity" resulting from "over conformity". He believed that bureaucrats are more likely to defend their own entrenched interests than to act to benefit the organization as a whole but that pride in their craft makes them resistant to changes in established routines. Merton stated that bureaucrats emphasize formality over interpersonal relationships, and have been trained to ignore the special circumstances of particular cases, causing them to come across as "arrogant" and "haughty".

In his book “A General Theory of Bureaucracy”, first published in 1976, Dr. Elliott Jaques describes the discovery of a universal and uniform underlying structure of managerial or work levels in the bureaucratic hierarchy for any type of employment systems.

Elliott Jaques argues and presents evidence that for the bureaucracy to provide a valuable contribution to the open society some of the following conditions must be met:
The definition of effective bureaucratic hierarchy by Elliott Jaques is of importance not only to sociology but to social psychology, social anthropology, economics, politics, and social philosophy. They also have a practical application in business and administrative studies.




</doc>
<doc id="59539440" url="https://en.wikipedia.org/wiki?curid=59539440" title="Artificial intelligence in government">
Artificial intelligence in government

Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with government (through the use of virtual assistants, for example). According to the Harvard Business Review, "Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world." Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters. The use of AI in government comes with significant benefits (including efficiencies resulting in cost savings), but also carries risks.

The potential uses of AI in government are wide and varied, with Deloitte considering that "Cognitive technologies could eventually revolutionize every facet of government operations". Mehr suggests that six types of government problems are appropriate for AI applications:


Meher stats that "While applications of AI in government work has not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector."

Potential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with government; and other uses.

There are a range of examples of where AI can contribute to public policy objectives. These include:


AI can be used to assist members of the public to interact with government and access government services, for example by:


Examples of virtual assistants or chatbots being used by government include the following:


Other uses of AI in government include:


AI offers potential efficiencies and cost savings for government. For example, Deloitte has estimated that automation could save US Government employees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between $3.3 billion to $41.1 billion a year. The Harvard Business Review has stated that while this may lead a government to reduce employee numbers, "Governments could instead choose to invest in the quality of its services. They can re-employ workers’ time towards more rewarding work that requires lateral thinking, empathy, and creativity — all things at which humans continue to outperform even the most sophisticated AI program."

Potential risks associated with the use of AI in government include AI becoming susceptible to bias, a lack of transparency in how an AI application may make decisions, and the accountability for any such decisions.



</doc>
<doc id="375091" url="https://en.wikipedia.org/wiki?curid=375091" title="Policy">
Policy

A policy is a deliberate system of principles to guide decisions and achieve rational outcomes. A policy is a statement of intent, and is implemented as a procedure or protocol. Policies are generally adopted by a governance body within an organization. Policies can assist in both "subjective" and "objective" decision making. Policies to assist in subjective decision making usually assist senior management with decisions that must be based on the relative merits of a number of factors, and as a result are often hard to test objectively, e.g. work-life balance policy. In contrast policies to assist in objective decision making are usually operational in nature and can be objectively tested, e.g. password policy.

The term may apply to government, private sector organizations and groups, as well as individuals. Presidential executive orders, corporate privacy policies, and parliamentary rules of order are all examples of policy. Policy differs from rules or law. While law can compel or prohibit behaviors (e.g. a law requiring the payment of taxes on income), policy merely guides actions toward those that are most likely to achieve a desired outcome.

Policy or policy study may also refer to the process of making important organizational decisions, including the identification of different alternatives such as programs or spending priorities, and choosing among them on the basis of the impact they will have. Policies can be understood as political, managerial, financial, and administrative mechanisms arranged to reach explicit goals. In public corporate finance, a critical accounting policy is a policy for a firm/company or an industry that is considered to have a notably high subjective element, and that has a material impact on the financial statements.

The intended effects of a policy vary widely according to the organization and the context in which they are made. Broadly, policies are typically instituted to avoid some negative effect that has been noticed in the 
organization, or to seek some positive benefit. 

Corporate purchasing policies provide an example of how organizations attempt to avoid negative effects. Many large companies have policies that all purchases above a certain value must be performed through a purchasing process. By requiring this standard purchasing process through policy, the organization can limit waste and standardize the way purchasing is done. 

The State of California provides an example of benefit-seeking policy. In recent years, the numbers of hybrid cars in California has increased dramatically, in part because of policy changes in Federal law that provided USD $1,500 in tax credits (since phased out) as well as the use of high-occupancy vehicle lanes to hybrid owners (no loew hybrid vehicles). In this case, the organization (state and/or federal government) created an effect (increased ownership and use of hybrid vehicles) through policy (tax breaks, highway lanes). 

Policies frequently have side effects or unintended consequences. Because the environments that policies seek to influence or manipulate are typically complex adaptive systems (e.g. governments, societies, large companies), making a policy change can have counterintuitive results. For example, a government may make a policy decision to raise taxes, in hopes of increasing overall tax revenue. Depending on the size of the tax increase, this may have the overall effect of reducing tax revenue by causing capital flight or by creating a rate so high that citizens are deterred from earning the money that is taxed. (See the Laffer curve.) 

The policy formulation process theoretically includes an attempt to assess as many areas of potential policy impact as possible, to lessen the chances that a given policy will have unexpected or unintended consequences. 

In political science, the policy cycle is a tool used for the analyzing of the development of a policy item. It can also be referred to as a "stagist approach", "stages heuristic" or "stages approach". It is thus a rule of thumb rather than the actual reality of how policy is created, but has been influential in how political scientists looked at policy in general. It was developed as a theory from Harold Lasswell's work.

One version by James E. Anderson, in his "Public Policy-Making" (1974) has the following stages:

An eight step policy cycle is developed in detail in "The Australian Policy Handbook" by Peter Bridgman and Glyn Davis: (now with Catherine Althaus in its 4th and 5th editions)


The Althaus, Bridgman & Davis model is heuristic and iterative. It is and not meant to be or predictive. Policy cycles are typically characterized as adopting a classical approach, and tend to describe processes from the perspective of policy decision makers. Accordingly, some postpositivist academics challenge cyclical models as unresponsive and unrealistic, preferring systemic and more complex models. They consider a broader range of actors involved in the policy space that includes civil society organisations, the media, intellectuals, think tanks or policy research institutes, corporations, lobbyists, etc.

Policies are typically promulgated through official written documents. Policy documents often come with the endorsement or signature of the executive powers within an organization to legitimize the policy and demonstrate that it is considered in force. Such documents often have standard formats that are particular to the organization issuing the policy. While such formats differ in form, policy documents usually contain certain standard components including :

Some policies may contain additional sections, including:

The American political scientist Theodore J. Lowi proposed four types of policy, namely distributive, redistributive, regulatory and constituent in his article 'Four systems of Policy, Politics and Choice' and in 'American Business, Public Policy, Case Studies and Political Theory'.
Policy addresses the intent of the organization, whether government, business, professional, or voluntary. Policy is intended to affect the 'real' world, by guiding the decisions that are made. Whether they are formally written or not, most organizations have identified policies.

Policies may be classified in many different ways. The following is a sample of several different types of policies broken down by their effect on members of the organization.

Distributive policies extend goods and services to members of an organization, as well as distributing the costs of the goods/services amongst the members of the organization. Examples include government policies that impact spending for welfare, public education, highways, and public safety, or a professional organization's benefits plan.

Regulatory policies, or mandates, limit the discretion of individuals and agencies, or otherwise compel certain types of behavior. These policies are generally thought to be best applied when good behavior can be easily defined and bad behavior can be easily regulated and punished through fines or sanctions. An example of a fairly successful public regulatory policy is that of a highway speed limit.

Constituent policies create executive power entities, or deal with laws. Constituent policies also deal with Fiscal Policy in some circumstances.

Policies are dynamic; they are not just static lists of goals or laws. Policy blueprints have to be implemented, often with unexpected results. Social policies are what happens 'on the ground' when they are implemented, as well as what happens at the decision making or legislative stage.

When the term policy is used, it may also refer to:

The actions the organization actually takes may often vary significantly from stated policy. This difference is sometimes caused by political compromise over policy, while in other situations it is caused by lack of policy implementation and enforcement. Implementing policy may have unexpected results, stemming from a policy whose reach extends further than the problem it was originally crafted to address. Additionally, unpredictable results may arise from selective or idiosyncratic enforcement of policy.

Types of policy analysis include: 


These qualifiers can be combined, so one could, for example, have a stationary-memoryless-index policy.







</doc>
<doc id="12229" url="https://en.wikipedia.org/wiki?curid=12229" title="Government">
Government

A government is the system or group of people governing an organized community, often a state.

In the case of its broad associative definition, government normally consists of legislature, executive, and judiciary. Government is a means by which organizational policies are enforced, as well as a mechanism for determining policy. Each government has a kind of constitution, a statement of its governing principles and philosophy. Typically the philosophy chosen is some balance between the principle of individual freedom and the idea of absolute state authority (tyranny).

While all types of organizations have governance, the word "government" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as subsidiary organizations.

Historically prevalent forms of government include monarchy, aristocracy, timocracy, oligarchy, democracy, theocracy and tyranny. The main aspect of any philosophy of government is how political power is obtained, with the two main forms being electoral contest and hereditary succession.

A government is the system to govern a state or community.

The word "government" derives, ultimately, from the Greek verb κυβερνάω ["kubernáo"] (meaning "to steer" with gubernaculum (rudder), the metaphorical sense being attested in Plato's Ship of State).

The Columbia Encyclopedia defines government as "a system of social control under which the right to make laws, and the right to enforce them, is vested in a particular group in society".

While all types of organizations have governance, the word "government" is often used more specifically to refer to the approximately 200 independent national governments on Earth, as well as their subsidiary organizations.

In the Commonwealth of Nations, the word "government" is also used more narrowly to refer to the ministry (collective executive), a collective group of people that exercises executive authority in a state or, metonymically, to the governing cabinet as part of the executive.

Finally, "government" is also sometimes used in English as a synonym for governance.

The moment and place that the phenomenon of human government developed is lost in time; however, history does record the formations of early governments. About 5,000 years ago, the first small city-states appeared. By the third to second millenniums BC, some of these had developed into larger governed areas: Sumer, Ancient Egypt, the Indus Valley Civilization, and the Yellow River Civilization.

The development of agriculture and water control projects were a catalyst for the development of governments. For many thousands of years when people were hunter-gatherers and small scale farmers, humans lived in small, non-hierarchical and self-sufficient communities. On occasion a chief of a tribe was elected by various rituals or tests of strength to govern his tribe, sometimes with a group of elder tribesmen as a council. The human ability to precisely communicate abstract, learned information allowed humans to become ever more effective at agriculture, and that allowed for ever increasing population densities. David Christian explains how this resulted in states with laws and governments:

Starting at the end of the 17th century, the prevalence of republican forms of government grew. The Glorious Revolution in England, the American Revolution, and the French Revolution contributed to the growth of representative forms of government. The Soviet Union was the first large country to have a Communist government. Since the fall of the Berlin Wall, liberal democracy has become an even more prevalent form of government.

In the nineteenth and twentieth century, there was a significant increase in the size and scale of government at the national level. This included the regulation of corporations and the development of the welfare state.

In political science, it has long been a goal to create a typology or taxonomy of polities, as typologies of political systems are not obvious. It is especially important in the political science fields of comparative politics and international relations. Like all categories discerned within forms of government, the boundaries of government classifications are either fluid or ill-defined.

Superficially, all governments have an official or ideal form. The United States is a constitutional republic, while the former Soviet Union was a socialist republic. However self-identification is not objective, and as Kopstein and Lichbach argue, defining regimes can be tricky. For example, elections are a defining characteristic of an electoral democracy, but in practice elections in the former Soviet Union were not "free and fair" and took place in a one-party state. Voltaire argued that "the Holy Roman Empire is neither Holy, nor Roman, nor an Empire". Many governments that officially call themselves a "democratic republic" are not democratic, nor a republic; they are usually a dictatorship "de facto". Communist dictatorships have been especially prone to use this term. For example, the official name of North Vietnam was "The Democratic Republic of Vietnam". China uses a variant, "The People's Republic of China". Thus in many practical classifications it would not be considered democratic.

Identifying a form of government is also difficult because many political systems originate as socio-economic movements and are then carried into governments by parties naming themselves after those movements; all with competing political-ideologies. Experience with those movements in power, and the strong ties they may have to particular forms of government, can cause them to be considered as forms of government in themselves.

Other complications include general non-consensus or deliberate "distortion or bias" of reasonable technical definitions to political ideologies and associated forms of governing, due to the nature of politics in the modern era. For example: The meaning of "conservatism" in the United States has little in common with the way the word's definition is used elsewhere. As Ribuffo notes, "what Americans now call conservatism much of the world calls liberalism or neoliberalism". Since the 1950s conservatism in the United States has been chiefly associated with the Republican Party. However, during the era of segregation many Southern Democrats were conservatives, and they played a key role in the Conservative Coalition that controlled Congress from 1937 to 1963.

Every country in the world is ruled by a system of governance that combines at least three or more political or economic attributes. Additionally, opinions vary by individuals concerning the types and properties of governments that exist. "Shades of gray" are commonplace in any government and its corresponding classification. Even the most liberal democracies limit rival political activity to one extent or another while the most tyrannical dictatorships must organize a broad base of support thereby creating difficulties for "pigeonholing" governments into narrow categories. Examples include the claims of the United States as being a plutocracy rather than a democracy since some American voters believe elections are being manipulated by wealthy Super PACs.

The Classical Greek philosopher Plato discusses five types of regimes: aristocracy, timocracy, oligarchy, democracy and tyranny. Plato also assigns a man to each of these regimes to illustrate what they stand for. The tyrannical man would represent tyranny for example. These five regimes progressively degenerate starting with aristocracy at the top and tyranny at the bottom.

One method of classifying governments is through which people have the authority to rule. This can either be one person (an autocracy, such as monarchy), a select group of people (an aristocracy), or the people as a whole (a democracy, such as a republic).

The division of governments as monarchy, aristocracy and democracy has been used since Aristotle's Politics. In his book Leviathan, Thomas Hobbes expands on this classification.

An autocracy is a system of government in which supreme power is concentrated in the hands of one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control (except perhaps for the implicit threat of a coup d'état or mass insurrection).

A despotism is a government ruled by a single entity with absolute power, whose decisions are subject to neither external legal restraints nor regular mechanisms of popular control (except perhaps for implicit threat). That entity may be an individual, as in an autocracy, or it may be a group, as in an oligarchy. The word despotism means to "rule in the fashion of despots".

A monarchy is where a family or group of families (rarely another type of group), called the royalty, represents national identity, with power traditionally assigned to one of its individuals, called the monarch, who mostly rule kingdoms. The actual role of the monarch and other members of royalty varies from purely symbolical (crowned republic) to partial and restricted (constitutional monarchy) to completely despotic (absolute monarchy). Traditionally and in most cases, the post of the monarch is inherited, but there are also elective monarchies where the monarch is elected.

Aristocracy (Greek ἀριστοκρατία "aristokratía", from ἄριστος "" "excellent", and κράτος "" "power") is a form of government that places power in the hands of a small, privileged ruling class.

Many monarchies were aristocracies, although in modern constitutional monarchies the monarch himself or herself has little real power. The term "Aristocracy" could also refer to the non-peasant, non-servant, and non-city classes in the Feudal system. 

An oligarchy is ruled by a small group of segregated, powerful or influential people who usually share similar interests or family relations. These people may spread power and elect candidates equally or not equally. An oligarchy is different from a true democracy because very few people are given the chance to change things. An oligarchy does not have to be hereditary or monarchic. An oligarchy does not have one clear ruler but several rulers.

Some historical examples of oligarchy are the former Union of Soviet Socialist Republics. Some critics of representative democracy think of the United States as an oligarchy. The Athenian democracy used sortition to elect candidates, almost always male, Greek, educated citizens holding a minimum of land, wealth and status.

A theocracy is rule by a religious elite; a system of governance composed of religious institutions in which the state and the church are traditionally or constitutionally the same entity. The Vatican's (see Pope), Iran's (see Supreme Leader), Tibetan government's (see Dalai Lama), Caliphates and other Islamic states are historically considered "theocracies".

In a general sense, in a democracy, all the people of a state or polity are involved in making decisions about its affairs. Also refer to the rule by a government chosen by election where most of the populace are enfranchised. The key distinction between a democracy and other forms of constitutional government is usually taken to be that the right to vote is not limited by a person's wealth or race (the main qualification for enfranchisement is usually having reached a certain age). A democratic government is, therefore, one supported (at least at the time of the election) by a majority of the populace (provided the election was held fairly). A "majority" may be defined in different ways. There are many "power-sharing" (usually in countries where people mainly identify themselves by race or religion) or "electoral-college" or "constituency" systems where the government is not chosen by a simple one-vote-per-person headcount.

In democracies, large proportions of the population may vote, either to make decisions or to choose representatives to make decisions. Commonly significant in democracies are political parties, which are groups of people with similar ideas about how a country or region should be governed. Different political parties have different ideas about how the government should handle different problems.

Liberal democracy is a variant of democracy. It is a form of government in which representative democracy operates under the principles of liberalism. It is characterised by fair, free, and competitive elections between multiple distinct political parties, a separation of powers into different branches of government, the rule of law in everyday life as part of an open society, and the protection of human rights and civil liberties for all persons. To define the system in practice, liberal democracies often draw upon a constitution, either formally written or uncodified, to delineate the powers of government and enshrine the social contract. After a period of sustained expansion throughout the 20th century, liberal democracy became the predominant political system in the world. A liberal democracy may take various constitutional forms: it may be a republic, such as France, Germany, India, Ireland, Italy, Taiwan, or the United States; or a constitutional monarchy, such as Japan, Spain, or the United Kingdom. It may have a presidential system (Argentina, Brazil, Mexico, or the United States), a semi-presidential system (France, Portugal, or Taiwan), or a parliamentary system (Australia, Canada, Germany, Ireland, India, Italy, New Zealand, or the United Kingdom).

A republic is a form of government in which the country is considered a "public matter" (Latin: "res publica"), not the private concern or property of the rulers, and where offices of states are subsequently directly or indirectly elected or appointed rather than inherited. The people, or some significant portion of them, have supreme control over the government and where offices of state are elected or chosen by elected people. A common simplified definition of a republic is a government where the head of state is not a monarch. Montesquieu included both democracies, where all the people have a share in rule, and aristocracies or oligarchies, where only some of the people rule, as republican forms of government.

Other terms used to describe different republics include Democratic republic, Parliamentary republic, Federal republic, and Islamic Republic.

Rule by authoritarian governments is identified in societies where a specific set of people possess the authority of the state in a republic or union. It is a political system controlled by unelected rulers who usually permit some degree of individual freedom. Rule by a totalitarian government is characterised by a highly centralised and coercive authority that regulates nearly every aspect of public and private life.

In contrast, a constitutional republic is rule by a government whose powers are limited by law or a formal constitution, and chosen by a vote amongst at least some sections of the populace (Ancient Sparta was in its own terms a republic, though most inhabitants were disenfranchised). Republics that exclude sections of the populace from participation will typically claim to represent all citizens (by defining people without the vote as "non-citizens"). Examples include the United States, South Africa, India, etc. 

Federalism is a political concept in which a "group" of members are bound together by covenant (Latin: "foedus", covenant) with a governing representative head. The term "federalism" is also used to describe a system of government in which sovereignty is constitutionally divided between a central governing authority and constituent political units (such as states or provinces). Federalism is a system based upon democratic rules and institutions in which the power to govern is shared between national and provincial/state governments, creating what is often called a federation. Proponents are often called federalists.

Historically, most political systems originated as socioeconomic ideologies. Experience with those movements in power and the strong ties they may have to particular forms of government can cause them to be considered as forms of government in themselves.


Certain major characteristics are defining of certain types; others are historically associated with certain types of government.

This list focuses on differing approaches that political systems take to the distribution of sovereignty, and the autonomy of regions within the state.





</doc>
<doc id="3488351" url="https://en.wikipedia.org/wiki?curid=3488351" title="People">
People

A people is a plurality of persons considered as a whole, as is the case with an ethnic group or nation, but that is distinct from a nation which is more abstract, and more overtly political. Collectively, for example, the contemporary Frisians and Danes are two related Germanic peoples, while various Middle Eastern ethnic groups are often linguistically categorized as Semitic peoples.

Various states govern, or claim to govern, in the name of "the people". Both the Roman Republic and the Roman Empire used the Latin term "Senatus Populusque Romanus", (the Senate and People of Rome). This term was fixed to Roman legionary standards, and even after the Roman Emperors achieved a state of total personal autarchy, they continued to wield their power in the name of the Senate and People of Rome.

A People's Republic is typically a Marxist or socialist one-party state that claims to govern on behalf of the people even if it in practice often turns out to be a dictatorship. Populism is another umbrella term for various political tendencies that claim to represent the people, usually with an implication that they serve the "common people" instead of the elite.

Chapter One, Article One of the Charter of the United Nations states that peoples have the right to self-determination.

In criminal law, in certain jurisdictions, criminal prosecutions are brought in the name of "the People". Several U.S. states, including California, Illinois, and New York, use this style. Citations outside the jurisdictions in question usually substitute the name of the state for the words "the People" in the case captions. Four states — Massachusetts, Virginia, Pennsylvania, and Kentucky — refer to themselves as "the Commonwealth" in case captions and legal process. Other states, such as Indiana, typically refer to themselves as "the State" in case captions and legal process. Outside the United States, criminal trials in Ireland and the Philippines are prosecuted in the name of the people of their respective states.

The political theory underlying this format is that criminal prosecutions are brought in the name of the sovereign; thus, in these U.S. states, the "people" are judged to be the sovereign, even as in the United Kingdom and other dependencies of the British Crown, criminal prosecutions are typically brought in the name of the Crown. "The people" identifies the entire body of the citizens of a jurisdiction invested with political power or gathered for political purposes.



</doc>
<doc id="219599" url="https://en.wikipedia.org/wiki?curid=219599" title="Person">
Person

A person is a being that has certain capacities or attributes such as reason, morality, consciousness or self-consciousness, and being a part of a culturally established form of social relations such as kinship, ownership of property, or legal responsibility. The defining features of personhood and consequently what makes a person count as a person differ widely among cultures and contexts.

In addition to the question of personhood, of what makes a being count as a person to begin with, there are further questions about personal identity and self: both about what makes any particular person that particular person instead of another, and about what makes a person at one time the same person as they were or will be at another time despite any intervening changes.

The common plural of "person", "people", is often used to refer to an entire nation or ethnic group (as in "a people"). The plural "persons" is often used in philosophical and legal writing.

Personhood is the status of being a person. Defining personhood is a controversial topic in philosophy and law, and is closely tied to legal and political concepts of citizenship, equality, and liberty. According to common worldwide general legal practice, only a natural person or legal personality has rights, protections, privileges, responsibilities, and legal liability.
Personhood continues to be a topic of international debate, and has been questioned during the abolition of slavery and the fight for women's rights, in debates about abortion, fetal rights, and in animal rights advocacy.

Various debates have focused on questions about the personhood of different classes of entities. Historically, the personhood of animals, women, and slaves has been a catalyst of social upheaval. In most societies today, living adult humans are usually considered persons, but depending on the context, theory or definition, the category of "person" may be taken to include or not children or such non-human entities as animals, artificial intelligences, or extraterrestrial life, as well as legal entities such as corporations, sovereign states and other polities, or estates in probate.

Personal identity is the unique identity of persons through time. That is to say, the necessary and sufficient conditions under which a person at one time and a person at another time can be said to be the "same" person, persisting through time. In the modern philosophy of mind, this concept of personal identity is sometimes referred to as the "diachronic" problem of personal identity. The "synchronic" problem is grounded in the question of what features or traits characterize a given person at one time.

Identity is an issue for both continental philosophy and analytic philosophy. A key question in continental philosophy is in what sense we can maintain the modern conception of identity, while realizing many of our prior assumptions about the world are incorrect.

Proposed solutions to the problem of personal identity include continuity of the physical body, continuity of an immaterial mind or soul, continuity of consciousness or memory, the bundle theory of self, continuity of personality after the death of the physical body, and proposals that there are actually no persons or selves who persist over time at all.

In ancient Rome, the word "persona" (Latin) or "prosopon" (πρόσωπον; Greek) originally referred to the masks worn by actors on stage. The various masks represented the various "personae" in the stage play.

The concept of person was further developed during the Trinitarian and Christological debates of the 4th and 5th centuries in contrast to the word nature. During the theological debates, some philosophical tools (concepts) were needed so that the debates could be held on common basis to all theological schools. The purpose of the debate was to establish the relation, similarities and differences between the Λóγος/"Verbum" and God. The philosophical concept of person arose, taking the word "prosopon" (πρόσωπον) from the Greek theatre. Therefore, Christus (the Λóγος/"Verbum") and God were defined as different "persons". This concept was applied later to the Holy Ghost, the angels and to all human beings.

Since then, a number of important changes to the word's meaning and use have taken place, and attempts have been made to redefine the word with varying degrees of adoption and influence.




</doc>
<doc id="1742287" url="https://en.wikipedia.org/wiki?curid=1742287" title="Risk society">
Risk society

Risk society is the manner in which modern society organizes in response to risk. The term is closely associated with several key writers on modernity, in particular Ulrich Beck and Anthony Giddens. The term was coined in the 1980s and its popularity during the 1990s was both as a consequence of its links to trends in thinking about wider modernity, and also to its links to popular discourse, in particular the growing environmental concerns during the period.

According to British sociologist Anthony Giddens, a risk society is "a society increasingly preoccupied with the future (and also with safety), which generates the notion of risk," whilst the German sociologist Ulrich Beck defines it as "a systematic way of dealing with hazards and insecurities induced and introduced by modernisation itself (Beck 1992:21)".

Beck (1992:50) defined modernization as,

Beck and Giddens both approach the risk society firmly from the perspective of modernity, "a shorthand term for modern society or industrial civilization... modernity is vastly more dynamic than any previous type of social order. It is a society... which unlike any preceding culture lives in the future rather than the past." (Anthony Giddens) They also draw heavily on the concept of reflexivity, the idea that as a society examines itself, it in turn changes itself in the process. In classical industrial society, the modernist view is based on assumption of realism in science creating a system in which scientists work in an exclusive, inaccessible environment of modern period.

In 1986, right after the Chernobyl disaster, Ulrich Beck, a sociology professor at the University of Munich, published the original German text, "Risikogesellschaft", of his highly influential and catalytic work (Suhrkamp, Frankfurt 1986). "Risikogesellschaft" was published in English as "Risk Society: Towards a New Modernity" in 1992. The ecological crisis is central to this social analysis of the contemporary period. Beck argued that environmental risks had become the predominant product, not just an unpleasant, manageable side-effect, of industrial society.

Giddens and Beck argued that whilst humans have always been subjected to a level of risk – such as natural disasters – these have usually been perceived as produced by non-human forces. Modern societies, however, are exposed to risks such as pollution, newly discovered illnesses, crime, that are the result of the modernization process itself. Giddens defines these two types of risks as external risks and manufactured risks. Manufactured risks are marked by a high level of human agency involved in both producing, and mitigating such risks.

As manufactured risks are the product of human activity, authors like Giddens and Beck argue that it is possible for societies to assess the level of risk that is being produced, or that is about to be produced. This sort of reflexive introspection can in turn alter the planned activities themselves. As an example, disasters such as Chernobyl and the Love Canal Crisis, public faith in the modern project has declined leaving public distrust in industry, government and experts.

Social concerns led to increased regulation of the nuclear power industry and to the abandonment of some expansion plans, altering the course of modernization itself. This increased critique of modern industrial practices is said to have resulted in a state of reflexive modernization, illustrated by concepts such as sustainability and the precautionary principle that focus on preventive measures to decrease levels of risk.

There are differing opinions as to how the concept of a risk society interacts with social hierarchies and class distinctions. Most agree that social relations have altered with the introduction of manufactured risks and reflexive modernization. Risks, much like wealth, are distributed unevenly in a population and will influence quality of life.

Beck has argued that older forms of class structure – based mainly on the accumulation of wealth – atrophy in a modern, risk society, in which people occupy social risk positions that are achieved through risk aversion. "In some of their dimensions these follow the inequalities of class and strata positions, but they bring a fundamentally different distribution logic into play". Beck contends that widespread risks contain a 'boomerang effect', in that individuals producing risks will also be exposed to them. This argument suggests that wealthy individuals whose capital is largely responsible for creating pollution will also have to suffer when, for example, the contaminants seep into the water supply. This argument may seem oversimplified, as wealthy people may have the ability to mitigate risk more easily by, for example, buying bottled water. Beck, however, has argued that the distribution of this sort of risk is the result of knowledge, rather than wealth. Whilst the wealthy person may have access to resources that enable him or her to avert risk, this would not even be an option were the person unaware that the risk even existed. However, Risks do not only affect those of a certain social class or place, risk is not bias and can affect everybody no matter your class, nobody is free from risk.

By contrast, Giddens has argued that older forms of class structure maintain a somewhat stronger role in a risk society, now being partly defined "in terms of differential access to forms of self-actualization and empowerment". Giddens has also tended to approach the concept of a risk society more positively than Beck, suggesting that there "can be no question of merely taking a negative attitude towards risk. Risk needs to be disciplined, but active risk-taking is a core element of a dynamic economy and an innovative society."




</doc>
<doc id="34407451" url="https://en.wikipedia.org/wiki?curid=34407451" title="Parallel society">
Parallel society

Parallel society refers to the self-organization of an ethnic or religious minority, often immigrant groups, with the intent of a reduced or minimal spatial, social and cultural contact with the majority society into which they immigrate.

The term was introduced into the debate about migration and integration in the early 1990s by the German sociologist Wilhelm Heitmeyer. It rose to prominence in the European public discourse following the murder of Dutch director and critic of Islam Theo van Gogh. In 2004, it was elected by the Association for the German Language second as Word of the year.



</doc>
<doc id="36055387" url="https://en.wikipedia.org/wiki?curid=36055387" title="Planetary consciousness">
Planetary consciousness

Planetary consciousness is the idea that human beings are members of a planetary society of Earth as much as they are members of their nations, provinces, districts, islands, cities or villages.

In his 1906 book "American Character", author Brander Matthews mentions the idea of a "league of nations" and a "planetary consciousness", believing it would be created by American politicians in the coming centuries. Key planetary consciousness events of the 20th century include the creation of the League of Nations, the signing of Kellogg-Briand Pact, the creation of the United Nations, and the creation of the Bretton Woods system. Democratic globalization advocate Abhay Kumar points to the International Corporation of Assigned Names and Numbers (ICANN) board of directors election in 2000, which were conducted globally, as the first example of global democracy. In September 2001, Ervin László and the Dalai Lama wrote an essay titled "Manifesto on Planetary Consciousness", which was adopted at a meeting at the Hungarian Academy of Sciences in Budapest. Its introduction begins:

Andreas Bummel, CEO of the Committee for a Democratic UN, says, "The first step into the direction of a world parliament would be the establishment of a Parliamentary Assembly at the United Nations".

Advocacy for the idea of planetary consciousness is based on the technological advancements made by the mankind in the fields of transport and telecommunications during the 20th century and in the first decade of the 21st century. Kumar claims that these technological advancements have turned the whole planet into an interdependent economic, political and communication community. He specifically cites the invention of the Internet and the mobile phones as key technological achievements of the 20th century which brought humans into more continuous interconnected communication. He believes that these inventions will lead to a second Renaissance and global democracy, just as the Gutenberg press in 1439 led to the first Renaissance, the Age of Enlightenment, and Nation states. Bummel describes planetary consciousness as integral, insofar as it does not conflict with other levels of social identity, but instead is a holistic perspective on humanity and the planet as a whole. Steven Kull writes that while nation states are reluctant to work cooperatively, individuals seem more willing.

Author Shashi Tharoor feels that an Earth Anthem sung by people across the world can inspire planetary consciousness and global citizenship among people.



</doc>
<doc id="14131017" url="https://en.wikipedia.org/wiki?curid=14131017" title="Contemporary society">
Contemporary society

Contemporary society, according to social and political scientists, is characterised by at least three fundamental directions:

These presentations are the result of a number of fundamental changes that are irreversibly transforming our daily lives, our way of thinking and perceiving the world and our way of living together. Among these fundamental changes are: improvements in life conditions, life expectancy, literacy and gender equality; changes in domestic and international political institutions; and the breakdown of natural equilibria.

The UN estimates that, at the beginning of the 20th century, about 60% of the world population lived in conditions of extreme poverty. In 1981, 40% of the world population lived extreme poverty. In 2001, the percentage had been halved to 20%.
Several developing countries, in particular in Sub-Saharan Africa, still suffer from social and economic backwardness, but life conditions have significantly improved in most regions of the world, in particular in Asia. 
The overall improvement in life conditions and the role of technologies now available have contributed to increase gross domestic product per capita by one and a half times in less than half a century (1960–2005), with peaks of over eight times in Eastern Asia. Only in a few countries, concentrated in Sub-Saharan Africa, growth of per capita income has been very slow.

In 1960, the average life expectancy of the world population was 50 years. Forty-five years later, in 2004, life expectancy had improved by over 30% to 67 years. Improvements in health care and the reduction in child mortality have led to a jump forward in middle-income countries, where life expectancy is now over 70 years. In high-income countries life expectancy is now over 80 years, extending well beyond the traditional length of working life, causing social and economic problems. It has led to people having an extra 4 hours of free-time during working days.

The ability to read and write is next to universal: in 2004, 80% of adult men and 73% of adult women had basic literacy skills. Of great social importance is the rapid growth of female school enrolment and the increasing presence of women in the labour market. These deep changes constitute a primary driver of economic growth in developing countries.
Female literacy has great consequences in terms of fertility. When female school enrollment and employment rates increase, fertility rates decline rapidly and tend to stabilise around the natural rate of reproduction of 2.1 children per women (see E. Todd, "After the Empire"). Several demographers believe that, as a consequence, world population will stabilise over the next few decades, at a level compatible with the resources of the planet [reference].

The world population has a number of "passive" (broadcasting) communication technologies (radio, television) that cover the whole globe. Moreover, a large portion of the population uses "active" communication technologies (telephone, internet). Internet connections are expanding rapidly: in 2004 there were 140 Internet users every 1000 inhabitants (according to data from the "International Communication Union"). The spread of information and communication technologies (ICT) is remodelling the material fundamentals of society. The sociologist Castells believes that these technologies have started a revolution of the productive structures of society and of daily life.

The new communication technologies represent a critical instrument to obtain consensus and as a result they are transforming the organisational models of the State and of politics. The power system becomes less visible but more pervasive in the way it can influence choices and ways of thinking M. Castells, "Galassia Internet", Feltrinelli, 2002).

The economic success of authoritarian regimes, mainly in Asia, suggests that (at least in the short term) economic growth is independent from the democratisation of political institutions. However, economic development favours the development of democratic institutions—but only if economic growth leads to substantial changes in cultural and social structures. (R. Inglehart, "La società postmoderna").
The "World Values Survey", which captures political values in 43 countries, shows that no country with a per capita income below the poverty line has democratic or free institutions. Almost the totality of nations with high per capita income are classified as democratic.

Over the last fifty years, world gross domestic product has increased by about five times, while trade has increased tenfold over the same period. These data suggests that the intensity of the commercial exchange between countries has developed faster than the overall economy. However, globalisation has gone beyond the exchange of physical commodities and it is progressively modelling also the lifestyles and consumption patterns of individuals and societies. The Swiss think-tank KOF has developed a number of globalisation indicators that show the increasing development of global individual, social and commercial networks.

New international flows have diminished the role of traditional political institutions—sometimes with negative consequences for social stability. In many societies, stability (or slow evolution) has been substituted by unstoppable and irreversible transformations.
As a result, individuals and communities perceive a high degree of insecurity—insecurity that touches every aspect of their lives. Growing masses of people feel threatened by the changes that affect their material (work, income, house), psychological (personal relationships), and cultural life (with the need to continuously update knowledge and professional skills).

The social improvement of the masses—resulting from increasing literacy and income, universal means of communication and a new social role of women—has eroded the traditional role of the elites and have weakened the traditional regulatory role of the state.
As the speed of social and cultural evolution sweeps away old life habits, religious beliefs, ancestral moral convictions and radicated political opinions, the anxiety towards a future that is mutating and unknown causes a cultural opposition that is at the root of fundamentalism. Opposition against new life conditions is justified also by increasing economic inequality: "the gap between the wealth of the North and that of the South of the world has increased by a multiple of five since the beginning of the 20th century" ( , "Trois leçons sur la société postindustrielle").

When demographic growth is multiplied by the growth of per capita income and consumption, one can have a measure of the global impact on environmental sustainability. Demographic and economic development is endangering our current forms of civilization and social co-living and our future ability to inhabit our planet.
Alternative scenarios developed by international organizations suggest the possibility of a serious breakdown of natural equilibrium unless political, scientific and economic tools are directed to a correction towards an acceptable equilibrium between humankind and with nature.





</doc>
<doc id="37235" url="https://en.wikipedia.org/wiki?curid=37235" title="Society">
Society

A society is a group of individuals involved in persistent social interaction, or a large social group sharing the same geographical or social territory, typically subject to the same political authority and dominant cultural expectations. Societies are characterized by patterns of relationships (social relations) between individuals who share a distinctive culture and institutions; a given society may be described as the sum total of such relationships among its constituent of members. In the social sciences, a larger society often exhibits stratification or dominance patterns in subgroups.

Insofar as it is collaborative, a society can enable its members to benefit in ways that would not otherwise be possible on an individual basis; both individual and social (common) benefits can thus be distinguished, or in many cases found to overlap. A society can also consist of like-minded people governed by their own norms and values within a dominant, larger society. This is sometimes referred to as a subculture, a term used extensively within criminology.

More broadly, and especially within structuralist thought, a society may be illustrated as an economic, social, industrial or cultural infrastructure, made up of, yet distinct from, a varied collection of individuals. In this regard society can mean the objective relationships people have with the material world and with other people, rather than "other people" beyond the individual and their familiar social environment.

The term "society" came from the Latin word "", which in turn was derived from the noun "socius" ("comrade, friend, ally"; adjectival form "socialis") used to describe a bond or interaction between parties that are friendly, or at least civil. Without an article, the term can refer to the entirety of humanity (also: "society in general", "society at large", etc.), although those who are unfriendly or uncivil to the remainder of society in this sense may be deemed to be "antisocial". However, the Scottish economist, Adam Smith taught instead that a society "may subsist among different men, as among different merchants, from a sense of its utility without any mutual love or affection, if only they refrain from doing injury to each other."

Used in the sense of an association, a society is a body of individuals outlined by the bounds of functional interdependence, possibly comprising characteristics such as national or cultural identity, social solidarity, language, or hierarchical structure.

Society, in general, addresses the fact that an individual has rather limited means as an autonomous unit. The great apes have always been more ("Bonobo", "Homo", "Pan") or less ("Gorilla", "Pongo") social animals, so Robinson Crusoe-like situations are either fictions or unusual corner cases to the ubiquity of social context for humans, who fall between presocial and eusocial in the spectrum of animal ethology.

Cultural relativism as a widespread approach or ethic has largely replaced notions of "primitive", better/worse, or "progress" in relation to cultures (including their material culture/technology and social organization).

According to anthropologist Maurice Godelier, one critical novelty in society, in contrast to humanity's closest biological relatives (chimpanzees and bonobos), is the parental role assumed by the males, which supposedly would be absent in our nearest relatives for whom paternity is not generally determinable.

Societies may also be structured politically. In order of increasing size and complexity, there are bands, tribes, chiefdoms, and state societies. These structures may have varying degrees of political power, depending on the cultural, geographical, and historical environments that these societies must contend with. Thus, a more isolated society with the same level of technology and culture as other societies is more likely to survive than one in close proximity to others that may encroach on their resources. A society that is unable to offer an effective response to other societies it competes with will usually be subsumed into the culture of the competing society.

Sociologist Peter L. Berger defines society as "...a human product, and nothing but a human product, that yet continuously acts upon its producers." According to him, society was created by humans but this creation turns back and creates or molds humans every day. 

Sociologist Gerhard Lenski differentiates societies based on their level of technology, communication, and economy: (1) hunters and gatherers, (2) simple agricultural, (3) advanced agricultural, (4) industrial, and (5) special (e.g. fishing societies or maritime societies). This is similar to the system earlier developed by anthropologists Morton H. Fried, a conflict theorist, and Elman Service, an integration theorist, who have produced a system of classification for societies in all human cultures based on the evolution of social inequality and the role of the state. This system of classification contains four categories:

In addition to this there are:

Over time, some cultures have progressed toward more complex forms of organization and control. This cultural evolution has a profound effect on patterns of community. Hunter-gatherer tribes settled around seasonal food stocks to become agrarian villages. Villages grew to become towns and cities. Cities turned into city-states and nation-states.

Many societies distribute largess at the behest of some individual or some larger group of people. This type of generosity can be seen in all known cultures; typically, prestige accrues to the generous individual or group. Conversely, members of a society may also shun or scapegoat members of the society who violate its norms. Mechanisms such as gift-giving, joking relationships and scapegoating, which may be seen in various types of human groupings, tend to be institutionalized within a society. Social evolution as a phenomenon carries with it certain elements that could be detrimental to the population it serves.

Some societies bestow status on an individual or group of people when that individual or group performs an admired or desired action. This type of recognition is bestowed in the form of a name, title, manner of dress, or monetary reward. In many societies, adult male or female status is subject to a ritual or process of this type. Altruistic action in the interests of the larger group is seen in virtually all societies. The phenomena of community action, shunning, scapegoating, generosity, shared risk, and reward are common to many forms of society.

Societies are social groups that differ according to subsistence strategies, the ways that humans use technology to provide needs for themselves. Although humans have established many types of societies throughout history, anthropologists tend to classify different societies according to the degree to which different groups within a society have unequal access to advantages such as resources, prestige, or power. Virtually all societies have developed some degree of inequality among their people through the process of social stratification, the division of members of a society into levels with unequal wealth, prestige, or power. Sociologists place societies in three broad categories: pre-industrial, industrial, and postindustrial.

In a pre-industrial society, food production, which is carried out through the use of human and animal labor, is the main economic activity. These societies can be subdivided according to their level of technology and their method of producing food. These subdivisions are hunting and gathering, pastoral, horticultural, agricultural, and feudal.

The main form of food production in such societies is the daily collection of wild plants and the hunting of wild animals. Hunter-gatherers move around constantly in search of food. As a result, they do not build permanent villages or create a wide variety of artifacts, and usually only form small groups such as bands and tribes. However, some hunting and gathering societies in areas with abundant resources (such as people of tlingit) lived in larger groups and formed complex hierarchical social structures such as chiefdom. The need for mobility also limits the size of these societies. They generally consist of fewer than 60 people and rarely exceed 100. Statuses within the tribe are relatively equal, and decisions are reached through general agreement. The ties that bind the tribe are more complex than those of the bands. Leadership is personal—charismatic—and used for special purposes only in tribal society. There are no political offices containing real power, and a chief is merely a person of influence, a sort of adviser; therefore, tribal consolidations for collective action are not governmental. The family forms the main social unit, with most members being related by birth or marriage. This type of organization requires the family to carry out most social functions, including production and education.

Pastoralism is a slightly more efficient form of subsistence. Rather than searching for food on a daily basis, members of a pastoral society rely on domesticated herd animals to meet their food needs. Pastoralists live a nomadic life, moving their herds from one pasture to another. Because their food supply is far more reliable, pastoral societies can support larger populations. Since there are food surpluses, fewer people are needed to produce food. As a result, the division of labor (the specialization by individuals or groups in the performance of specific economic activities) becomes more complex. For example, some people become craftworkers, producing tools, weapons, and jewelry. The production of goods encourages trade. This trade helps to create inequality, as some families acquire more goods than others do. These families often gain power through their increased wealth. The passing on of property from one generation to another helps to centralize wealth and power. Over time emerge hereditary chieftainships, the typical form of government in pastoral societies.

Fruits and vegetables grown in garden plots that have been cleared from the jungle or forest provide the main source of food in a horticultural society. These societies have a level of technology and complexity similar to pastoral societies. Some horticultural groups use the slash-and-burn method to raise crops. The wild vegetation is cut and burned, and ashes are used as fertilizers. Horticulturists use human labor and simple tools to cultivate the land for one or more seasons. When the land becomes barren, horticulturists clear a new plot and leave the old plot to revert to its natural state. They may return to the original land several years later and begin the process again. By rotating their garden plots, horticulturists can stay in one area for a fairly long period of time. This allows them to build semipermanent or permanent villages. The size of a village's population depends on the amount of land available for farming; thus villages can range from as few as 30 people to as many as 2000.

As with pastoral societies, surplus food leads to a more complex division of labor. Specialized roles in horticultural societies include craftspeople, shamans (religious leaders), and traders. This role specialization allows people to create a wide variety of artifacts. As in pastoral societies, surplus food can lead to inequalities in wealth and power within horticultural political systems, developed because of the settled nature of horticultural life.

Agrarian societies use agricultural technological advances to cultivate crops over a large area. Sociologists use the phrase agricultural revolution to refer to the technological changes that occurred as long as 8,500 years ago that led to cultivating crops and raising farm animals. Increases in food supplies then led to larger populations than in earlier communities. This meant a greater surplus, which resulted in towns that became centers of trade supporting various rulers, educators, craftspeople, merchants, and religious leaders who did not have to worry about locating nourishment.

Greater degrees of social stratification appeared in agrarian societies. For example, women previously had higher social status because they shared labor more equally with men. In hunting and gathering societies, women even gathered more food than men. However, as food stores improved and women took on lesser roles in providing food for the family, they increasingly became subordinate to men. As villages and towns expanded into neighboring areas, conflicts with other communities inevitably occurred. Farmers provided warriors with food in exchange for protection against invasion by enemies. A system of rulers with high social status also appeared. This nobility organized warriors to protect the society from invasion. In this way, the nobility managed to extract goods from “lesser” members of society.

Feudalism was a form of society based on ownership of land. Unlike today's farmers, vassals under feudalism were bound to cultivating their lord's land. In exchange for military protection, the lords exploited the peasants into providing food, crops, crafts, homage, and other services to the landowner. The estates of the realm system of feudalism was often multigenerational; the families of peasants may have cultivated their lord's land for generations.

Between the 15th and 16th centuries, a new economic system emerged that began to replace feudalism. Capitalism is marked by open competition in a free market, in which the means of production are privately owned. Europe's exploration of the Americas served as one impetus for the development of capitalism. The introduction of foreign metals, silks, and spices stimulated great commercial activity in European societies.

Industrial societies rely heavily on machines powered by fuels for the production of goods. This produced further dramatic increases in efficiency. The increased efficiency of production of the industrial revolution produced an even greater surplus than before. Now the surplus was not just agricultural goods, but also manufactured goods. This larger surplus caused all of the changes discussed earlier in the domestication revolution to become even more pronounced.

Once again, the population boomed. Increased productivity made more goods available to everyone. However, inequality became even greater than before. The breakup of agricultural-based feudal societies caused many people to leave the land and seek employment in cities. This created a great surplus of labor and gave capitalists plenty of laborers who could be hired for extremely low wages.

Post-industrial societies are societies dominated by information, services, and high technology more than the production of goods. Advanced industrial societies are now seeing a shift toward an increase in service sectors over manufacturing and production. The United States is the first country to have over half of its work force employed in service industries. Service industries include government, research, education, health, sales, law, and banking.

The term "society" is currently used to cover both a number of political and scientific connotations as well as a variety of associations.

The development of the Western world has brought with it the emerging concepts of Western culture, politics, and ideas, often referred to simply as "Western society". Geographically, it covers at the very least the countries of Western Europe, North America, Australia, and New Zealand. It sometimes also includes Eastern Europe, South America, and Israel.

The cultures and lifestyles of all of these stem from Western Europe. They all enjoy relatively strong economies and stable governments, allow freedom of religion, have chosen democracy as a form of governance, favor capitalism and international trade, are heavily influenced by Judeo-Christian values, and have some form of political and military alliance or cooperation.

Although the concept of information society has been under discussion since the 1930s, in the modern world it is almost always applied to the manner in which information technologies have impacted society and culture. It therefore covers the effects of computers and telecommunications on the home, the workplace, schools, government, and various communities and organizations, as well as the emergence of new social forms in cyberspace.

One of the European Union's areas of interest is the information society. Here policies are directed towards promoting an open and competitive digital economy, research into information and communication technologies, as well as their application to improve social inclusion, public services, and quality of life.

The International Telecommunications Union's World Summit on the Information Society in Geneva and Tunis (2003 and 2005) has led to a number of policy and application areas where action is envisaged.

As access to electronic information resources increased at the beginning of the 21st century, special attention was extended from the information society to the knowledge society. An analysis by the Irish government stated, "The capacity to manipulate, store and transmit large quantities of information cheaply has increased at a staggering rate over recent years. The digitisation of information and the associated pervasiveness of the Internet are facilitating a new intensity in the application of knowledge to economic activity, to the extent that it has become the predominant factor in the creation of wealth. As much as 70 to 80 percent of economic growth is now said to be due to new and better knowledge."

The Second World Summit on the Knowledge Society, held in Chania, Crete, in September 2009, gave special attention to the following topics:

People of many nations united by common political and cultural traditions, beliefs, or values are sometimes also said to form a society (such as Judeo-Christian, Eastern, and Western). When used in this context, the term is employed as a means of contrasting two or more "societies" whose members represent alternative conflicting and competing worldviews.

Some academic, professional, and scientific associations describe themselves as "societies" (for example, the American Mathematical Society, the American Society of Civil Engineers, or the Royal Society).

In some countries, e.g. the United States, France, and Latin America, the term "society' is used in commerce to denote a partnership between investors or the start of a business. In the United Kingdom, partnerships are not called societies, but co-operatives or mutuals are often known as societies (such as friendly societies and building societies).




</doc>
<doc id="11033682" url="https://en.wikipedia.org/wiki?curid=11033682" title="Societal marketing">
Societal marketing

The societal marketing is a marketing concept that holds that a company should make marketing decisions not only by considering consumers' wants, the company's requirements, but also society's long-term interests.

The societal marketing concept holds that the organization's task is to determine the needs, wants, and interests of a target market and to deliver the desired satisfactions more effectively and efficiently than competitors in a way that preserves or enhances the well being of both the individual consumer and society in general. Therefore, marketers must endeavor to satisfy the needs and wants of their target markets in ways that preserve and enhance the well-being of consumers and society as a whole. It is closely linked with the principles of corporate social responsibility and of sustainable development.

Societal marketing can be defined as a "marketing with a social dimension or marketing that includes non-economic criteria". Societal marketing "concerns for society's long term interests". It is about "the direct benefits for the organization and secondary benefit for the community". Societal marketing distinguishes between the consumer's immediate satisfaction and longer term consumer and social benefits. Accordingly, Andreas Kaplan defines societal management as "management that takes into account society's overall welfare in addition to mere profitability considerations." Its a 3 dimensional concept of marketing ;Social welfare, Individual welfare, Company profits.

Various attempts to define the objectives of societal marketing have been noted, such as:

The concept of societal marketing emerged in the early 1970s, promoting a more socially responsible, moral and ethical model of marketing in an effort to counter some of the more serious criticisms of marketing that had arisen out of the consumerist movement around that time. 

Philip Kotler is generally credited with introducing the societal marketing concept to the literature in a 1972 article "What Consumerism Means for Marketers" in the "Harvard Business Review" of 1972. Certainly Kotler believed that he had coined the term, "societal marketing" and was the first to codify it within the marketing literature. Some marketing historians, notably Wilkie and Moore, have argued that a societal perspective was not new, and that evidence for it could be found in marketing theory and in marketing texts, since the discipline's inception in the early 1900s. Kotler introduced both the concept of social marketing (extending marketing technologies into non-business areas) and societal marketing, arguing that the marketing concept and its technologies must be tempered and ultimately revised by adopting a more explicit social orientation. The novelty of Kotler's concept was the idea of "long-run consumer welfare", emphasizing that the short-term desires might not support the consumer's long term interests or be good for the society as a whole.

The societal marketing concept adopts the position that marketers have a greater social responsibility than simply satisfying customers and providing them with superior value. Instead, marketing activities should strive to benefit society's overall well-being. Marketing organisations that have embraced the societal marketing concept typically identify key stakeholder groups including: employees, customers, local communities, the wider public and government and consider the impact of their activities on all stakeholders. They ensure that marketing activities do not damage the environment and are not hazardous to broader society. Societal marketing developed into sustainable marketing. Societal marketing requires businesses to include social, ethical and ecological considerations in product and market planning. 

Kotler identified four categories of products, classified in terms of long term benefits and immediate satisfaction: 


Kotler's concept of societal marketing suggested that for the well-being of society, deficient products should be eliminated from the market, pleasing and salutary products should go through a product modification process to acquire desirable status, by incorporating missing short term benefits into salutary products and long term benefits into pleasing products, and the companies' ultimate goal should be to develop desirable products. Rather than focusing on selling a products, which can be good or bad for the consumers, companies should focus on consumer and society's well-being.

Most companies recognize that socially responsible activities improve their image among customers, stockholders, the financial community, and other relevant publics. Ethical and socially responsible practices are simply good business, resulting not only in favorable image, but ultimately in increased sales.


Societal marketing should not be confused with social marketing. Societal marketing is a philosophy or mindset that informs marketing decisions whereas social marketing is a distinct branch within the marketing discipline. Societal marketing is concerned with the consideration of the social and ethical aspects of marketing planning. Social marketing is concerned with facilitating social change. A key difference is that the greater 'social good' is the "principal" consideration in social marketing while social benefits are one of a number of considerations in societal marketing. 

On the other hand, social marketing is a sub-branch of marketing that began in 1971, with the publication of an article by Kotler and Zaltman, emphasising a planned approach to achieving social change. It is primarily concerned with encouraging pro-social behaviours (e.g. recycling, sun-safety, safe driving practices) and discouraging anti-social behaviours (e.g. littering, drink-driving). It is defined as an "adaptation of commercial marketing technologies to programs designed to influence the voluntary behavior of target audiences to improve their personal welfare and that of the society of which they are a part". 

Social marketing uses more traditional commercial techniques and strategies, focusing on persuasion, to achieve goals for the greater social good. Its campaigns can either encourage merit goods, as for example fund raising for not-for-profit organizations or dissuade the use of demerit goods promoting society's well being, as non-smoking campaigns or promote the use of seat belts. Another characteristic of social marketing is that is planned to influence individual behaviour to improve well-being. It includes more than just advertising in traditional mass media, and may extend to educational programs and formal enforcement regimes in the case of road safety campaigns. It planned campaigns, implemented by governmental and non-governmental organisations. A clear example that differentiates societal from social marketing is a marketing campaign on non-smoking. A smoking cessation advertisement is an example of social marketing, but if the marketing strategies and techniques used in that campaign focus on increasing the well-being of society, that same campaign can be an example of societal marketing.

The societal marketing concept was a forerunner of sustainable marketing in integrating issues of social responsibility into commercial marketing strategies. In contrast to that, social marketing uses commercial marketing theories, tools and techniques to influence social change. Social marketing applies a "customer orientated" approach and uses the concepts and tools used by commercial marketers in pursuit of social goals like Anti-Smoking-Campaigns or fund raising for NGOs.

Unlike societal marketing, CSR has existed for many years. Another difference is that CSR "focuses more in a corporate level and stakeholders", while societal marketing is more concerned about the consumer and their long term benefits. CSR social and environmental concerns are integrated into all business operations. CSR is mainly run by companies, while social marketing mainly by Government or Non profits organizations. One example of CSR among companies is what Häagen-Dazs is doing with their "microsite" to raise awareness to the general public about the preservation of the honeybee.

Corporations are the one who are striving during the whole time for improvements. They are turning to all kind of forms of corporate societal marketing programs to help build and repair their brand images.

Corporate Social Marketing, or CSM, usually refers to marketing efforts that have at least one social related objective, such as charity founding, among its goals. Typical examples are releasing a certain percentage of the final sale product to a charity related to the product, or sponsoring events that encourage social well-being such as the Olympic Games. Corporate Social Marketing benefits a company in many ways, but its main goal is to improve the image the public has of the company. A company that appears committed to improving the lives or others, the environment or other worthy causes is seen in a better light than one who doesn't, and more and more business are hoping to benefit from that.

So, it can be so, that CSM programs are becoming extremely popular because the leaders believe that it is a good business to be viewed as a socially responsible company. However, even though past research suggests that CSM may be effective in improving brand equity and increasing market share, there are limits to the effectiveness of these initiatives.

An example of his is how corporate social initiatives adversely affected purchase intentions if consumers perceived that the company would forgo product quality in order to be socially responsible.

Depending on the nature of the CSM program, the intent of the corporation may not be as obvious to the consumers. This happens if the benefits to the corporation are not apparent or conflicts with what the consumer already believes about a specific firm or industry.

Since firms exist to make a profit, consumers may spend considerable energy in an attempt to infer motives related to the profit-oriented goals. As an example, a consumer may be suspicious of a tobacco company that undertakes a campaign to prevent underage smoking. If this is successful, the company would be affected and the cigarette sales will be lowered. So, in this situation, consumers' suspicions may lead them to infer motives that would actually protect the companies financial condition – as they are trying to improve their image to sell more cigarettes to adults. However, if a tobacco company undertook a CSM Campaign, that would sustain their business consumers may be able to infer profit motives more easily and then have a more favorable attitude toward the partnership. Therefore, it can be concluded that the attitude of the consumers could be better if they knew more about the motives of the companies and they were more obvious.

Another aspect that may cause suspicion with consumers is the amount of harm that a company has already done, either because of unsafe products or harmful production practices. It is logical that consumers are more suspicious to companies that sell harmful products. Again examples are the tobacco companies and alcohol companies as well. They will meet resistance from consumers when they undertake socially-oriented campaigns aimed at mitigating the effects of their products. That is why when different industries are separated, two very general dimensions are used – the harmful nature of the products and the harmful nature of the production methods.

This classification can briefly show how consumer are influenced by the various CSM efforts. Companies that work in this "dangerous" industries are not that successful always, because the consumers may be suspicious of any societal efforts the company attempts to undertake. Consumers will infer less society-serving motives and more self-serving motives for corporate societal marketing programs undertaken by firms that operate in mixed or sin industries.

Based on how easily consumers could infer profit-driven motives, are classified the types of CSM campaigns: Positively tied to product sales, positively tied to product sales, not directly tied to sales but aimed at sustaining the company's business, completely unrelated.

Societal marketing has been the subject of a number of criticisms: 

A key issue concerns the question of who decides what is in the public's best interests. The moral agenda implicit in the societal marketing concept is underdeveloped and often implicit. Gaski argued that marketers should step away from their classic goal of customer satisfaction and profit maximization while respecting the minimum governmental standards imposed by law and enter this public policy area, since marketers themselves would have to decide what actions are consistent with public welfare. Marketers might have neither the competence nor the right to determine the "public interest." Instead, it should be the customers who decide what is good for them, or their political representatives and dictate that to the industry. 

Some scholars have argued that societal marketing is not a distinct concept, but rather it is a mere extension of the marketing concept. Others have pointed out that the literature in the field is vague, poorly defined and underdeveloped. The societal marketing concept has become an excellent strategy for promotions with social dimensions and for exploring consumers' behavioural response to such corporate 'doing good'.

Societal marketing is gaining the marketers and consumer attention and there is every reason to expect it to continue to evolve in practice. It focuses on providing win-win opportunities to companies, consumers and society. But achieving the compelling benefits for each party involved is very complicated. So much more research is needed. To achieve a win situation for organization involved, is dependent largely upon how the key constituents react. In this context, anticipating consumer reaction is really challenging which can be affected by number of factors that often vary across different segments. The several research questions remain to be answered like how different factors affects reaction to societal marketing and how do the various factors interact? How can societal initiatives be designed to leverage positive reaction and mitigate negative ones? 

For consumers to win, societal marketing must provide them with compelling benefits that increase their overall welfare. What benefits did societal marketing initiative actually provided to consumers? Are there direct benefits such as increased satisfaction with their interaction with commercial or nonprofit organization?
Determining whether there is a win situation for society by societal marketing initiative is the most difficult question to be answered. We turn to the two questions proposed by Bloom, Hussien and Szykmann (1995): Is the society better off because of this program? Does corporate involvement result in better performance than if it would have been managed by NGOs or government agencies? 
Societal marketing is becoming globally popular but there exist a scarcity of research in this field. Therefore, extensive future research is needed particularly investigating questions with respect to its impact on consumer attitudes to corporate image, product image and their purchase intention or brand choice as well as on positive impact on society.




</doc>
<doc id="251368" url="https://en.wikipedia.org/wiki?curid=251368" title="Power structure">
Power structure

A power structure is an overall system of influence between any individual and every other individual within any selected group of people. A description of a power structure would capture the way in which power or authority is distributed between people within groups such as a government, nation, institution, organization, or a society. Such structures are of interest to various fields, including sociology, government, economics, and business. A power structure may be formal and intentionally constructed to maximize values like fairness or efficiency, as in a hierarchical organization wherein every entity, except one, is subordinate to a single other entity. Conversely, a power structure may be an informal set of roles, such as those found in a dominance hierarchy in which members of a social group interact, often aggressively, to create a ranking system. A culture that is organised in a dominance hierarchy is a dominator culture, the opposite of an egalitarian culture of partnership. A visible, dominant group or elite that holds power or authority within a power structure is often referred to as being the Establishment. Power structures are fluid, with changes occurring constantly, either slowly or rapidly, evolving or revolutionary, peacefully or violently.



</doc>
<doc id="273856" url="https://en.wikipedia.org/wiki?curid=273856" title="Industrial society">
Industrial society

In sociology, industrial society is a society driven by the use of technology to enable mass production, supporting a large population with a high capacity for division of labour. Such a structure developed in the Western world in the period of time following the Industrial Revolution, and replaced the agrarian societies of the pre-modern, pre-industrial age. Industrial societies are generally mass societies, and may be succeeded by an information society. They are often contrasted with traditional societies.

Industrial societies use external energy sources, such as fossil fuels, to increase the rate and scale of production. The production of food is shifted to large commercial farms where the products of industry, such as combine harvesters and fossil fuel-based fertilizers, are used to decrease required human labor while increasing production. No longer needed for the production of food, excess labor is moved into these factories where mechanization is utilized to further increase efficiency. As populations grow, and mechanization is further refined, often to the level of automation, many workers shift to expanding service industries.

Industrial society makes urbanization desirable, in part so that workers can be closer to centers of production, and the service industry can provide labor to workers and those that benefit financially from them, in exchange for a piece of production profits with which they can buy goods. This leads to the rise of very large cities and surrounding suburb areas with a high rate of economic activity.

These urban centers require the input of external energy sources in order to overcome the diminishing returns of agricultural consolidation, due partially to the lack of nearby arable land, associated transportation and storage costs, and are otherwise unsustainable. This makes the reliable availability of the needed energy resources high priority in industrial government policies.

Some theoreticians (namely Ulrich Beck, Anthony Giddens, and Manuel Castells) argue that we are located in the middle of a transformation or transition from industrial societies to post-industrial societies. The triggering technology for the change from an agricultural to an industrial organization was steam power, allowing mass production and reducing the agricultural work necessary. Thus, many industrial cities have been built around rivers. Identified as catalyst or trigger for the transition to post-modern or informational society is global information technology.

Some, such as Theodore Kaczynski, have argued that an industrialized society leads to psychological pain and that citizens must actively work to return to a more primitive society. His essay, "Industrial Society and Its Future", describes different political factions and laments the direction of technology and the modern world.




</doc>
<doc id="33524705" url="https://en.wikipedia.org/wiki?curid=33524705" title="Origins of society">
Origins of society

The origins of society — the evolutionary emergence of distinctively human social organization — is an important topic within evolutionary biology, anthropology, prehistory and palaeolithic archaeology. While little is known for certain, debates since Hobbes and Rousseau have returned again and again to the philosophical, moral and evolutionary questions posed.

Arguably the most influential theory of human social origins is that of Thomas Hobbes, who in his "Leviathan" argued that without strong government, society would collapse into "Bellum omnium contra omnes" — "the war of all against all":

If Hobbes' idea is accepted, it follows that society could not have emerged prior to the state. This school of thought has remained influential to this day. Prominent in this respect is British archaeologist Colin Renfrew (Baron Renfrew of Kaimsthorn), who points out that the state did not emerge until long after the evolution of "Homo sapiens". The earliest representatives of our species, according to Renfrew, may well have been "anatomically" modern, but they were not yet "cognitively" or "behaviourally" modern. For example, they lacked political leadership, large-scale cooperation, food production, organised religion, law or symbolic artefacts. Humans were simply hunter-gatherers, who — much like extant apes — ate whatever food they could find in the vicinity. Renfrew controversially suggests that hunter-gatherers to this day think and socialise along lines not radically different from those of their nonhuman primate counterparts. In particular, he says that they do not "ascribe symbolic meaning to material objects" and for that reason "lack fully developed 'mind.'"

However, hunter-gatherer ethnographers emphasise that extant foraging peoples certainly do have social institutions — notably institutionalised rights and duties codified in formal systems of kinship. Elaborate rituals such as initiation ceremonies serve to cement contracts and commitments, quite independently of the state. Other scholars would add that insofar as we can speak of "human revolutions" — "major transitions" in human evolution — the first was not the Neolithic Revolution but the rise of symbolic culture that occurred toward the end of the Middle Stone Age.

Arguing the exact opposite of Hobbes's position, anarchist anthropologist Pierre Clastres views the state and society as mutually incompatible: genuine society is always struggling to survive "against" the state.

Like Hobbes, Jean-Jacques Rousseau argued that society was born in a social contract. In Rousseau's case, however, sovereignty is vested in the entire populace, who enter into the contract directly with one another. "The problem", he explained, "is to find a form of association which will defend and protect with the whole common force the person and goods of each associate, and in which each, while uniting himself with all, may still obey himself alone, and remain as free as before." This is the fundamental problem of which the Social Contract provides the solution. The contract's clauses, Rousseau continued, may be reduced to one — "the total alienation of each associate, together with all his rights, to the whole community. Each man, in giving himself to all, gives himself to nobody; and as there is no associate over whom he does not acquire the same right as he yields others over himself, he gains an equivalent for everything he loses, and an increase of force for the preservation of what he has". In other words: "Each of us puts his person and all his power in common under the supreme direction of the general will, and, in our corporate capacity, we receive each member as an indivisible part of the whole." At once, in place of the individual personality of each contracting party, this act of association creates a moral and collective body, composed of as many members as the assembly contains votes, and receiving from this act its unity, its common identity, its life and its will. By this means, each member of the community acquires not only the capacities of the whole but also, for the first time, rational mentality:

In his influential book, "Ancient Law" (1861), Maine argued that in early times, the basic unit of human social organisation was the patriarchal family:

Hostile to French revolutionary and other radical social ideas, Maine's motives were partly political. He sought to undermine the legacy of Rousseau and other advocates of man’s natural rights by asserting that originally, no one had any rights at all – ‘every man, living during the greater part of his life under the patriarchal despotism, was practically controlled in all his actions by a regimen not of law but of caprice’. Not only were the patriarch’s children subject to what Maine calls his ‘despotism’: his wife and his slaves were equally affected. The very notion of kinship, according to Maine, was simply a way of categorizing those who were forcibly subjected to the despot’s arbitrary rule. Maine later added a Darwinian strand to this argument. In his "The Descent of Man," Darwin had cited reports that a wild-living male gorilla would monopolise for itself as large a harem of females as it could violently defend. Maine endorsed Darwin’s speculation that ‘primeval man’ probably 'lived in small communities, each with as many wives as he could support and obtain, whom he would have jealously guarded against all other men’. Under pressure to spell out exactly what he meant by the term 'patriarchy', Maine clarified that ‘sexual jealousy, indulged through power, might serve as a definition of the Patriarchal Family’.

In his influential book, "Ancient Society" (1877), its title echoing Maine's "Ancient Law," Lewis Henry Morgan proposed a very different theory. Morgan insisted that throughout the earlier periods of human history, neither the state nor the family existed.

Frederick Engels built on Morgan's ideas in his 1884 essay, "The Origin of the Family, Private Property and the State in the light of the researches of Lewis Henry Morgan." His primary interest was the position of women in early society, and — in particular — Morgan's insistence that the matrilineal clan preceded the family as society's fundamental unit. 'The mother-right gens', wrote Engels in his survey of contemporary historical materialist scholarship, 'has become the pivot around which the entire science turns...' Engels argued that the matrilineal clan represented a principle of self-organization so vibrant and effective that it allowed no room for patriarchal dominance or the territorial state.

Emile Durkheim considered that in order to exist, any human social system must counteract the natural tendency for the sexes to promiscuously conjoin. He argued that social order presupposes sexual morality, which is expressed in prohibitions against sex with certain people or during certain periods — in traditional societies particularly during menstruation.

The incest taboo, wrote Durkheim in 1898, is no more than a particular example of something more basic and universal - the ritualistic setting apart of 'the sacred' from 'the profane'. This begins as the segregation of the sexes, each of which - at least on important occasions - is 'sacred' or 'set apart' from the other. 'The two sexes', as Durkheim explains, 'must avoid each other with the same care as the profane flees from the sacred and the sacred from the profane.' Women as sisters act out the role of 'sacred' beings invested 'with an isolating power of some sort, a power which holds the masculine population at a distance.' Their menstrual blood in particular sets them in a category apart, exercising a 'type of repulsing action which keeps the other sex far from them'. In this way, the earliest ritual structure emerges — establishing morally regulated 'society' for the first time.

Charles Darwin pictured early human society as resembling that of apes, with one or more dominant males jealously guarding a harem of females. In his myth of the 'Primal Horde', Sigmund Freud later took all this as his starting point but then postulated an insurrection mounted by the tyrant's own sons: Following this, the band of brothers were about to take sexual possession of their mothers and sisters when suddenly they were overcome with remorse. In their contradictory emotional state, their dead father now became stronger than the living one had been. In memory of him, the brothers revoked their deed by forbidding the killing and eating of the 'totem' (as their father had now become) and renouncing their claim to the women who had just been set free. In this way, the two fundamental taboos of primitive society – not to eat the totem and not to marry one's sisters – were established for the first time.

A related but less dramatic version of Freud's 'sexual revolution' idea was proposed in 1960 by American social anthropologist Marshall Sahlins. Somehow, he writes, the world of primate brute competition and sexual dominance was turned upside-down: 

If we accept Rousseau's line of reasoning, no single dominant individual is needed to embody society, to guarantee security or to enforce social contracts. The people themselves can do these things, combining to enforce the general will. A modern origins theory along these lines is that of evolutionary anthropologist Christopher Boehm. Boehm argues that ape social organisation tends to be despotic, typically with one or more dominant males monopolising access to the locally available females. But wherever there is dominance, we can also expect resistance. In the human case, resistance to being personally dominated intensified as humans used their social intelligence to form coalitions. Eventually, a point was reached when the costs of attempting to impose dominance became so high that the strategy was no longer evolutionarily stable, whereupon social life tipped over into 'reverse dominance' — defined as a situation in which only the entire community, on guard against primate-style individual dominance, is permitted to use force to suppress deviant behaviour.

Human beings, writes social anthropologist Ernest Gellner, are not genetically programmed to be members of this or that social order. You can take a human infant and place it into any kind of social order and it will function acceptably. What makes human society so distinctive is the fabulous range of quite different forms it takes across the world. Yet in any given society, the range of permitted behaviours is quite narrowly constrained. This is not owing to the existence of any externally imposed system of rewards and punishments. The constraints come from within — from certain compulsive moral concepts which members of the social order have internalised. The society installs these concepts in each individual's psyche in the manner first identified by Emile Durkheim, namely, by means of collective rituals such as initiation rites. Therefore, the problem of the origins of society boils down to the problem of the origins of collective ritual.

Feminist scholars — among them palaeoanthropologists Leslie Aiello and Camilla Power — take similar arguments a step further, arguing that any reform or revolution which overthrew male dominance must surely have been led by women. Evolving human females, Power and Aiello suggest, actively separated themselves from males on a periodic basis, using their own blood (and/or pigments such as red ochre) to mark themselves as fertile and defiant: In similar vein, anthropologist Chris Knight argues that Boehm's idea of a 'coalition of everyone' is hard to envisage, unless — along the lines of a modern industrial picket line — it was formed to co-ordinate 'sex-strike' action against badly behaving males: In virtually all hunter-gatherer ethnographies, according to Knight, a persistent theme is that 'women like meat', and that they determinedly use their collective bargaining power to motivate men to hunt for them and bring home their kills — on pain of exclusion from sex. Arguments about women's crucial role in domesticating males — motivating them to cooperate — have also been advanced by anthropologists Kristen Hawkes, Sarah Hrdy and Bruce Knauft among others. Meanwhile, other evolutionary scientists continue to envisage uninterrupted male dominance, continuity with primate social systems and the emergence of society on a gradualist basis without revolutionary leaps.

In his 1985 book, "Social Evolution", Robert Trivers outlines the theoretical framework used today by most evolutionary biologists to understand how and why societies are established. Trivers sets out from the fundamental fact that genes survive beyond the death of the bodies they inhabit, because copies of the same gene may be replicated in multiple different bodies. From this, it follows that a creature should behave altruistically to the extent that those benefiting carry the same genes — 'inclusive fitness', as this source of cooperation in nature is termed. Where animals are unrelated, cooperation should be limited to 'reciprocal altruism' or 'tit-for-tat'.
Where previously, biologists took parent-offspring cooperation for granted, Trivers predicted on theoretical grounds both cooperation and conflict — as when a mother needs to wean an existing baby (even against its will) in order to make way for another. Previously, biologists had interpreted male infanticidal behaviour as aberrant and inexplicable or, alternatively, as a necessary strategy for culling excess population. Trivers was able to show that such behaviour was a logical strategy by males to enhance their own reproductive success at the expense of conspecifics including rival males. Ape or monkey females whose babies are threatened have directly opposed interests, often forming coalitions to defend themselves and their offspring against infanticidal males.
Human society, according to Trivers, is unusual in that it involves the male of the species investing parental care in his own offspring — a rare pattern for a primate. Where such cooperation occurs, it's not enough to take it for granted: in Trivers' view we need to "explain" it using an overarching theoretical framework applicable to humans and nonhumans alike.

Robin Dunbar originally studied gelada baboons in the wild in Ethiopia, and has done much to synthesise modern primatological knowledge with Darwinian theory into a comprehensive overall picture. The components of primate social systems 'are essentially alliances of a political nature aimed at enabling the animals concerned to achieve more effective solutions to particular problems of survival and reproduction'. Primate societies are in essence 'multi-layered sets of coalitions'. Although physical fights are ultimately decisive, the social mobilisation of allies usually decides matters and requires skills that go beyond mere fighting ability. The manipulation and use of coalitions demands sophisticated social — more precisely "political" — intelligence.
Usually but not always, males exercise dominance over females. Even where male despotism prevails, females typically gang up with one another to pursue agendas of their own. When a male gelada baboon attacks a previously dominant rival so as to take over his harem, the females concerned may insist on their own say in the outcome. At various stages during the fighting, the females may 'vote' among themselves on whether to accept the provisional outcome. Rejection is signalled by refusing to groom the challenger; acceptance is signalled by going up to him and grooming him. According to Dunbar, the ultimate outcome of an inter-male 'sexual fight' always depends on the female 'vote'.
Dunbar points out that in a primate social system, lower-ranking females will typically suffer the most intense harassment. Consequently, they will be the first to form coalitions in self-defence. But maintaining commitment from coalition allies involves much time-consuming manual grooming, putting pressure on time-budgets. In the case of evolving humans, who were living in increasingly large groups, the costs would soon have outweighed the benefits — unless some more efficient way of maintaining relationships could be found. Dunbar argues that 'vocal grooming' — using the voice to signal commitment — was the time-saving solution adopted, and that this led eventually to speech. Dunbar goes on to suggest (citing evolutionary anthropologist Chris Knight) that "distinctively human" society may have been evolved under pressure from female ritual and 'gossiping' coalitions established to dissuade males from fighting one another and instead cooperate in hunting for the benefit of the whole camp: Dunbar stresses that this is currently a minority theory among specialists in human origins — most still support the 'bison-down-at-the-lake' theory attributing early language and cooperation to the imperatives of men's activities such as hunting. Despite this, he argues that 'female bonding may have been a more powerful force in human evolution than is sometimes supposed'. Although still controversial, the idea that female coalitions may have played a decisive role has subsequently received strong support from a number of anthropologists including Sarah Hrdy, Camilla Power, Ian Watts. and Jerome Lewis. It is also consistent with recent studies by population geneticists (see Verdu et al. 2013 for Central African Pygmies; Schlebusch 2010 for Khoisan) showing a deep-time tendency to matrilocality among African hunter-gatherers.




</doc>
<doc id="4228181" url="https://en.wikipedia.org/wiki?curid=4228181" title="Stateless society">
Stateless society

A stateless society is a society that is not governed by a state, or, especially in common American English, has no government. In stateless societies, there is little concentration of authority; most positions of authority that do exist are very limited in power and are generally not permanently held positions; and social bodies that resolve disputes through predefined rules tend to be small. Stateless societies are highly variable in economic organization and cultural practices.

While stateless societies were the norm in human prehistory, few stateless societies exist today; almost the entire global population resides within the jurisdiction of a sovereign state. In some regions nominal state authorities may be very weak and wield little or no actual power. Over the course of history most stateless peoples have been integrated into the state-based societies around them.

Some political philosophies, particularly anarchism, consider the state an unwelcome institution and stateless societies the ideal.

In archaeology, cultural anthropology and history, a stateless society denotes a less complex human community without a state, such as a tribe, a clan, a band society or a chiefdom. The main criterion of "complexity" used is the extent to which a division of labor has occurred such that many people are permanently "specialized" in particular forms of production or other activity, and depend on others for goods and services through trade or sophisticated reciprocal obligations governed by custom and laws. An additional criterion is population size. The bigger the population, the more relationships have to be reckoned with.

Evidence of the earliest known city-states has been found in ancient Mesopotamia around 3700 BC, suggesting that the history of the state is less than 6,000 years old; thus, for most of human prehistory the state did not exist.

Generally speaking, the archaeological evidence suggests that the state emerged from stateless communities only when a fairly large population (at least tens of thousands of people) was more or less settled together in a particular territory, and practiced agriculture. Indeed, one of the typical functions of the state is the defense of territory. Nevertheless, there are exceptions: Lawrence Krader for example describes the case of the Tatar state, a political authority arising among confederations of clans of nomadic or semi-nomadic herdsmen.

Characteristically the state functionaries (royal dynasties, soldiers, scribes, servants, administrators, lawyers, tax collectors, religious authorities etc.) are mainly not self-supporting, but rather materially supported and financed by taxes and tributes contributed by the rest of the working population. This assumes a sufficient level of labor-productivity per capita which at least makes possible a "permanent" surplus product (principally foodstuffs) appropriated by the state authority to sustain the activities of state functionaries. Such permanent surpluses were generally not produced on a significant scale in smaller tribal or clan societies.

The archaeologist Gregory Possehl has argued that there is no evidence that the relatively sophisticated, urbanized Harappan civilization, which flourished from about 2,500 to 1,900 BC in the Indus region, featured anything like a centralized state apparatus. No evidence has yet been excavated locally of palaces, temples, a ruling sovereign or royal graves, a centralized administrative bureaucracy keeping records, or a state religion—all of which are elsewhere usually associated with the existence of a state apparatus.

Similarly, in the earliest large-scale human settlements of the stone age which have been discovered, such as Çatal Höyük and Jericho, no evidence was found of the existence of a state authority. The Çatal Höyük settlement of a farming community (7,300 BC to circa 6,200 BC) spanned circa 13 hectares (32 acres) and probably had about 5,000 to 10,000 inhabitants.

Modern state-based societies regularly pushed out stateless indigenous populations as their settlements expanded.

Uncontacted peoples may be considered remnants of prehistoric stateless societies. To varying extents they may be unaware of and unaffected by the states that have nominal authority over their territory.

Some political philosophies consider the state undesirable, and thus consider the formation of a stateless society a goal to be achieved.

A central tenet of anarchism is the advocacy of society without states. The type of society sought for varies significantly between anarchist schools of thought, ranging from extreme individualism to complete collectivism.

In Marxism, Marx's theory of the state considers that in a post-capitalist society the state, an undesirable institution, would be unnecessary and wither away. A related concept is that of stateless communism, a phrase sometimes used to describe Marx's anticipated post-capitalist society.

Anthropologists have found that social stratification is not the standard among all societies. John Gowdy writes, "Assumptions about human behaviour that members of market societies believe to be universal, that humans are naturally competitive and acquisitive, and that social stratification is natural, do not apply to many hunter-gatherer peoples."

The economies of stateless agricultural societies tend to focus and organize subsistence agriculture at the community level, and tend to diversify their production rather than specializing in a particular crop.

In many stateless societies, conflicts between families or individuals are resolved by appealing to the community. Each of the sides of the dispute will voice their concerns, and the community, often voicing its will through village elders, will reach a judgment on the situation. Even when there is no legal or coercive authority to enforce these community decisions, people tend to adhere to them, due to a desire to be held in esteem by the community.



</doc>
<doc id="1728007" url="https://en.wikipedia.org/wiki?curid=1728007" title="Foodservice">
Foodservice

Food service (US English) or catering industry (British English) defines those businesses, institutions, and companies responsible for any meal prepared outside the home. This industry includes restaurants, school and hospital cafeterias, catering operations, and many other formats.

The companies that supply foodservice operators are called foodservice distributors. Foodservice distributors sell goods like small wares (kitchen utensils) and foods. Some companies manufacture products in both consumer and foodservice versions. The consumer version usually comes in individual-sized packages with elaborate label design for retail sale. The foodservice version is packaged in a much larger industrial size and often lacks the colorful label designs of the consumer version.

The food system, including food service and food retailing supplied $1.24 trillion worth of food in 2010 in the US, $594 billion of which was supplied by food service facilities, defined by the USDA as any place which prepares food for immediate consumption on site, including locations that are not primarily engaged in dispensing meals such as recreational facilities and retail stores. Full-service and Fast-food restaurants account for 77% of all foodservice sales, with full-service restaurants accounting for just slightly more than fast food in 2010. The shifts in the market shares between fast food and full-service restaurants to market demand changes the offerings of both foods and services of both types of restaurants.

According to the National Restaurant Association a growing trend among US consumers for the foodservice industry is global cuisine with 66% of US consumers eating more widely in 2015 than in 2010, 80% of consumers eating 'ethnic' cuisines at least once a month, and 29% trying a new 'ethnic' cuisine within the last year.

The Foodservice distributor market size is as of 2015 $231 billion in the US; the national broadline market is controlled by US Foods and Sysco which combined have 60-70% share of the market and were blocked from merging by the FTC for reasons of market power.

Foodservice tends to be, on average, higher in calories and lower in key nutrients than foods prepared at home. Most restaurants, including fast food, have added more salads and fruit offerings and either by choice or in response to local legislation provided nutrition labeling.

In the US the FDA is moving towards establishing uniform guidelines for fast food and restaurant labeling, proposed rules were published in 2011 and final regulations published on 1 December 2014 which supersede State and local menu-labeling provisions, going into effect 1 December 2015. Research has shown that the new labels may influence consumer choices, but primarily if it provides unexpected information and that health-conscious consumers are resistant to changing behaviors based on menu labeling Fast food restaurants are expected by the ERS to do better under the new menu labeling than full service restaurants as full-service restaurants tend to offer much more calorie dense foods, with 50% of fast food meals being between 400 and 800 calories and less than 20% above 1000 calories, in contrast, full-service restaurants 20% of meals are above 1,400 calories. When consumers are aware of the calorie counts at full-service restaurants 20% choose lower calorie options and consumers also reduce their calorie intake over the rest of the day.

Eating one meal away from home each week translates to 2 extra pounds each year or a daily increase of 134 calories and a decrease in diet quality by 2 points on the Healthy Eating Index.

In addition; the likelihood of contracting a food-borne illness (such as E. coli, hepatitis B, H. pylori, listeria, salmonella, norovirus and typhoid) is greatly increased due to food not being kept below 40 degrees Fahrenheit or cooked to a temperature of higher than 160 degrees Fahrenheit, not washing hands for at least 20 seconds for food handlers or not washing contaminated cutting boards and other kitchen tools in hot water.

Counter service is food ordered by the customer at the counter and either picked up at the counter by the customer or delivered to the table by restaurant staff. It is common in fast food restaurants in the United States, and in pubs and bars in the United Kingdom.

Table service is food ordered by the customer at the table and served to the customer's table by waiters and waitresses, also known as "servers". Table service is common in most restaurants. With table service, the customer generally pays at the end of meal. Various methods of table service can be provided, such as silver service.

Gueridon service is a form of food service provided by restaurants to their customers. This type of service encompasses preparing food (primarily salads, main dishes such as beef tartare, or desserts) in direct view of the guests, using a gueridon. A gueridon typically consists of a trolley that is equipped to transport, prepare, cook and serve food. There is a gas hob, chopping board, cutlery drawer, cold store (depending on the trolley type), and general working area.



</doc>
<doc id="29678" url="https://en.wikipedia.org/wiki?curid=29678" title="Trade">
Trade

Trade involves the transfer of goods or services from one person or entity to another, often in exchange for money. A system or network that allows trade is called a market.

An early form of trade, barter, saw the direct exchange of goods and services for other goods and services. Barter involves trading things without the use of money. Later, one bartering party started to involve precious metals, which gained symbolic as well as practical importance. Modern traders generally negotiate through a medium of exchange, such as money. As a result, buying can be separated from selling, or earning. The invention of money (and later credit, paper money and non-physical money) greatly simplified and promoted trade. Trade between two traders is called bilateral trade, while trade involving more than two traders is called multilateral trade.

Trade exists due to specialization and the division of labor, a predominant form of economic activity in which individuals and groups concentrate on a small aspect of production, but use their output in trades for other products and needs. Trade exists between regions because different regions may have a comparative advantage (perceived or real) in the production of some trade-able commodity—including production of natural resources scarce or limited elsewhere, or because different regions' sizes may encourage mass production. In such circumstances, trade at market prices between locations can benefit both locations.

Retail trade consists of the sale of goods or merchandise from a very fixed location (such as a department store, boutique or kiosk), online or by mail, in small or individual lots for direct consumption or use by the purchaser. Wholesale trade is defined as traffic in goods that are sold as merchandise to retailers, or to industrial, commercial, institutional, or other professional business users, or to other wholesalers and related subordinated services.

"Commerce" is derived from the Latin "commercium", from "cum" "together" and "merx", "merchandise."

"Trade" from Middle English "trade" ("path, course of conduct"), introduced into English by Hanseatic merchants, from Middle Low German "trade" ("track, course"), from Old Saxon "trada" ("spoor, track"), from Proto-Germanic "*tradō" ("track, way"), and cognate with Old English "tredan" ("to tread").

Trade originated with human communication in prehistoric times. Trading was the main facility of prehistoric people, who bartered goods and services from each other before the innovation of modern-day currency. Peter Watson dates the history of long-distance commerce from circa 150,000 years ago.

In the Mediterranean region the earliest contact between cultures were of members of the species "Homo sapiens" principally using the Danube river, at a time beginning 35,000–30,000 BCE.

Some trace the origins of commerce to the very start of transaction in prehistoric times. Apart from traditional self-sufficiency, trading became a principal facility of prehistoric people, who bartered what they had for goods and services from each other.

Trade is believed to have taken place throughout much of recorded human history. There is evidence of the exchange of obsidian and flint during the stone age. Trade in obsidian is believed to have taken place in Guinea from 17,000 BCE.
Trade in the stone age was investigated by Robert Carr Bosanquet in excavations of 1901. Trade is believed to have first begun in south west Asia.

Archaeological evidence of obsidian use provides data on how this material was increasingly the preferred choice rather than chert from the late Mesolithic to Neolithic, requiring exchange as deposits of obsidian are rare in the Mediterranean region.

Obsidian is thought to have provided the material to make cutting utensils or tools, although since other more easily obtainable materials were available, use was found exclusive to the higher status of the tribe using "the rich man's flint".

Obsidian was traded at distances of 900 kilometres within the Mediterranean region.

Trade in the Mediterranean during the Neolithic of Europe was greatest in this material. Networks were in existence at around 12,000 BCE Anatolia was the source primarily for trade with the Levant, Iran and Egypt according to Zarins study of 1990. Melos and Lipari sources produced among the most widespread trading in the Mediterranean region as known to archaeology.

The Sari-i-Sang mine in the mountains of Afghanistan was the largest source for trade of lapis lazuli. The material was most largely traded during the Kassite period of Babylonia beginning 1595 BCE.

Ebla was a prominent trading centre during the third millennia, with a network reaching into Anatolia and north Mesopotamia.

Materials used for creating jewelry were traded with Egypt since 3000 BCE. Long-range trade routes first appeared in the 3rd millennium BCE, when Sumerians in Mesopotamia traded with the Harappan civilization of the Indus Valley. The Phoenicians were noted sea traders, traveling across the Mediterranean Sea, and as far north as Britain for sources of tin to manufacture bronze. For this purpose they established trade colonies the Greeks called emporia.

From the beginning of Greek civilization until the fall of the Roman empire in the 5th century, a financially lucrative trade brought valuable spice to Europe from the far east, including India and China. Roman commerce allowed its empire to flourish and endure. The latter Roman Republic and the Pax Romana of the Roman empire produced a stable and secure transportation network that enabled the shipment of trade goods without fear of significant piracy, as Rome had become the sole effective sea power in the Mediterranean with the conquest of Egypt and the near east.

In ancient Greece Hermes was the god of trade (commerce) and weights and measures, for Romans "Mercurius" also god of merchants, whose festival was celebrated by traders on the 25th day of the fifth month. The concept of free trade was an antithesis to the will and economic direction of the sovereigns of the ancient Greek states. Free trade between states was stifled by the need for strict internal controls (via taxation) to maintain security within the treasury of the sovereign, which nevertheless enabled the maintenance of a "" of civility within the structures of functional community life.

The fall of the Roman empire, and the succeeding Dark Ages brought instability to Western Europe and a near collapse of the trade network in the western world. Trade however continued to flourish among the kingdoms of Africa, Middle East, India, China and Southeast Asia. Some trade did occur in the west. For instance, Radhanites were a medieval guild or group (the precise meaning of the word is lost to history) of Jewish merchants who traded between the Christians in Europe and the Muslims of the Near East.

Archaeological evidence (Greenberg 1951) of the first use of trade-marks are from China dated about 2700 BCE.

The emergence of exchange networks in the Pre-Columbian societies of and near to Mexico are known to have occurred within recent years before and after 1500 BCE.

Trade networks reached north to Oasisamerica. There is evidence of established maritime trade with the cultures of northwestern South America and the Caribbean.

During the Middle Ages, commerce developed in Europe by trading luxury goods at trade fairs. Wealth became converted into movable wealth or capital. Banking systems developed where money on account was transferred across national boundaries. Hand to hand markets became a feature of town life, and were regulated by town authorities.

Western Europe established a complex and expansive trade network with cargo ships being the main workhorse for the movement of goods, Cogs and Hulks are two examples of such cargo ships. Many ports would develop their own extensive trade networks. The English port city of Bristol traded with peoples from what is modern day Iceland, all along the western coast of France, and down to what is now Spain.

During the Middle Ages, Central Asia was the economic center of the world. The Sogdians dominated the East-West trade route known as the Silk Road after the 4th century CE up to the 8th century CE, with Suyab and Talas ranking among their main centers in the north. They were the main caravan merchants of Central Asia.

From the 8th to the 11th century, the Vikings and Varangians traded as they sailed from and to Scandinavia. Vikings sailed to Western Europe, while Varangians to Russia. The Hanseatic League was an alliance of trading cities that maintained a trade monopoly over most of Northern Europe and the Baltic, between the 13th and 17th centuries.

Vasco da Gama pioneered the European Spice trade in 1498 when he reached Calicut after sailing around the Cape of Good Hope at the southern tip of the African continent. Prior to this, the flow of spice into Europe from India was controlled by Islamic powers, especially Egypt. The spice trade was of major economic importance and helped spur the Age of Discovery in Europe. Spices brought to Europe from the Eastern world were some of the most valuable commodities for their weight, sometimes rivaling gold.

From 1070 onward, kingdoms in West Africa became significant members of global trade. This came initially through the movement of gold and other resources sent out by Muslim traders on the Trans-Saharan trading network. Later, West Africa exported gold, spices, cloth, and slaves to European traders such as the Portuguese, Dutch, and English. This was often in exchange for cloth, iron, or cowrie shells which were used locally as currency.

In the 16th and 17th centuries, the Portuguese gained economic advantage in the Kingdom of Kongo due to different philosophies of trade. Whereas Portuguese traders concentrated on the accumulation of capital, in Kongo spiritual meaning was attached to many objects of trade. According to economic historian Toby Green, in Kongo "giving more than receiving was a symbol of spiritual and political power, and privilege."

In the 16th century, the Seventeen Provinces were the centre of free trade, imposing no exchange controls, and advocating the free movement of goods. Trade in the East Indies was dominated by Portugal in the 16th century, the Dutch Republic in the 17th century, and the British in the 18th century. The Spanish Empire developed regular trade links across both the Atlantic and the Pacific Oceans.
In 1776, Adam Smith published the paper "An Inquiry into the Nature and Causes of the Wealth of Nations". It criticised Mercantilism, and argued that economic specialisation could benefit nations just as much as firms. Since the division of labour was restricted by the size of the market, he said that countries having access to larger markets would be able to divide labour more efficiently and thereby become more productive. Smith said that he considered all rationalisations of import and export controls "dupery", which hurt the trading nation as a whole for the benefit of specific industries.

In 1799, the Dutch East India Company, formerly the world's largest company, became bankrupt, partly due to the rise of competitive free trade.
In 1817, David Ricardo, James Mill and Robert Torrens showed that free trade would benefit the industrially weak as well as the strong, in the famous theory of comparative advantage. In Principles of Political Economy and Taxation Ricardo advanced the doctrine still considered the most counterintuitive in economics:

The ascendancy of free trade was primarily based on national advantage in the mid 19th century. That is, the calculation made was whether it was in any particular country's self-interest to open its borders to imports.

John Stuart Mill proved that a country with monopoly pricing power on the international market could manipulate the terms of trade through maintaining tariffs, and that the response to this might be reciprocity in trade policy. Ricardo and others had suggested this earlier. This was taken as evidence against the universal doctrine of free trade, as it was believed that more of the economic surplus of trade would accrue to a country following "reciprocal", rather than completely free, trade policies. This was followed within a few years by the infant industry scenario developed by Mill promoting the theory that government had the duty to protect young industries, although only for a time necessary for them to develop full capacity. This became the policy in many countries attempting to industrialise and out-compete English exporters. Milton Friedman later continued this vein of thought, showing that in a few circumstances tariffs might be beneficial to the host country; but never for the world at large.

The Great Depression was a major economic recession that ran from 1929 to the late 1930s. During this period, there was a great drop in trade and other economic indicators.

The lack of free trade was considered by many as a principal cause of the depression causing stagnation and inflation. Only during the World War II the recession ended in the United States. Also during the war, in 1944, 44 countries signed the Bretton Woods Agreement, intended to prevent national trade barriers, to avoid depressions. It set up rules and institutions to regulate the international political economy: the International Monetary Fund and the International Bank for Reconstruction and Development (later divided into the World Bank and Bank for International Settlements). These organisations became operational in 1946 after enough countries ratified the agreement. In 1947, 23 countries agreed to the General Agreement on Tariffs and Trade to promote free trade.

The European Union became the world's largest exporter of manufactured goods and services, the biggest export market for around 80 countries.

Today, trade is merely a subset within a complex system of companies which try to maximize their profits by offering products and services to the market (which consists both of individuals and other companies) at the lowest production cost. A system of international trade has helped to develop the world economy but, in combination with bilateral or multilateral agreements to lower tariffs or to achieve free trade, has sometimes harmed third-world markets for local products.

Free trade advanced further in the late 20th century and early 2000s:

Protectionism is the policy of restraining and discouraging trade between states and contrasts with the policy of free trade. This policy often takes of form of tariffs and restrictive quotas. Protectionist policies were particularly prevalent in the 1930s, between the Great Depression and the onset of World War II.

Islamic teachings encourage trading (and condemn usury or interest).

Judeao-Christian teachings prohibit fraud and dishonest measures, and historically also forbade the charging of interest on loans.

The first instances of money were objects with intrinsic value. This is called commodity money and includes any commonly available commodity that has intrinsic value; historical examples include pigs, rare seashells, whale's teeth, and (often) cattle. In medieval Iraq, bread was used as an early form of money. In Mexico under Montezuma cocoa beans were money. 

Currency was introduced as a standardised money to facilitate a wider exchange of goods and services. This first stage of currency, where metals were used to represent stored value, and symbols to represent commodities, formed the basis of trade in the Fertile Crescent for over 1500 years.

Numismatists have examples of coins from the earliest large-scale societies, although these were initially unmarked lumps of precious metal.

The Doha round of World Trade Organization negotiations aimed to lower barriers to trade around the world, with a focus on making trade fairer for developing countries. Talks have been hung over a divide between the rich developed countries, represented by the G20, and the major developing countries. Agricultural subsidies are the most significant issue upon which agreement has been hardest to negotiate. By contrast, there was much agreement on trade facilitation and capacity building. The Doha round began in Doha, Qatar, and negotiations were continued in: Cancún, Mexico; Geneva, Switzerland; and Paris, France and Hong Kong.

Beginning around 1978, the government of the People's Republic of China (PRC) began an experiment in economic reform. In contrast to the previous Soviet-style centrally planned economy, the new measures progressively relaxed restrictions on farming, agricultural distribution and, several years later, urban enterprises and labor. The more market-oriented approach reduced inefficiencies and stimulated private investment, particularly by farmers, that led to increased productivity and output. One feature was the establishment of four (later five) Special Economic Zones located along the South-east coast.

The reforms proved spectacularly successful in terms of increased output, variety, quality, price and demand. In real terms, the economy doubled in size between 1978 and 1986, doubled again by 1994, and again by 2003. On a real per capita basis, doubling from the 1978 base took place in 1987, 1996 and 2006. By 2008, the economy was 16.7 times the size it was in 1978, and 12.1 times its previous per capita levels. International trade progressed even more rapidly, doubling on average every 4.5 years. Total two-way trade in January 1998 exceeded that for all of 1978; in the first quarter of 2009, trade exceeded the full-year 1998 level. In 2008, China's two-way trade totaled US$2.56 trillion.

In 1991 China joined the Asia-Pacific Economic Cooperation group, a trade-promotion forum.<https://www.apec.org/About-Us/About-APEC/Member-Economies> In 2001, it also joined the World Trade Organization.<https://www.wto.org/english/thewto_e/countries_e/china_e.htm>

International trade is the exchange of goods and services across national borders. In most countries, it represents a significant part of GDP. While international trade has been present throughout much of history (see Silk Road, Amber Road), its economic, social, and political importance have increased in recent centuries, mainly because of Industrialization, advanced transportation, globalization, multinational corporations, and outsourcing.

Empirical evidence for the success of trade can be seen in the contrast between countries such as South Korea, which adopted a policy of export-oriented industrialization, and India, which historically had a more closed policy. South Korea has done much better by economic criteria than India over the past fifty years, though its success also has to do with effective state institutions.

Trade sanctions against a specific country are sometimes imposed, in order to punish that country for some action. An embargo, a severe form of externally imposed isolation, is a blockade of all trade by one country on another. For example, the United States has had an embargo against Cuba for over 40 years.

International trade, which is governed by the World Trade Organization, can be restricted by both tariff and non-tariff barriers. International trade is usually regulated by governmental quotas and restrictions, and often taxed by tariffs. Tariffs are usually on imports, but sometimes countries may impose export tariffs or subsidies. Non-tariff barriers include Sanitary and Phytosanitary rules, labeling requirements and food safety regulations. All of these are called "trade barriers". If a government removes all trade barriers, a condition of free trade exists. A government that implements a protectionist policy establishes trade barriers. There are usually few trade restrictions within countries although a common feature of many developing countries is police and other road blocks along main highways, that primarily exist to extract bribes.

The "fair trade" movement, also known as the "trade justice" movement, promotes the use of labour, environmental and social standards for the production of commodities, particularly those exported from the Third and Second Worlds to the First World. Such ideas have also sparked a debate on whether trade itself should be codified as a human right.

Importing firms voluntarily adhere to fair trade standards or governments may enforce them through a combination of employment and commercial law. Proposed and practiced fair trade policies vary widely, ranging from the common prohibition of goods made using slave labour to minimum price support schemes such as those for coffee in the 1980s. Non-governmental organizations also play a role in promoting fair trade standards by serving as independent monitors of compliance with labeling requirements. As such, it is a form of Protectionism.




</doc>
<doc id="228047" url="https://en.wikipedia.org/wiki?curid=228047" title="Human communication">
Human communication

Human communication, or anthroposemiotics, is the field dedicated to understanding how humans communicate. Human communication is grounded in cooperative and shared intentions.

Humans have communication abilities that other animals do not. Being able to communicate aspects like time and place as though they were solid objects are a few examples. It is said that humans communicate to request help, to inform others, and to share attitudes as a way of bonding. Communication is a joint activity which largely depends on the ability to keep common attention, to share the relevant background knowledge and joint experience in order to get the content across and make sense in the exchanges.

The current study of human communication can be branched off into two major categories; rhetorical and relational. The focus of rhetorical communication is primarily on the study of influence; the art of rhetorical communication is based on the idea of persuasion. The relational approach examines communication from a transactional perspective; two or more people interact to reach an agreed perspective.

In its early stages, rhetoric was developed to help ordinary people prove their claims in court; this shows how persuasion is key in this form of communication. Aristotle stated that effective rhetoric is based on argumentation. As explained in the text, rhetoric involves a dominant party and a submissive party or a party that succumbs to that of the most dominant party. While the rhetorical approach stems from Western societies, the relational approach stems from Eastern societies. Eastern societies hold higher standards for cooperation, which makes sense as to why they would sway more toward a relational approach for that matter. "Maintaining valued relationships is generally seen as more important than exerting influence and control over others". "The study of human communication today is more diversified than ever before in its history".

Classification of human communication can be found in the workplace, especially for group work. Co-workers need to argue with each other to gain the best solutions for their projects, while they also need to nurture their relationship to maintain their collaboration. For example, in their group work, they may use the communication tactic of "saving face".

Spoken language involves speech, a mostly human quality to acquire. For example, chimpanzees are humans' closest relative, but they are unable to produce speech. Chimpanzees are the closest living species to humans. Chimpanzees are closer to humans, in genetic and evolutionary terms, than they are to gorillas or other apes. The fact that a chimpanzee will not acquire speech, even when raised in a human home with all the environmental input of a normal human child, is one of the central puzzles we face when contemplating the biology of our species. In repeated experiments, starting in the 1910s, chimpanzees raised in close contact with humans have universally failed to speak, or even to try to speak, despite their rapid progress in many other intellectual and motor domains. Each normal human is born with a capacity to rapidly and unerringly acquire their mother tongue, with little explicit teaching or coaching. In contrast, no nonhuman primate has spontaneously produced even a word of the local language.

Human communication can be subdivided into a variety of types:



</doc>
<doc id="10401954" url="https://en.wikipedia.org/wiki?curid=10401954" title="Family">
Family

In the context of human society, a family (from ) is a group of people related either by consanguinity (by recognized birth), affinity (by marriage or other relationship), or co-residence (as implied by the etymology of the English word "family") or some combination of these. Members of the immediate family may include spouses, parents, brothers, sisters, sons, and daughters. Members of the extended family may include grandparents, aunts, uncles, cousins, nephews, nieces, and siblings-in-law. Sometimes these are also considered members of the immediate family, depending on an individual's specific relationship with them.

In most societies, the family is the principal institution for the socialization of children. As the basic unit for raising children, anthropologists generally classify most family organizations as matrifocal (a mother and her children); conjugal (a wife, her husband, and children, also called the nuclear family); avuncular (for example, a grandparent, a brother, his sister, and her children); or extended (parents and children co-reside with other members of one parent's family). Sexual relations among the members are regulated by rules concerning incest such as the incest taboo.

The word "family" can be used metaphorically to create more inclusive categories such as community, nationhood, global village, and humanism.

The field of genealogy aims to trace family lineages through history.

The family is also an important economic unit studied in family economics.

One of the primary functions of the family involves providing a framework for the production and reproduction of persons biologically and socially. This can occur through the sharing of material substances (such as food); the giving and receiving of care and nurture (nurture kinship); jural rights and obligations; and moral and sentimental ties. Thus, one's experience of one's family shifts over time. From the perspective of children, the family is a "family of orientation": the family serves to locate children socially and plays a major role in their enculturation and socialization. From the point of view of the parent(s), the family is a "family of procreation", the goal of which is to produce and enculturate and socialize children. However, producing children is not the only function of the family; in societies with a sexual division of labor, marriage, and the resulting relationship between two people, it is necessary for the formation of an economically productive household.

Christopher Harris notes that the western conception of family is ambiguous and confused with the household, as revealed in the different contexts in which the word is used. Olivia Harris states this confusion is not accidental, but indicative of the familial ideology of capitalist, western countries that pass social legislation that insists members of a nuclear family should live together, and that those not so related should not live together; despite the ideological and legal pressures, a large percentage of families do not conform to the ideal nuclear family type.

The total fertility rate of women varies from country to country, from a high of 6.76 children born/woman in Niger to a low of 0.81 in Singapore (as of 2015). Fertility is low in most Eastern European and Southern European countries; and high in most Sub-Saharan African countries.

In some cultures, the mother's preference of family size influences that of the children through early adulthood. A parent's number of children strongly correlates with the number of children that they will eventually have.

Although early western cultural anthropologists and sociologists considered family and kinship to be universally associated with relations by "blood" (based on ideas common in their own cultures) later research has shown that many societies instead understand family through ideas of living together, the sharing of food (e.g. milk kinship) and sharing care and nurture. Sociologists have a special interest in the function and status of family forms in stratified (especially capitalist) societies.

According to the work of scholars Max Weber, Alan Macfarlane, Steven Ozment, Jack Goody and Peter Laslett, the huge transformation that led to modern marriage in Western democracies was "fueled by the religio-cultural value system provided by elements of Judaism, early Christianity, Roman Catholic canon law and the Protestant Reformation".

Much sociological, historical and anthropological research dedicates itself to the understanding of this variation, and of changes in the family that form over time. Levitan claims:
"Times have changed; it is more acceptable and encouraged for mothers to work and fathers to spend more time at home with the children. The way roles are balanced between the parents will help children grow and learn valuable life lessons. There is [the] great importance of communication and equality in families, in order to avoid role strain."
The term "nuclear family" is commonly used, especially in the United States of America, to refer to conjugal families. A "conjugal" family includes only the spouses and unmarried children who are not of age. Some sociologists distinguish between conjugal families (relatively independent of the kindred of the parents and of other families in general) and nuclear families (which maintain relatively close ties with their kindred).
Other family structures - with (for example) blended parents, single parents, and domestic partnerships – have begun to challenge the normality of the nuclear family.

A "single-parent family" consist one parent together with his or her children, where the parent is either widowed, divorced and not remarried, or never married. The parent may either have sole custody of the children, or, the parents may have a shared parenting arrangement, where the children divide their time equally between two different single-parent families or between one single-parent family and one blended family. Physical, mental and social well-being is lower for sole custody children, compared to children in nuclear families or shared parenting arrangements. The number of single-parent families have been increasing, and about half of all children in the United States have lived in a single-parent family at some point before they reach the age of 18. Most single-parent families are headed by a mother, but the number of single-parent families headed by fathers is increasing.

A "matrifocal" family consists of a mother and her children. Generally, these children are her biological offspring, although adoption of children is a practice in nearly every society. This kind of family occurs commonly where women have the resources to rear their children by themselves, or where men are more mobile than women. As a definition, "a family or domestic group is matrifocal when it is centred on a woman and her children. In this case, the father(s) of these children are intermittently present in the life of the group and occupy a secondary place. The children's mother is not necessarily the wife of one of the children's fathers."

The term "extended family" is also common, especially in the United States. This term has two distinct meanings:


These types refer to ideal or normative structures found in particular societies. Any society will exhibit some variation in the actual composition and conception of families.

The term "family of choice," also sometimes referred to as "chosen family," is common within the LGBT community, both in academic literature and in colloquial vocabulary. It refers to the group of people in an individual's life that satisfies the typical role of family as a support system. The term differentiates between the "family of origin" (the biological family or that in which people are raised) and those that actively assume that ideal role.
The family of choice may or may not include some or all of the members of the family of origin. This terminology stems from the fact that many LGBT individuals, upon coming out, face rejection or shame from the families they were raised in. The term family of choice is also used by individuals in the 12 step communities, who create close-knit "family" ties through the recovery process.

The term "blended family" or "stepfamily" describes families with mixed parents: one or both parents remarried, bringing children of the former family into the new family. Also in sociology, particularly in the works of social psychologist Michael Lamb, "traditional family" refers to "a middle-class family with a bread-winning father and a stay-at-home mother, married to each other and raising their biological children," and "nontraditional" to exceptions from this rule. Most of the US households are now non-traditional under this definition. Critics of the term "traditional family" point out that in most cultures and at most times, the extended family model has been most common, not the nuclear family, though it has had a longer tradition in England than in other parts of Europe and Asia which contributed large numbers of immigrants to the Americas. The nuclear family became the most common form in the U.S. in the 1960s and 1970s.

In terms of communication patterns in families, there are a certain set of beliefs within the family that reflect how its members should communicate and interact. These family communication patterns arise from two underlying sets of beliefs. One being conversation orientation (the degree to which the importance of communication is valued) and two, conformity orientation (the degree to which families should emphasize similarities or differences regarding attitudes, beliefs, and values).

A monogamous family is based on a legal or social monogamy. In this case, an individual has only one (official) partner during their lifetime or at any one time (i.e. serial monogamy). This means that a person may not have several different legal spouses at the same time, as this is usually prohibited by bigamy laws, in jurisdictions that require monogamous marriages.

Polygamy is a marriage that includes more than two partners. When a man is married to more than one wife at a time, the relationship is called polygyny; and when a woman is married to more than one husband at a time, it is called polyandry. If a marriage includes multiple husbands and wives, it can be called polyamory, group or conjoint marriage.

Polygyny is a form of plural marriage, in which a man is allowed more than one wife . In modern countries that permit polygamy, polygyny is typically the only form permitted. Polygyny is practiced primarily (but not only) in parts of the Middle East and Africa; and is often associated with Islam, however, there are certain conditions in Islam that must be met to perform polygyny.

Polyandry is a form of marriage whereby a woman takes two or more husbands at the same time.<ref name="Starkweather/Hames 2012"></ref> Fraternal polyandry, where two or more brothers are married to the same wife, is a common form of polyandry. Polyandry was traditionally practiced in areas of the Himalayan mountains, among Tibetans in Nepal, in parts of China and in parts of northern India. Polyandry is most common in societies marked by high male mortality or where males will often be apart from the rest of the family for a considerable period of time.

A first-degree relative is one who shares 50% of your DNA through direct inheritance, such as a full sibling, parent or progeny.

There is another measure for the degree of relationship, which is determined by counting up generations to the first common ancestor and back down to the target individual, which is used for various genealogical and legal purposes. 

In his book "Systems of Consanguinity and Affinity of the Human Family", anthropologist Lewis Henry Morgan (1818–1881) performed the first survey of kinship terminologies in use around the world. Although much of his work is now considered dated, he argued that kinship terminologies reflect different sets of distinctions. For example, most kinship terminologies distinguish between sexes (the difference between a brother and a sister) and between generations (the difference between a child and a parent). Moreover, he argued, kinship terminologies distinguish between relatives by blood and marriage (although recently some anthropologists have argued that many societies define kinship in terms other than "blood").

Morgan made a distinction between kinship systems that use "classificatory" terminology and those that use "descriptive" terminology. Classificatory systems are generally and erroneously understood to be those that "class together" with a single term relatives who actually do not have the same type of relationship to ego. (What defines "same type of relationship" under such definitions seems to be genealogical relationship. This is problematic given that any genealogical description, no matter how standardized, employs words originating in a folk understanding of kinship.) What Morgan's terminology actually differentiates are those (classificatory) kinship systems that do not distinguish lineal and collateral relationships and those (descriptive) kinship systems that do. Morgan, a lawyer, came to make this distinction in an effort to understand Seneca inheritance practices. A Seneca man's effects were inherited by his sisters' children rather than by his own children. Morgan identified six basic patterns of kinship terminologies:

Most Western societies employ Eskimo kinship terminology. This kinship terminology commonly occurs in societies based on conjugal (or nuclear) families, where nuclear families have a degree of relative mobility. Members of the nuclear use descriptive kinship terms:

Such systems generally assume that the mother's husband is also the biological father. In some families, a woman may have children with more than one man or a man may have children with more than one woman. The system refers to a child who shares only one parent with another child as a "half-brother" or "half-sister". For children who do not share biological or adoptive parents in common, English-speakers use the term "stepbrother" or "stepsister" to refer to their new relationship with each other when one of their biological parents marries one of the other child's biological parents. Any person (other than the biological parent of a child) who marries the parent of that child becomes the "stepparent" of the child, either the "stepmother" or "stepfather". The same terms generally apply to children adopted into a family as to children born into the family. In the United States, one in five mothers have children by different fathers; among mothers with two or more children the figure is higher, with 28% having children with at least two different men. Such families are more common among Blacks and Hispanics, and among the lower socioeconomic class.

Typically, societies with conjugal families also favor neolocal residence; thus upon marriage, a person separates from the nuclear family of their childhood (family of orientation) and forms a new nuclear family (family of procreation). However, in western society, the single parent family has been growing more accepted and has begun to make an impact on culture. Single parent families are more commonly single mother families than single father. These families sometimes face difficult issues besides the fact that they have to rear their children on their own, for example, low income making it difficult to pay for rent, child care, and other necessities for a healthy and safe home. Members of the nuclear families of members of one's own (former) nuclear family may class as lineal or as collateral. Kin who regard them as lineal refer to them in terms that build on the terms used within the nuclear family:

For collateral relatives, more classificatory terms come into play, terms that do not build on the terms used within the nuclear family:

When additional generations intervene (in other words, when one's collateral relatives belong to the same generation as one's grandparents or grandchildren), the prefixes "great-" or "grand-" modifies these terms. Also, as with grandparents and grandchildren, as more generations intervene the prefix becomes "great-grand-," adding another "great-" for each additional generation. Most collateral relatives have never had membership of the nuclear family of the members of one's own nuclear family.

Cousins of an older generation (in other words, one's parents' first cousins), although technically first cousins once removed, are often classified with "aunts" and "uncles." Similarly, a person may refer to close friends of one's parents as "aunt" or "uncle," or may refer to close friends as "brother" or "sister," using the practice of fictive kinship. English-speakers mark relationships by marriage (except for wife/husband) with the tag "-in-law." The mother and father of one's spouse become one's mother-in-law and father-in-law; the female spouse of one's child becomes one's daughter-in-law and the male spouse of one's child becomes one's son-in-law. The term "sister-in-law" refers to three essentially different relationships, either the wife of one's sibling, or the sister of one's spouse, or, in some uses, the wife of one's spouse's sibling. "Brother-in-law" expresses a similar ambiguity. The terms "half-brother" and "half-sister" indicate siblings who share only one biological or adoptive parent.

Patrilineality, also known as "the male line" or "agnatic kinship", is a form of kinship system in which an individual's family membership derives from and is traced through his or her father's lineage. It generally involves the inheritance of property, rights, names, or titles by persons related through male kin.

A patriline ("father line") is a person's father, and additional ancestors that are traced only through males. One's patriline is thus a record of descent from a man in which the individuals in all intervening generations are male. In cultural anthropology, a patrilineage is a consanguineal male and female kinship group, each of whose members is descended from the common ancestor through male forebears.

Matrilineality is a form of kinship system in which an individual's family membership derives from and is traced through his or her mother's lineage.

It may also correlate with a societal system in which each person is identified with their matriline—their mother's lineage—and which can involve the inheritance of property and titles. A matriline is a line of descent from a female ancestor to a descendant in which the individuals in all intervening generations are mothersin other words, a "mother line".

In a matrilineal descent system, an individual is considered to belong to the same descent group as her or his mother. This matrilineal descent pattern is in contrasts to the more common pattern of patrilineal descent pattern.

Bilateral descent is a form of kinship system in which an individual's family membership derives from and is traced through both the paternal and maternal sides. The relatives on the mother's side and father's side are equally important for emotional ties or for transfer of property or wealth. It is a family arrangement where descent and inheritance are passed equally through both parents. Families who use this system trace descent through both parents simultaneously and recognize multiple ancestors, but unlike with cognatic descent it is not used to form descent groups.

Traditionally, this is found among some groups in West Africa, India, Australia, Indonesia, Melanesia, Malaysia and Polynesia. Anthropologists believe that a tribal structure based on bilateral descent helps members live in extreme environments because it allows individuals to rely on two sets of families dispersed over a wide area.

Early scholars of family history applied Darwin's biological theory of evolution in their theory of evolution of family systems. American anthropologist Lewis H. Morgan published "Ancient Society" in 1877 based on his theory of the three stages of human progress from Savagery through Barbarism to Civilization. Morgan's book was the "inspiration for Friedrich Engels' book" "The Origin of the Family, Private Property and the State" published in 1884.

Engels expanded Morgan's hypothesis that economical factors caused the transformation of primitive community into a class-divided society. Engels' theory of resource control, and later that of Karl Marx, was used to explain the cause and effect of change in family structure and function. The popularity of this theory was largely unmatched until the 1980s, when other sociological theories, most notably structural functionalism, gained acceptance.

Contemporary society generally views the family as a haven from the world, supplying absolute fulfillment. Zinn and Eitzen discuss the image of the "family as haven [...] a place of intimacy, love and trust where individuals may escape the competition of dehumanizing forces in modern society".
During industrialization, "[t]he family as a repository of warmth and tenderness (embodied by the mother) stands in opposition to the competitive and aggressive world of commerce (embodied by the father). The family's task was to protect against the outside world." However, Zinn and Eitzen note, "The protective image of the family has waned in recent years as the ideals of family fulfillment have taken shape. Today, the family is more compensatory than protective. It supplies what is vitally needed but missing in other social arrangements."

"The popular wisdom", according to Zinn and Eitzen, sees the family structures of the past as superior to those today, and families as more stable and happier at a time when they did not have to contend with problems such as illegitimate children and divorce. They respond to this, saying, "there is no golden age of the family gleaming at us in the far back historical past." "Desertion by spouses, illegitimate children, and other conditions that are considered characteristics of modern times existed in the past as well."

Others argue that whether or not one views the family as "declining" depends on one's definition of "family". "Married couples have dropped below half of all American households. This drop is shocking from traditional forms of the family system. Only a fifth of households were following traditional ways of having married couples raising a family together." In the Western World, marriages are no longer arranged for economic, social or political gain, and children are no longer expected to contribute to family income. Instead, people choose mates based on love. This increased role of love indicates a societal shift toward favoring emotional fulfilment and relationships within a family, and this shift necessarily weakens the institution of the family.

Margaret Mead considers the family as a main safeguard to continuing human progress. Observing, "Human beings have learned, laboriously, to be human", she adds: "we hold our present form of humanity on trust, [and] it is possible to lose it" ... "It is not without significance that the most successful large-scale abrogations of the family have occurred not among simple savages, living close to the subsistence edge, but among great nations and strong empires, the resources of which were ample, the populations huge, and the power almost unlimited"

Many countries (particularly Western) have, in recent years, changed their family laws in order to accommodate diverse family models. For instance, in the United Kingdom, in Scotland, the "Family Law (Scotland) Act 2006" provides cohabitants with some limited rights. In 2010, Ireland enacted the Civil Partnership and Certain Rights and Obligations of Cohabitants Act 2010. There have also been moves at an international level, most notably, the Council of Europe "European Convention on the Legal Status of Children Born out of Wedlock" which came into force in 1978. Countries which ratify it must ensure that children born outside marriage are provided with legal rights as stipulated in the text of this Convention. The Convention was ratified by the UK in 1981 and by Ireland in 1988.

The model, common in the western societies, of the family triangle, husband-wife-children isolated from the outside, is also called the "oedipal model of the family", and it is a form of patriarchal family. Many philosophers and psychiatrists have analyzed such a model.
In such a family, they argue, the young develop in a perverse relationship, wherein they learn to love the same person who beats and oppresses them. They believe that young children grow up and develop loving a person who is oppressing them physically or mentally, and that these children are not taught in a way that will raise affectionate children. Such philosophers claim that the family therefore constitutes the first cell of the fascist society, as the children will carry this attitude of love for oppressive figures in their adult life. They claim that fathers torment their sons. Deleuze and Guattari, in their analysis of the dynamics at work within a family, "track down all varieties of fascism, from the enormous ones that surround and crush us to the petty ones that constitute the tyrannical bitterness of our everyday lives".

As it has been explained by Deleuze, Guattari and Foucault, as well as other philosophers and psychiatrists such as Laing and Reich, the patriarchal-family conceived in the West tradition serves the purpose of perpetuating a propertarian and authoritarian society. The child grows according to the oedipal model, which is typical of the structure of capitalist societies, and he becomes in turn owner of submissive children and protector of the woman.

As the young undergoes physical and psychological repression from someone for whom they develop love, they develop a loving attitude towards authority figures. They will bring such attitude in their adult life, when they will desire social repression and will form docile subjects for society. Michel Foucault, in his systematic study of sexuality, argued that rather than being merely repressed, the desires of the individual are efficiently mobilized and used, to control the individual, alter interpersonal relationships and control the masses. Foucault believed organized religion, through moral prohibitions, and economic powers, through advertising, make use of unconscious sex drives. Dominating desire, they dominate individuals. According to the analysis of Michel Foucault, in the west:

Domestic violence (DV) is violence that happens within the family. The legal and social understanding of the concept of DV differs by culture. The definition of the term "domestic violence" varies, depending on the context in which it is used. It may be defined differently in medical, legal, political or social contexts. The definitions have varied over time, and vary in different parts of the world.

The Convention on preventing and combating violence against women and domestic violence states that:

In 1993, the United Nations Declaration on the Elimination of Violence against Women identified domestic violence as one of three contexts in which violence against women occurs, describing it as:

Family violence is a broader definition, often used to include child abuse, elder abuse, and other violent acts between family members.

Child abuse is defined by the WHO as:

Elder abuse is, according to the WHO: "a single, or repeated act, or lack of appropriate action, occurring within any relationship where there is an expectation of trust which causes harm or distress to an older person".

Child abuse is the physical, sexual or emotional maltreatment or neglect of a child or children. In the United States, the Centers for Disease Control and Prevention (CDC) and the Department for Children and Families (DCF) define child maltreatment as any act or series of acts of commission or omission by a parent or other caregiver that results in harm, potential for harm, or threat of harm to a child. Child abuse can occur in a child's home, or in the organizations, schools or communities the child interacts with. There are four major categories of child abuse: neglect, physical abuse, psychological or emotional abuse, and sexual abuse.

Abuse of parents by their children is a common but under reported and under researched subject. Parents are quite often subject to levels of childhood aggression in excess of normal childhood aggressive outbursts, typically in the form of verbal or physical abuse. Parents feel a sense of shame and humiliation to have that problem, so they rarely seek help and there is usually little or no help available anyway.

Elder abuse is "a single, or repeated act, or lack of appropriate action, occurring within any relationship where there is an expectation of trust, which causes harm or distress to an older person." This definition has been adopted by the World Health Organization from a definition put forward by Action on Elder Abuse in the UK. Laws protecting the elderly from abuse are similar to, and related to, laws protecting dependent adults from abuse.

The core element to the harm of elder abuse is the "expectation of trust" of the older person toward their abuser. Thus, it includes harms by people the older person knows or with whom they have a relationship, such as a spouse, partner or family member, a friend or neighbor, or people that the older person relies on for services. Many forms of elder abuse are recognized as types of domestic violence or family violence.

Forced and child marriages are practiced in certain regions of the world, particularly in Asia and Africa, and these types of marriages are associated with a high rate of domestic violence.

A forced marriage is a marriage where one or both participants are married without their freely given consent. The line between forced marriage and consensual marriage may become blurred, because the social norms of many cultures dictate that one should never oppose the desire of one's parents/relatives in regard to the choice of a spouse; in such cultures it is not necessary for violence, threats, intimidation etc. to occur, the person simply "consents" to the marriage even if he/she doesn't want it, out of the implied social pressure and duty. The customs of bride price and dowry, that exist in parts of the world, can lead to buying and selling people into marriage.

A child marriage is a marriage where one or both spouses are under 18. Child marriage was common throughout history but is today condemned by international human rights organizations. Child marriages are often arranged between the families of the future bride and groom, sometimes as soon as the girl is born. Child marriages can also occur in the context of marriage by abduction.

Family honor is an abstract concept involving the perceived quality of worthiness and respectability that affects the social standing and the self-evaluation of a group of related people, both corporately and individually. The family is viewed as the main source of honor and the community highly values the relationship between honor and the family. The conduct of family members reflects upon family honor and the way the family perceives itself, and is perceived by others. In cultures of honor maintaining the family honor is often perceived as more important than either individual freedom, or individual achievement. In extreme cases, engaging in acts that are deemed to tarnish the honor of the family results in honor killings. An honor killing is the homicide of a member of a family or social group by other members, due to the perpetrators' belief that the victim has brought shame or dishonor upon the family or community, usually for reasons such as refusing to enter an arranged marriage, being in a relationship that is disapproved by their relatives, having sex outside marriage, becoming the victim of rape, dressing in ways which are deemed inappropriate, or engaging in homosexual relations.

A family is often part of a sharing economy with common ownership.

Dowry is property (money, goods, or estate) that a wife or wife's family gives to her husband when the wife and husband marry. Offering dowry was common in many cultures historically (including in Europe and North America), but this practice today is mostly restricted to some areas primarily in the Indian subcontinent.

Bride price, (also bridewealth or bride token), is property paid by the groom or his family to the parents of a woman upon the marriage of their daughter to the groom. It is practiced mostly in Sub-Saharan Africa, parts of South-East Asia (Thailand, Cambodia), and parts of Central Asia.

Dower is property given to the bride herself by the groom at the time of marriage, and which remains under her ownership and control.

In some countries married couples benefit from various taxation advantages not available to a single person or to unmarried couples. For example, spouses may be allowed to average their combined incomes. Some jurisdictions recognize common law marriage or "de facto" relations for this purposes. In some jurisdictions there is also an option of civil partnership or domestic partnership.

Different property regimes exist for spouses. In many countries, each marriage partner has the choice of keeping their property separate or combining properties. In the latter case, called community property, when the marriage ends by divorce each owns half. In lieu of a will or trust, property owned by the deceased generally is inherited by the surviving spouse.

Reproductive rights are legal rights and freedoms relating to reproduction and reproductive health. These include the right to decide on issues regarding the number of children born, family planning, contraception, and private life, free from coercion and discrimination; as well as the right to access health services and adequate information. According to UNFPA, reproductive rights "include the right to decide the number, timing and spacing of children, the right to voluntarily marry and establish a family, and the right to the highest attainable standard of health, among others". Family planning refers to the factors that may be considered by individuals and couples in order for them to control their fertility, anticipate and attain the desired number of children and the spacing and timing of their births.

The state and church have been, and still are in some countries, involved in controlling the size of families, often using coercive methods, such as bans on contraception or abortion (where the policy is a natalist one—for example through tax on childlessness) or conversely, discriminatory policies against large families or even forced abortions (e.g., China's one-child policy in place from 1978 to 2015). Forced sterilization has often targeted ethnic minority groups, such as Roma women in Eastern Europe, or indigenous women in Peru (during the 1990s).

The parents' rights movement is a movement whose members are primarily interested in issues affecting parents and children related to family law, specifically parental rights and obligations. Mothers' rights movements focus on maternal health, workplace issues such as labor rights, breastfeeding, and rights in family law. The fathers' rights movement is a movement whose members are primarily interested in issues related to family law, including child custody and child support, that affect fathers and their children.

Children's rights are the human rights of children, with particular attention to the rights of special protection and care afforded to minors, including their right to association with both parents, their right to human identity, their right to be provided in regard to their other basic needs, and their right to be free from violence and abuse.

Each jurisdiction has its own marriage laws. These laws differ significantly from country to country; and these laws are often controversial. Areas of controversy include women's rights as well as same-sex marriage.

Legal reforms to family laws have taken place in many countries during the past few decades. These dealt primarily with gender equality within marriage and with divorce laws. Women have been given equal rights in marriage in many countries, reversing older family laws based on the dominant legal role of the husband. Coverture, which was enshrined in the common law of England and the US for several centuries and throughout most of the 19th century, was abolished. In some European countries the changes that lead to gender equality were slower. The period of 1975–1979 saw a major overhaul of family laws in countries such as Italy, Spain, Austria, West Germany, and Portugal. In 1978, the Council of Europe passed the "Resolution (78) 37 on equality of spouses in civil law". Among the last European countries to establish full gender equality in marriage were Switzerland. In 1985, a referendum guaranteed women legal equality with men within marriage. The new reforms came into force in January 1988. In Greece, in 1983, legislation was passed guaranteeing equality between spouses, abolishing dowry, and ending legal discrimination against illegitimate children. In 1981, Spain abolished the requirement that married women must have their husbands’ permission to initiate judicial proceedings the Netherlands, and France in the 1980s. In recent decades, the marital power has also been abolished in African countries that had this doctrine, but many African countries that were former French colonies still have discriminatory laws in their marriages regulations, such regulations originating in the Napoleonic Code that has inspired these laws. In some countries (predominantly Roman Catholic) divorce was legalized only recently (e.g. Italy (1970), Portugal (1975), Brazil (1977), Spain (1981), Argentina (1987), Ireland (1996), Chile (2004) and Malta (2011)) although annulment and legal separation were options. Philippines still does not allow divorce. (see Divorce law by country). The laws pertaining to the situation of children born outside marriage have also been revised in many countries (see Legitimacy (family law)).

Family medicine is a medical specialty devoted to comprehensive health care for people of all ages; it is based on knowledge of the patient in the context of the family and the community, emphasizing disease prevention and health promotion. The importance of family medicine is being increasingly recognized.

Maternal mortality or maternal death is defined by WHO as "the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management but not from accidental or incidental causes." Historically, maternal mortality was a major cause of women's death. In recent decades, advances in healthcare have resulted in rates of maternal mortality having dropped dramatically, especially in Western countries. Maternal mortality however remains a serious problem in many African and Asian counties.

Infant mortality is the death of a child less than one year of age. Child mortality is the death of a child before the child's fifth birthday. Like maternal mortality, infant and child mortality were common throughout history, but have decreased significantly in modern times.

Family policies differ significantly between countries. Depending on jurisdiction, family policy may have a multiplicity of functions: horizontal redistribution, the enhancement of individual choices, increasing fertility rates, supporting economic growth and productivity, as well as reducing gender inequalities (Ferragina and Seeleib-Kaiser 2015). From a societal perspective, family policies can contribute to "horizontal redistribution" between generations, as well as between households with and without children; to favour individual choices by supporting the reconciliation between care and paid work; and to reduce the costs of having children and child poverty. From an economic perspective, employment-oriented family policy is part of an overall redesign of welfare states geared to foster "active citizenship", also among mothers who were formerly not employed, through the development of an "enabling state". More generous family policies are said to lead to higher employment rates for women, mitigate the risk of unemployment for mothers after a substantial period of leave, support a social investment strategy, and offset some of the costs of raising children. From many feminist perspectives, family policies should aim at equalising opportunities between men and women through de-familialising care, encouraging men's involvement in care work, and facilitating employment opportunities for women. Profound social, economic, and cultural changes have led in many societies to the decline of the "male breadwinner model" and the move towards a variety of "adult worker models" (Daly 2011). Nevertheless, family policy expansion has not always fundamentally challenged gender inequalities: overall men have not increased their contribution to care work sufficiently to "compensate" for women's increased labour force participation and slightly reduced participation in care.

In an era of perceived permanent austerity and overall welfare state retrenchment, rich OECD countries have not been prevented from expanding family policies (Ferragina and Seeleib-Kaiser 2015). In fact, in many of these countries there has been an expansion of family policies, leading to a socialisation of family care responsibilities, traditionally disproportionately performed by women (Daly and Lewis 2000). Although at the institutional policy level, the expansion of family policy might be characterised as a "silent revolution", relevant for gender equality, a cautious interpretation might be necessary: gender inequalities in income, opportunities, leisure and other significant outcomes remain and are sometimes sustained by policy, even if there is an observed shift in their character towards support for women's employment (Ferragina and Seeleib-Kaiser 2015).

The policy shift has been particularly significant in countries that had previously emphasised more conservative approaches to family policies, such as Germany, Ireland, Japan, and Norway. Hence, it can no longer be assumed that in the majority of rich OECD countries care for young children will be mainly provided through unpaid work within the family. Nevertheless, a certain number of countries still fail to provide adequate childcare arrangements, constituting a barrier for full-time maternal employment. Furthermore, in some countries, such as the United States and the United Kingdom, gender discrimination continues to strongly intersect with class; high childcare costs constitute a disincentive to labour force participation, especially among less educated and unskilled women (Esping-Andersen 2009). This means that higher-class and more educated women tend to have better opportunities than women belonging to a lower social class.

Partisanship and women's political agency have been the main drivers for family policy change during the 1980s and 1990s in many countries. For the 2000s, however, the importance of these drivers has significantly declined. As societal preferences have undergone profound changes—to some extent driven by the activities of women's equality movements, as well as by the experience of women's employment—the policy preferences of voters have also changed. Electorates in western democracies increasingly want policies supporting "modern" family lifestyles which depend on women's employment (Ferragina and Seeleib-Kaiser 2015). As political parties react to these changed policy preferences, the traditional differences in family policy positions between political parties decline. The extent to which this translates into support for gender equality, and how such equality might be defined, is as yet not decided. However, societal policy preferences, long believed to be set in stone, are undergoing profound changes; and public opinion increasingly matters for changing policies. The changed policy preferences are also mirrored in new political discourses that prioritise social investment and the preservation of the human capital of women, especially of those who are highly skilled. The expansion of family policies geared to supporting women's employment and investment in children is very likely to continue in western democracies.(Ferragina and Seeleib-Kaiser 2015).

While in many parts of the world family policies seek to promote a gender-equal organization of the family life, in others the male-dominated family continues to be the official policy of the authorities, which is also supported by law. For instance, the Civil Code of Iran states at Article 1105: "In relations between husband and wife; the position of the head of the family is the exclusive right of the husband".

In some parts of the world, some governments promote a specific form of family, such as that based on traditional family values. The term "family values" is often used in political discourse in some countries, its general meaning being that of traditional or cultural values that pertain to the family's structure, function, roles, beliefs, attitudes, and ideals, usually involving the "traditional family"—a middle-class family with a breadwinner father and a homemaker mother, raising their biological children. Any deviation from this family model is considered a "nontraditional family". These family ideals are often advanced through policies such as marriage promotion. Some jurisdictions outlaw practices which they deem as socially or religiously unacceptable, such as fornication, cohabitation or adultery.

Work-family balance is a concept involving proper prioritizing between work/career and family life. It includes issues relating to the way how work and families intersect and influence each other. At a political level, it is reflected through policies such maternity leave and paternity leave. Since the 1950s, social scientists as well as feminists have increasingly criticized gendered arrangements of work and care, and the male breadwinner role, and policies are increasingly targeting men as fathers, as a tool of changing gender relations.

Article 8 of the European Convention on Human Rights provides a right to respect for one's "private and family life, his home and his correspondence", subject to certain restrictions that are "in accordance with law" and "necessary in a democratic society".

Certain social scientists have advocated the abolition of the family. An early opponent of the family was Socrates whose position was outlined by Plato in "The Republic". In Book 5 of "The Republic", Socrates tells his interlocutors that a just city is one in which citizens have no family ties.

The family being such a deep-rooted and much-venerated institution, few intellectuals have ventured to speak against it. Familialism has been atypically defined as a “social structure where … a family's values are held in higher esteem than the values of the individual members of the family.” Favoritism granted to relatives regardless of merit is called nepotism.

The Russian-American rationalist and individualist philosopher, novelist and playwright Ayn Rand compared partiality towards consanguinity with racism, as a small-scale manifestation of the latter. “The worship of the family is merely racism, like a crudely primitive first installment on the worship of the tribe. It places the accident of birth above a man's values and duty to the tribe above a man's right to his own life.” Additionally, she spoke in favor of childfree lifestyle, while following it herself.

The British social critic, poet, mountaineer and occultist Aleister Crowley censured the institution of family in his works: “Horrid word, family! Its very etymology accuses it of servility and stagnation. / Latin, "famulus", a servant; Oscan, "Faamat", he dwells. … [T]hink what horrid images it evokes from the mind. Not only Victorian; wherever the family has been strong, it has always been an engine of tyranny. Weak members or weak neighbours: it is the mob spirit crushing genius, or overwhelming opposition by brute arithmetic. … In every Magical, or similar system, it is invariably the first condition which the Aspirant must fulfill: he must once and for all and for ever put his family outside his magical circle.”

One of the controversies regarding the family is the application of the concept of social justice to the private sphere of family relations, in particular with regard to the rights of women and children. Throughout much of the history, most philosophers who advocated for social justice focused on the public political arena, not on the family structures; with the family often being seen as a separate entity which needed to be protected from outside state intrusion. One notable exception was John Stuart Mill, who, in his work "The Subjection of Women", advocated for greater rights for women within marriage and family. Second wave feminists argued that the personal is political, stating that there are strong connections between personal experiences and the larger social and political structures. In the context of the feminist movement of the 1960s and 1970s, this was a challenge to the nuclear family and family values, as they were understood then. Feminists focused on domestic violence, arguing that the reluctance—in law or in practice—of the state to intervene and offer protection to women who have been abused within the family, is in violation of women's human rights, and is the result of an ideology which places family relations outside the conceptual framework of human rights.

In 2015, Nicholas Eberstadt, political economist at the American Enterprise Institute in Washington, described a "global flight from family" in an opinion piece in the "Wall Street Journal". Statistics from an infographic by Olivier Ballou showed that,

However, Swedish statisticians reported in 2013 that, in contrast to many countries, since the 2000s, fewer children have experienced their parents' separation, childlessness had decreased in Sweden and marriages had increased. It had also become more common for couples to have a third child suggesting that the nuclear family was no longer in decline in Sweden.




</doc>
<doc id="1546216" url="https://en.wikipedia.org/wiki?curid=1546216" title="Spirit of place">
Spirit of place

Spirit of place (or soul) refers to the unique, distinctive and cherished aspects of a place; often those celebrated by artists and writers, but also those cherished in folk tales, festivals and celebrations. It is thus as much in the invisible weave of culture (stories, art, memories, beliefs, histories, etc.) as it is the tangible physical aspects of a place (monuments, boundaries, rivers, woods, architectural style, rural crafts styles, pathways, views, and so on) or its interpersonal aspects (the presence of relatives, friends and kindred spirits, and the like).

Often the term is applied to a rural or a relatively unspoiled or regenerated place — whereas the very similar term sense of place would tend to be more domestic, urban, or suburban in tone. For instance, one could logically apply 'sense of place' to an urban high street; noting the architecture, the width of the roads and pavements, the plantings, the style of the shop-fronts, the street furniture, and so on, but one could not really talk about the 'spirit of place' of such an essentially urban and commercial environment. However, an urban area that looks faceless or neglected to an adult may have deep meaning in children's street culture.

The Roman term for spirit of place was Genius loci, by which it is sometimes still referred. This has often been historically envisaged as a guardian animal or a small supernatural being (puck, fairy, elf, and the like) or a ghost. In the developed world these beliefs have been, for the most part, discarded. A new layer of less-embodied superstition on the subject, however, has arisen around ley lines, feng shui and similar concepts, on the one hand, and urban leftover spaces, such as back alleys or gaps between buildings in some North-American downtown areas, on the other hand.

The western cultural movements of Romanticism and Neo-romanticism are often deeply concerned with creating cultural forms that 're-enchant the land', in order to establish or re-establish a spirit of place.

Modern earth art (sometimes called environment art) artists such as Andy Goldsworthy have explored the contribution of natural/ephemeral sculpture to spirit of place.

Many indigenous and tribal cultures around the world are deeply concerned with spirits of place in their landscape. Spirits of place are explicitly recognized by some of the world's main religions: Shinto has its Kami which may incorporate spirits of place; Christianity has spirits of place in the Angelic Choirs of Dominions and Principalities, Hinduism, Vajrayana and Bonpo traditions.

Between 1988 and 1993, five annual Spirit of Place Symposiums were produced by environmental psychologist James Swan and Roberta Swan, in the United States and Japan. The programs sought to explore the relationship between ancient wisdom about the importance of a sense or spirit of a place, and modern science and design. The programs—held in Davis, CA; Grace Cathedral, San Francisco' Mesa Verde National Park; Tenri, Japan; and San Rafael, CA—featured over 350 speakers, including scientists, designers, and representatives of natives cultures. They also included concerts by Paul Horn, Anna Halprin, Steven Halpern, and R. Carlos Nakai. The five programs attracted an audience of 10,000, and featured on national TV in the United States and Japan. The 1992 symposium held in Tenri, Japan, was the largest environmental conference in Japan that year. Representatives of 20 different American Indian tribes, Ainu, Inuit, Aborigine, and African traditional cultures were speakers in the series. Two anthologies edited by James and Roberta Swan, "The Power of Place" (Quest, 1991) and "Dialogues with the Living Earth" (Quest, 1996) present a selection of presentations at the Spirit of Place Symposiums.




</doc>
<doc id="863862" url="https://en.wikipedia.org/wiki?curid=863862" title="Artes Mechanicae">
Artes Mechanicae

Artes Mechanicae or mechanical arts, are a medieval concept of ordered practices or skills, often juxtaposed to the traditional seven liberal arts Artes liberales. Also called "servile" and "vulgar", from antiquity they had been deemed unbecoming for a free man, as ministering to baser needs.

Johannes Scotus Eriugena (9th century) divides them somewhat arbitrarily into seven parts: 

In his "Didascalicon", Hugh of St Victor (12th century) includes navigation, medicine and theatrical arts instead of commerce, agriculture and cooking. Hugh's treatment somewhat elevates the mechanical arts as ordained to the improvement of humanity, a promotion which was to represent a growing trend among late medievals.

The classification of the "Artes Mechanicae" as applied geometry was introduced to Western Europe by Dominicus Gundissalinus (12th century) under the influence of his readings in Arabic scholarship.

In the 19th century "mechanic arts" referred to fields of which some are now known as engineering. Use of the term was apparently an attempt to distinguish these fields from creative and artistic endeavors like the performing arts and the fine arts which were for the upper class of the time, and the intelligentsia. The mechanic arts were also considered practical fields for those that did not come from good families.

Related phrases, "useful arts," or "applied arts" probably encompass the mechanic arts as well as craftsmanship in general.

The most famous usage of the term "mechanic arts" (and the one in which it is most commonly encountered today) is in the Morrill Land-Grant Colleges Act.




</doc>
<doc id="690245" url="https://en.wikipedia.org/wiki?curid=690245" title="Art world">
Art world

The art world comprises everyone involved in producing, commissioning, presenting, preserving, promoting, chronicling, criticizing, and selling fine art.

"Art world" is indeed a wider term than art market, though that is a large part of it. Howard S. Becker describes it as "the network of people whose cooperative activity, organized via their joint knowledge of conventional means of doing things, produce(s) the kind of art works that art world is noted for" (Becker, 1982). In her book, "Seven Days in the Art World", Sarah Thornton describes it as "a loose network of overlapping subcultures held together by a belief in art. They span the globe but cluster in art capitals like New York, London, Los Angeles, and Berlin." Other cities sometimes called "art capitals" include Beijing, Brussels, Hong Kong, Miami, Paris, Rome and Tokyo; due to their large art festivals, followings, and being the centers of art production.

The notion of the singular "art world" is problematic, since Becker and others show art worlds are, instead, independent multiplicities scattered worldwide that are always in flux: there is no "center" to the art world any more. In her analysis of the "net art world" (referring to network-aided art or net art), Amy Alexander states "net.art had a movement, at the very least it had coherence, and although it aimed to subvert the art world, eventually its own sort of art world formed around it. It developed a culture, hype and mystique through lists and texts; it had a centre, insiders, outsiders, even nodes. This is of course not a failure; this is unavoidable: groups form; even anarchism is an institution." 
Art worlds exist at local and regional levels, as hidden or obscured subcultures, via primary and secondary art markets, through gallery circuits, around design movements, and, esoterically, as shared or perceived experiences.

The one globalized, all-encompassing art world exists only as myth; rather, there are multiplicities of intersecting, overlapping, self-similar art worlds, each expressing different views of the world as they see it.

"Whitehot Magazine" artist/publisher Noah Becker has published over 3500 articles about the Art World."

"New York Magazine" art critic Jerry Saltz has referred to William Powhida's and Jade Townsend's drawing "Art Basel Miami Beach Hooverville" as "a great big art-world stinkbomb."




</doc>
<doc id="7524684" url="https://en.wikipedia.org/wiki?curid=7524684" title="Showpiece">
Showpiece

A showpiece is:





</doc>
<doc id="27561" url="https://en.wikipedia.org/wiki?curid=27561" title="Stendhal syndrome">
Stendhal syndrome

Stendhal syndrome, Stendhal's syndrome or Florence syndrome is a psychosomatic condition involving rapid heartbeat, dizziness, fainting, confusion and even hallucinations, allegedly occurring when individuals become exposed to objects or phenomena of great beauty. 

The affliction is named after 19th-century French author Stendhal (pseudonym of Marie-Henri Beyle), who described his experience with the phenomenon during his 1817 visit to Florence in his book "Naples and Florence: A Journey from Milan to Reggio". When he visited the Basilica of Santa Croce, where Niccolò Machiavelli, Michelangelo and Galileo Galilei are buried, he was overcome with profound emotion. Stendhal wrote:

Although psychologists have long debated whether Stendhal's syndrome exists, the apparent effects on some individuals are severe enough to warrant medical attention. The staff at Florence's Santa Maria Nuova hospital are accustomed to tourists suffering from dizzy spells or disorientation after viewing the statue of David, the artworks of the Uffizi Gallery, and other historic relics of the Tuscan city.

Though there are numerous accounts of people fainting while taking in Florentine art, dating from the early 19th century on, the syndrome was only named in 1979; when it was described by Italian psychiatrist Graziella Magherini, who observed over a hundred similar cases among tourists in Florence. There exists no scientific evidence to define Stendhal syndrome as a specific psychiatric disorder; however there is evidence that the same cerebral areas involved in emotional responses are activated during exposure to art. The syndrome is not listed as a recognised condition in the "Diagnostic and Statistical Manual of Mental Disorders".




</doc>
<doc id="3574096" url="https://en.wikipedia.org/wiki?curid=3574096" title="Community arts">
Community arts

Community art, also sometimes known as "dialogical art", "community-engaged art", or "community-based art", refers to the practice of art based in and generated in a community setting. Works in this form can be of any media and are characterized by interaction or dialogue with the community. Professional artists may collaborate with communities which may not normally engage in the arts. The term was defined in the late 1960s as the practice grew in the United States, Canada, the Netherlands, the United Kingdom, Ireland, and Australia. In Scandinavia, the term "community art" more often refers to contemporary art projects.

Community art is a community-oriented, grassroots approach, often useful in economically depressed areas. When local community members come together to express concerns or issues through this artistic practice, professional artists or actors may be involved. This artistic practice can act as a catalyst to trigger events or changes within a community or at a national or international level.

In English-speaking countries, community art is often seen as the work of community arts centers, where visual arts (fine art, video, new media art), music, and theater are common media. Many arts organizations in the United Kingdom do community-based work, which typically involves developing participation by non-professional members of local communities.

The term "community art" may also apply to public art efforts when, in addition to the collaborative community artistic process, the resulting product is intended as public art and installed in public space. Popular community art approaches to public art can include environmental sustainability themes associated with urban revitalization projects.

Models of community-engaged arts can vary with three forms of collaborative practices emerging from among the sets of common practices. In the artist-driven model, artists are seen as the catalysts for social change through the social commentary addressed in their works. A muralist whose work elicits and sustains political dialogue would be a practitioner of this model. In the second model, artists engage with community groups to facilitate specialized forms of art creation, often with the goal of presenting the work in a public forum to promote awareness and to further discourse within a larger community. In the process-driven or dialogic model, artists may engage with a group in order to facilitate an artistic process that addresses particular concerns specific to the group. The use of an artistic process (such as dance or social circus) for problem-solving, therapeutic, group-empowerment or strategic planning purposes may result in artistic works that are not intended for public presentation. In the second and third models, the individuals who collaborate on the artistic creation may not define themselves as artists but are considered practitioners of an art-making process that produces social change.

Due to its roots in social justice and collaborative, community-based nature, art for social change may be considered a form of cultural democracy. Often, the processes (or the works produced by these processes) intend to create or promote spaces for participatory public dialogue.

In Canada, the field of community-engaged arts has recently seen broader use of art for social change practices by non-arts change organizations. The resultant partnerships have enabled these collaborative communities to address systemic issues in health, education, as well as empowerment for indigenous, immigrant, LGBT and youth communities. A similar social innovation trend has appeared where business development associations have engaged with artists/artistic organizations to co-produce cultural festivals or events that address social concerns.

As the field diversifies and practices are adopted by various organizations from multiple disciplines, ethics and safety have become a concern to practitioners. As a result, opportunities for cross-disciplinary training in art for social change practices have grown within the related field of arts education.

A community can be seen in many ways, it can refer to different kind of groups. There are also virtual communities or online communities. Internet art has many different forms, but often there is some kind of community that is created for a project or it is an effect of an art project.

Community theatre includes theatre made by, with, and for a community—it may refer to theatre that is made almost by a community with no outside help, or to a collaboration between community members and professional theatre artists, or to performance made entirely by professionals that is addressed to a particular community. Community theatres range in size from small groups led by single individuals that perform in borrowed spaces to large permanent companies with well-equipped facilities of their own. Many community theatres are successful, non-profit businesses with a large active membership and, often, a full-time professional staff. Community theatre is often devised and may draw on popular theatrical forms, such as carnival, circus, and parades, as well as performance modes from commercial theatre. Community theatre is understood to contribute to the social capital of a community, insofar as it develops the skills, community spirit, and artistic sensibilities of those who participate, whether as producers or audience-members.




</doc>
<doc id="340336" url="https://en.wikipedia.org/wiki?curid=340336" title="Artistic license">
Artistic license

Artistic license (also known as art license, historical license, dramatic license, poetic license, narrative license, licentia poetica, creative license, or simply license) is a colloquial term, sometimes a euphemism, used to denote the distortion of fact, alteration of the conventions of grammar or language, or rewording of pre-existing text made by an artist in the name of art.

The artistic license may also refer to the ability of an artist to apply smaller distortions, such as a poet ignoring some of the minor requirements of grammar for poetic effect. For example, Mark Antony's "Friends, Romans, Countrymen, lend me your ears" from Shakespeare's "Julius Caesar" would technically require the word "and" before "countrymen", but the conjunction "and" is omitted to preserve the rhythm of iambic pentameter (the resulting conjunction is called an asyndetic tricolon). Conversely, on the next line, the end of "I come to bury Caesar, not to praise him" has an extra syllable because omitting the word "him" would make the sentence unclear, but adding a syllable at the end would not disrupt the meter. Both of these are examples of artistic license. 

Another example of artistic license is the way in which stylized images of an object (for instance in a painting or an animated movie) are different from their real life counterparts, but are still intended to be interpreted by the viewer as representing the same thing. This can mean the omission of details, or the simplification of shapes and color shades, even to the point that the image is nothing more than a pictogram. It can also mean the addition of non-existing details, or exaggeration of shapes and colours, as in fantasy art or a caricature.

Certain stylizations have become fixed conventions in art; an agreement between artist and viewer that is understood and undebatable. A striking example is how in simple cartoon drawings' monochromatic white parts on a dark colored surface are immediately recognized by most viewers to represent the reflection of light on a smooth or wet surface.

In summary, artistic license is:


Artistic license is often referred to as dramatic license when it involves the glamorization of real-world occupations for the sake of exciting television or cinematic experience. For example, "CSI: Crime Scene Investigation" and other police procedural programs typically omit completely the more mundane aspects of the occupation such as paperwork, reports, administrative duties and other daily "business-oriented" aspects which in reality often constitute the majority of police work. They will also present other duties with much more action, suspense or drama than would be experienced in reality. The same is also true for many military-oriented adventure stories which often show high-ranking characters being allowed to continuously enter dangerous situations when in reality, they would usually be restricted to command-oriented and administrative duties.

Artistic license often provokes controversy by offending those who resent the reinterpretation of cherished beliefs or previous works. Artists often respond to these criticisms by pointing out that their work was not intended to be a verbatim portrayal of something previous and should be judged only on artistic merit. Artistic license is a generally accepted practice, particularly when the result is widely acclaimed. William Shakespeare's historical plays, for example, are gross distortions of historical fact but are nevertheless lauded as outstanding literary works.

Critical voices are sometimes raised when artistic license is applied to cinematic and other depictions of real historical events. While slight manipulation for dramatic effect of chronology and character traits are generally accepted, some critics feel that depictions that present a significantly altered reality are irresponsible, particularly because many viewers and readers do not know the actual events and may thus take the dramatized depiction to be true to reality. Examples of films and television series criticized for excessive use of dramatic license include Disney's "Pocahontas", Mel Gibson's "Braveheart", Oliver Stone's "Alexander", the HBO series "Rome", 20th Century Fox's "The Greatest Showman" and Showtime's "The Tudors". 

Writers adapting a work for another medium (e.g., a film screenplay from a book) often make significant changes, additions to, or omissions from the original plot in the book, on the grounds that these changes were necessary to make a good film. These changes are sometimes to the dismay of fans of the original work.


</doc>
<doc id="8395427" url="https://en.wikipedia.org/wiki?curid=8395427" title="Economics of the arts and literature">
Economics of the arts and literature

Economics of the arts and literature or cultural economics (used below for convenience) is a branch of economics that studies the economics of creation, distribution, and the consumption of works of art, literature and similar creative and/or cultural products. For a long time, the concept of the "arts" were confined to visual arts (e.g., painting) and performing arts (music, theatre, dance) in the Anglo-Saxon tradition. Usage has widened since the beginning of the 1980s with the study of cultural industry (cinema, television programs, book and periodical publishing and music publishing) and the economy of cultural institutions (museums, libraries, historic buildings). The field is coded as in the "Journal of Economic Literature" classification system used for article searches.

Cultural economics is concerned with the arts in a broad sense. The goods considered have creative content, but that is not enough to qualify as a cultural good. Designer goods such as clothes and drapes are not considered usually to be works of art or culture. Cultural goods are those with a value determined by symbolic content rather than physical characteristics. (For further considerations, see also Cultural Institutions Studies). Economic thinking has been applied in ever more areas in the last decennia, including pollution, corruption and education.

Works of art and culture have a specific quality, which is their uniqueness. While other economic goods, such as crude oil or wheat are generic, interchangeable commodities (given a specific grade of the product), there is only one example of a famous painting such as the Mona Lisa, and only one example of Rodin's well-known sculpture "The Thinker". While copies or reproductions can be made of these works of art, and while many inexpensive posters of the Mona Lisa and small factory-made replicas of "The Thinker" are sold, neither full-size copies nor inexpensive reproductions are viewed as substitutes for the real artworks, in the way that a consumer views a pound of Grade A sugar from Cuba as a fully equivalent substitute for a pound of Grade A sugar from United States or Dominican Republic. As there is no equivalent item or substitute for these famous works of art, classical economist Adam Smith held it was impossible to value them. Alfred Marshall noted that the demand for a certain kind of cultural good can depend on its consumption: The more you have listened to a particular kind of music, the more you appreciate. In his economic framework, these goods do not have the usual decreasing marginal utility.

Key academic works in cultural economics include those of Baumol and Bowen (Performing Arts, The Economic Dilemma, 1966), of Gary Becker on addictive goods, and of Alan Peacock (public choice). This summary has been divided into sections on the economic study of the performing arts, on the market of individual pieces of art, the art market in cultural industries, the economics of cultural heritage and the labour market in the art sector.

The seminal paper by William Baumol and Bowen introduced the term cost disease for a relative cost growth of live performances. This cost growth explains the increasing dependency of this kind of art on state subsidies. It occurs when the consumable good is labour itself. To understand this phenomenon, compare the change in the cost of performing the Molière play "Tartuffe" in 1664 and in 2007 with the change in cost of calculating a large number of sums from an accounting ledger. In 1664, you needed two hours and twelve actors to perform Molière's play, and it would take, say, twelve accountants working for two hours to add up all the sums in an accounting ledger. In 2007, a single accountant with a $10 calculator can add the sums in 20 minutes, but you still need two hours and twelve actors for the Molière play. Artists must make a considerable investment in human capital (e.g., training), and needs to be paid accordingly. The artists' pay needs to rise along with that of the population in general. As the latter is following the general productivity in the economy, the cost of a play will rise with general productivity, while the actors' productivity does not rise.

There are two lines of thought in subsequent literature on the economics of the performing arts:


Two segments of the market in the visual arts can be distinguished: works of art that are familiar and have a history, and contemporary works that are more easily influenced by fashion and new discoveries. Both markets, however, are oligopolistic, i.e., there are limited numbers of sellers and buyers (oligopsony). Two central questions on the working of the markets are: How are prices determined, and what is the return on artworks, compared to the return on financial assets?

Components of a work of art, like raw stone, tubes of paint or unpainted canvas, in general have a value much lower than the finished products, such as a sculpture or a finished painting. Also, the amount of labour needed to produce an item does not explain the big price differences between works of art. It seems that the value is much more dependent on potential buyers', and experts' perception of it. This perception has three elements: First, social value, which is the social status the buyer has by owning it. The artist thus has an "artistic capital". Second, the artistic value, compared to contemporary works, or as importance to later generations. Third, the price history of the item, if a buyer uses this for his expectation of a future price at which he might sell the item again (given the oligopolistic market structure).
Three kinds of economic agents determine these values. Specific experts like gallery owners or museum directors use the first, social value. Experts like art historians and art professors use the second, artistic value. Buyers who buy works of art as an investment use the third, the price history and expectations for future price increases.

Some major financial institutions, banks and insurance companies, have had considerable return rates on investments in art works in the 1990s. These rates have not slowed down at the same time as the rates on stock exchanges, in the early 90's. This may indicate a diversification opportunity. Apart from this evidence of successful investment, the amount of data available has stimulated study of the market. Many works are sold on auctions. These transactions are thus very transparent. This has made it possible to establish price databases, with prices of some items going back to 1652. Empirical studies have shown that, on average, the return on works of art has been lower than that on equity, with a volatility that is at least as high. An intangible gain in terms of pleasure of having a work of art could explain this partly. However, before interpreting the figures, it should be borne in mind that art is often exempt of many kinds of taxes. In 1986, Baumol made an estimate of an average yearly rate of return of 0.55 percent for works of art, against a rate of return of 2.5 percent for financial assets, over a 20-year period.

Some famous artworks such as the "Mona Lisa" painting are not reproducible (at least in the sense of creating another copy that would be seen as equivalent in value), but there are many cultural goods whose value does not depend on a single, individual copy. Books, recordings, movies get some of their value from the existence of many copies of the original. These are the products of major cultural industries, which are the book industry, the music industry and the film industry. These markets are characterized by:

The important cultural industries tend to have an oligopolistic market structure. The market is dominated by a few major companies, with the rest of the market consisting of many small companies. The latter may act as a filter or as "gatekeepers" for the artistic supply. A small company with a successful artist or good quality roster can be bought by one of the major companies. Big conglomerates, pooling TV and film production, have existed for decades. The 1990s have seen some mergers extending beyond the industry as such, and mergers of hardware producers with content providers. Anticipated gains from synergy and market power have not been realised, and from the early 2000s there has been a trend towards organisation along sector lines.

Cultural heritage is reflected in goods and real estate. Management and regulation of museums has come under study in this area.

Museums, which have a conservatory role, and provide exhibitions to the general public, can be commercial, or on a non-profit base. In the second case, as they provide a public good, they pose the problems related to these goods: should they be self-financing, or be subsidized ? One of the specific issues is the imbalance between the huge value of the collections in museums, and their budgets. Also, they are often located in places (city centres) where the cost of land is high, which limits their expansion possibilities. American museums exhibit only about half of their collection. Some museums in Europe, like the Pompidou Centre in France, show less than 5 percent of their collection. Apart from providing exhibitions, museums get proceeds from derived products, like catalogues and reproductions. They also produce at a more intangible level: They make collections. Out of so many pieces in the public domain, they make a selection based on their expertise, thus adding value to the mere existence of the items.

The dual goal of conservation and providing exhibitions obviously presents a choice. On the one hand the museum has, for conservation reasons, an interest in exhibiting as few items as possible, and it would select lesser known works and a specialized audience, to promote knowledge and research. On the other hand, the exhibition argument requires showing the major pieces from different cultures, to satisfy the demands from the public and to attract a large audience. When a government has made a choice about this, application of economic contract theory will help to implement this choice by showing how to use incentives to different managers (on the financial, conservatory side) to obtain the required result.

Many countries have systems that protect historically significant buildings and structures. These are buildings or other structures that are deemed to have cultural importance or which are deemed to have heritage value. Owners get tax deductions or subsidies for restoration, in return for which they accept restrictions on modifications to the buildings or provide public access. Buildings that are often classified as heritage buildings include former or current Parliament buildings, cathedrals, courthouses, houses built in a recognized historical style, and even fairly regular houses, if the house was formerly the home of a famous politician, artist or inventor. Buildings with heritage status cannot typically be demolished. Depending on the nature of the heritage restrictions, the current owner may or may not be allowed to modify the outside or inside of the building. Such a system poses the same choice problems as museums do. There has been little study of this issue.

The labour market for artists is characterized by:

The term "star system", coined by Sherwin Rosen, is used to explain why a very small number of the artists and creators in the market, such as the celebrity A-list actors and top pop singers, earn most of the total earnings in a sector. Rosen's 1981 paper examined the economics of superstars to determine why “relatively small numbers of people earn enormous amounts of money and seem to dominate the fields in which they engage.” Rosen argues that in superstar markets, "small differences in talent at the top of the distribution will translate into large differences in revenue." Rosen points out that "...sellers of higher talent charge only slightly higher prices than those of lower talent, but sell much larger quantities; their greater earnings come overwhelmingly from selling larger quantities than from charging higher prices".

In cultural industries, the uncertainty about the quality of a product plays a key role in this. The consumer does not really know how good the product is, until he or she has consumed it (think of a movie), and the producer is confronted with the typical uncertainty in a cultural industry. The consumer looks for guidance in the price, reputation, or a famous name on the cover or poster. As the producer understands this using a famous director, actor or singer affects demand, he or she is prepared to pay a lot for a name considered a sign of quality (a star). Indeed, authors like Adler and Ginsburgh have given evidence that star status is determined by chance: in a musical contest, results were highly correlated with the order of performance. This randomness has been used to explain why the labor supply in the sector remains excessive: given the extreme gains of a star, and an irrational behaviour, or particular preferences, with respect to chance, unsuccessful artists keep trying, even when they are earning their money mostly in a different trade, such as waiting tables. A second argument is the possibility of intangible returns to artists' labour in terms of social status and lifestyle. For example, even a struggling DJ spends most of her time onstage on nightclubs and raves, which for some people is a desirable outcome.

A case has been made for the existence of a different structure in the production of cultural goods . (See Cultural Institutions Studies.) An artist often considers a product to be an expression of himself, while the ordinary craftsperson is only concerned with his product, as far as it affects his/her pay or salary. For example, a painter who creates artworks that are displayed in museums may view her paintings as her artistic expression. On the other hand, a scene painter for a music theatre company may see herself as a craftsperson who is paid by the hour for doing painting. The artist may thus want restrict the use of his or her product, and he/she may object if a museum uses a reproduction of his/her painting to help sell cars or liquor. On the other hand, the scene painter may not object to commercial re-uses of her set painting, as she/he may see it just as a regular job.





</doc>
<doc id="18368611" url="https://en.wikipedia.org/wiki?curid=18368611" title="Arts criticism">
Arts criticism

Arts criticism is the process of describing, analyzing, interpreting, and judging works of art. It is distinct from art criticism (which focuses on visual arts) due to its broader remit. The disciplines of arts criticism can be defined by the object being considered rather than the methodology (through analysis of its philosophy): buildings (architecture criticism), paintings (visual art criticism), performances (dance criticism, theatre criticism), music (music journalism), visual media (film criticism, television criticism), or literary texts (literary criticism).

Criticism of the arts can be broadly divided into two types. There is academic criticism such as that found in scholarly works and specialist journals, then there is criticism of a more journalistic nature (often called 'a review') which is seen by a wider public through newspapers, television and radio. The academic criticism will be of a more vigorous and analytical nature than the journalistic, the journalistic may even focus on entertaining the reader at the expense of detail about the art under discussion.



</doc>
<doc id="20443993" url="https://en.wikipedia.org/wiki?curid=20443993" title="Gifted art">
Gifted art

Gifted art (or free art) is any form or piece of art that is given freely, whether to a city, a group of people, a community or an individual. It refers to any art that is distributed at no direct cost. It is a form of conceptual art. It comes from a belief that art should be available for all people to enjoy, whether rich or poor, university graduate or junior high dropout. Since Gifted Art is an expressive form of art - an idea, it encompasses virtually all forms of art: movies, literature, music recordings, sculpture, painting, graffiti, digital art, street performances, performance art, sticker art, comics, coffeehouse poetry and Internet-distributed art etc.

Gifted Art has a long history in the arts. Many artists have been known to give art freely to each other, in an effort to pass along ideas, etc. Picasso and many of his contemporaries were fond of this.
Duchamp was one of the first modern day artists to Gift to the public with his piece, "Fountain", for an art show in 1917. He gave the piece under the name of "R. Mutt," presumably to hide his identity as the artist. He actually had to pay $7 to have the piece exhibited in the show, and never intended on making any money from it. He gave it freely for all to see and to push forward the Dada movement.
In the 50's, Ray Johnson started doing Mail art. He spread the idea to many other artists and it still occurs today. In 2002, a 6 ft X 53 ft mural by Roy Lichtenstein was installed in the Times Square Subway Station in New York City. It was gifted by the artist to all New Yorkers.
In recent years, Gifted Art has primarily been embraced by graffiti and urban artists. Many graffiti artists consider the work they do to be art for the public. Banksy writes in his books that he believes art is more important and more enjoyable when it is out on the street where people can view it in their everyday lives than when it hangs in a museum.



http://www.featherandfathomfreeart.wordpress.com
There are now several art shows and organizations where art is gifted. Here are a few.


</doc>
<doc id="12721996" url="https://en.wikipedia.org/wiki?curid=12721996" title="Human figure">
Human figure

In aesthetics, the human figure or human form in art, sculpture and other art forms involves a study and appreciation of the beauty of the human body in its depiction or presentation. The study involves an appreciation of the body shape, including body postures - sitting, standing or even sleeping, and movements - walking, running, dancing etc. Kant refers to the human figure as the ideal of beauty. The human figure conforms very well to the law that states that form follows function, which is believed to be a result of evolution over thousands of generations.

The human figure is one of the most enduring themes in the visual arts. Very few art forms are not related to human figure such as music, though it figures in lyrics. A study of the human figure includes a detailed study of the following subjects:

"Body proportions" are the study of relation of human body, or in general, animal body, parts to each other and the whole, essential for depiction of the overall figure.

A figure drawing is a study of the human form in its various shapes and postures. It is a study or stylized depiction of the human form, with the line and form of the human figure as the primary objective, rather than the subject person. It is a composed image of the subject in a still position. A life drawing is a work that has been drawn from an observation of a live model. 




</doc>
<doc id="24989989" url="https://en.wikipedia.org/wiki?curid=24989989" title="Interdisciplinary arts">
Interdisciplinary arts

Interdisciplinary Arts is an academic department in the School of Media Arts at Columbia College Chicago. As one of the earliest interdisciplinary arts programs in the United States, it has been an incubator for new approaches towards art making that has shaped the development of arts professionals for over thirty-three years. Guided by the principle that interdisciplinarity "is a defining characteristic of contemporary art practice" and "a necessary prerequisite for those artists who will shape the future of creative practice", the artists who work in the Interdisciplinary Arts department investigate new terrain.

Examining concepts, forms and techniques from across the fine, performing and media arts, students work with a diverse array of unique and experimental approaches that interrogate artist books, installations, gesture and movement, sound art, durational performance, interactive media, video, performance media, papermaking, letterpress, etching and offset printing, electronically controlled artworks, online artwork, performance in artificial spaces, democratic multiples, written, spoken and performed text, dramatic forms, DIY/DIT collaborative strategies and relational art forms.

Interdisciplinary Arts was formulated in Chicago Il, 1976, by Suzanne Cohan-Lange, Jean Unsworth, and Rebecca Ruben. Originally accepted by the Chicago Consortium of Colleges as a program taught across several universities, in 1981, it was established as a department with a permanent home at Columbia College Chicago. At that time, the Interdisciplinary Arts MA program was formalized within the context of the department. Since then, two MFA degrees were added: the Interdisciplinary Book & Paper Arts MFA, launched in 1994; and the Interdisciplinary Arts & Media MFA, launched in 2002. The founding of the Interdisciplinary Arts program is preceded by the undergraduate program in the Studio for Interrelated Media program founded by Harris Barron in 1969 at the Massachusetts College of Art and Design in Boston.

The Interdisciplinary Arts MA is for art teachers who want to expand their repertoire of techniques, as well as assist practicing artists in expanding their practice to include new media. It is immersed in the five traditional art media that make up the heart of the program: visual art, movement, sound, writing, and drama.

The Interdisciplinary Book & Paper Arts MFA program enables students to participate in the contemporary art world by encouraging them to consider book and paper as a site for interdisciplinary practice. It promotes the understanding of hand papermaking and the book arts as artistic media with applications in cultural discourse, community building, and collaborative practice.

The Interdisciplinary Arts & Media MFA fosters an innovative dialog between the fine, performing and media arts. It is a graduate program for traditional and performing artists who want to incorporate media into their artistic practice and for media artists who want to expand into areas such as performance, installation, interactive, and relational art forms.

In the 916 S. Wabash building are housed the departmental offices, faculty offices, conference room, lecture hall, two smart classrooms, large computer lab, three installation labs and the media equipment center. Additionally, the Center for Book and Paper Arts is part of the Interdisciplinary Arts department. The Center for Book and Paper Arts occupies the entire second floor of the historic Ludington Building at 1104 South Wabash and includes a papermaking studio, a letterpress facility, a bookbindery, a gallery, a smart classroom, a multi-purpose space for performance and lectures, a computer laboratory, a critique room, studio space for artists, a resource room, and offices for the staff. Also at 1104 South Wabash are studio spaces for the MFA students.




</doc>
<doc id="21084897" url="https://en.wikipedia.org/wiki?curid=21084897" title="Artist's portfolio">
Artist's portfolio

An artist's portfolio is an edited collection of their best artwork intended to showcase an artist's style or method of work. A portfolio is used by artists to show employers their versatility by showing different samples of current work. Typically, the work reflects an artist's best work or a depth in one specific area of work.

Historically, portfolios were printed out and placed into a book. With the increased use of the internet and email, however, there are now websites that host online portfolios that are available to a wider audience. Sometimes an artist's portfolio can be referred to as a lookbook.

A photography portfolio can focus on a single subject. It can be a collection of photographs taken with a certain type of camera, in one geographic area, of one person or a group of people, only black & white or sepia photos, a special event, etc.

Many photographers use portfolios to show their best work when looking for jobs in the photography industry. For example, wedding photographers may put together a book of their best wedding photos to show to engaged couples who are looking for a wedding photographer. Photojournalists may take a collection of their best freelance work with them when looking for a job.

An artist design book is a collection of photographs meant to show off a model, photographer, style, or clothing line. Sometimes they are made to compile the looks of other people such as a celebrity, politician or socialite. This is an especially popular term with fashion bloggers.

Artist design books, or ADBs, in their online form, can be described as "fashion diaries" because bloggers are constantly updating them on a daily or weekly basis. 

It is common for stores or clothing designers to use an ADB to show off products. They may include photos of multiple types of clothes, shoes and other accessories from a season or line.


</doc>
<doc id="101140" url="https://en.wikipedia.org/wiki?curid=101140" title="Cultural resources management">
Cultural resources management

In the broadest sense, cultural resource management (CRM) is the vocation and practice of managing cultural resources, such as the arts and heritage. It incorporates Cultural Heritage Management which is concerned with traditional and historic culture. It also delves into the material culture of archaeology. Cultural resource management encompasses current culture, including progressive and innovative culture, such as urban culture, rather than simply preserving and presenting traditional forms of culture.

However, the broad usage of the term is relatively recent and as a result it is most often used as synonymous with heritage management. In the United States, cultural resources management is not usually divorced from the heritage context. The term is, "used mostly by archaeologists and much more occasionally by architectural historians and historical architects, to refer to managing historic places of archaeological, architectural, and historical interests and considering such places in compliance with environmental and historic preservation laws." 

Cultural resources include both physical assets such as archaeology, architecture, paintings and sculptures and also intangible culture such as folklore and interpretative arts, such as storytelling and drama. Cultural resource managers are typically in charge of museums, galleries, theatres etc., especially those that emphasize culture specific to the local region or ethnic group. Cultural tourism is a significant sector of the tourism industry.

At a national and international level, cultural resource management may be concerned with larger themes, such as languages in danger of extinction, public education, the ethos or operation of multiculturalism, and promoting access to cultural resources. The Masterpieces of the Oral and Intangible Heritage of Humanity is an attempt by the United Nations to identify exemplars of intangible culture.

Cultural resource management can trace its beginning to the environment/conservation movement in the 1960s and 1970s. During this time, there was growth in legislation concerning the protection of cultural resources. The Archaeological and Historic Preservation Act of 1974, commonly known as the Moss-Bennett Act, helped to fuel the creation of CRM, while creating “growth in archaeological jobs in the federal government, academia, and private sector.” Federal legislation had passed earlier in 1906 under the Antiquities Act, but it was not until the 1970s when the term “cultural resources” was coined by the National Park Service. This term came into more popular usage after two meetings in 1974: the Cultural Resource Management conference and the Airlie House conference. Following these conferences, the National Park Service (NPS) defined cultural resources in the Cultural Resource Management Guidelines as being:

“Those tangible and intangible aspects of cultural systems, both living and dead, that are valued by or representative of a given culture or that contain information about a culture…[They] include but are not limited to sites, structures, districts, objects, and historic documents associated with or representative of peoples, cultures, and human activities and events, either in the present or in the past. Cultural resources also can include primary written and verbal data for interpretation and understanding of those tangible resources.” 

Cultural resource management in the heritage context is mainly concerned with the investigation of sites with archaeological potential, the preservation and interpretation of historic sites and artifacts, and the culture of indigenous people. The subject developed from initiatives in rescue archaeology, sensitivities to the treatment of indigenous people, and subsequent legislation to protect cultural heritage.

In the 1970s, archaeologists created the term "cultural resource management" as a parallel to natural resource management to address the following resources:

A significant proportion of the archaeological investigation in countries that have heritage management legislation including the United States and United Kingdom is conducted on sites under threat of development. In the US, such investigations are now done by private companies on a consulting basis, and a national organization exists to support the practice of CRM. Museums, besides being popular tourist attractions, often play roles in conservation of, and research on, threatened sites, including as repositories for collections from sites slated for destruction.

In the United States, a common Cultural Resource Management task is the implementation of a Section 106 review: CRM archaeologists determine whether federally funded projects are likely to damage or destroy archaeological sites that may be eligible for the National Register of Historic Places. This process commonly entails one or more archaeological field surveys.

Cultural resource management features people from a wide array of disciplines. The general education of most involved in CRM includes, but is not limited to, sociology, archaeology, architectural history, cultural anthropology, social and cultural geography, and other fields in the social sciences.

In the field of cultural resource management there are many career choices. One could obtain a career with an action agency that works directly with the NEPA or even more specifically, Native American resources. There are also careers that can be found in review agencies like the Advisory Council on Historic Preservation (ACHP), or the state historic preservation office (SHPO) Beyond these choices, one could also obtain a career as part of the local government and work with planning agencies, housing agencies, social service agencies, local museums, libraries, or educational institutions. Jobs at private cultural resource management companies can range from field technicians (see shovelbum) to principal investigators, project archaeologists, historic preservationists, and laboratory work. One could also become a part of an advocacy organization, such as the National Trust for Historic Preservation.

It is commonly debated in cultural resource management how to determine whether cultural or archaeological sites should be considered significant or not. The criteria that is stated by the National Register of Historic Places is said to be able to be “interpreted in different ways so that the significance… may be subjectively argued for many cultural resources.” Another issue that arises among scholars is that “protection does not necessarily mean preservation.” Any public projects occurring near the cultural resource can have adverse effects. Development plans for a proposed project may not be able to be changed to limit impact and to avoid damage to the resource.

The vocation of management in cultural and creative sectors is the subject of research and improvement initiatives, by organizations such as Arts and Business which take a partnership approach to involving professional business people in running and mentoring arts organizations. Some universities now offer vocational degrees.

The management of cultural heritage is underpinned by academic research in archaeology, ethnography and history. The broader subject is also underpinned by research in sociology and culture studies.

Understanding the traditional cultures of all peoples (Indigenous or not) is essential in mitigating the adverse impact of development and ensuring that intervention by more developed nations is not prejudicial to the interests of local people or results in the extinction of cultural resources.

Cultural resources policies have developed over time with the recognition of the economic and social importance of heritage and other cultural assets.

The exploitation of cultural resources can be controversial, particularly where the finite cultural heritage resources of developing countries are exported to satisfy the demand for antiquities market in the developed world. The exploitation of the potential intellectual property of traditional remedies in identifying candidates for new drugs has also been controversial. On the other hand, traditional crafts can be important elements of income from tourism, performance of traditional dances, and music that is popular with tourists and traditional designs can be exploited in the fashion industry. Popular culture can also be an important economic asset.





</doc>
<doc id="2583747" url="https://en.wikipedia.org/wiki?curid=2583747" title="Work of art">
Work of art

A work of art, artwork, art piece, piece of art or art object is an aesthetic physical item or artistic creation. Apart from "work of art", which may be used of any work regarded as art in its widest sense, including works from literature and music, these terms apply principally to tangible, portable forms of visual art: 
Used more broadly, the term is less commonly applied to:

This article is concerned with the terms and concept as used in and applied to the visual arts, although other fields such as aural-music and written word-literature have similar issues and philosophies. The term "objet d'art" is reserved to describe works of art that are not paintings, prints, drawings or large or medium-sized sculptures, or architecture (e.g. household goods, figurines, etc., some purely aesthetic, some also practical). The term oeuvre is used to describe the complete body of work completed by an artist throughout a career.

A "work of art" in the visual arts is a physical two- or three- dimensional object that is professionally determined or otherwise considered to fulfill a primarily independent aesthetic function. A singular art object is often seen in the context of a larger art movement or artistic era, such as: a genre, aesthetic convention, culture, or regional-national distinction. It can also be seen as an item within an artist's "body of work" or "oeuvre". The term is commonly used by: museum and cultural heritage curators, the interested public, the art patron-private art collector community, and art galleries.

Physical objects that document immaterial or conceptual art works, but do not conform to artistic conventions can be redefined and reclassified as art objects. Some Dada and Neo-Dada conceptual and readymade works have received later inclusion. Also, some architectural renderings and models of unbuilt projects, such as by Vitruvius, Leonardo da Vinci, Frank Lloyd Wright, and Frank Gehry, are other examples.

The products of environmental design, depending on intention and execution, can be "works of art" and include: land art, site-specific art, architecture, gardens, landscape architecture, installation art, rock art, and megalithic monuments.

Legal definitions of "work of art" are used in copyright law; "see ".

Marcel Duchamp critiqued the idea that the work of art should be a unique product of an artist's labour, representational of their technical skill or artistic caprice. Theorists have argued that objects and people do not have a constant meaning, but their meanings are fashioned by humans in the context of their culture, as they have the ability to make things mean or signify something.

Artist Michael Craig-Martin, creator of "An Oak Tree", said of his work – "It's not a symbol. I have changed the physical substance of the glass of water into that of an oak tree. I didn't change its appearance. The actual oak tree is physically present, but in the form of a glass of water."

Some art theorists and writers have long made a distinction between the physical qualities of an art object and its identity-status as an artwork. For example, a painting by Rembrandt has a physical existence as an "oil painting on canvas" that is separate from its identity as a masterpiece "work of art" or the artist's "magnum opus". Many works of art are initially denied "museum quality" or artistic merit, and later become accepted and valued in museum and private collections. Works by the Impressionists and non-representational abstract artists are examples. Some, such as the "Readymades" of Marcel Duchamp including his infamous urinal "Fountain", are later reproduced as museum quality replicas.

There is an indefinite distinction, for current or historical aesthetic items: between "fine art" objects made by "artists"; and folk art, craft-work, or "applied art" objects made by "first, second, or third-world" designers, artisans and craftspeople. Contemporary and archeological indigenous art, industrial design items in limited or mass production, and places created by environmental designers and cultural landscapes, are some examples. The term has been consistently available for debate, reconsideration, and redefinition.




</doc>
<doc id="21342679" url="https://en.wikipedia.org/wiki?curid=21342679" title="The arts and politics">
The arts and politics

A strong relationship between the arts and politics, particularly between various kinds of art and power, occurs across historical epochs and cultures. As they respond to contemporaneous events and politics, the arts take on political as well as social dimensions, becoming themselves a focus of controversy and even a force of political as well as social change.

A widespread observation is that a great talent has a free spirit. For instance Pushkin, who some scholars regard as Russia's first great writer, attracted the mad irritation of the Russian officialdom and particularly of the Tsar, since he "instead of being a good servant of the state in the rank and file of the administration and extolling conventional virtues in his vocational writings (if write he must), composed extremely arrogant and extremely independent and extremely wicked verse in which a dangerous freedom of thought was evident in the novelty of his versification, in the audacity of his sensual fancy, and in his propensity for making fun of major and minor tyrants."

According to Groys, "Art has its own power in the world, and is as much a force in the power play of global politics today as it once was in the arena of cold war politics."

Pertaining to such politically-intractable phenomena as the Modern conflicts in the Middle East, however, some artists and social critics believe that "art is useless as a tool for political change." There are, nevertheless, examples where artists employ art in the service of political change.

The Italian poet Ungaretti, when interviewed on transgression by director Pasolini for the 1964 "Love Meetings" documentary, said that the foundation of poetry is to transgress all laws: "I am a poet and as such I begin transgressing all the laws by doing poetry".

The Situationist International (SI), a small group of international political and artistic agitators with roots in Marxism, Lettrism and the early 20th-century European artistic and political avant-gardes formed in 1957, aspired to major social and political transformations; before disbanding in 1972 and splitting into a number of different groups, including the Situationist Bauhaus, the Antinational, and the Second Situationist International, the first SI became active in Europe through the 1960s and elsewhere throughout the world and was characterized by an anti-capitalist and surrealist perspective on aesthetics and politics, according to Italian art historian Francesco Poli. In the works of the situationists, Italian scholar Mirella Bandini observes, there is no separation between art and politics; the two confront each other in revolutionary terms .

Historically, revolutionary ideas have emerged first among artists and intellectuals. That's why a precise mechanism to defuse the role of artists and intellectuals is to relegate them into specialized, compartmentalized disciplines, in order to impose unnatural dichotomies as the "separation of art from politics". Once artistic-intellectual works are separated from current events and from a comprehensive critique of society, they are sterilized and can be safely integrated into the official culture and the public discourse, where they can add new flavours to old dominant ideas and play the role of a gear wheel in the mechanism of the society of the spectacle.

"Not content with claiming leftwing music", posters for the Conservative Party in the UK recycled iconic art styles of "socialist revolution" to communicate its political message in 2008.

Czech sculptor David Černý's "Entropa", a sculpture commissioned to mark the Czech presidency of the European Union Council during the first semester of 2009, illustrates how art can come into conflict with politics, creating various kinds of controversy in the process, both intentionally and unintentionally. "Entropa" attracted controversy both for its stereotyped depictions of the various EU member states and for having been a creation of Černý and two friends rather than, as Černý purported, a collaboration of 27 artists from each of the member states. Some EU members states reacted negatively to the depiction of their country, with Bulgaria, for instance, deciding to summon the Czech Ambassador to Sofia in order to discuss the illustration of the Balkan country as a collection of squat toilets (ČTK). This "Europe-wide hoax … reveals deeper truths" not only about the countries but "about art itself" (Gavrilova).

According to Esti Sheinberg, a lecturer in music at the University of Edinburgh, in her book "Irony, Satire, Parody and the Grotesque in the Music of Shostakovich", in "the traditional Russian perception of the arts", an "interrelationship between artistic technique and ideological content is the main aesthetic criterion" (ix; cf. Blois).

Ludwig van Beethoven did not use the original title "Ode to Freedom" of Friedrich Schiller's lyric, known in English as "Ode to Joy" (1785), in setting it to music in the final movement of his Ninth Symphony (1824), which "Napoleonic censors had forced the poet to change to 'Ode to Joy'." After the fall of the Berlin Wall, on 9 November 1989, that Christmas Day, when Leonard Bernstein conducted a performance of Beethoven's Ninth at the site of the former East German–West German border in Berlin, a concert telecast nationally in the United States, he substituted "Freedom" for "Joy" to reflect his own "personal message".

In February 1952, the United States Customs Service seized the passport of Paul Robeson, preventing him from leaving the United States to travel to the Fourth Canadian Convention of the International Union of Mine, Mill, and Smelter Workers, in Vancouver, British Columbia, Canada; but, after "The convention heard Robeson sing over the telephone", the union organized "a concert on the US-Canada border". According to the account of the "Paul Robeson Centennial Celebration": "Robeson sang and spoke for 45 minutes. He introduced his first song stating 'I stand here today under great stress because I dare, as do you—all of you, to fight for peace and for a decent life for all men, women and children' … [and, accompanied by Lawrence Brown on piano,] proceeded to sing spirituals, folk songs, labour songs, and a passionate version of Old Man River, written for him in the [1920s], slowly enunciating 'show a little grit and you land in jail', underlining the fact that his government had turned the entire country into a prison for Robeson and many others."

In the 1960s the songs of Pete Seeger, Joan Baez, Bob Dylan, and others protested further racism, war, and the military-industrial complex, continuing an American artistic tradition of political protest founded during its colonial era.

In force from July 1985 until May 2002 and considered by its opponents a Draconian "anti-music law", the Teen Dance Ordinance (TDO), imposing restrictions on clubs admitting those under the legal drinking age of 21 in Seattle, Washington, was still the subject of protracted political and legal opposition in U.S. Federal Court in early 2002, when a suit filed by the Joint Artists and Music Promotions Action Committee (JAMPAC) in 2000 was still being adjudicated. In May 2002, Judge Lasnik ruled for the City of Seattle on JAMPAC's suit, finding no Constitutional infringement of the First Amendment and deciding that the matter is a political one for the Seattle City Council to decide, not the courts; during the course of the suit, Mayor Schell's successor, Greg Nickels, a proponent of the bill, resubmitted the ordinance to the Seattle City Council, and, on 12 August 2002, the new All-Ages Dance Ordinance (AADO) replaced the TDO, but was not considered much of an improvement by its critics.

In May 2008 a "Promoters Ordinance" proposed by the Chicago City Council aroused opposition in Chicago, Illinois, for being regarded as overly restrictive and stifling free expression.

Following the implementation of the Licensing Act 2003, the London Borough of Hillingdon cited "the interest of public order and the prevention of terrorism" as reasons for expecting promoters of live music events to complete the Metropolitan Police's Form 696. Though later clarified by a police spokesperson as not "", the perceived "demand" for the information solicited on such "risk assessment" forms motivated Jon McClure, lead singer with Reverend and The Makers, to post an electronic petition in the "E-Petitions" section of the official website of Gordon Brown, the UK Prime Minister, at Number10.gov.uk, in order to facilitate protest against what McClure alleges is "racial discrimination" occasioned by such bureaucratic constraints, which some have deemed "police authoritarianism". It begins: "We the undersigned petition the Prime Minister to Scrap the unnecessary and draconian usage of the 696 Form from London music events". By 11 November 2008, according to Orlowski, "A dozen London boroughs [had] implemented a 'risk assessment' [Form 696] policy for live music that permits the police to ban any live music if they fail to receive personal details from the performers 14 days in advance." Orlowski points out: The demand explicitly singles out performances and musical styles favoured by the black community: garage and R&B, and MCs and DJs. ... However all musical performances – from one man playing a guitar on up – are subject to the demands once implemented by the council. And the threat is serious: failure to comply 'may jeopardise future events by the promoter or the venue'. ... UK Music chief Feargal Sharkey ... speaking to the Department of Culture Media and Sport's hearing on venue licensing today [11 Nov. 2008] [concluded that] ... 'Live music is now a threat to the prevention of terrorism'. ... In response, Detective Superintendent Dave Eyles from the Met's clubs and vice office told us that 10,000 such Risk Assessments would be processed this year. He said they weren't : ... 'We can't demand it – we recommend that you provide it as best practice. But you're bloody silly if you don't, because you're putting your venue at risk.' By early March 2009, over 16,000 British citizens or residents had signed McClure's E-Petition, which remained open to potential signatories until 1 December 2009.



</doc>
<doc id="33563052" url="https://en.wikipedia.org/wiki?curid=33563052" title="Anthropotechnic">
Anthropotechnic

Anthropotechnic is a term used in art, science and literature to denote something with aspects of both man and machine. In this case, it is claimed that the "modified" does not set a limit but instead opens an infinite horizon that is as wide and limitless as human desires. Another conceptualization is that anthropotechnic is a set of rules that we make to tame, teach, and train ourselves. The concept is distinguished from anthropotechnology, which focuses on the study and improvement of working and living conditions.

Paintings such as Max Ernst's "Oedipus Rex" are early examples of the use of this quality. In technology, it is any field of science that attempts to make machines and automation more user-friendly. In sociology, it is used to describe the relationship between man and anything that is perceived as inflexible or inhumane such as slavery, religion or animals. There is a claim that a part of freemasonry, particularly during the 18th century, could also be considered anthropotechnic in the sense that it acted as a caste of technocrats managing society and the human sphere.




</doc>
<doc id="13601902" url="https://en.wikipedia.org/wiki?curid=13601902" title="Arts and letters">
Arts and letters

Arts and letters is a traditional term for the fine arts and literature considered together. The category defined as "arts and letters" may also include the performing arts, visual arts, or liberal arts.

By the late 19th century the term was used to name a few arts-related institutions in the United States. A subscription-based Theatre of Arts and Letters opened in New York in 1892, but closed within a year. The American Academy of Arts and Letters was founded in 1904.

The term has also figured in higher education. For example, by 1889 students at Swarthmore College were sorted into two categories of study: "Arts and Letters" or "Science and Engineering." Since 2010, course requirements at the University of Pennsylvania College of Arts and Sciences have included a "sector" (category) called "Arts and Letters." The college defines the sector as including "visual arts, literature and music, together with the criticism surrounding them."


</doc>
<doc id="33962158" url="https://en.wikipedia.org/wiki?curid=33962158" title="Context art">
Context art

The term Context art () was introduced through the seminal exhibition and an accompanying publication "Kontext Kunst. The Art of the 90s" curated by Peter Weibel at the Neue Galerie im Künstlerhaus Graz (Austria) in 1993 (02.10.–07.11.1993).

Both exhibition and publication aimed to establish grounds for recognizing a new form of artistic practice emerging in the early 1990s. The presentation displayed different approaches though all shared an interest in the use of methods of contextualization to reveal connections between the art works and their conditions of production, whether these were formal, social, or ideologically defined. Institutional critique, feminist positions, later also critiques of precarious economic conditions and issues of globalization, all closely related to social and political changes, became relevant subjects of artistic production.

“It is no longer purely about critiquing the art system, but about critiquing reality and analyzing and creating social processes. In the ’90s, non-art contexts are being increasingly drawn into the art discourse. Artists are becoming autonomous agents of social processes, partisans of the real. The interaction between artists and social situations, between art and non-art contexts has led to a new art form, where both are folded together: Context art. The aim of this social construction of art is to take part in the social construction of reality.” 

It might be due to the fact that the term was introduced under the German translation "KontextKunst" instead of "Context Art" or its likewise politically tended orientation (see Maria Lind’s reference), but it never spread far beyond Europe’s language based barriers. Instead vaguely similar strategies were labeled as "Models of Participatory Practice" in 1998 by Christian Kravagna’s attempt to define the field or the later appearing and quite moderate Relational Art based on the 2002 book "Relational Aesthetics" by Nicolas Bourriaud. 

The accompanying catalog is described to document ”a wide-ranging exhibition designed to illustrate the emergence over the past decade of a new international art movement, .. “ featuring “… an anthology of 22 substantial essays (some reprinted) discussing from diverse perspectives the artistic issues and social and political themes that distinguish "Context Art" from related forms of conceptual and installation art…. “.

Fareed Armaly, Cosima von Bonin, Tom Burr, Clegg & Guttmann, Meg Cranston, Mark Dion, Peter Fend, Andrea Fraser, Inspection Medhermeneutics, Ronald Jones, Louise Lawler, Thomas Locher, Dorit Margreiter, Kasimir Malewitsch, Katrin von Maltzahn, Regina Möller, Reinhard Mucha, Christian Philipp Müller, Anton Olschwang, Hirsch Perlman, Dan Peterman, Adrian Piper, Mathias Poledna, Stephan Prina, Florian Pumhösl, Gerwald Rockenschaub, Julia Scher, Oliver Schwarz, Jason Simon, Rudolf Stingel, Lincoln Tobier, Olga Tschernyschewa, Christopher Williams, Peter Zimmermann, Heimo Zobernig 


</doc>
<doc id="34644396" url="https://en.wikipedia.org/wiki?curid=34644396" title="Pounce (art)">
Pounce (art)

Pouncing is an art technique used for transferring an image from one surface to another. It is similar to tracing, and is useful for creating copies of a sketch outline to produce finished works.

Pouncing has been a common technique for centuries, used to create copies of portraits and other works that would be finished as oil paintings, engravings, and so on. The most common method involves laying semi-transparent paper over the original image, then tracing along the lines of the image by creating pricked marks on the top sheet of paper. This pounced drawing made of pricked holes is laid over a new working surface. A powder such as chalk, graphite or pastel is forced through the holes to leave an outline on the working surface below, thus transferring the image. The powder is applied by being placed into a small bag of thin fabric such as cheesecloth, then dabbed onto the pricked holes of the pounced drawing.

1. Calligraphy in black "nasta'liq" script on a beige paper decorated with bird and leaf designs painted in gold. The main text panel is bordered by a number of other verses in both diagonal and vertical registers forming a frame. The entire composition is pasted to a larger sheet of paper decorated with a pounced vegetal motif in green and backed by cardboard.

2. Black chalk over pounce marks, traces of stylus, watermark of encircled Saint Anthony's cross.

3. Ink and color on paper, pounced for transfer.

4. The original drawing, which has been reinforced in ink and wash by other hands, was used as the pattern for a number of copies, including this example. Pounce marks on the outlines reveal that this copy was traced not from the original but from another copy. It was previously mounted on thin paper, which was cut out and stuck onto thicker paper.



</doc>
<doc id="805897" url="https://en.wikipedia.org/wiki?curid=805897" title="Useful art">
Useful art

Useful art, or useful arts or technics, is concerned with the skills and methods of practical subjects such as manufacture and craftsmanship. The phrase has now gone out of fashion, but it was used during the Victorian era and earlier as an antonym to the performing art and the fine art.

The term "useful Arts" is used in the United States Constitution, which is the basis of United States patent and copyright law:

"To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries;..."
According to the US Supreme Court, the phrase "useful Arts" is meant to reference inventions. There is controversy in the Court as to whether or not this includes business methods. In the majority opinion for "In re Bilski", Justice Anthony Kennedy states "the Patent Act leaves open the possibility that there are at least some processes that can be fairly described as business methods that are within patentable subject matter under §101." In his dissenting opinion, however, Judge Mayer disagreed because he did not consider the claimed business method to be within the useful arts.



</doc>
<doc id="37151950" url="https://en.wikipedia.org/wiki?curid=37151950" title="Bibliography of encyclopedias: art and artists">
Bibliography of encyclopedias: art and artists

This is a list of encyclopedias and encyclopedic/biographical dictionaries published on the subject of art and artists in any language. Entries are in the English language unless stated as otherwise.














</doc>
<doc id="36315801" url="https://en.wikipedia.org/wiki?curid=36315801" title="Social artistry">
Social artistry

Social artistry is the attempt to address or recognize a particular social issue using art and creativity. Social artists are people who use creative skills to work with people or organizations in their community to affect change. While a traditional artist uses their creative skills to express their take on the world, a social artist puts their skills to use to help promote and improve communities. Thus, the main aim of a social artist is to improve society as a whole and to help other people find their own means of creative expression.

Social artists may address issues such as youth alienation or the breakdown of communities. Most commonly, social artists will address these problems by helping people express themselves and find their voice, or by bringing people together and using art to help them to foster an understanding of each other.

Social artistry can incorporate several different art forms including theatre, poetry, music and visual art. 

Findings from 2013 confirm the shift from individual expression to community engagement, or "from autonomous to socially engaged." Lingo and Tepper cite several examples:



</doc>
<doc id="35857691" url="https://en.wikipedia.org/wiki?curid=35857691" title="Carving">
Carving

Carving is the act of using tools to shape something from a material by scraping away portions of that material. The technique can be applied to any material that is solid enough to hold a form even when pieces have been removed from it, and yet soft enough for portions to be scraped away with available tools. Carving, as a means for making sculpture, is distinct from methods using soft and malleable materials like clay, fruit, and melted glass, which may be shaped into the desired forms while soft and then harden into that form. Carving tends to require much more work than methods using malleable materials.

Kinds of carving include:



</doc>
<doc id="3603659" url="https://en.wikipedia.org/wiki?curid=3603659" title="Aesthetic relativism">
Aesthetic relativism

Aesthetic relativism is the idea that views of beauty are relative to differences in perception and consideration, and intrinsically, have no absolute truth or validity.

Aesthetic relativism might be regarded as a sub-set of an overall philosophical relativism, which denies any absolute standards of truth or morality as well as of aesthetic judgement. (A frequently-cited source for philosophical relativism in postmodern theory is a fragment by Nietzsche, entitled "On Truth and Lie in an Extra-Moral Sense".) 

For example, in historical terms, the female form as depicted in the Venus of Willendorf and the women in the paintings of Rubens would today be regarded as over-weight, while the slim models on the covers of contemporary fashion magazines would no doubt be regarded in a negative light by our predecessors. In contemporary (cross-cultural) terms, body modification among "primitive" peoples is sometimes regarded as grotesque by Western society. 

Aesthetic relativism is a variety of the philosophy known generally as relativism, which casts doubt on the possibility of direct epistemic access to the "external world", and which therefore rejects the positive claim that statements made about the external world can be known to be objectively true. Other varieties of relativism include cognitive relativism (the general claim that all truth and knowledge is relative) and ethical relativism (the claim that moral judgments are relative). Aesthetic and ethical relativism are sub-categories of cognitive relativism. Philosophers who have been influential in relativist thinking include David Hume, particularly his "radical scepticism" as set out in "A Treatise of Human Nature"; Thomas Kuhn, with regard to the history and philosophy of science, and particularly his work "The Structure of Scientific Revolutions"; Friedrich Nietzsche, in moral philosophy and epistemology; and Richard Rorty, on the contingency of language.

Philosophers who have given influential objectivist accounts include Plato, and in particular his Theory of the Forms; Immanuel Kant, who argued that the judgment of beauty, despite being subjective, is a universally practiced function of the mind; Noam Chomsky, whose "nativist" theory of linguistics argues for a universal grammar (i.e., that language is not as contingent as relativists have argued that it is).

The most prominent philosophical opponent of aesthetic relativism was Immanuel Kant, who argued that the judgment of beauty, while subjective, is universal. 


</doc>
<doc id="39272508" url="https://en.wikipedia.org/wiki?curid=39272508" title="Artist's impression">
Artist's impression

An artist's impression, artist's interpretation, or artist's rendition is the representation of an object or a scene created by an artist, when no other accurate representation is available. It could be an image, a sound, a video or a model. Artist's impressions are often created to represent concepts and objects that cannot be seen by the naked eye; that are very big, very small, in the past, in the future, fictional, or otherwise abstract in other ways. For example, in architecture, artists' impressions are used to showcase the design of planned buildings and associated landscape. Artists' impressions are particularly prominent in space art.



</doc>
<doc id="725885" url="https://en.wikipedia.org/wiki?curid=725885" title="Studio">
Studio

A studio is an artist or worker's workroom. This can be for the purpose of acting, architecture, painting, pottery (ceramics), sculpture, origami, woodworking, scrapbooking, photography, graphic design, filmmaking, animation, industrial design, radio or television production broadcasting or the making of music. The term is also used for the workroom of dancers, often specified to dance studio.

The word "studio" is derived from the , from , from "studere", meaning to or zeal.

The French term for studio, atelier, in addition to designating an artist's studio is used to characterize the studio of a fashion designer. Atelier also has the connotation of being the home of an alchemist or wizard.

"Studio" is also a metonym for the group of people who work within a particular studio. 

The studio of any artist, especially from the 15th to the 19th centuries, characterized all the assistants, thus the designation of paintings as "from the workshop of..." or "studio of..."
An art studio is sometimes called an atelier, especially in earlier eras. In contemporary, English language use, "atelier" can also refer to the Atelier Method, a training method for artists that usually takes place in a professional artist's studio. 

The above-mentioned "method" calls upon that zeal for study to play a significant role in the production which occurs in a studio space. A studio is more or less artful to the degree that the artist who occupies it is committed to the continuing education in his or her formal discipline. Academic curricula categorize studio classes in order to prepare students for the rigors of building sets of skills which require a continuity of practice in order to achieve growth and mastery of their artistic expression. A versatile and creative mind will embrace the opportunity of such practice to innovate and experiment, which develops uniquely individual qualities of each artist's expression. Thus the method raises and maintains an art studio space above the level of a mere production facility or workshop.

Safety is or may be a concern in studios, with some painting materials required to be handled, stored, or used properly to prevent poisoning, chemical burns, or fire.

In educational studios, students learn to develop skills related to design, ranging from architecture to product design. In specific, educational studios are studio settings where large numbers of students learn to draft and design with instructional help at a college. Educational studios are colloquially referred to as "studio" by students, who are known for staying up late hours into the night doing projects and socializing.

The studio environment is characterized by 2 types in education:

Studio pottery is made by an individual potter working on his own in his studio, rather than in a ceramics factory (although there may be a design studio within a larger manufacturing site).

Production studios are those studios which act as centres for the production in any of the arts; alternatively they can also be the financial and commercial entity behind such endeavours. In radio and television production studio is the place where programs and radio commercial and television advertising are recorded for further emission.

Animation studios, like movie studios, may be production facilities, or financial entities. In some cases, especially in anime, they continue the tradition of a studio where a master or group of talented individuals oversee the work of lesser artists and crafts persons in realising their vision. Animation studios are a fast rising entity and they include established firms such as Walt Disney and Pixar.

Artists or writers, predominantly those producing comics, still employ small studios of staff to assist in the creation of a comic strip, comic book or graphic novel. In the early days of Dan Dare, Frank Hampson employed a number of staff at his studio to help with the production of the strip. Eddie Campbell is another creator who has assembled a small studio of colleagues to help him in his art, and the comic book industry of the United States has based its production methods upon the studio system employed at its beginnings.

Many universities are creating studio settings for courses outside the artist's realm. There are several different projects along these lines, most notably the SCALE-UP (Student-Centered Active Learning Environment for Undergraduate Programs) initiated at NC State.

In audio, a mastering studio is a facility specialised in audio mastering. Tasks may include but not be limited to audio restoration, corrective and tone-shaping EQ, dynamic control, stereo or 5.1 surround editing, vinyl and tape transfers, vinyl cutting, and CD compilation. Depending on the quality of the original mix, the mastering engineer's role can change from small corrections to improving the overall sound of a mix drastically. Typically studios contain a combination of high-end analogue equipment with low-noise circuitry and digital hardware and plug-ins. Some may contain tape machines and vinyl lathes. They may also contain full-range monitoring systems and be acoustically tuned to provide an accurate reproduction of the sound information contained in the original medium. The mastering engineer must prepare the file for its intended destination, which may be radio, CD, vinyl or digital distribution.

In video production, a mastering studio is a facility specialized in the post-production of video recordings. Tasks may include but not be limited to: video editing, colour grading correction, mixing, DVD authoring and audio mastering. The mastering engineer must prepare the file for its intended destination, which may be broadcast, DVD or digital distribution.

An "acting studio" is an institution or workspace (similar to a dance studio) in which actors rehearse and refine their craft. The Neighborhood Playhouse and Actors Studio are legendary acting studios in New York.

A movie studio is a company which develops, equips and maintains a controlled environment for filmmaking. This environment may be interior (sound stage), exterior (backlot) or both.

A photographic studio is both a workspace and a corporate body. As a workspace it provides space to take, develop, print and duplicate photographs.

A radio studio is a room in which a radio program or show is produced, either for live broadcast or for recording for a later broadcast. The room is soundproofed to avoid unwanted noise being mixed into the broadcast.

A recording studio is a facility for sound recording which generally consists of at least two rooms: the studio or live room, and the control room, where the sound from the studio is recorded and manipulated. They are designed so that they have good acoustics and so that there is good isolation between the rooms.

A television studio is an installation in which television or video productions take place, for live television, for recording video tape, or for the acquisition of raw footage for post-production. The design of a studio is similar to, and derived from, movie studios, with a few amendments for the special requirements of television production. A professional television studio generally has several rooms, which are kept separate for noise and practicality reasons.

Many of the healing arts and activities such as yoga, judo, karate are "studied" in a studio. It is very common to see yoga studios and martial arts studios established in settings that might previously have been for other uses, described as studios. These are not really recreation centers, or gyms in the traditional sense, but places where students of these activities practice or study their art.



</doc>
<doc id="3410311" url="https://en.wikipedia.org/wiki?curid=3410311" title="Acknowledgment (creative arts and sciences)">
Acknowledgment (creative arts and sciences)

In the creative arts and scientific literature, an acknowledgement (also spelled acknowledgment in American and Canadian English) is an expression of a gratitude for assistance in creating an original work.

Receiving credit by way of acknowledgement rather than authorship indicates that the person or organization did not have a direct hand in producing the work in question, but may have contributed funding, criticism, or encouragement to the author(s). Various schemes exist for classifying acknowledgements; Cronin et al. give the following six categories:

Apart from citation, which is not usually considered to be an acknowledgement, acknowledgement of conceptual support is widely considered to be the most important for identifying intellectual debt. Some acknowledgements of financial support, on the other hand, may simply be legal formalities imposed by the granting institution. Occasionally, bits of science humor can also be found in acknowledgements.

There have been some attempts to extract bibliometric indices from the acknowledgments section (also called "acknowledgments paratext") of research papers in order to evaluate the impact of the acknowledged individuals, sponsors and funding agencies.

The spelling "acknowledgement" is used in British English, Australian English, and other English-speaking areas outside North America, whereas the spelling "acknowledgment" (without the "e" after the "g") is often used in American English and Canadian English.



</doc>
<doc id="40476642" url="https://en.wikipedia.org/wiki?curid=40476642" title="Poet as legislator">
Poet as legislator

The theme of poet as legislator reached its grandiose peak in the Romantic era, epitomised in the view of the lonely, alienated poet as 'unacknowledged legislator' to the whole world.

However the concept had a long prehistory in Western culture, with classical figures like Orpheus or Solon being appealed to as precedents for the poet's civilising role.

Plato's opposition to poets in his ideal Republic was predicated on the contemporary existence of Homeric expounders who claimed that "a man ought to regulate the tenour of his whole life by this poet's directions". Plato only allowed the already censured poet to guide the young, to be an acknowledged legislator at the price of total external control.

Less threatened by the poetic role, the Romans by contrast saw poetry, with Horace, as primarily pleasing, and only secondarily as instructive.

Building the view of the fifteenth century Florentine Neoplatonists on the poet as seer, however, Sir Philip Sidney developed a more powerful concept of the poet as overtopping the philosopher, historian and lawyer to stand out as "the monarch...of all sciences.

Such a viewpoint was more or less institutionalised in Augustan literature, Johnson's Rasselas maintaining for example that the poet "must write as the interpreter of nature and the legislator of mankind" – a fully public, even patriotic role moreover

By contrast the Romantic view of the poet as "unackowledged" legislator emerges at the turm of the century in the writing of William Godwin, with his anarchic view of the poet as "the legislator of generations and the moral instructor of the world".

It received its most memorable formulation however in Percy Bysshe Shelley's 1820 "A Defence of Poetry". Shelley maintained that, through their powers of imaginative understanding, 'Poets' (in the widest sense, of ancient Greece) were able to identify and formulate emerging socio-cultural trends; and were as a result "the hierophants of an unapprehended inspiration...the unacknowledged legislators of the world".

The grand claims of the Romantics began to give way in the twentieth century to a more ironic stance – Yeats speaking for his calling in general when he wrote "We have no gift to set a statesman right".

What remained of the Shelley claim was to be further diminished by Postmodernism's distrust for grand narratives, if not perhaps destroyed entirely.


</doc>
<doc id="5134465" url="https://en.wikipedia.org/wiki?curid=5134465" title="Social practice">
Social practice

Social practice is a theory within psychology that seeks to determine the link between practice and context within social situations. Emphasized as a commitment to change, social practice occurs in two forms: activity and inquiry. Most often applied within the context of human development, social practice involves knowledge production and the theorization and analysis of both institutional and intervention practices.

Through research, Sylvia Scribner sought to understand and create a decent life for all people regardless of geographical position, race, gender, and social class. Using anthropological field research and psychological experimentation, Scribner tried to dig deeper into human mental functioning and its creation through social practice in different societal and cultural settings. She therefore aimed to enact social reform and community development through an ethical orientation that accounts for the interaction of historical and societal conditions of different institutional settings with human social and mental functioning and development.

Social practice involves engagement with communities of interest by creating a practitioner-community relationship wherein there remains a focus on the skills, knowledge, and understanding of people in their private, family, community, and working lives. In this approach to social practice, activity is used for social change without the agenda of research. Activity theory suggests the use of a system of participants that work toward an object or goal that brings about some form of change or transformation in the community.

Within research, social practice aims to integrate the individual with his or her surrounding environment while assessing how context and culture relate to common actions and practices of the individual. Just as social practice is an activity itself, inquiry focuses on how social activity occurs and identifies its main causes and outcomes. It has been argued that research be developed as a specific theory of social practice through which research purposes are defined not by philosophical paradigms but by researchers' commitments to specific forms of social action.

In education, social practice refers to the use of adult-child interaction for observation in order to propose intentions and gauge the reactions of others. Under social practice, literacy is seen as a key dimension of community regeneration and a part of the wider lifelong learning agenda. In particular, literacy is considered to be an area of instruction for the introduction of social practice through social language and social identity. According to social practice in education, literacy and numeracy are complex capabilities rather than a simple set of basic skills. Furthermore, adult learners are more likely to develop and retain knowledge, skills, and understanding if they see them as relevant to their own problems and challenges. Social practice perspectives focus on local literacies and how literacy practices are affected by settings and groups interacting around print.

As literature is repeatedly studied in education and critiqued in discourse, many believe that it should be a field of social practice as it evokes emotion and discussion of social interactions and social conditions. Those that believe literature may be construed as a form of social practice believe that literature and society are essentially related to each other. As such, they attempt to define specific sociological practices of literature and share expressions of literature as works comprising text, institution, and individual. Overall, literature becomes a realm of social exchange through fiction, poetry, politics, and history.

Social practice is also considered a medium for making art. Social practice art came about in response to increasing pressure within art education to work collaboratively through social and participatory formats from artists' desires and art viewers' increasing media sophistication. "Social practice art" is a term for artwork that uses social engagement as a primary medium, and is also referred to by a range of different names: socially engaged art, community art, new-genre public art, participatory art, interventionist art, and collaborative art.

Artists working in the medium of social practice develop projects by inviting collaboration with individuals, communities, institutions, or a combination of these, creating participatory art that exists both within and outside of the traditional gallery and museum system. Artists working in social practice art co-create their work with a specific audience or propose critical interventions within existing social systems that inspire debate or catalyze social exchange. Social practice art work focuses on the interaction between the audience, social systems, and the artist through topics such as aesthetics, ethics, collaboration, persona, media strategies, and social activism. The social interaction component inspires, drives, or, in some instances, completes the project. Although projects may incorporate traditional studio media, they are realized in a variety of visual or social forms (depending on variable contexts and participant demographics) such as performance, social activism, or mobilizing communities towards a common goal.


</doc>
<doc id="35922455" url="https://en.wikipedia.org/wiki?curid=35922455" title="Water marble nail">
Water marble nail

Water marble nails are a finger nail art technique involving dropping nail lacquers into clear water and creating a pattern on the water surface, the pattern is then transferred to the nails.

The water marble nail technique was originally developed by professional nail technicians in Japanese nail salons. In the 1990s, it was popularized by commercial publications released by shopping centers in Chiba, Japan. In 2010, water marble nail art was adapted to use acrylic artificial nails and gels.

The water marble nail technique has gained popularity across the globe through features in magazines, websites, polish makers and videos.

There are two main types of water marble nail art methods: free-dragging and free-dropping. Free-dragging is more common.

Dragged patterns range from simple circular shapes to complicated drawings. Patterns such as marble, hearts/peacocks, animals, flowers, leaves, parallel lines, psychedelics, spiderwebs, and random patterns in the style of designer Emilio Pucci are generally performed with nail lacquers and some kind of tool.

Dropped patterns range from colored shapes to complicated drawings such as spirals and geometric designs. The free patterns are created by colored drops of nail lacquer. For free-dropping, lacquer colors are dropped straight or diagonally onto the water. The pattern floats on the surface of the water.

Water marble nail art requires clean water, nail lacquers for free-dropping, and a stick for drawing patterns. Before patterns are created, the nails are painted with a light-colored nail polish that establishes a good contrast with the colors chosen to create the water marble. Some techniques use a matte coat to provide contrast and maintain an even look.

After the base coat has dried, tape is put around the fingers to gather the excess nail polish after the marbling. One or two drops of colored lacquer are chosen for the design and are added to a cup of clean water. The drops will create a circle on the water surface. The next color is added on top of the circle created by the previous drop. The resulting pattern is ready for application to the nail, but it can still be modified with a stick or toothpick to create different shapes. The nail is dipped into the pattern on the water and kept under water as a Q-tip is used to "grab" the remaining polish.


</doc>
<doc id="41520090" url="https://en.wikipedia.org/wiki?curid=41520090" title="Impalement in myth and art">
Impalement in myth and art

The use of impalement in myth, art, and literature includes mythical representations of it as a method of execution and other uses in paintings, sculptures, and the like, folklore and other tales in which impalement is related to magical or supernatural properties, and the use of simulated impalement for the purposes of entertainment.

The idea that the vampire "can "only" be slain with a stake driven through its heart" has been pervasive in European fiction. Examples such as Bram Stoker's Dracula and the more recent Buffy The Vampire Slayer and Twilight series' all incorporate that idea. In classic European folklore, it was believed that one method, "among several", to "kill" a vampire, or prevent a corpse from rising as a vampire, was to drive a wooden stake through the heart before interment. In one story, an Istrian peasant named Jure Grando died and was buried in 1656. It was believed that he returned as a vampire, and at least one villager tried to drive a stake through his heart, but failed in the attempt. Finally, in 1672, the corpse was decapitated, and the vampire terror was put to rest. Although the Eastern European, in particular Slavic (but also Romanian), conception of the vampire as an undead creature in which impaling it was central to either destroying it, or at least immobilizing it, is the most well-known European tradition, such traditions can also be found elsewhere in Europe. In Greece, the troublesome undead were usually called a vrykolakas. The archaeologist Susan-Marie Cronkite describes an odd grave found at Mytilene, at Lesbos, a find the archeologists connected with the vrykolakas superstition.

The Norse draugr, or haugbui (mound-dweller), was a type of undead typically (but not exclusively) associated with those put (supposedly) to rest in burial mounds/tumuli. The approved methods of killing a draugr are "to sever his head from his body and set the same beneath his rump, or impale his body with a stake or burn it to ashes".

Although in modern vampiric lore, the stake is regarded as a very effective tool against the undead, people in pre-modern Europe could have their doubts. Edward Payson Evans tells the following story, from the city Kadaň:

A graphic description of the vertical impalement of a Serbian rebel by Ottoman authorities can be found in Ivo Andrić's novel "The Bridge on the Drina". Andrić was later awarded the Nobel Prize for Literature for the whole of his literary contribution, though this novel was the "magnum opus".

Some anecdotes of the behavior and fates of the impaled remain which, if true, would be unique in the history of impalement. The first was narrated as a proof of the efficacy of praying to Saint Barbara. In the woods of Bohemia around 1552, there was a robber band roaming, plundering and murdering innocent travelers. A manhunt was organized, and the robber chief was apprehended and sentenced to be impaled. While one of his associates, likewise impaled, swiftly expired, the chief was not so lucky.
All day long, he writhed on his stake, begging to be killed, but all in vain. That night, in his despair, he prayed to St. Barbara that he was truly sorry for all his evil doings in life and that all he hoped for was to reconcile with God and to be graced with a good death. Seemingly in response, the man's stake broke, and with great effort and pain, he managed to "de-impale" himself. Crawling along, he came to a house, and his cries of help were heard. He was helped into a bed, and a priest was sent for. The former robber chief then gave his death bed confession, grieving over his misspent life, but properly grateful to God and St. Barbara. He then died in peace, his hands folded over his chest.

Another incident, which was, allegedly, partially witnessed by the editor of a "Ladies' Journal", is said to have occurred in Wallachia in the 1770s. He had been present in Arad when 27 robbers had been impaled. It was strictly forbidden to give the impaled persons any water, but one woman took mercy on one of the robbers, and fetched water for him in a kettle. As she was glancing anxiously about to check if anyone took notice of her forbidden act of mercy, the robber smashed her head in with the kettle, killing her on the spot. The editor avers he was present when the robber was asked why he had done such a thing, and he merely replied he had done it on a whim, and just had felt like killing her then and there.

In British Columbia, a folk tale from the Lillooet People is preserved in which impalement occurs as a central element. A man became suspicious of his wife because she went out each day to gather roots and cedar-bark but hardly ever brought anything home. One day, he spied on her, and discovered that she was cavorting with Lynx, rather than doing her wifely duties. The next day, he asked to accompany her, and they went out in the forest, and came at last to a very tall tree. The man climbed to the top of it, the wife following. The jealous man then sharpened the top of the tree with his knife, and impaled his wife on it. On his way down, he removed the bark of the tree, so it became slick. The woman cried out her pain and her brothers heard her. They and animals they called to help them tried to rescue her, but the stem was too slick for them to climb up to reach her. Then Snail offered to help her, and slowly crawled up the tree. But alas, Snail moved too slowly, and by the time it took him to reach the top of the tree, the woman was dead.

Among tribes living around the Titicaca, tales circulated in the sixteenth century that prior to the Incas, a mysterious group of white men lived there, and their banishment was somehow connected with the birth of the Sun. A sixteenth century tale collected by a Spanish missionary tells of such an individual, called Tanupa or Taapac, who was impaled by other Indians around the Titicaca, and a shrine was set up there to commemorate the events.

The renowned Sufi mystic was in AD 922 to be impaled for blasphemy in Baghdad, for having said such things as "I am God". However, the executioners were unable to do so, because al-Hallaj floated in the air just above their reach. Then, al-Hallaj's spirit ascended to Heaven, and conversed with Muhammed, the Prophet of Islam, and al-Hallaj asked the Prophet if he should let himself be impaled. The Prophet acknowledged that al-Hallaj's spiritual state was so heightened that his utterance "I am God" was both just and true, but that for the sake of ordinary people, he should let himself be impaled, because their spiritual state was such that they would be mislead from practical religion if they were to believe in such sayings like "I am God". And thus, for the sake of preserving the religion of ordinary people, al-Hallaj let himself be impaled at last.

Tales and anecdotes concerning how dreadfully swift and harsh Ottoman justice was for comparatively trivial offenses abound. Dimitrie Cantemir, a Moldavian noble living in Constantinople at the end of the 17th century, and often engaged in pleas of cases towards Ottoman authorities, narrates a tale from the building of a great mosque there in 1461. The Greek architect was amply rewarded by the sultan, so that a whole street was privileged to the Greek populace in recognition of his efforts. However, some asked the architect if he could even build a greater and more beautiful mosque than the one completed. Incautiously, the architect said sure enough, if I were given the materials. The sultan, upon hearing this, was so fearful that his successors might create an even more beautiful mosque than his own, that just in case, he chose to impale the architect to deprive successors of that genius, commemorating the event by erecting a huge iron spike in the middle of the mosque. Not even bothering to refute this tale of impalement, Cantemir says that he "does", however, believe in the grand gift of the street, because he had used the original charter from the sultan to protect the Greek interest when somebody wanted to deprive the Greeks of the privilege. Cantemir won his case. In 1632, under Murad IV (r.1623–40), a hapless interpreter in a fierce dispute between the French ambassador and Ottoman authorities was impaled alive for faithfully translating the insolent words of the ambassador. Furthermore, Murad IV sought to ban the use of tobacco, and reportedly impaled alive a man and a woman for breaking the law, the one for selling tobacco, the other for using it. Another such anecdote, is said to have occurred in 1695 under Mustafa II: The Grand Vizier prevented access to the sultan to a poor shoemaker who had a petition for his sovereign. Once the sultan learnt of it, he promptly ordered the Grand Vizier to be impaled, although the Grand Vizier was the son of the sultan's favourite concubine.

In the Hindu Draupadi cult, impalement of animals, demons, and humans is a recurring motif within legends and symbolic re-enactments during holidays/festivals.

According to a Shaivite story from India, under the old Pandyan Dynasty, ruling from 500 BC-1500 CE, the 7th century King Koon Pandiyan had 8000 Jains impaled in Madurai. Some historians regard the story as a legend rather than historically accurate, and that it might have been created by the Shaivites to prove their superiority over the Jains. This act, legendary or not, is still commemorated in "lurid mural representations" in several Hindu temples in Tamil Nadu. An example of such depictions in temples can be found in the Meenakshi Amman Temple in Madurai, around the holy tank enclosure to the shrine of Meenakshi. There, a long line of impaled Jaines are depicted, with dogs at their feet, licking up the blood, and crows flying around to pick out their eyes.

In Bengal, tales existed about a foolish king in the Pala Empire, Bhava Chandra, and his equally foolish minister. They are a pair not unlike the Wise Men of Gotham, bereft of common sense as a result of a curse laid upon them. In their last judgment, they had condemned two robbers to be impaled, but when the robbers began quarreling about who should get impaled on the longest pole, Bhava Chandra and his minister became deeply intrigued. The robbers told them that whoever died on the longest pole would be reincarnated as the ruler of the Earth, while the other would become his minister. Thinking it unseemly that two mere robbers should gain such a high position in their next life, Bhava Chandra chose to impale himself on the longest pole, while his minister happily chose to die on the shorter.

The remains of persons impaled have, occasionally, been thought imbued with certain magical properties. For example, the Arthashastra states that if one wishes to make a fire at one place, and prevent any other fire to be lit there, one could make that fire "by the friction of a black-and-white bamboo stick on the rib bone of the left side of a man who has been slain with a sword, or impaled". Virginia Saunders also mentions from the same text how to become invisible:
The ascetic Mandavya, when he was five years old, had amused himself with sticking a reed into a living locust. Lord Yama, the god of death, bided his time to exact what he thought was a proper punishment. As an old man, Mandavya was sitting outside his cave in deep meditation, oblivious to some thieves placing their stolen goods there. Wrongfully believing Mandavya had stolen the goods, the authorities placed Mandavya on trial. He could not answer the judge on how the goods had come to be in his hermitage, so the king declared he was to be impaled. Mandavya seemed unperturbed by the whole affair, and when he was still alive, in deep contemplation, on the stake after 4 years, the king declared Mandavya had to be innocent, and ordered him pulled down. However, the stake broke inside Mandavya's body, and the excruciating pain destroyed Mandavya's trance. In deep bitterness, he asked the gods how he had deserved such a fate, and Yama answered it was because of the locust he had tortured as a young boy. Mandavya became infuriated at Yama and pointed out how disproportionate the punishment had been. He then cursed Yama to be born as a human being, namely as Vidura, the son of a mere serving maid.

A tale from Kashmir of reincarnation after death on the stake concerns the sage Samdhimati. Samdhimati was minister under King Jayendra, when a mysterious prophecy spread through the populace: "To Samdhimati will belong the kingdom". Jayendra, on hearing of this, threw Samdhimati in prison for 10 years. When the king was on his death bed, he was unwilling to let Samdhimati have the prophecy fulfilled, so he ordered Samdhimati impaled. When Samdhimati's guru Isana heard of this, he went to the cemetery where Samhimati was impaled in order to perform the proper funeral rites. The wolves had devoured all the flesh of the body, and Isana was amazed that the prophecy was inscribed on Samdhimati's skull that he was to inherit the kingdom. Keeping watch, one night Isana saw the graveyard was filled with Yoginis (female mystics/witches). The Yoginis were drunk and "lustful for a man", and provided the skeleton with flesh (not the least, a penis) from their own bodies. They then captured Samdhimati's spirit, which was still hovering around, within the fleshed skeleton, and "spent the rest of the night sporting with him". As dawn approached, Isana, afraid that Samdhimati's new body should be dissolved by the witches, rushed out from his hiding place, and chased them away. In his new body and life, Samdhimati became known as Aryaraja, and was, indeed, crowned as King of Kashmir, thereby fulfilling the prophecy.

In the Buddhist conception of the eight Hells, as John Bowring relates from Siam, those consigned for the Sixth Hell are impaled on spits and roasted. When well roasted, enormous dogs with iron teeth devour them. But, the damned are reborn, and must relive this punishment for 16000 years, over and over again ... Another tale popular in Siam was about Devadatta, a wily antagonist to Buddha seeking to undermine Gautama's position among his followers. For this crime, Devadatta was sent off into the very deepest Hell, the Avici, being impaled on three great iron spears in a sea of flames.

The 1980 Italian film, "Cannibal Holocaust", directed by Ruggero Deodato, graphically depicts impalement. The story follows a rescue party searching for a missing documentary film crew in the Amazon Rainforest. The film's depiction of indigenous tribes, death of animals on set, and the graphic violence (notably the impalement scene) brought on a great deal of controversy, legal investigations, boycotts and protests by concerned social groups, bans in many countries (some of which are still in effect), and heavy censorship in countries where it has not been banned. The impalement scene was so realistic, that Deodato was charged with murder at one point. Deodato had to produce evidence that the "impaled" actress was alive in the aftermath of the scene, and had to further explain how the special effect was done: the actress sat on a bicycle seat mounted to a pole while she looked up and held a short stake of balsa wood in her mouth. The charges were dropped.

In stage magic, the "illusion of impalement" is a popular feat of magic that appears to be an act of impalement. Impaling tricks are not, however, a modern European invention, and some dervish orders performed such acts already in the 18th century. Carsten Niebuhr, traveling the Middle East 1761–67 on a Danish funded expedition, saw such a display at Basra: 



</doc>
<doc id="35853255" url="https://en.wikipedia.org/wiki?curid=35853255" title="International trade in fine art">
International trade in fine art

The international trade of fine art is most precisely defined as the trade across nations of unique, non-reproducible works by an artist. The art trade contradicts typical international trade models since it is a culturally significant good. It is not treated by consumers the same way any other commodity would because of the aesthetic value that is unique to each piece. Despite existing as a finite physical piece, unique art is still considered intellectual property. This sparks the debate as to whether art exports should be restricted for nationalistic and cultural reasons, or liberalized for the sake of a healthier international market.

The trade commodities included in the definition of "visual art" include the following: painting, drawing, sculpture in various materials, printmaking, photography, maps, performance art, installation art, mail art, assemblage art, textile arts, fashion design, video art, digital art, and product design. These works are non-functional, emotional, social, political, traditional, and cultural statements, and in comparison to other goods, are not greatly affected by commercial-sector constraints. Though visual art is a physical, hand-made good, it is often culturally rooted and created for aesthetic appeal. Therefore, art is considered intellectual property.

The 4-digit Standard International Trade Classification (SITC) classifies "Works of Art, Collectors Pieces and Antiques" under category 8960, which includes paintings, drawings, pastels, original sculptures, original prints, stamps, and antiques over 100 years old. This is the only SITC category that consists of unique, non-reproducible art, which is typically thought of as "fine" art. The 4-digit harmonized commodity description and coding system, otherwise known as the harmonized system (HS) code for "fine" art is 9701, which is classified as "Paintings, Drawings and Pastels, Executed Entirely By Hand".

The earliest known regulation of cultural property dates to 1464, when Pope Pius II prohibited the exportation of works of art from the Papal States. It wasn’t until the mid-1500s that any sizable amount of formal artwork was transferred between nations in licit markets. Previously, local demand had satisfied the supply of artwork, but it couldn’t keep up as the number of artists increased. Consequently, artists exported their works to foreign markets. Between 1540 and 1670, an average of 144 paintings per year were transported between the Netherlands and New Spain.

Throughout much of the early modern period, if an artist could not domestically sell his art, he sold it instead to dealers who exported the works abroad. Starting in the 17th Century, however, most art was traded at the massive auction houses of Christie's and Sotheby's of London, which both still survive today.

Like any other traded good, art has been historically subjected to import duties. For example, in the more enlightened years of the 19th Century, art escaped high tariffs in America because the government viewed art as an important cultural good. At other times, though, tariff revenue was considered more important than free intellectual property.

During World War II, neutral Switzerland became the primary trafficker of art on the European continent. Most "degenerate" works of art that the Nazi government purged from German museums were sold there, where they largely entered black markets. Since the war ended, there has been a massive, ongoing effort to recover all of these works. For fifteen years following the war, 45,000 pieces were returned to France, mostly to Jewish owners.

Today, almost every country in the world has restrictions and regulations on the export of cultural property. Currently, most art auctions are facilitated on online sites such as eBay and Lauritz.com.

Fine art proves complicated for economists to analyze, mostly because trade in unique art is in large part trade between consumers – the "secondary market" – rather than the "primary market" trade between the producer (artist) and the consumer. For example, when a museum buys a sculpture from a private collector, the exchange is between two consumers and considered a transaction in the secondary market because neither of them produced the sculpture. It is more complicated for economists, then, to capture these transactions in their data.

Comparative advantage is also more difficult to pinpoint in the case of cultural goods. There is a certain degree of cultural nationalism of art, making some nations reluctant to part with their cultural property. Additionally, relative advantage cannot simply be calculated by the marginal cost of producing a unit of art, since aesthetic value plays so heavily into its price.

Trade theory demonstrates how much and at what price countries trade goods if they have different endowments or different preferences. But this model is not very useful because of retentive nationalism: a country that is relatively less endowed with art-producing resources will not stop producing simply because they can import art from abroad at cheaper prices.

Traditional trade theory treats art as a homogenous, non-differentiated good, which is where it fails to reliably predict trade trends. Unique art is valued precisely because of its uniqueness. Since each piece of art is different, and because each piece does not appear on the market very often, the determination of changes in market value prove difficult to determine.

Economists use the hedonic regression (HR) estimation method to calculate prices in art. This is used to predict prices based on various attributes of the artwork such as its dimensions, the artist, and the subject matter attended to.

Protectionism is a nationalistic viewpoint that contends that a healthy cultural industry is necessary to assert national sovereignty and identity. Countries with small domestic markets are often overwhelmed by imports from larger markets in which producers can make up their costs of production by dumping content abroad. America is the largest exporter of artwork in the world, and English speaking countries are especially vulnerable to American imports. Protectionists view this as a modern form of American imperialism, which reduces cultural diversity when national industries are unable to compete.

Protectionism is less concerned with direct economic interest as it is with preserving cultural integrity. Australian economist David Throsby argues that investment in "cultural capital" may be necessary for the sustainability of a culture. Paul M. Bator, who helped negotiate and draft the UNESCO convention on the International Trade of Art, argues that larger countries are responsible for the cultural interests of smaller ones. He explains in "The International Trade in Art" that "art-rich countries should create tax and other financial and psychological incentives to persuade those with important antiquities and archaeological objects to keep them at home".
Proponents of liberalization in international art markets stress that open markets contribute to efficient production and distribution of cultural products. Reliance on competitive market forces also frees creative individuals from the hindrance of government oversight. Many special characteristics of cultural products – the lack of rivalry in content, economies of scale, and agglomeration economies – enhance the case for opening markets. Protection raises rather than lowers domestic prices, which detracts from consumer welfare.

The "commodification objection" is the reluctance to classify an important cultural good as a simple trade commodity, and is frequently cited by cultural protectionists. Those who advocate openness believe that free-market nations can provide the most effective political force for development of an active market. This argument diametrically opposes Bator’s: free-trade advocates contend that large countries are better suited to help smaller nations finance organization of their cultural resources for more effective participation in international trade.

The issue of the illicit art trade also factors into the debate. In terms of value transferred, the illegal trafficking of art ranks in black market activities second only to narcotics. More problematic than the actual theft of the work is its subsequent transport, which proves difficult to prosecute since most paintings are easily concealable. By maintaining an open market, liberalists argue that much of the illicit trade of art can be eliminated.

The following tables list the top world import/export values, according to data from the UN Comtrade database using SITC code 8960. The sums here are greater than those included in HS 9701 (works done exclusively by hand) because the SITC classification includes works such as antiques, stamps, and sculpture that aren’t captured in the HS definition.

The United States both imports and exports the greatest value of art. The following tables use data from HS code 9701 code to rank the USA’s biggest trading partners in art.



</doc>
<doc id="65821" url="https://en.wikipedia.org/wiki?curid=65821" title="Pastiche">
Pastiche

A pastiche is a work of visual art, literature, theatre, or music that imitates the style or character of the work of one or more other artists. Unlike parody, pastiche celebrates, rather than mocks, the work it imitates.

The word "pastiche" is a French cognate of the Italian noun "pasticcio", which is a pâté or pie-filling mixed from diverse ingredients. Metaphorically, "pastiche" and "pasticcio" describe works that are either composed by several authors, or that incorporate stylistic elements of other artists' work. Pastiche is an example of eclecticism in art.

Allusion is not pastiche. A literary allusion may refer to another work, but it does not reiterate it. Moreover, allusion requires the audience to share in the author's cultural knowledge. Both allusion and pastiche are mechanisms of intertextuality.

In literature usage, the term denotes a literary technique employing a generally light-hearted tongue-in-cheek imitation of another's style; although jocular, it is usually respectful. The word implies a lack of originality or coherence, an imitative jumble, but with the advent of postmodernism pastiche has become positively constructed as deliberate, witty homage or playful imitation.

For example, many stories featuring Sherlock Holmes, originally penned by Arthur Conan Doyle, have been written as pastiches since the author's time. Ellery Queen and Nero Wolfe are other popular subjects of mystery parodies and pastiches.

A similar example of pastiche is the posthumous continuations of the Robert E. Howard stories, written by other writers without Howard's authorization. This includes the Conan the Barbarian stories of L. Sprague de Camp and Lin Carter.
David Lodge's novel "The British Museum Is Falling Down" (1965) is a pastiche of works by Joyce, Kafka, and Virginia Woolf. In 1991 Alexandra Ripley wrote the novel "Scarlett", a pastiche of "Gone with the Wind", in an unsuccessful attempt to have it recognized as a canonical sequel. 

In 2017, John Banville published "Mrs. Osmond", a sequel to Henry James's "The Portrait of a Lady", written in a style similar to that of James. In 2018, Ben Schott published "Jeeves and the King of Clubs", an homage to P. G. Wodehouse's character Jeeves, with the blessing of the Wodehouse estate.

Charles Rosen has characterized Mozart's various works in imitation of Baroque style as pastiche, and Edvard Grieg's "Holberg Suite" was written as a conscious homage to the music of an earlier age. Some of Pyotr Ilyich Tchaikovsky's works, such as his "Variations on a Rococo Theme" and "Serenade for Strings", employ a poised "classical" form reminiscent of 18th-century composers such as Mozart (the composer whose work was his favorite). Perhaps one of the best examples of pastiche in modern music is that of George Rochberg, who used the technique in his String Quartet No. 3 of 1972 and Music for the Magic Theater. Rochberg turned to pastiche from serialism after the death of his son in 1963.

"Bohemian Rhapsody" by Queen is unusual as it is a pastiche in both senses of the word, as there are many distinct styles imitated in the song, all "hodge-podged" together to create one piece of music. A similar earlier example is "Happiness is a Warm Gun" by The Beatles. One can find musical "pastiches" throughout the work of the American composer Frank Zappa.

A "pastiche Mass" is a musical Mass where the constituent movements come from different Mass settings. Most often this convention has been chosen for concert performances, particularly by early-music ensembles. Masses are composed of movements: Kyrie, Gloria, Credo, Sanctus, Agnus Dei; for example, the "Missa Solemnis" by Beethoven and the "Messe de Nostre Dame" by Guillaume de Machaut. In a pastiche Mass, the performers may choose a Kyrie from one composer, and a Gloria from another; or choose a Kyrie from one setting of an individual composer, and a Gloria from another.

In musical theatre pastiche is often an indispensable tool for evoking the sounds of a particular era for which a show is set. For the 1971 musical "Follies", a show about a reunion of performers from a musical revue set between the World Wars, Stephen Sondheim wrote over a dozen songs in the style of Broadway songwriters of the 1920s and 1930s. Sondheim imitates not only the music of composers such as Cole Porter, Irving Berlin, Jerome Kern, and George Gershwin but also the lyrics of writers such as Ira Gershwin, Dorothy Fields, Otto Harbach, and Oscar Hammerstein II. For example, Sondheim notes that the torch song "Losing My Mind" sung in the show contains "near-stenciled rhythms and harmonies" from the Gershwins' "The Man I Love" and lyrics written in the style of Dorothy Fields. Examples of musical pastiche also appear in other Sondheim shows including "Gypsy", "Saturday Night", "Assassins", and "Anyone Can Whistle".

Pastiche can also be a cinematic device whereby filmmakers pay homage to another filmmaker's style and use of cinematography, including camera angles, lighting, and mise en scène. A film's writer may also offer a pastiche based on the works of other writers (this is especially evident in historical films and documentaries but can be found in non-fiction drama, comedy and horror films as well). Italian director Sergio Leone`s "Once Upon a Time in the West" is a pastiche of earlier American Westerns. Another major filmmaker, Quentin Tarantino, often uses various plots, characteristics and themes from many lesser-known films to create his films, among them from the films of Sergio Leone, in effect creating a pastiche of a pastiche. Tarantino has openly stated that "I steal from every single movie ever made." Director Todd Haynes' 2002 film "Far From Heaven" was a conscious attempt to replicate a typical Douglas Sirk melodrama - in particular "All That Heaven Allows". The film works as a mostly reverential and unironic tribute to Sirk's filmmaking, lovingly re-creating the stylized mise-en-scene, colors, costumes, cinematography and lighting of Sirkian melodrama.

In cinema, the influence of George Lucas' "Star Wars" films (spawning their own pastiches, such as the 1983 3D film "") can be regarded as a function of postmodernity.

In discussions of urban planning, the term "pastiche" may describe developments as imitations of the building styles created by major architects: with the implication that the derivative work is unoriginal and of little merit, and the term is generally attributed without reference to its urban context. Many post-war European developments can in this way be described as pastiches of the work of architects and planners such as Le Corbusier or Ebenezer Howard. The term itself is not pejorative, however Alain de Botton describes pastiche as "an unconvincing reproduction of the styles of the past".



</doc>
<doc id="43727113" url="https://en.wikipedia.org/wiki?curid=43727113" title="Dematerialization (art)">
Dematerialization (art)

Dematerialization of the art object is an idea in conceptual art. In "Six Years: The Dematerialization of the Art Object" Lucy Lippard characterizes the period of 1966 to 1972 as one in which the art object was dematerialised through the new artistic practices of conceptual art.


</doc>
<doc id="43946450" url="https://en.wikipedia.org/wiki?curid=43946450" title="Micro miniature">
Micro miniature

Micro miniature (also called micro art or micro sculpture) is a fine art form. Micro miniatures are made by microscope. It originated at the end of 20th century.



</doc>
<doc id="3547336" url="https://en.wikipedia.org/wiki?curid=3547336" title="Artist-in-residence">
Artist-in-residence

Artist-in-residence programs exist to invite artists, academicians, and curators to reside within the premises of an institution.

Some residency programs are incorporated within larger institutions, such as museums, universities, or galleries. Other organizations exist solely to support residential exchange programs.

Various Composer-in-Residence programs exist, for example with orchestras and with universities.

The comic artists program is specifically called "Comic Artist in Residence".


</doc>
<doc id="90317" url="https://en.wikipedia.org/wiki?curid=90317" title="Fine art">
Fine art

In European academic traditions, fine art is art developed primarily for aesthetics or beauty, distinguishing it from decorative art or applied art, which also has to serve some practical function, such as pottery or most metalwork. In the aesthetic theories developed in the Italian Renaissance, the highest art was that which allowed the full expression and display of the artist's imagination, unrestricted by any of the practical considerations involved in, say, making and decorating a teapot. It was also considered important that making the artwork did not involve dividing the work between different individuals with specialized skills, as might be necessary with a piece of furniture, for example. Even within the fine arts, there was a hierarchy of genres based on the amount of creative imagination required, with history painting placed higher than still life.

Historically, the five main fine arts were painting, sculpture, architecture, music, and poetry, with performing arts including theatre and dance. In practice, outside education the concept is typically only applied to the visual arts. The old master print and drawing were included as related forms to painting, just as prose forms of literature were to poetry. Today, the range of what would be considered fine arts (in so far as the term remains in use) commonly includes additional modern forms, such as film, photography, video production/editing, design, and conceptual art.

One definition of "fine art" is "a visual art considered to have been created primarily for aesthetic and intellectual purposes and judged for its beauty and meaningfulness, specifically, painting, sculpture, drawing, watercolor, graphics, and architecture." In that sense, there are conceptual differences between the fine arts and the decorative arts or applied arts (these two terms covering largely the same media). As far as the consumer of the art was concerned, the perception of aesthetic qualities required a refined judgment usually referred to as having good taste, which differentiated fine art from popular art and entertainment.
The word "fine" does not so much denote the quality of the artwork in question, but the purity of the discipline according to traditional Western European canons. Except in the case of architecture, where a practical utility was accepted, this definition originally excluded the "useful" applied or decorative arts, and the products of what were regarded as crafts. In contemporary practice these distinctions and restrictions have become essentially meaningless, as the concept or intention of the artist is given primacy, regardless of the means through which this is expressed.

The term is typically only used for Western art from the Renaissance onwards, although similar genre distinctions can apply to the art of other cultures, especially those of East Asia. The set of "fine arts" are sometimes also called the "major arts", with "minor arts" equating to the decorative arts. This would typically be for medieval and ancient art. 

According to some writers the concept of a distinct category of fine art is an invention of the early modern period in the West. Larry Shiner in his "" (2003) locates the invention in the 18th century:
"There was a traditional “system of the arts” in the West before the eighteenth century. (Other traditional cultures still have a similar system.) In that system, an artist or artisan was a skilled maker or practitioner, a work of art was the useful product of skilled work, and the appreciation of the arts was integrally connected with their role in the rest of life. “Art”, in other words, meant approximately the same thing as the Greek word techne, or in English “skill”, a sense that has survived in phrases like “the art of war”, “the art of love”, and “the art of medicine.”
Similar ideas have been expressed by Paul Oskar Kristeller, Pierre Bourdieu, and Terry Eagleton (e.g. The Ideology of the Aesthetic), though the point of invention is often placed earlier, in the Italian Renaissance; Anthony Blunt notes that the term "arti di disegno", a similar concept, emerged in Italy in the mid-16th century.

But it can be argued that the classical world, from which very little theoretical writing on art survives, in practice had similar distinctions. The names of artists preserved in literary sources are Greek painters and sculptors, and to a lesser extent the carvers of engraved gems. Several individuals in these groups were very famous, and copied and remembered for centuries after their deaths. The cult of the individual artistic genius, which was an important part of the Renaissance theoretical basis for the distinction between "fine" and other art, drew on classical precedent, especially as recorded by Pliny the Elder. Some other types of object, in particular Ancient Greek pottery, are often signed by their makers, or the owner of the workshop, probably partly to advertise their products. 

The decline of the concept of "fine art" is dated by George Kubler and others to around 1880, when it "fell out of fashion" as, by about 1900, folk art was also coming to be regarded as significant. Finally, at least in circles interested in art theory, ""fine art" was driven out of use by about 1920 by the exponents of industrial design ... who opposed a double standard of judgment for works of art and for useful objects". This was among theoreticians; it has taken far longer for the art trade and popular opinion to catch up. However, over the same period of the late 19th and early 20th centuries, the movement of prices in the art market was in the opposite direction, with works from the fine arts drawing much further ahead of those from the decorative arts.

In the art trade the term retains some currency for objects from before roughly 1900, and may be used to define the scope of auctions or auction house departments and the like. The term also remains in use in tertiary education, appearing in the names of colleges, faculties, and courses. In the English-speaking world this is mostly in North America, but the same is true of the equivalent terms in other European languages, such as "beaux-arts" in French or "bellas artes" in Spanish.

The conceptual separation of arts and decorative arts or crafts that has often dominated in Europe and the US is not shared by all other cultures. But traditional Chinese art had comparable distinctions, distinguishing within Chinese painting between the mostly landscape literati painting of scholar gentlemen and the artisans of the schools of court painting and sculpture. Although high status was also given to many things that would be seen as craft objects in the West, in particular ceramics, jade carving, weaving, and embroidery, this by no means extended to the workers who created these objects, who typically remained even more anonymous than in the West. Similar distinctions were made in Japanese and Korean art. In Islamic art, the highest status was generally given to architects and the painters of Persian miniatures and related traditions, but these were still court employees. Typically they also supplied designs for the best Persian carpets, architectural tiling and other decorative media, more consistently than happened in the West.

Latin American art was dominated by European colonialism until the 20th-century, when indigenous art began to reassert itself inspired by the Constructivist Movement, which reunited arts with crafts based upon socialist principles. In Africa, Yoruba art often has a political and spiritual function. As with the art of the Chinese, the art of the Yoruba is also often composed of what would ordinarily be considered in the West to be craft production. Some of its most admired manifestations, such as textiles, fall in this category.

Painting as a fine art means applying paint to a flat surface (as opposed for example to painting a sculpture, or a piece of pottery), typically using several colours. Prehistoric painting that has survived was applied to natural rock surfaces, and wall painting, especially on wet plaster in the fresco technique was a major form until recently. Portable paintings on wood panel or canvas have been the most important in the Western world for several centuries, mostly in tempera or oil painting. Asian painting has more often used paper, with the monochrome ink and wash painting tradition dominant in East Asia. Paintings that are intended to go in a book or album are called "miniatures", whether for a Western illuminated manuscript or in Persian miniature and its Turkish equivalent, or Indian paintings of various types. Watercolour is the western version of painting in paper; forms using gouache, chalk and similar mediums without brushes are really forms of drawing. 

Drawing is one of the major forms of the visual arts, and painters need drawing skills as well. Common instruments include: graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoals, chalk, pastels, markers, stylus, or various metals like silverpoint. There are a number of subcategories of drawing, including cartooning and creating comics. 

Mosaics are images formed with small pieces of stone or glass, called "tesserae". They can be decorative or functional. An artist who designs and makes mosaics is called a mosaic artist or a mosaicist. Ancient Greeks and Romans created realistic mosaics. Mythological subjects, or scenes of hunting or other pursuits of the wealthy, were popular as the centrepieces of a larger geometric design, with strongly emphasized borders. Early Christian basilicas from the 4th century onwards were decorated with wall and ceiling mosaics. The most famous Byzantine basilicas decorated with mosaics are the Basilica of San Vitale from Ravenna (Italy) and Hagia Sofia from Istanbul (Turkey).

Printmaking covers the making of images on paper that can be reproduced multiple times by a printing process. It has been an important artistic medium for several centuries, in the West and East Asia. Major historic techniques include engraving, woodcut and etching in the West, and woodblock printing in East Asia, where the Japanese ukiyo-e style is the most important. The 19th-century invention of lithography and then photographic techniques have partly replaced the historic techniques. Older prints can be divided into the fine art Old Master print and popular prints, with book illustrations and other practical images such as maps somewhere in the middle. 

Except in the case of monotyping, the process is capable of producing multiples of the same piece, which is called a print. Each print is considered an original, as opposed to a copy. The reasoning behind this is that the print is not a reproduction of another work of art in a different medium — for instance, a painting — but rather an image designed from inception as a print. An individual print is also referred to as an impression. Prints are created from a single original surface, known technically as a matrix. Common types of matrices include: plates of metal, usually copper or zinc for engraving or etching; stone, used for lithography; blocks of wood for woodcuts, linoleum for linocuts and fabric in the case of screen-printing. But there are many other kinds. Multiple nearly identical prints can be called an edition. In modern times each print is often signed and numbered forming a "limited edition." Prints may also be published in book form, as artist's books. A single print could be the product of one or multiple techniques.

Calligraphy is a type of visual art. A contemporary definition of calligraphic practice is "the art of giving form to signs in an expressive, harmonious and skillful manner". Modern calligraphy ranges from functional hand-lettered inscriptions and designs to fine-art pieces where the abstract expression of the handwritten mark may or may not compromise the legibility of the letters. Classical calligraphy differs from typography and non-classical hand-lettering, though a calligrapher may create all of these; characters are historically disciplined yet fluid and spontaneous, improvised at the moment of writing.

"Fine art photography" refers to photographs that are created to fulfill the creative vision of the artist. Fine art photography stands in contrast to photojournalism and commercial photography. Photojournalism visually communicates stories and ideas, mainly in print and digital media. Fine art photography is created primarily as an expression of the artist’s vision, but has also been important in advancing certain causes.

Architecture is frequently considered a fine art, especially if its aesthetic components are spotlighted — in contrast to structural-engineering or construction-management components. Architectural works are perceived as cultural and political symbols and works of art. Historical civilizations often are known primarily through their architectural achievements. Such buildings as the pyramids of Egypt and the Roman Colosseum are cultural symbols, and are important links in public consciousness, even when scholars have discovered much about past civilizations through other means. Cities, regions and cultures continue to identify themselves with, and are known by, their architectural monuments.

With some modern exceptions, pottery is not considered as fine art, but "fine pottery" remains a valid technical term, especially in archaeology. "Fine wares" are high-quality pottery, often painted, moulded or otherwise decorated, and in many periods distinguished from "coarse wares", which are basic utilitarian pots used by the mass of the population, or in the kitchen rather than for more formal purposes.

Even when, as with porcelain figurines, a piece of pottery has no practical purpose, the making of it is typically a collaborative and semi-industrial one, involving many participants with different skills.

Sculpture is three-dimensional artwork created by shaping hard or plastic material, commonly stone (either rock or marble), metal, or wood. Some sculptures are created directly by ; others are assembled, built up and fired, welded, molded, or cast. Because sculpture involves the use of materials that can be moulded or modulated, it is considered one of the plastic arts. The majority of public art is sculpture. Many sculptures together in a garden setting may be referred to as a sculpture garden.

Sculpture in stone survives far better than works of art in perishable materials, and often represents the majority of the surviving works (other than pottery) from ancient cultures, though conversely traditions of sculpture in wood may have vanished almost entirely. However, most ancient sculpture was brightly painted, and this has been lost.

Conceptual art is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic and material concerns.
The inception of the term in the 1960s referred to a strict and focused practice of "idea-based art" that often defied traditional visual criteria associated with the visual arts in its presentation as text. However, through its association with the Young British Artists and the Turner Prize during the 1990s, its popular usage, particularly in the UK, developed as a synonym for all contemporary art that does not practice the traditional skills of painting and sculpture.

Poetry (the term derives from a variant of the Greek term, poiesis, "making") is a form of literature that uses aesthetic and rhythmic qualities of language—such as sound symbolism, phonaesthetics and metre—to evoke meanings in addition to, or in place of, the prosaic ostensible meaning.

Music is an art form and cultural activity whose medium is sound organized in time. The common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the "color" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek μουσική (mousike; "art of the Muses").

"Dance" is an art form that generally refers to movement of the body, usually rhythmic, and to music, used as a form of expression, social interaction or presented in a spiritual or performance setting.
Dance is also used to describe methods of nonverbal communication (see body language) between humans or animals (bee dance, patterns of behaviour such as a mating dance), motion in inanimate objects "(the leaves danced in the wind)", and certain musical genres. In sports, gymnastics, figure skating and synchronized swimming are "dance disciplines" while the "Katas" of the martial arts are often compared to dances.

Modern Western theatre is dominated by realism, including drama and comedy. Another popular Western form is musical theatre. Classical forms of theatre, including Greek and Roman drama, classic English drama (Shakespeare and Marlowe included), and French theater (Molière included), are still performed today. In addition, performances of classic Eastern forms such as Noh and Kabuki can be found in the West, although with less frequency.

"Fine arts film" is a term that encompasses motion pictures and the field of film as a fine art form. A "fine arts movie theater" is a venue, usually a building, for viewing such movies. Films are produced by recording images from the world with cameras, or by creating images using animation techniques or special effects. Films are cultural artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them. Film is considered to be an important art form, a source of popular entertainment and a powerful method for educating — or indoctrinating — citizens. The visual elements of cinema give motion pictures a universal power of communication. Some films have become popular worldwide attractions by using dubbing or subtitles that translate the dialogue.

Cinematography is the discipline of making lighting and camera choices when recording photographic images for the cinema. It is closely related to the art of still photography, though many additional issues arise when both the camera and elements of the scene may be in motion.

Independent filmmaking often takes place outside of Hollywood, or other major studio systems. An independent film (or "indie film)" is a film initially produced without financing or distribution from a major movie studio. Creative, business, and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century.





In the United States an academic course of study in fine art may include the Bachelor of Arts in Fine Art, or a Bachelor of Fine Arts, and/or a Master of Fine Arts degree — traditionally the terminal degree in the field. Doctor of Fine Arts degrees —earned, as opposed to honorary degrees— have begun to emerge at some US academic institutions, however.
Major schools of art in the US:




</doc>
<doc id="30041335" url="https://en.wikipedia.org/wiki?curid=30041335" title="Repurposing">
Repurposing

Repurposing is the process by which an object with one use value is transformed or redeployed as an object with an alternative use value.

Repurposing is as old as human civilization, with many contemporary scholars investigating that way that different societies re-appropriate the artifacts of older cultures in new and creative ways. More recently, repurposing has been celebrated by 21st century hobbyists and arts-and-crafts organizations such as Instructables and other Maker culture communities as a means of creatively responding to the ecological and economic crises of the 21st century. Recent scholarship has attempted to relate these activities to American left- and right-libertarianism.

Repurposing is the use of a tool being re-channeled into being another tool, usually for a purpose unintended by the original tool-maker. Typically, repurposing is done using items usually considered to be junk, garbage, or obsolete. A good example of this would be the Earthship style of house, that uses tires as insulating walls and bottles as glass walls. Reuse is not limited to repeated uses for the same purpose. Examples of repurposing include using tires as boat fenders and steel drums or plastic drums as feeding troughs and/or composting bins. Incinerator and power plant exhaust stack fly-ash is used extensively as an additive to concrete, providing increased strength. This type of reuse can sometimes make use of items which are no longer usable for their original purposes, for example using worn-out clothes as rags.

Not all repurposing is necessarily environmentally friendly, take for instance the idea of repurposing older work trucks for businesses in their infancy, in which their poor fuel economy can negate long term benefits since greater spending of money for fuel, and more fumes output to the sky can prove to be environmentally unfriendly, in which repurposing vehicles for electric car conversion can be the recommended alternative to that, though its cost can be negligible upfront.









</doc>
<doc id="18816" url="https://en.wikipedia.org/wiki?curid=18816" title="Mural">
Mural

A mural is any piece of artwork painted or applied directly on a wall, ceiling or other permanent surfaces. A distinguishing characteristic of mural painting is that the architectural elements of the given space are harmoniously incorporated into the picture.

Some wall paintings are painted on large canvases, which are then attached to the wall (e.g., with marouflage), but the technique has been in common use since the late 19th century.

Murals of sorts date to Upper Paleolithic times such as the cave paintings in the Lubang Jeriji Saléh cave in Borneo (40,000-52,000 BP), Chauvet Cave in Ardèche department of southern France (around 32,000 BP). Many ancient murals have been found within ancient Egyptian tombs (around 3150 BC), the Minoan palaces (Middle period III of the Neopalatial period, 1700–1600 BC), the Oxtotitlán cave and Juxtlahuaca in Mexico (around 1200-900 BC) and in Pompeii (around 100 BC – AD 79).

During the Middle Ages murals were usually executed on dry plaster (secco). The huge collection of Kerala mural painting dating from the 14th century are examples of fresco secco. In Italy, circa 1300, the technique of painting of frescos on wet plaster was reintroduced and led to a significant increase in the quality of mural painting.
In modern times, the term became more well-known with the Mexican muralism art movement (Diego Rivera, David Siqueiros and José Orozco). There are many different styles and techniques. The best-known is probably "fresco", which uses water-soluble paints with a damp lime wash, rapid use of the resulting mixture over a large surface, and often in parts (but with a sense of the whole). The colors lighten as they dry. The "marouflage" method has also been used for millennia.

Murals today are painted in a variety of ways, using oil or water-based media. The styles can vary from abstract to "trompe-l'œil" (a French term for "fool" or "trick the eye"). Initiated by the works of mural artists like Graham Rust or Rainer Maria Latzke in the 1980s, trompe-l'oeil painting has experienced a renaissance in private and public buildings in Europe.
Today, the beauty of a wall mural has become much more widely available with a technique whereby a painting or photographic image is transferred to poster paper or canvas which is then pasted to a wall surface "(see wallpaper, Frescography)" to give the effect of either a hand-painted mural or realistic scene.

A special type of mural painting is Lüftlmalerei, still practised today in the villages of the Alpine valleys. Well-known examples of such façade designs from the 18th and 19th centuries can be found in Mittenwald, Garmisch, Unter- and Oberammergau.

In the history of mural several methods have been used:

"A fresco" painting, from the Italian word "affresco" which derives from the adjective "fresco" ("fresh"), describes a method in which the paint is applied on plaster on walls or ceilings. 

The "buon fresco" technique consists of painting in pigment mixed with water on a thin layer of wet, fresh, lime mortar or plaster. The pigment is then absorbed by the wet plaster; after a number of hours, the plaster dries and reacts with the air: it is this chemical reaction which fixes the pigment particles in the plaster. After this the painting stays for a long time up to centuries in fresh and brilliant colors.

"Fresco-secco" painting is done on dry plaster ("secco" is "dry" in Italian). The pigments thus require a binding medium, such as egg (tempera), glue or oil to attach the pigment to the wall.

"Mezzo-fresco" is painted on nearly-dry plaster, and was defined by the sixteenth-century author Ignazio Pozzo as "firm enough not to take a thumb-print" so that the pigment only penetrates slightly into the plaster. By the end of the sixteenth century this had largely displaced the "buon fresco" method, and was used by painters such as Gianbattista Tiepolo or Michelangelo. This technique had, in reduced form, the advantages of "a secco" work.

In Greco-Roman times, mostly encaustic colors applied in a cold state were used.

Tempera painting is one of the oldest known methods in mural painting. In tempera, the pigments are bound in an albuminous medium such as egg yolk or egg white diluted in water.

In 16th-century Europe, oil painting on canvas arose as an easier method for mural painting. The advantage was that the artwork could be completed in the artist's studio and later transported to its destination and there attached to the wall or ceiling. Oil paint may be a less satisfactory medium for murals because of its lack of brilliance in colour. Also, the pigments are yellowed by the binder or are more easily affected by atmospheric conditions. 

Different muralists tend to become experts in their preferred medium and application, whether that be oil paints, emulsion or acrylic paints applied by brush, roller or airbrush/aerosols. Clients will often ask for a particular style and the artist may adjust to the appropriate technique.

A consultation usually leads to detailed design and layout of the proposed mural with a price quote that the client approves before the muralist starts on the work. The area to be painted can be gridded to match the design allowing the image to be scaled accurately step by step. In some cases, the design is projected straight onto the wall and traced with pencil before painting begins. Some muralists will paint directly without any prior sketching, preferring the spontaneous technique.

Once completed the mural can be given coats of varnish or protective acrylic glaze to protect the work from UV rays and surface damage.

In modern, quick form of muralling, young enthusiasts also use POP clay mixed with glue or bond to give desired models on canvas board. The canvas is later set aside to let the clay dry. Once dried, the canvas and the shape can be painted with your choice of colors and later coated with varnish.

As an alternative to a hand-painted or airbrushed mural, digitally printed murals can also be applied to surfaces. Already existing murals can be photographed and then be reproduced in near-to-original quality.

The disadvantages of pre-fabricated murals and decals are that they are often mass-produced and lack the allure and exclusivity of original artwork. They are often not fitted to the individual wall sizes of the client and their personal ideas or wishes cannot be added to the mural as it progresses. The Frescography technique, a digital manufacturing method (CAM) invented by Rainer Maria Latzke addresses some of the personalisation and size restrictions.

Digital techniques are commonly used in advertisements. A "wallscape" is a large advertisement on or attached to the outside wall of a building. Wallscapes can be painted directly on the wall as a mural, or printed on vinyl and securely attached to the wall in the manner of a billboard. Although not strictly classed as murals, large scale printed media are often referred to as such. Advertising murals were traditionally painted onto buildings and shops by sign-writers, later as large scale poster billboards.

Murals are important in that they bring art into the public sphere. Due to the size, cost, and work involved in creating a mural, muralists must often be commissioned by a sponsor. Often it is the local government or a business, but many murals have been paid for with grants of patronage. For artists, their work gets a wide audience who otherwise might not set foot in an art gallery. A city benefits by the beauty of a work of art.

Murals can be a relatively effective tool of social emancipation or achieving a political goal. Murals have sometimes been created against the law, or have been commissioned by local bars and coffee shops. Often, the visual effects are an enticement to attract public attention to social issues.
State-sponsored public art expressions, particularly murals, are often used by totalitarian regimes as a tool of propaganda. However, despite the propagandist character of that works, some of them still have an artistic value.

Murals can have a dramatic impact whether consciously or subconsciously on the attitudes of passers-by, when they are added to areas where people live and work. It can also be argued that the presence of large, public murals can add aesthetic improvement to the daily lives of residents or that of employees at a corporate venue.

Other world-famous murals can be found in Mexico, New York City, Philadelphia, Belfast, Derry, Los Angeles, Nicaragua, Cuba and in India. They have functioned as an important means of communication for members of socially, ethnically and racially divided communities in times of conflict. They also proved to be an effective tool in establishing a dialogue and hence solving the cleavage in the long run.
The Indian state Kerala has exclusive murals. These Kerala mural painting are on walls of Hindu temples. They can be dated from 9th century AD.

The San Bartolo murals of the Maya civilization in Guatemala, are the oldest example of this art in Mesoamerica and are dated at 300 BC.

Many rural towns have begun using murals to create tourist attractions in order to boost economic income. Colquitt, Georgia, is one such town. Colquitt was chosen to host the 2010 Global Mural Conference. The town has more than twelve murals completed, and will host the Conference along with Dothan, Alabama, and Blakely, Georgia. In the summer of 2010, Colquitt will begin work on their Icon Mural.

The Mexican mural movement in the 1930s brought new prominence to murals as a social and political tool. Diego Rivera, José Orozco and David Siqueiros were the most famous artists of the movement. Between 1932 and 1940, Rivera also painted murals in San Francisco, Detroit, and New York City. In 1933, he completed a famous series of twenty-seven fresco panels entitled "Detroit Industry" on the walls of an inner court at the Detroit Institute of Arts. During the McCarthyism of the 1950s, a was placed in the courtyard defending the artistic merit of the murals while attacking his politics as "detestable."

In 1948, the Colombian government hosted the IX Pan-American Conference to establish the Marshall plan for the Americas. The director of the OEA and the Colombian government commissioned master Santiago Martinez Delgado, to paint a mural in the Colombian congress building to commemorate the event. Martinez decided to make it about the Cúcuta Congress, and painted Bolívar in front of Santander, making liberals upset; so, due to the murder of Jorge Elieser Gaitan the mobs of el bogotazo tried to burn the capitol, but the Colombian Army stopped them. Years later, in the 1980s, with liberals in charge of the Congress, they passed a resolution to turn the whole chamber in the Elliptic Room 90 degrees to put the main mural on the side and commissioned Alejandro Obregon to paint a non-partisan mural in the surrealist style.

Northern Ireland contains some of the most famous political murals in the world. Almost 2,000 murals have been documented in Northern Ireland since the 1970s. In recent times, many murals are non-sectarian, concerning political and social issues such as racism and environmentalism, and many are completely apolitical, depicting children at play and scenes from everyday life. (See Northern Irish murals.)

A not political, but social related mural covers a wall in an old building, once a prison, at the top of a cliff in Bardiyah, in Libya. It was painted and signed by the artist in April 1942, weeks before his death on the first day of the First Battle of El Alamein. Known as the Bardia Mural, it was created by English artist, private John Frederick Brill.

In 1961 East Germany began to erect a wall between East and West Berlin, which became famous as the Berlin Wall. While on the East Berlin side painting was not allowed, artists painted on the Western side of the Wall from the 80s until the fall of the Wall in 1989.

Many unknown and known artists such as Thierry Noir and Keith Haring painted on the Wall, the "World's longest canvas". The sometimes detailed artwork were often painted over within hours or days. On the Western side the Wall was not protected, so everybody could paint on the Wall. After the fall of the Berlin Wall in 1989, the Eastern side of the Wall became also a popular "canvas" for many mural and graffiti artists.
Orgosolo, in Sardinia, is a most important center of murals politics.

It is also common for mural graffiti to be used as a memoir. In the book "Somebody Told Me," Rick Bragg writes about a series of communities, mainly located in New York, that have walls dedicated to the people who died. These memorials, both written word and mural style, provide the deceased to be present in the communities in which they lived. Bragg states that the "murals have woven themselves in the fabric of the neighborhoods, and the city." These memorials remind people of the deaths caused by inner city violence.

Many people like to express their individuality by commissioning an artist to paint a mural in their home. This is not an activity exclusively for owners of large houses. A mural artist is only limited by the fee and therefore the time spent on the painting; dictating the level of detail; a simple mural can be added to the smallest of walls.

Private commissions can be for dining rooms, bathrooms, living rooms or, as is often the case- children's bedrooms. A child's room can be transformed into the 'fantasy world' of a forest or racing track, encouraging imaginative play and an awareness of art.

The current trend for feature walls has increased commissions for muralists in the UK. A large hand-painted mural can be designed on a specific theme, incorporate personal images and elements and may be altered during the course of painting it. The personal interaction between client and muralist is often a unique experience for an individual not usually involved in the arts.

In the 1980s, illusionary wall painting experienced a renaissance in private homes. The reason for this revival in interior design could, in some cases be attributed to the reduction in living space for the individual. Faux architectural features, as well as natural scenery and views, can have the effect of 'opening out' the walls. Densely built-up areas of housing may also contribute to people's feelings of being cut off from nature in its free form. A mural commission of this sort may be an attempt by some people to re-establish a balance with nature.

Commissions of murals in schools, hospitals, and retirement homes can achieve a pleasing and welcoming atmosphere in these caring institutions. Murals in other public buildings, such as public houses are also common.

Recently, graffiti and street art have played a key role in contemporary wall painting. Such graffiti/street artists as Keith Haring, Shepard Fairey, Above, Mint&Serf, Futura 2000, Os Gemeos, and Faile among others have successfully transcended their street art aesthetic beyond the walls of urban landscape and onto walls of private and corporate clients. As graffiti/street art became more mainstream in the late 1990s, youth-oriented brands such as Nike and Red Bull, with Wieden Kennedy, have turned to graffiti/street artists to decorate walls of their respective offices. This trend continued through 2000's with graffiti/street art gaining more recognition from art institutions worldwide.

Many homeowners choose to display the traditional art and culture of their society or events from their history in their homes. Ethnic murals have become an important form of interior decoration. Warli painting murals are becoming a preferred mode of wall decor in India. Warli painting is an ancient Indian art form in which the tribal people used to depict different phases of their life on the walls of their mud houses.

Tile murals are murals made out of stone, ceramic, porcelain, glass and or metal tiles that are installed within, or added onto the surface of an existing wall. They are also inlaid into floors. Mural tiles are painted, glazed, sublimation printed (as described below) or more traditionally cut or broken into pieces. Unlike the traditional painted murals described above, tile murals are always made with the use of tiles.

Mosaic murals are made by combining small 1/4" to 2" size pieces of colorful stone, ceramic, or glass tiles which are then laid out to create a picture. Modern day technology has allowed commercial mosaic mural makers to use computer programs to separate photographs into colors that are automatically cut and glued onto sheets of mesh creating precise murals fast and in large quantities.

The azulejo (, ) refers to a typical form of Portuguese or Spanish painted, tin-glazed, ceramic tilework. They have become a typical aspect of Portuguese culture, manifesting without interruption during five centuries, the consecutive trends in art.

Azulejos can be found inside and outside churches, palaces, ordinary houses and even railway stations or subway stations.

They were not only used as an ornamental art form, but also had a specific functional capacity like temperature control in homes. Many "azulejos" chronicle major historical and cultural aspects of Portuguese history.

Custom-printed tile murals can be produced using digital images for kitchen splashbacks, wall displays, and flooring. Digital photos and artwork can be resized and printed to accommodate the desired size for the area to be decorated. Custom tile printing uses a variety of techniques including dye sublimation and ceramic-type laser toners. The latter technique can yield fade-resistant custom tiles which are suitable for long term exterior exposure.





</doc>
<doc id="50110539" url="https://en.wikipedia.org/wiki?curid=50110539" title="Dedication (art)">
Dedication (art)

In art, a dedication is the creation or attribution of a work of art as a tribute to or in honor of a person, place, or thing. The dedicatee may be the commissioner, conductor, premiere performer or musical ensemble, or patron. The work may be memorial.

In music, examples include many of Beethoven's works, such as the Piano Sonata Op. 109, dedicated to his friend Antonie Brentano's daughter Maximiliane.

In literature, examples include Vladimir Nabokov's dedication of works to his wife Véra.



</doc>
<doc id="50113632" url="https://en.wikipedia.org/wiki?curid=50113632" title="United Art Rating">
United Art Rating

United Art Rating is a reference book published since 1999 by Artists Trade Union of Russia as one of the instruments of art market regulations.

United Art Rating (before 2012, referred to as United Artists Rating) is a rating published as a periodical reference book, registered as mass media. As of 2016, it has stood 22 printed editions, total number of printed copies is more than 90000. The electronic version is also available.

On January 1, 2016, United Art Rating included the names, dates of life and rating categories of 58965 artists (painters, graphic and poster artists, theatre decorators, batik painters, illustrators, animators, sculptors, jewelers, ceramists, authors of installations etc.).

Editor-in-chief: Sergey Zagraevsky, chairman of Artists Trade Union of Russia, member of Russian Academy of Arts and Russian Academy of Art-critics, Honored culture worker of Russian Federation.

In 2002, United Art Rating was awarded the gratitude of the Minister of culture of Russia.

The Rating Centre acts under Artists Trade Union of Russia and has a status of an independent professional jury, free in its judges and estimations. Defining rating categories is made on the basis of all available information about works of art, biographies, expositions, collections and sales of works, catalogues and Internet and press publications, art historians' and art-managers' opinions, public opinion polls etc.

United Art Rating consists of 2 sections:

Artists are subdivided into categories "A" and "B": an artist of "A" category does not virtually submit to the market's demand. an artist of "B" category is mainly oriented to the market demand.

Each category has its levels from the first (the top of recognition and fame; is referred to the deceased classic artists, whose works of art are widely recognized in museums) to the seventh one (amateur artist).

According to the declared principles, defining of rating categories is made on available information about expositions, sales and collections of works, catalogues, publications, biographies, public-opinion polls etc. Rating criteria are: art level of works, their humanistic significance, professionalism, exhibitions, works in museums, recognition by the public, galleries and art critics, social and civil significance of works, price level of works etc.

United Art Rating contains pricing recommendations about sale and purchase of works of art.

United Art Rating is constantly changed, refined and supplemented.


International Art Rating includes the first three categories of United Art Rating (i.e. 1, 1A, 1B, 2A, 2B, 3A and 3B). These artists formed and are forming the history of world art of 18th–21st centuries.

International Art Rating includes artists of Argentina, Australia, Austria, Belgium, Bolivia, Brazil, United Kingdom, Bulgaria, Canada, Colombia, Cuba, Czech Republic, Germany, Greece, Hungary, Israel, Italy, Mexico, New Zealand, Norway, Poland, Portugal, Romania, Russia and former Soviet republics, the republics of the former Yugoslavia, United States, Finland, France, Spain, Switzerland, Sweden, Japan and some other countries, such as Namibia.

All famous artists of the period mentioned in the project are represented such as Vincent van Gogh, Pablo Picasso, Henri Matisse, Wassily Kandinsky, Emil Nolde, Joseph Beuys, Andy Warhol just to name a few. From Namibia e.g. the artists Adolph Jentsch, Hans Anton Aschenborn, Dieter Aschenborn and Uli Aschenborn are listed in the project (see " 'Internet-project' " under 'External links' below). The art of the rated artists form the world art heritage. 

Internet-project "Greatest world artists of XVIII–XXI centuries" is created on the base of International Art Rating (about 11000 artists). Before 2014 the project was called "10000 best world artists of XVIII–XXI centuries".




</doc>
<doc id="16300571" url="https://en.wikipedia.org/wiki?curid=16300571" title="Computational creativity">
Computational creativity

Computational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.

The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:


The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.

As measured by the amount of activity in the field (e.g., publications, conferences and workshops), computational creativity is a growing area of research. But the field is still hampered by a number of fundamental problems. Creativity is very difficult, perhaps even impossible, to define in objective terms. Is it a state of mind, a talent or ability, or a process? Creativity takes many forms in human activity, some "eminent" (sometimes referred to as "Creativity" with a capital C) and some "mundane".

These are problems that complicate the study of creativity in general, but certain problems attach themselves specifically to "computational" creativity:


Indeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do—a key point in favor of computational creativity.

Because no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:

Whereas the above reflects a "top-down" approach to computational creativity, an alternative thread has developed among "bottom-up" computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms. Experiments involving recurrent nets were successful in hybridizing simple musical melodies and predicting listener expectations.

Concurrent with such research, a number of computational psychologists took the perspective, popularized by Stephen Wolfram, that system behaviors perceived as complex, including the mind's creative output, could arise from what would be considered simple algorithms. As neuro-philosophical thinking matured, it also became evident that language actually presented an obstacle to producing a scientific model of cognition, creative or not, since it carried with it so many unscientific aggrandizements that were more uplifting than accurate. Thus questions naturally arose as to how "rich," "complex," and "wonderful" creative cognition actually was.

Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner. In 1992, Todd
extended this work, using the so-called distal teacher approach that had been developed by
Paul Munro, Paul Werbos, D. Nguyen and Bernard Widrow, Michael I. Jordan and David Rumelhart. In the new approach there are two neural networks, one of which is supplying training patterns to another. 
In later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new "interpolated" melodies that the network generates corresponding to intermediate points in the 2-d plane.

More recently a neurodynamical model of semantic networks has been developed to study how the connectivity structure of these networks relates to the richness of the semantic constructs, or ideas, they can generate. It was demonstrated that semantic neural networks that have richer semantic dynamics than those with other connectivity structures may provide insight into the important issue of how the physical structure of the brain determines one of the most profound features of the human mind – its capacity for creative thought.

Some high-level and philosophical themes recur throughout the field of computational creativity.

Margaret Boden refers to creativity that is novel "merely to the agent that produces it" as "P-creativity" (or "psychological creativity"), and refers to creativity that is recognized as novel "by society at large" as "H-creativity" (or "historical creativity"). Stephen Thaler has suggested a new category he calls "V-" or "Visceral creativity" wherein significance is invented to raw sensory inputs to a Creativity Machine architecture, with the "gateway" nets perturbed to produce alternative interpretations, and downstream nets shifting such interpretations to fit the overarching context. An important variety of such V-creativity is consciousness itself, wherein meaning is reflexively invented to activation turnover within the brain.

Boden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as "exploratory creativity" and the latter as "transformational creativity", seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are more recently also the subject of formalization, most notably in the work by Geraint Wiggins.

The criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs are then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith, which is a psychological model of creative generation based on empirical observation of human creativity.

A great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects . Common strategies for combinatorial creativity include:
The combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.

Mark Turner and Gilles Fauconnier propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity as well as more recent work by Lakoff and Johnson, by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:


Fauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.

Some computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures. More recently, Francisco Câmara Pereira presented an implementation of blending theory that employs ideas both from GOFAI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.

Language provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes. Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, and some have found their way to the dictionary. The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.

In the seminal work of applied linguist Ronald Carter, he hypothesized two main creativity types involving words and word patterns: pattern-reforming creativity, and pattern-forming creativity. Pattern-reforming creativity refers to creativity by the breaking of rules, reforming and reshaping patterns of language often through individual innovation, while pattern-forming creativity refers to creativity via conformity to language rules rather than breaking them, creating convergence, symmetry and greater mutuality between interlocutors through their interactions in the form of repetitions. 

Substantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN

The company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.

Example of a metaphor: "She was an ape."

Example of a simile: "Felt like a tiger-fur blanket."
The computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin, Dan Fass, John Barnden, and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., "as hard as steel") or ironic (e.g., "as hairy as a bowling ball", "as pleasant as a root canal"); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms "pencil", "whip", "whippet", "rope", "stick-insect" and "snake" are suggested).

The process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME, the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper, which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.

Humour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie. This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system of Oliviero Stock and Carlo Strapparava.

The blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called "blends" or "portmanteau words" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., "food traveller" for "gastronaut" and "time traveller" for "chrononaut"). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel ("H-creative") and useful. Neurolinguistic inspirations have been used to analyze the process of novel word creation in the brain, understand neurocognitive processes responsible for intuition, insight, imagination and creativity and to create a server that invents novel names for products, based on their description. Further, the system Nehovah blends two source words into a neologism that blends the meanings of the two source words. Nehovah searches WordNet for synonyms and TheTopTens.com for pop culture hyponyms. The synonyms and hyponyms are blended together to create a set of candidate neologisms. The neologisms are then scored based on their word structure, how unique the word is, how apparent the concepts are conveyed, and if the neologism has a pop culture reference. Nehovah loosely follows conceptual blending. 

A corpus linguistic approach to the search and extraction of neologism have also shown to be possible. Using Corpus of Contemporary American English as a reference corpus, Locky Law has performed an extraction of neologism, portmanteaus and slang words using the hapax legomena which appeared in the scripts of American TV drama House M.D. 

In terms of linguistic research in neologism, Stefan Th. Gries has performed a quantitative analysis of blend structure in English and found that "the degree of recognizability of the source words and that the similarity of source words to the blend plays a vital role in blend formation." The results were validated through a comparison of intentional blends to speech-error blends.

Like jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.

Computational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz. Most notably, David Cope has written a software system called "Experiments in Musical Intelligence" (or "EMI") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.

In the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD, which "New Scientist" described as "The first major work composed by a computer and performed by a full orchestra". Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.

Creativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next.
The robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation. Virtual improvisation software based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from live performer.

In 1994, a Creativity Machine architecture (see above) was able to generate 11,000 musical hooks by training a synaptically perturbed neural net on 100 melodies that had appeared on the top ten list over the last 30 years. In 1996, a self-bootstrapping Creativity Machine observed audience facial expressions through an advanced machine vision system and perfected its musical talents to generate an album entitled "Song of the Neurons"

In the field of musical composition, the patented works by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies so-called "coherent" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real time while listening to the song). The patented invention "Medal-Composer" raises problems of copyright.

Computational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. The most famous program in this domain is Harold Cohen's AARON, which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.

Other software artists of note include the NEvAr system (for "Neuro-Evolutionary Art") of Penousal Machado. NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.

The Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, in more recent work, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.

The artist Krasimira Dimtchevska and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art. The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.

An emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system which can generate short segments of code which act as simple game mechanics. ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.

In July 2015 Google released "DeepDream" – an open source computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.

In August 2015 researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.

In early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases. The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.

Creativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has recently been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem solving. The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.

Some researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as "creative" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a "general theory of creativity". Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are "general theories". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities. Likewise, the Formal Theory of Creativity is based on a simple computational principle published by Jürgen Schmidhuber in 1991. The theory postulates that creativity and curiosity and selective attention in general are by-products of a simple algorithmic principle for measuring and optimizing learning progress.

Traditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions . As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms . For some related discussions and references to related work are captured in some recent work on philosophical foundations of simulation.

Mathematically, the same set of arguments against creativity has been made by Chaitin. Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well defined algorithms.

The International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity. Events in the series include:

Previously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:


The 1st Conference on Computer Simulation of Musical Creativity will be held

Design Computing and Cognition is one conference that addresses computational creativity. The ACM Creativity and Cognition conference is another forum for issues related to computational creativity. Journées d'Informatique Musicale 2016 keynote by Shlomo Dubnov was on Information Theoretic Creativity.

A number of recent books provide either a good introduction or a good overview of the field of Computational Creativity. These include:


In addition to the proceedings of conferences and workshops, the computational creativity community has thus far produced these special journal issues dedicated to the topic:


In addition to these, a new journal has started which focuses on computational creativity within the field of music.







</doc>
<doc id="50400956" url="https://en.wikipedia.org/wiki?curid=50400956" title="Pandemonia">
Pandemonia

Pandemonia is a character and persona created as conceptual art by an anonymous London-based artist that has appeared in the art and fashion world since 2009. Clad in a latex full-head mask with stylized hair and latex dresses, Pandemonia is seven feet tall and was described by Katia Ganfield of "Vice" as "Roy Lichtenstein's blonde caricatures ... brought to life as a 7 ft Jeff Koons inflatable". She is often accompanied by an inflatable white dog named Snowy.

Pandemonia told "Stylist":
Initially a "crasher" at fashion and social events, Pandemonia eventually became a London Fashion Week VIP guest.

Pandemonia is a critical reflection and, as such, an intervention upon ideas of celebrity and femininity. She is a pointed manifestation of how these ideas intersect with mass media, social media, and the marketability of desire. The art of Pandemonia herself is that of a constructed figure placed in the landscape of media, fashion and art events that has instigated the media response by feeding back to the media its own language, imagery and ideals.

The growth of Pandemonia's celebrity is one of the themes in her art, which also explores archetypes of pop myth and reality.

Pandemonia’s art is not only cross-media (sculpture, digital art, photography, and performance), but also cross-generational as she ties the earliest moments of Pop Art to the most current worlds of celebrity, fashion and contemporary art, creating an arc and evolution which continues its ongoing exploration.

In 2016, Pandemonia was chosen by Camper as the protagonist and muse for its Kobarah shoe style, and has been featured in stores and billboards in several major cities including Paris, London, New York, and Tokyo.


</doc>
<doc id="50504032" url="https://en.wikipedia.org/wiki?curid=50504032" title="Leaf carving">
Leaf carving

Leaf carving is an artwork involving the delicate trimming of leaves to develop a picture or landscape. The process of carving is performed by artists using tools to carefully cut the surface without cutting or removing the veins. The veins add detail into the subject matter of the carving. Leaf carving originated out of China and gained popularity in 1994 by artist Huag Tai Sheng after he got the Guinness Book of World Records to recognize his work. The art may related to Chinese paper cutting. The material or most common leaf used in leaf carving is that of the Chinar tree. The Chinar tree is native to India, Pakistan and China. Chinar leaves have a close resemblance to maple leaves.


</doc>
<doc id="54087097" url="https://en.wikipedia.org/wiki?curid=54087097" title="Contemporary-Traditional Art">
Contemporary-Traditional Art

Contemporary-Traditional Art refers to an art produced at the present period of time that reflects the current culture by utilizing classical techniques in drawing, painting, and sculpting. Practicing artists are mainly concerned with the preservation of time-honored skills in creating works of figurative and representational forms of fine art as a means to express human emotions and experiences. Subjects are based on the aesthetics of balancing external reality with the intuitive, internal conscience driven by emotion, philosophical thought, or the spirit. The term is used broadly to encompass all styles and practices of representational art, such as Classicism, Impressionism, Realism, and Plein Air (En plein air) painting. Technical skills are founded in the teachings of the Renaissance, Academic Art, and American Impressionism.

Organized groups of practicing artists and institutions dedicated to furthering classical techniques include the Art Renewal Center, California Art Club, Florence Academy of Art, Laguna College of Art and Design, Los Angeles Academy of Figurative Art, New York Academy of Art, and Portrait Society of America.

Publications referencing the term, Contemporary-Traditional or living artists working in traditional styles: 

Peter Seitz Adams (b. 1950)

Jacob Collins (b. 1964)

Karl Dempwolf (b. 1939)

Frederick Hart (1943-1999, Sculptor)

Everett Raymond Kinstler (b. 1926)

Jeremy Lipking (b. 1974)

Richard Schmid (b. 1934)

Nelson Shanks (1937-2015)

Tim Solliday (b. 1952)

Alexey Steele (b. 1967)

Patricia Watwood (b. 1971)

Bruce Wolfe (b. 1941, Sculptor)


</doc>
<doc id="29560452" url="https://en.wikipedia.org/wiki?curid=29560452" title="The arts">
The arts

The arts refers to the theory and physical expression of creativity found in human societies and cultures. Major constituents of the arts include literature (including drama, poetry, and prose), performing arts (among them dance, music, and theatre), and visual arts (including drawing, painting, filmmaking, architecture, ceramics, sculpting, and photography). 

Some art forms combine a visual element with performance (e.g., cinematography) or artwork with the written word (e.g., comics). From prehistoric cave paintings to modern day films, art serves as a vessel for storytelling and conveying humankind's relationship with the environment.

In its most basic abstract definition, art is a documented expression of a sentient being through or on an accessible medium so that anyone can view, hear or experience it. The act itself of producing an expression can also be referred to as a certain art, or as art in general. If this solidified expression, or the act of producing it, is "good" or has value depends on those who access and rate it and this public rating is dependent on various subjective factors. Merriam-Webster defines "the arts" as "painting, sculpture, music, theatre, literature, etc., considered as a group of activities done by people with skill and imagination." Similarly, the United States Congress, in the National Foundation on the Arts and Humanities Act, defined "the arts" as follows:

In Ancient Greece, all art and craft was referred to by the same word, "techne". Thus, there was no distinction among the arts. Ancient Greek art brought the veneration of the animal form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features (e.g. Zeus' thunderbolt). In Byzantine and Gothic art of the Middle Ages, the dominance of the church insisted on the expression of biblical and not material truths. Eastern art has generally worked in a style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade and reflection). A characteristic of this style is that the local colour is often defined by an outline (a contemporary equivalent is the cartoon). This is evident in, for example, the art of India, Tibet and Japan. Religious Islamic art forbids iconography, and expresses religious ideas through geometry instead.

In the Middle Ages, the "Artes Liberales" (liberal arts) were taught in universities as part of the Trivium, an introductory curriculum involving grammar, rhetoric, and logic, and of the Quadrivium, a curriculum involving the "mathematical arts" of arithmetic, geometry, music, and astronomy. The "Artes Mechanicae" (consisting of "vestiaria –" tailoring and weaving; "agricultura" – agriculture; "architectura" – architecture and masonry; "militia" and "venatoria –" warfare, hunting, military education, and the martial arts; "mercatura" – trade; "coquinaria" – cooking; and "metallaria" – blacksmithing and metallurgy) were practised and developed in guild environments. The modern distinction between "artistic" and "non-artistic" skills did not develop until the Renaissance. In modern academia, the arts are usually grouped with or as a subset of the humanities. Some subjects in the humanities are history, linguistics, literature, theology, philosophy, and logic.

The arts have also been classified as seven: painting, architecture, sculpture, literature, music, performing and cinema.
Some view literature, painting, sculpture, and music as the main four arts, of which the others are derivative; drama is literature with acting, dance is music expressed through motion, and song is music with literature and voice.

Architecture is the art and science of designing buildings and structures. The word "architecture" comes from the Greek "arkhitekton", "master builder, director of works," from "αρχι-" (arkhi) "chief" + "τεκτων" (tekton) "builder, carpenter". A wider definition would include the design of the built environment, from the macrolevel of town planning, urban design, and landscape architecture to the microlevel of creating furniture. Architectural design usually must address both feasibility and cost for the builder, as well as function and aesthetics for the user.

In modern usage, architecture is the art and discipline of creating, or inferring an implied or apparent plan of, a complex object or system. The term can be used to connote the "implied architecture" of abstract things such as music or mathematics, the "apparent architecture" of natural things, such as geological formations or the structure of biological cells, or explicitly "planned architectures" of human-made things such as software, computers, enterprises, and databases, in addition to buildings. In every usage, an architecture may be seen as a "subjective mapping" from a human perspective (that of the "user" in the case of abstract or physical artifacts) to the elements or components of some kind of structure or system, which preserves the relationships among the elements or components. Planned architecture manipulates space, volume, texture, light, shadow, or abstract elements in order to achieve pleasing aesthetics. This distinguishes it from applied science or engineering, which usually concentrate more on the functional and feasibility aspects of the design of constructions or structures.

In the field of building architecture, the skills demanded of an architect range from the more complex, such as for a hospital or a stadium, to the apparently simpler, such as planning residential houses. Many architectural works may be seen also as cultural and political symbols, or works of art. The role of the architect, though changing, has been central to the successful (and sometimes less than successful) design and implementation of pleasingly built environments in which people live.

Ceramic art is art made from ceramic materials (including clay), which may take forms such as pottery, tile, figurines, sculpture, and tableware. While some ceramic products are considered fine art, some are considered to be decorative, industrial, or applied art objects. Ceramics may also be considered artefacts in archaeology.Ceramic art can be made by one person or by a group of people. In a pottery or ceramic factory, a group of people design, manufacture, and decorate the pottery. Products from a pottery are sometimes referred to as "art pottery." In a one-person pottery studio, ceramists or potters produce studio pottery. In modern ceramic engineering usage, "ceramics" is the art and science of making objects from inorganic, non-metallic materials by the action of heat. It excludes glass and mosaic made from glass "tesserae."

Conceptual art is art in which the concept(s) or idea(s) involved in the work takes precedence over traditional aesthetic and material concerns.
The inception of the term in the 1960s referred to a strict and focused practice of idea-based art that often defied traditional visual criteria associated with the visual arts in its presentation as text. Through its association with the Young British Artists and the Turner Prize during the 1990s, its popular usage, particularly in the UK, developed as a synonym for all contemporary art that does not practise the traditional skills of painting and sculpture.

Drawing is a means of making an image, using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax colour pencils, crayons, charcoals, pastels, and markers. Digital tools which can simulate the effects of these are also used. The main techniques used in drawing are line drawing, hatching, crosshatching, random hatching, scribbling, stippling, and blending. An artist who excels in drawing is referred to as a "drafter", "draftswoman", or "draughtsman". Drawing can be used to create art used in cultural industries such as illustrations, comics and animation.

Painting is a mode of creative expression, and can be done in numerous forms. Drawing, gesture (as in gestural painting), composition, narration (as in narrative art), or abstraction (as in abstract art), among other aesthetic modes, may serve to manifest the expressive and conceptual intention of the practitioner. Paintings can be naturalistic and representational (as in a still life or landscape painting), photographic, abstract, narrative, symbolistic (as in Symbolist art), emotive (as in Expressionism), or political in nature (as in Artivism).

Modern painters have extended the practice considerably to include, for example, collage. Collage is not painting in the strict sense since it includes other materials. Some modern painters incorporate different materials such as sand, cement, straw, wood or strands of hair for their artwork texture. Examples of this are the works of Elito Circa, Jean Dubuffet or Anselm Kiefer.

Photography as an art form refers to photographs that are created in accordance with the creative vision of the photographer. Art photography stands in contrast to photojournalism, which provides a visual account for news events, and commercial photography, the primary focus of which is to advertise products or services.

Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, as clay), in stone, metal, ceramics, wood and other materials; but since modernism, shifts in sculptural process led to an almost complete freedom of materials and process. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or moulded, or cast.

Literature is literally "acquaintance with letters" as in the first sense given in the "Oxford English Dictionary". The noun "literature" comes from the Latin word "littera" meaning "an individual written character (letter)." The term has generally come to identify a collection of writings, which in Western culture are mainly prose (both fiction and non-fiction), drama and poetry. In much, if not all of the world, the artistic linguistic expression can be oral as well, and include such genres as epic, legend, myth, ballad, other forms of oral poetry, and as folktale. Comics, the combination of drawings or other visual arts with narrating literature, are often called the "ninth art" (le neuvième art) in Francophone scholarship.

Performing arts comprise dance, music, theatre, opera, mime, and other art forms in which a human performance is the principal product. Performing arts are distinguished by this performance element in contrast with disciplines such as visual and literary arts where the product is an object that does not require a performance to be observed and experienced. Each discipline in the performing arts is temporal in nature, meaning the product is performed over a period of time. Products are broadly categorized as being either repeatable (for example, by script or score) or improvised for each performance. Artists who participate in these arts in front of an audience are called "performers", including actors, magicians, comedians, dancers, musicians, and singers. Performing arts are also supported by the services of other artists or essential workers, such as songwriting and stagecraft. Performers often adapt their appearance with tools such as costume and stage makeup.

Music is an art form whose medium is sound and silence, occurring in time. Common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, metre, and articulation), dynamics, and the sonic qualities of timbre and texture. The creation, performance, significance, and even the definition of music vary according to culture and social context. Music ranges from strictly organized compositions (and their reproduction in performance) through improvisational music to aleatoric pieces. Music can be divided into genres and subgenres, although the dividing lines and relationships between music genres are often subtle, sometimes open to individual interpretation, and occasionally controversial. Within "the arts," music may be classified as a performing art, a fine art, and auditory art.

Theatre or theater (from Greek "theatron" ("θέατρον)"; from "theasthai", "behold") is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound and spectacle – indeed, any one or more elements of the other performing arts. In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, Chinese opera and mummers' plays.

Dance (from Old French "dancier", of unknown origin) generally refers to human movement either used as a form of expression or presented in a social, spiritual or performance setting. "Dance" is also used to describe methods of non-verbal communication (see body language) between humans or animals (e.g. bee dance, mating dance), motion in inanimate objects (e.g. "the leaves danced in the wind"), and certain musical forms or genres. Choreography is the art of making dances, and the person who does this is called a choreographer. Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. In sports, gymnastics, figure skating and synchronized swimming are dance disciplines while Martial arts "kata" are often compared to dances.

Areas exist in which artistic works incorporate multiple artistic fields, such as film, opera and performance art. While opera is often categorized in the performing arts of music, the word itself is Italian for "works", because opera combines several artistic disciplines in a singular artistic experience. In a typical traditional opera, the entire work utilizes the following: the sets (visual arts), costumes (fashion), acting (dramatic performing arts), the libretto, or the words/story (literature), and singers and an orchestra (music). The composer Richard Wagner recognized the fusion of so many disciplines into a single work of opera, exemplified by his cycle "Der Ring des Nibelungen" ("The Ring of the Nibelung"). He did not use the term opera for his works, but instead Gesamtkunstwerk ("synthesis of the arts"), sometimes referred to as "Music Drama" in English, emphasizing the literary and theatrical components which were as important as the music. Classical ballet is another form which emerged in the 17th century in which orchestral music is combined with dance.

Other works in the late 19th, 20th and 21st centuries have fused other disciplines in unique and creative ways, such as performance art. Performance art is a performance over time which combines any number of instruments, objects, and art within a predefined or less well-defined structure, some of which can be improvised. Performance art may be scripted, unscripted, random or carefully organized; even audience participation may occur. John Cage is regarded by many as a performance artist rather than a composer, although he preferred the latter term. He did not compose for traditional ensembles. Cage's composition "Living Room Music" composed in 1940 is a "quartet" for unspecified instruments, really non-melodic objects, which can be found in a living room of a typical house, hence the title.

There is no clear line between art and culture. Cultural fields like gastronomy are sometimes considered as arts.

The applied arts are the application of design and decoration to everyday, functional, objects to make them aesthetically pleasing. The applied arts includes fields such as industrial design, illustration, and commercial art. The term "applied art" is used in distinction to the fine arts, where the latter is defined as arts that aims to produce objects which are beautiful or provide intellectual stimulation but have no primary everyday function. In practice, the two often overlap.

A debate exists in the fine arts and video game cultures over whether video games can be counted as an art form. Game designer Hideo Kojima professes that video games are a type of service, not an art form, because they are meant to entertain and attempt to entertain as many people as possible, rather than being a single artistic voice (despite Kojima himself being considered a gaming auteur, and the mixed opinions his games typically receive). However, he acknowledged that since video games are made up of artistic elements (for example, the visuals), game designers could be considered museum curators – not creating artistic pieces, but arranging them in a way that displays their artistry and sells tickets.

Within social sciences, cultural economists show how video games playing is conducive to the involvement in more traditional art forms and cultural practices, which suggests the complementarity between video games and the arts.

In May 2011, the National Endowment of the Arts included video games in its redefinition of what is considered a "work of art" when applying of a grant. In 2012, the Smithsonian American Art Museum presented an exhibit, "The Art of the Video Game". Reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.





</doc>
<doc id="55657946" url="https://en.wikipedia.org/wiki?curid=55657946" title="Adaptation (arts)">
Adaptation (arts)

An adaptation is a transfer of a work of art from one medium to another.

Some common examples are:

There is, however, no end to potential media involved in adaptation. Adaptation is the practice of transcoding (changing the code or 'language' used in a medium) as well as the assimilation of a work of art to other cultural, linguistic, semiotic, aesthetic or other norms. Recent approaches to the expanding field Adaptation Studies reflect these expansion of our perspective. Adaptation occurs as a special case of intertextual and intermedial exchange and the copy-paste culture of digital technologies has produced "new intertextual forms engendered by emerging technologies—mashups, remixes, reboots, samplings, remodelings, transformations— " that "further develop the impulse to adapt and appropriate, and the ways in which they challenge the theory and practice of adaptation and appropriation."




</doc>
<doc id="198778" url="https://en.wikipedia.org/wiki?curid=198778" title="Composer">
Composer

A composer (Latin "compōnō"; literally "one who puts together") is a musician who is an author of music in any form, including vocal music (for a singer or choir), instrumental music, electronic music, and music which combines multiple forms. A composer may create music in any music genre, including, for example, classical music, musical theatre, blues, folk music, jazz, and popular music. Composers often express their works in a written musical score using musical notation.

Many composers are, or were, also skilled performers of music.

Musical notation serves as a set of directions for a performer, but there is a whole continuum of possibilities concerning how much the performer determines the final form of the rendered work in performance. 

Even in a conventional Western piece of instrumental music, in which all of the melodies, chords, and basslines are written out in musical notation, the performer has a degree of latitude to add artistic interpretation to the work, by such means as by varying his or her articulation and phrasing, choosing how long to make fermatas (held notes) or pauses, and — in the case of bowed string instruments, woodwinds or brass instruments — deciding whether to use expressive effects such as vibrato or portamento. For a singer or instrumental performer, the process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.

Although a musical composition often has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, where the songs may be written by one person, the orchestration of the accompaniment parts and writing of the overture is done by an orchestrator, and the words may be written by a third person.

A piece of music can also be composed with words, images, or, in the 20th and 21st century, computer programs that explain or notate how the singer or musician should create musical sounds. Examples of this range from wind chimes jingling in a breeze, to avant-garde music from the 20th century that uses graphic notation, to text compositions such as "Aus den sieben Tagen", to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski.

The nature and means of individual variation of the music is varied, depending on the musical culture in the country and time period it was written. For instance, music composed in the Baroque era, particularly in slow tempos, often was written in bare outline, with the expectation that the performer would add improvised ornaments to the melody line during a performance. Such freedom generally diminished in later eras, correlating with the increased use by composers of more detailed scoring in the form of dynamics, articulation et cetera; composers becoming uniformly more explicit in how they wished their music to be interpreted, although how strictly and minutely these are dictated varies from one composer to another. Because of this trend of composers becoming increasingly specific and detailed in their instructions to the performer, a culture eventually developed whereby faithfulness to the composer's written intention came to be highly valued (see, for example, Urtext edition). This musical culture is almost certainly related to the high esteem (bordering on veneration) in which the leading classical composers are often held by performers.

The historically informed performance movement has revived to some extent the possibility of the performer elaborating in a serious way the music as given in the score, particularly for Baroque music and music from the early Classical period. The movement might be considered a way of creating "greater" faithfulness to the original in works composed at a time that expected performers to improvise. In genres other than classical music, the performer generally has more freedom; thus for instance when a performer of Western popular music creates a "cover" of an earlier song, there is little expectation of exact rendition of the original; nor is exact faithfulness necessarily highly valued (with the possible exception of "note-for-note" transcriptions of famous guitar solos).

In Western art music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a pop songwriter may not use notation at all, and instead compose the song in his or her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music. The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough the creation of popular and traditional music songs and instrumental pieces and to include spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

The level of distinction between composers and other musicians varies, which affects issues such as copyright and the deference given to individual interpretations of a particular piece of music. In the development of European classical music, the function of composing music initially did not have much greater importance than that of performing it. The preservation of individual compositions did not receive enormous attention and musicians generally had no qualms about modifying compositions for performance. In as much as the role of the composer in western art music has seen continued solidification, in alternative idioms (i.e. jazz, experimental music) it has in some ways become increasingly complex or vague. For instance, in certain contexts the line between composer and performer, sound designer, arranger, producer, and other roles, can be quite blurred.
The term "composer" is often used to refer to composers of instrumental music, such as those found in classical, jazz or other forms of art and traditional music. In popular and folk music, the composer is usually called a songwriter, since the music generally takes the form of a song. Since the mid-20th century, the term has expanded to accommodate creators of electroacoustic music, in which composers directly create sonic material in any of the various electronic media, such as reel-to-reel tape and electronic effects units, which may be presented to an audience by replaying a tape or other sound recording, or by having live instrumentalists and singers perform with prerecorded material. This is distinct from a 19th-century conception of instrumental composition, where the work was represented solely by a musical score to be interpreted by performers.

Music was an important part of social and cultural life in Ancient Greece. We know that composers wrote notated music during the Ancient Greek era because scholars have found the Seikilos epitaph. The epitaph, written around 200 BC to around AD 100 is the oldest surviving example of a complete musical composition, including musical notation, in the world. The song, the melody of which is recorded, alongside its lyrics, in the ancient Greek musical notation, was found engraved on a tombstone, a stele, near Aydın, Turkey (not far from Ephesus). It is a Hellenistic Ionic song in either the Phrygian octave species or Iastian tonos.

During the Medieval music era (476 to 1400), composers wrote monophonic (single melodic line) chanting into Roman Catholic Church services. Western Music then started becoming more of an art form with the advances in music notation. The only European Medieval repertory that survives from before about 800 is the monophonic liturgical plainsong of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin and Guillaume de Machaut.

During the Renaissance music era (c. 1400 to 1600) composers tended to focus more on writing songs about secular (non-religious) themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was hand-copied). The increased availability of sheet music helped to spread composers' musical styles more quickly and across a larger area. By the middle of the 15th century, composers were writing richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. As musical activity shifted from the church to the aristocratic courts, kings, queens and princes competed for the finest composers. Many leading important composers came from the Netherlands, Belgium, and northern France. They are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain.

During the Baroque era of music (1600 to 1750), composers expanded the range and complexity of the music they were writing. The Baroque music era began when composers looked back to Ancient Greek music for the inspiration to create operas (dramatic vocal music accompanied by orchestra). Another key style of music composers used during this era was contrapuntal music. This style of writing required composers to have an advanced knowledge of music theory, as contrapuntal music involves multiple, independent melody lines played by instruments or sung by voices. There were strict counterpoint rules that composers had to learn. German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as choirs and for keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period, composers developed several major music forms that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Some of the best-known composers from the Baroque era include Claudio Monteverdi, Heinrich Schütz, Jean-Baptiste Lully, Dieterich Buxtehude, Arcangelo Corelli, Henry Purcell, François Couperin, Antonio Vivaldi, Georg Philipp Telemann, Jean-Philippe Rameau, Johann Sebastian Bach and George Frideric Handel.

Composers of music of the Classical Period (1750 to 1830) looked to the art and philosophy of Ancient Greece and Rome, to the ideals of balance, proportion and disciplined expression. Apart from when writing religious works, composers moved towards writing in a lighter, clearer and considerably simpler texture, using instrumental melodies that tended to be almost voicelike and singable. New genres were developed by composers. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct.

Composers focused on instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata.

One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres such as opera and oratorio became more popular.

The best known composers of Classicism are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism.

During the Romantic music era (c. 1810 to 1900), composers turned the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces. Composers attempted to increase emotional expression and power of their music, and they tried to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. For composers, the emotional and expressive qualities of music came to take precedence over following textbooks and tradition. Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, composers explored new approaches to use for existing genres, forms, and functions. Also, composers created new forms that were deemed better suited to the new subject matter. Opera and ballet continued to develop.

In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colours". Composers in the Romantic era increased the size of the orchestra by adding players and using new instruments, creating a more powerful sound. Some Wagnerian orchestras included multiple harps, massive string sections and Wagner tubas.

In the 19th century, one of the key ways that new compositions became known to the public was by the sales of sheet music, which amateur music lovers would perform at home on their piano or other instruments. With 20th-century music, there was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music. In the 20th century, contemporary classical composers were also influenced by the African-American improvisation-based jazz music. The jazz influence can be seen in Third Stream music and in the compositions of Leonard Bernstein. The focus of art music was characterized by exploration of new rhythms, styles, and sounds. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. Cage wrote compositions for traditional classical instruments and unusual sound-producing devices not normally thought of as instruments, such as radios. The invention of sound recording and the ability to edit music on tape gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition, in which composers made pieces using reel-to-reel tape recorders and electronic equipment.

In 1993, American musicologist Marcia Citron asked "[w]hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers.

According to Abbey Philips, "women musicians have had a very difficult time breaking through and getting the credit they deserve." During the Medieval eras, most of the art music was created for liturgical (religious) purposes and due to the views about the roles of women that were held by religious leaders, few women composed this type of music, with the nun Hildegard von Bingen being among the exceptions. Most university textbooks on the history of music discuss almost exclusively the role of male composers. As well, very few works by women composers are part of the standard repertoire of classical music. In the "Concise Oxford History of Music", Clara Schumann is the only woman composer who is mentioned. Philips states that "[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts."

Women today are being taken more seriously in the realm of concert music, though the statistics of recognition, prizes, employment, and overall opportunities are still biased toward men.

Famous composers have a tendency to cluster in certain cities throughout history. Based on over 12,000 prominent composers listed in "Grove Music Online" and using word count measurement techniques, the most important cities for classical music can be quantitatively identified.

Paris has been the main hub for classical music in all periods. It was ranked fifth in the 15th and 16th centuries but first in the 17th to 20th centuries inclusive. London was the second most meaningful city: eighth in the 15th century, seventh in the 16th, fifth in the 17th, second in the 18th and 19th centuries, and fourth in the 20th century. Rome topped the rankings in the 15th century, dropped to second in the 16th and 17th centuries, eighth in the 18th century, ninth in the 19th century but back at sixth in the 20th century. Berlin appears in the top ten ranking only in the 18th century, and was ranked third most important city in both the 19th and 20th centuries. New York City entered the rankings in the 19th century (at fifth place) and stood at second rank in the 20th century. The patterns are very similar for a sample of 522 top composers.

Professional classical composers often have a background in performing classical music during their childhood and teens, either as a singer in a choir, as a player in a youth orchestra, or as a performer on a solo instrument (e.g., piano, pipe organ, or violin). Teens aspiring to be composers can continue their postsecondary studies in a variety of formal training settings, including colleges, conservatories, and universities. Conservatories, which are the standard musical training system in France and in Quebec (Canada) provide lessons and amateur orchestral and choral singing experience for composition students. Universities offer a range of composition programs, including bachelor's degrees, Master of Music degrees, and Doctor of Musical Arts degrees. As well, there are a variety of other training programs such as classical summer camps and festivals, which give students the opportunity to get coaching from composers.

Bachelor's degrees in composition (referred to as B.Mus. or B.M) are four-year programs that include individual composition lessons, amateur orchestra/choral experience, and a sequence of courses in music history, music theory, and liberal arts courses (e.g., English literature), which give the student a more well-rounded education. Usually, composition students must complete significant pieces or songs before graduating. Not all composers hold a B.Mus. in composition; composers may also hold a B.Mus. in music performance or music theory.

Master of music degrees (M.mus.) in composition consist of private lessons with a composition professor, ensemble experience, and graduate courses in music history and music theory, along with one or two concerts featuring the composition student's pieces. A Master's degree in music (referred to as an M.Mus. or M.M.) is often a required minimum credential for people who wish to teach composition at a university or conservatory. A composer with an M.Mus. could be an adjunct professor or instructor at a university, but it would be difficult in the 2010s to obtain a tenure track professor position with this degree.

To become a tenure track professor, many universities require a doctoral degree. In composition, the key doctoral degree is the Doctor of Musical Arts, rather than the PhD; the PhD is awarded in music, but typically for subjects such as musicology and music theory.

Doctor of Musical Arts (referred to as D.M.A., DMA, D.Mus.A. or A.Mus.D) degrees in composition provide an opportunity for advanced study at the highest artistic and pedagogical level, requiring usually an additional 54+ credit hours beyond a master's degree (which is about 30+ credits beyond a bachelor's degree). For this reason, admission is highly selective. Students must submit examples of their compositions. If available, some schools will also accept video or audio recordings of performances of the student's pieces. Examinations in music history, music theory, ear training/dictation, and an entrance examination are required.

Students must prepare significant compositions under the guidance of faculty composition professors. Some schools require DMA composition students to present concerts of their works, which are typically performed by singers or musicians from the school. The completion of advanced coursework and a minimum B average are other typical requirements of a D.M.A program. During a D.M.A. program, a composition student may get experience teaching undergraduate music students.

Some classical composers did not complete composition programs, but focused their studies on performance of voice or an instrument or on music theory, and developed their compositional skills over the course of a career in another musical occupation.

During the Middle Ages, most composers worked for the Catholic church and composed music for religious services such as plainchant melodies. During the Renaissance music era, composers typically worked for aristocratic employers. While aristocrats typically required composers to produce a significant amount of religious music, such as Masses, composers also penned many non-religious songs on the topic of courtly love: the respectful, reverential love of a great woman from afar. Courtly love songs were very popular during the Renaissance era. During the Baroque music era, many composers were employed by aristocrats or as church employees. During the Classical period, composers began to organize more public concerts for profit, which helped composers to be less dependent on aristocratic or church jobs. This trend continued in the Romantic music era in the 19th century. In the 20th century, composers began to seek employment as professors in universities and conservatories. In the 20th century, composers also earned money from the sales of their works, such sheet music publications of their songs or pieces or as sound recordings of their works.




</doc>
<doc id="57062676" url="https://en.wikipedia.org/wiki?curid=57062676" title="Portrayal of female bodies in Chinese contemporary art">
Portrayal of female bodies in Chinese contemporary art

Portrayal of female bodies in Chinese contemporary art has become a lot more diverse as women artists start to rise in China. Many contemporary Chinese women artists have employed the use of female bodies as the subject of their artworks. From the ancient and imperial period of China until early the 19th century, women's body images in Chinese art were predominantly portrayed through male artists' lenses. As a result, female bodies were often misrepresented. With the arrival of modernism in Chinese contemporary art, women now have more influences in the field of visual arts. Chinese women artists employ unconventional artistic expressions in order to speak about their experiences of being a female and an artist in a patriarchal society. Although the Chinese visual art field has been more inclusive in the recent decades, women artists’ representation in major art exhibitions is still significantly less than male artists. Therefore, artists and curators have made a joint effort to devote certain art exhibitions to just Chinese women artists.

Traditionally, female bodies were only valued as carriers for the family bloodline. In traditional paintings, nude women were a taboo for the artists. As a result, the portrayal of women's bodies was formulaic, always with slim figures, usually leaning on another object or have a slightly bent posture to further emphasize their delicate bodies.

Female images would also demonstrate ideal qualities of women at that time. Aside from feminine beauty and charm, she should also possess the virtues of subordinating herself in a Confucian patriarchal society. Therefore, women in these ancient paintings all wore long silk skirts that came down to the ankles. They could only show the part of their body above their chest. Their appearance, combined with their slender body figures, showed the submissive and demure qualities, which was desirable for male audiences.

A drastic change in ideal femininity occurred in 1966 and 1967 when the Cultural Revolution in China took place. The Cultural Revolution, led by Mao Zedong, denounced the traditional image of women's inferiority and calls for a women's liberation in the new China. Propaganda tools, such as performance art, films, and posters, strongly supported this movement and effectively constructed and broadcast a new female identity. Feminine beauty was no longer characterized by delicate figures, but instead by masculinized images of women. For example, "Iron girls" represented the permissible appearances of women in the 1970s. They should not wear anything that showed their female curves. Moreover, women were encouraged to dress like men and go to work like men did, given that Mao called for a "gender erasure" in order to make "Chinese women in new China."

Similar to "Iron Girls," "Strong Women" images were popularized through mass media, such as cartoons and local newspapers. Their appearance was defined by masculinized traits: tanned skin tone, sturdy figures, and blue and black work uniforms. Feminine features, like smooth skin and hands, were seen as something to be ashamed of because they were expected to be contributing to the nation's economy like men did.

In her installation series, "Trauma", Hung Liu portrays a bound-foot woman. Foot binding was practiced among Chinese women from the Song dynasty up until the early 20th century. Women would wrap their feet tightly in order to keep them small, which was characterized as a feminine beauty at the time. In Liu's installation pieces, she repeatedly shows an emotionless woman with her naked feet. As a result of the foot binding cultural, these women had deformed feet.

Feng Jiali is famous for her oil painting of explicit images of female bodies. A series of her paintings depict young, school-aged Chinese girls, usually skimpily dressed in the bath or laying in bed. The girls in her paintings almost always look directly at the viewers, some innocent-looking, some with a confrontational gaze.

Another series of her works involves photographs of her pregnant body. By revealing her own pregnant body fearlessly in front of the camera, she challenges the traditional views of pregnant body as something private and a mean to continue the family line, favoring boys over girls, and following the one-child policy. Because pregnant nude is normally not defined as the ideal beauty found in female bodies, Feng’s photo collage expresses another aspect of being a woman. She desired to express the hardships and anxieties of being pregnant, which contradicts with the conventional, sacred portrayal of childbirth by many male artists.

Cai Jin, another contemporary oil painter, also challenges the traditional taboos of nudity in ancient Chinese art history. Unlike Feng Jiali, however, Cai articulates female bodies in a semi-abstract way by using organic elements in her paintings. A series of her paintings depict women's bodies shaped like banana tree leaves. Unlike many conventional portrayals of the female body that serves in the interest of male audiences, Cai painted the bodies like decomposed flesh, which is then metamorphosed into dripping, red, banana leaves.

Traditionally, plants, especially flowers, are associated with femininity. However, Cai made a modern twist on this subject and made it into a somewhat unpleasant-looking and distorted image, rejecting the stereotypical delicate women and flowers association. Like how Feng declared that pregnant bodies can be art too, Cai had made an assertion that although women's bodies aren't always going to be maintained in a perfect way, they can still become a form of art.

In "Opening of the Great Wall", He Chengyao presented her own naked body in a public space in 2001 and took a series of photographs of herself. The photographs documented her walking confidently with her upper body being nude. She held her bright red bra on her right hand and tied her sweater around the waist. She had a deadpan expression on her face and a confident posture as she walks toward the camera. The photo was taken on the Great Wall of China, which is one of the most popular scenic spots in Beijing, China.

Despite the fact that she was surrounded by many curious tourists, He still maintained a confident look. She had also spontaneously joined in an installation by a male artist, H.A. Schult, which was exhibited on the Great Wall at the same time. Whether intended or not, she had made her appearance as a Chinese women artist in the mainstream of male art production. Furthermore, although her piece took place in 2001, women's nudity in public spaces was still considered taboo at the time.

He Chengyao completed this performance art piece in 2004 at the Shanghai Doland Museum of Modern Art, which combined the themes of body and identity. As the title of the piece suggested, He Chengyao performed a series of exercises in front of a large group of audiences, while her naked body was tightly wrapped with tape. Her movement caused the tape to be ripped apart one by one. She also wrapped herself with the sticky side of the tape out, which made her body parts stuck together and making it increasingly harder to perform the exercise.

Xiang Jing is a Beijing-based sculptor. She is best known for her depiction of realistic nude women sculptures. One of her sculptures, "Your Body", portrays a nude and bald woman slumping in a chair with her legs spread open. There's a prominent scar on the right side of her abdomen. She has sagging breasts, belly fat rolls, and blank expression on her face. None of these qualities fall under the expectation of the ideal female body.

Though efforts have been made in order to make the visual art field more inclusive, women's artworks have still been underrepresented in major exhibitions. In the late 1980s, when the first major Chinese Art exhibition took place, only five women, out of sixty-seven artists and participants, were included in the show. While artists from mainland China, Taiwan, and Hong Kong, were being recognized for their diverse identities, women artists' participation in this first Chinese exhibition were not acknowledged.

Similarly, in the traveling exhibition, "Transience: Chinese Experimental Art at the End of Twentieth Century" in 1998, only three women artists presented their artworks. The curator of this exhibition, Wu Hung, intended to provide a platform for experimental Chinese art to take place. However, it appears that women's innovative artistic expressions were still being excluded and didn't gain as many recognition as male Chinese artists have had.

The issue of unequal representation among male and female Chinese artists did receive some attention from the public. As a result, curators and artists have made a joint effort to devote certain exhibition and publications solely for female artists.

"Century Women" was one of the first exhibitions dedicated to Chinese women artists. It intended to raise awareness on female experiences, roles, subject matters, and most importantly, art practices through the perspective of women living in contemporary Chinese society. In addition to providing a platform to showcase artworks made by Chinese women, it also exhibited photos and documents showing women's art production in China. More than seventy artists participated in this exhibition and presented over five hundred pieces of artworks

This exhibition was held concurrently around the world, such as China, Singapore, Germany, and the United States, to spread pan-Asian feminist artworks. Specifically, the show in New York, "Against the Tide (1998)," featured five female artists who were born in China and grew up during the Cultural Revolution. Cai Jin, who also participated in this show, presented her Banana Plant series in the show, which is a suggestive form of presenting female nude bodies in a public space.

List of Chinese women artists


</doc>
<doc id="8560" url="https://en.wikipedia.org/wiki?curid=8560" title="Design">
Design

A design is a construction or activity specification or plan, or the result of that plan in the form of a prototype, finished product, or process. "Design" as a verb is the process of creating such a design. In some cases, the direct construction of an object without an explicit prior plan (such as in craftwork and some engineering, coding, and graphic design) is also considered to be a design activity. The same word is also used for the broad discipline of design creation, which spans engineering and applied art. Major examples of design are architectural blueprints, engineering drawings, business processes, circuit diagrams, and sewing patterns.

The person or organization who creates a design is a "designer", which is also a term used for people who work professionally in one of the various design areas usually specifying which area is being dealt with (such as a textile designer, fashion designer, product designer, concept designer, web designer or interior designer). A designer's sequence of activities is called a design process while the scientific study of design is called design science.A design is often crafted to meet certain design goals and constraints, is often expected to interact with a certain environment.}} Designs may take into account aesthetic, functional, economic, or socio-political considerations. The process of creating a design can be brief (a quick sketch) or lengthy and complicated, involving considerable research, negotiation, reflection, modelling, interactive adjustment and re-design. 

Substantial disagreement exists concerning how designers in many fields, whether amateur or professional, alone or in teams, produce designs. Kees Dorst and Judith Dijkhuis, both designers themselves, argued that "there are many ways of describing design processes" and discussed "two basic and fundamentally different ways", both of which have several names. The prevailing view has been called "the rational model", "technical problem solving" and "the reason-centric perspective". The alternative view has been called "reflection-in-action", "evolutionary design", "co-evolution", and "the action-centric perspective".

The rational model was independently developed by Herbert A. Simon, an American scientist, and Gerhard Pahl and Wolfgang Beitz, two German engineering design theorists. It posits that:

The rational model is based on a rationalist philosophy and underlies the waterfall model, systems development life cycle, and much of the engineering design literature. According to the rationalist philosophy, design is informed by research and knowledge in a predictable and controlled manner.

Typical stages consistent with the rational model include the following:

Each stage has many associated best practices.

The rational model has been widely criticized on two primary grounds:


The action-centric perspective is a label given to a collection of interrelated concepts, which are antithetical to the rational model. It posits that:


The action-centric perspective is based on an empiricist philosophy and broadly consistent with the agile approach and amethodical development. Substantial empirical evidence supports the veracity of this perspective in describing the actions of real designers. Like the rational model, the action-centric model sees design as informed by research and knowledge. However, research and knowledge are brought into the design process through the judgment and common sense of designers – by designers "thinking on their feet" – more than through the predictable and controlled process stipulated by the rational model.

At least two views of design activity are consistent with the action-centric perspective. Both involve three basic activities.

In the reflection-in-action paradigm, designers alternate between "framing", "making moves", and "evaluating moves". "Framing" refers to conceptualizing the problem, i.e., defining goals and objectives. A "move" is a tentative design decision. The evaluation process may lead to further moves in the design.

In the sensemaking–coevolution–implementation framework, designers alternate between its three titular activities. Sensemaking includes both framing and evaluating moves. Implementation is the process of constructing the design object. Coevolution is "the process where the design agent simultaneously refines its mental picture of the design object based on its mental picture of the context, and vice versa".

The concept of the design cycle is understood as a circular time structure, which may start with the thinking of an idea, then expressing it by the use of visual or verbal means of communication (design tools), the sharing and perceiving of the expressed idea, and finally starting a new cycle with the critical rethinking of the perceived idea. Anderson points out that this concept emphasizes the importance of the means of expression, which at the same time are means of perception of any design ideas.

There are countless philosophies for guiding design as design values and its accompanying aspects within modern design vary, both between different schools of thought and among practicing designers. Design philosophies are usually for determining design goals. A design goal may range from solving the least significant individual problem of the smallest element, to the most holistic influential utopian goals. Design goals are usually for guiding design. However, conflicts over immediate and minor goals may lead to questioning the purpose of design, perhaps to set better long term or ultimate goals. John Heskett, a 20th-century British writer on design claimed, "Design, stripped to its essence, can be defined as the human nature to shape and make our environment in ways without precedent in nature, to serve our needs and give meaning to our lives."

Design philosophies are fundamental guiding principles that dictate how a designer approaches his/her practice. Reflections on material culture and environmental concerns (sustainable design) can guide a design philosophy. One example is the First Things First manifesto which was launched within the graphic design community and states "We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication – a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design."

In "The Sciences of the Artificial" by polymath Herbert A. Simon, the author asserts design to be a meta-discipline of all professions. "Engineers are not the only professional designers. Everyone designs who devises courses of action aimed at changing existing situations into preferred ones. The intellectual activity that produces material artifacts is no different fundamentally from the one that prescribes remedies for a sick patient or the one that devises a new sales plan for a company or a social welfare policy for a state. Design, so construed, is the core of all professional training; it is the principal mark that distinguishes the professions from the sciences. Schools of engineering, as well as schools of architecture, business, education, law, and medicine, are all centrally concerned with the process of design."

A design approach is a general philosophy that may or may not include a guide for specific methods. Some are to guide the overall goal of the design. Other approaches are to guide the tendencies of the designer. A combination of approaches may be used if they don't conflict.

Some popular approaches include:

Design methods is a broad area that focuses on:

The word "design" is often considered ambiguous, as it is applied in varying contexts.
Today, the term design is generally used for what was formerly called the applied arts. The new term, for a very old thing, was perhaps initiated by Raymond Loewy and teachings at the Bauhaus and Ulm School of Design (HfG Ulm) in Germany during the 20th century.

The boundaries between art and design are blurred, largely due to a range of applications both for the term 'art' and the term 'design'. Applied arts has been used as an umbrella term to define fields of industrial design, graphic design, fashion design, etc. The term 'decorative arts' is a traditional term used in historical discourses to describe craft objects, and also sits within the umbrella of applied arts. In graphic arts (2D image making that ranges from photography to illustration), the distinction is often made between fine art and commercial art, based on the context within which the work is produced and how it is traded.

To a degree, some methods for creating work, such as employing intuition, are shared across the disciplines within the applied arts and fine art. Mark Getlein, writer, suggests the principles of design are "almost instinctive", "built-in", "natural", and part of "our sense of 'rightness'." However, the intended application and context of the resulting works will vary greatly.

In engineering, design is a component of the engineering process. Many overlapping methods and processes can be seen when comparing Product design, Industrial design and Engineering. The American Heritage Dictionary defines design as: "To conceive or fashion in the mind; invent," and "To formulate a plan", and defines engineering as: "The application of scientific and mathematical principles to practical ends such as the design, manufacture, and operation of efficient and economical structures, machines, processes, and systems.". Both are forms of problem-solving with a defined distinction being the application of "scientific and mathematical principles". The increasingly scientific focus of engineering in practice, however, has raised the importance of more new "human-centered" fields of design. How much science is applied in a design is a question of what is considered "science". Along with the question of what is considered science, there is social science versus natural science. Scientists at Xerox PARC made the distinction of design versus engineering at "moving minds" versus "moving atoms" (probably in contradiction to the origin of term "engineering – engineer" from Latin "in genio" in meaning of a "genius" what assumes existence of a "mind" not of an "atom").

The relationship between design and production is one of planning and executing. In theory, the plan should anticipate and compensate for potential problems in the execution process. Design involves problem-solving and creativity. In contrast, production involves a routine or pre-planned process. A design may also be a mere plan that does not include a production or engineering processes although a working knowledge of such processes is usually expected of designers. In some cases, it may be unnecessary or impractical to expect a designer with a broad multidisciplinary knowledge required for such designs to also have a detailed specialized knowledge of how to produce the product.

Design and production are intertwined in many creative professional careers, meaning problem-solving is part of execution and the reverse. As the cost of rearrangement increases, the need for separating design from production increases as well. For example, a high-budget project, such as a skyscraper, requires separating (design) architecture from (production) construction. A Low-budget project, such as a locally printed office party invitation flyer, can be rearranged and printed dozens of times at the low cost of a few sheets of paper, a few drops of ink, and less than one hour's pay of a desktop publisher.

This is not to say that production never involves problem-solving or creativity, nor that design always involves creativity. Designs are rarely perfect and are sometimes repetitive. The imperfection of a design may task a production position (e.g. production artist, construction worker) with utilizing creativity or problem-solving skills to compensate for what was overlooked in the design process. Likewise, a design may be a simple repetition (copy) of a known preexisting solution, requiring minimal, if any, creativity or problem-solving skills from the designer.

"Process design" (in contrast to "design process" mentioned above) refers to the planning of routine steps of a process aside from the expected result. Processes (in general) are treated as a product of design, not the method of design. The term originated with the industrial designing of chemical processes. With the increasing complexities of the information age, consultants and executives have found the term useful to describe the design of business processes as well as manufacturing processes.



</doc>
<doc id="504974" url="https://en.wikipedia.org/wiki?curid=504974" title="Patronage">
Patronage

Patronage is the support, encouragement, privilege, or financial aid that an organization or individual bestows to another. In the history of art, arts patronage refers to the support that kings, popes, and the wealthy have provided to artists such as musicians, painters, and sculptors. It can also refer to the right of bestowing offices or church benefices, the business given to a store by a regular customer, and the guardianship of saints. The word "patron" derives from the ("patron"), one who gives benefits to his clients (see Patronage in ancient Rome).

In some countries the term is used to describe political patronage, which is the use of state resources to reward individuals for their electoral support. Some patronage systems are legal, as in the Canadian tradition of the Prime Minister to appoint senators and the heads of a number of commissions and agencies; in many cases, these appointments go to people who have supported the political party of the Prime Minister. As well, the term may refer to a type of corruption or favoritism in which a party in power rewards groups, families, ethnicities for their electoral support using illegal gifts or fraudulently awarded appointments or government contracts.

From the ancient world onward, patronage of the arts was important in art history. It is known in greatest detail in reference to medieval and Renaissance Europe, though patronage can also be traced in feudal Japan, the traditional Southeast Asian kingdoms, and elsewhere—art patronage tended to arise wherever a royal or imperial system and an aristocracy dominated a society and controlled a significant share of resources. Samuel Johnson defined a patron as "one who looks with unconcern on a man struggling for life in the water, and, when he has reached ground, encumbers him with help".

Rulers, nobles and very wealthy people used patronage of the arts to endorse their political ambitions, social positions, and prestige. That is, patrons operated as sponsors. Most languages other than English still use the term "mecenate", derived from the name of Gaius Maecenas, generous friend and adviser to the Roman Emperor Augustus. Some patrons, such as the Medici of Florence, used artistic patronage to "cleanse" wealth that was perceived as ill-gotten through usury. Art patronage was especially important in the creation of religious art. The Roman Catholic Church and later Protestant groups sponsored art and architecture, as seen in churches, cathedrals, painting, sculpture and handicrafts.

While sponsorship of artists and the commissioning of artwork is the best-known aspect of the patronage system, other disciplines also benefited from patronage, including those who studied natural philosophy (pre-modern science), musicians, writers, philosophers, alchemists, astrologers, and other scholars. Artists as diverse and important as Chrétien de Troyes, Leonardo da Vinci and Michelangelo, William Shakespeare, and Ben Jonson all sought and enjoyed the support of noble or ecclesiastical patrons. Figures as late as Wolfgang Amadeus Mozart and Ludwig van Beethoven also participated in the system to some degree; it was only with the rise of bourgeois and capitalist social forms in the middle 19th century that European culture moved away from its patronage system to the more publicly supported system of museums, theaters, mass audiences and mass consumption that is familiar in the contemporary world. 
This kind of system continues across many fields of the arts. Though the nature of the sponsors has changed—from churches to charitable foundations, and from aristocrats to plutocrats—the term "patronage" has a more neutral connotation than in politics. It may simply refer to direct support (often financial) of an artist, for example by grants. In the latter part of the 20th century, the academic sub-discipline of patronage studies began to evolve, in recognition of the important and often neglected role that the phenomenon of patronage had played in the cultural life of previous centuries.

Charitable and other non-profit making organisations often seek an influential figurehead to act as patron. The relationship often does not involve money. As well as conferring credibility, these people can use their contacts and charisma to assist the organisation to raise funds or to affect government policy. The British Royal Family are especially prolific in this respect, devoting a large proportion of their time to a wide range of causes.

Sometimes consumers support smaller or local businesses or corporations out of loyalty even if less expensive options exist. Their regular custom is referred to as 'patronage'. Patronage may entitle members of a cooperative to a share of the surplus or profit generated by the co-op, called a "patronage refund". This refund is a form of dividend.

In the Church of England, "patronage" is the commonly used term for the right to present a candidate to a benefice.

The liturgical feast of the Patronage of Our Lady was first permitted by Decree of the Sacred Congregation of Rites on 6 May 1679, for all the ecclesiastical provinces of Spain, in memory of the victories obtained over the Saracens, heretics and other enemies from the sixth century to the reign of Philip IV of Spain. Pope Benedict XII ordered it to be kept in the Papal States on the third Sunday of November. To other places it is granted, on request, for some Sunday in November, to be designated by the ordinary. In many places the feast of the Patronage is held with an additional Marian title of Queen of All Saints, of Mercy, Mother of Graces. The Office is taken entirely from the Common of the Blessed Virgin, and the Mass is the "Salve sancta parens".

The Church Patronage (Scotland) Act 1711, (in force until 1874) resulted in multiple secessions from the Church of Scotland, including the secession of 1733, which led to the formation of the Associate Presbytery, the secession of 1761, which led to the formation of the Relief Church, and the Disruption of 1843, which led to the formation of the Free Church of Scotland.

While most news companies, particularly in North America are funded through advertising revenue, secondary funding sources include audience members and philanthropists who donate to for-profit and non-profit organizations.

Political leaders have at their disposal a great deal of patronage, in the sense that they make decisions on the appointment of officials inside and outside government (for example on quangos in the UK). Patronage is therefore a recognized power of the executive branch. In most countries the executive has the right to make many appointments, some of which may be lucrative (see also sinecures). In some democracies, high-level appointments are reviewed or approved by the legislature (as in the advice and consent of the United States Senate); in other countries, such as those using the Westminster system, this is not the case. Other types of political patronage may violate the laws or ethics codes, such as when political leaders engage in nepotism (hiring family members) and cronyism such as fraudulently awarding non-competitive government contracts to friends or relatives or pressuring the public service to hire an unqualified family member or friend.

Political patronage, also known as "Padrino System" also a slang call as "balimbing" (starfruit), in the Philippines, has been the source of many controversies and corruption. It has been an open secret that one cannot join the political arena of the Philippines without mastery of the Padrino System. From the lowest Barangay official, to the President of the Republic, it is expected that one gains political debts and dispenses political favor to advance one's career or gain influence, if not wealth.

After Soviet leader Vladimir Lenin's retirement from politics in March 1923 following a stroke, a power struggle began between Soviet Premier Alexei Rykov, Pravda editor Nikolai Bukharin, Profintern leader Mikhail Tomsky, Red Army founder Leon Trotsky, former Premier Lev Kamenev, Comintern leader Grigory Zinoviev, and General Secretary Joseph Stalin. Stalin used patronage to appoint many Stalinist delegates (such as Vyacheslav Molotov, Lazar Kaganovich, Grigory Ordzhonikidze, and Mikhail Kalinin) to the Party Politburo and Sovnarkom in order to sway the votes in his favour, making Stalin the effective leader of the country by 1929.

During 2012, the African National Congress (ANC) mayor of Beaufort West in the Western Cape Province wrote a letter which openly and illegally solicited funds from the Construction Education and Training Authority for the ANC's 2016 election campaign. This episode, amongst many others including instances revolving around president Jacob Zuma, revealed how the African National Congress as ruling political party utilized patronage to reward supporters and strengthen the leading faction of the party's control over governmental institutions.

In the United States during the Gilded Age, patronage became a controversial issue. Tammany boss William M. Tweed was an American politician who ran what is considered now to have been one of the most corrupt political machines in the country's history. Tweed and his cronies ruled for a brief time with absolute power over the city and state of New York. At the height of his influence, Tweed was the third-largest landowner in New York City, a director of the Erie Railway, the Tenth National Bank, and the New-York Printing Company, as well as proprietor of the Metropolitan Hotel. At times he was a member of the United States House of Representatives, the New York City Board of Advisors, and the New York State Senate. In 1873, Tweed was convicted for diverting between $40 million and $200 million of public monies.

Six months after James Garfield became president in 1881, Charles J. Guiteau, a disappointed office-seeker, assassinated him. To prevent further political violence and to assuage public outrage, Congress passed the Pendleton Act in 1883, which set up the Civil Service Commission. Henceforth, applicants for most federal government jobs would have to pass an examination. Federal politicians' influence over bureaucratic appointments waned, and patronage declined as a national political issue.

Beginning in 1969, a Supreme Court case in Chicago, "Michael L. Shakman v. Democratic Organization of Cook County", occurred involving political patronage and its constitutionality. Shakman claimed that much of the patronage going on in Chicago politics was unlawful on the grounds of the first and fourteenth amendments. Through a series of legal battle and negotiations, the two parties agreed upon The Shakman Decrees. Under these decrees it was declared that the employment status of most public employees could not be affected positively or negatively based on political allegiance, with exceptions for politically inclined positions. The case is still in negotiation today, as there are points yet to be decided.

Political patronage is not always considered corrupt. In the United States, the U.S. Constitution provides the president with the power to appoint individuals to government positions. He or she also may appoint personal advisers without congressional approval. Not surprisingly, these individuals tend to be supporters of the president. Similarly, at the state and local levels, governors and mayors retain appointments powers. Some scholars have argued that patronage may be used for laudable purposes, such as the "recognition" of minority communities through the appointment of their members to a high-profile positions. Bearfield has argued that patronage be used for four general purposes: create or strengthen a political organization; achieve democratic or egalitarian goals; bridge political divisions and create coalitions; and to alter the existing patronage system.

Boliburguesía is a term that was coined by journalist Juan Carlos Zapata in order to "define the oligarchy that has developed under the protection of the Chavez government". During Hugo Chávez's tenure, he seized thousands of properties and businesses while also reducing the footprint of foreign companies. Venezuela's economy was then largely state-run and was operated by military officers that had their business and government affairs connected. Senior fellow at the Brookings Institution, Harold Trinkunas, stated that involving the military in business was "a danger", with Trinkunas explaining that the Venezuelan military "has the greatest ability to coerce people, into business like they have". According to "Bloomberg Business", "[b]y showering contracts on former military officials and pro-government business executives, Chavez put a new face on the system of patronage".

There are historical examples where the noble classes financed scientific pursuits.

Many Barmakids were patrons of the sciences, which greatly helped the propagation of Indian science and scholarship from the neighbouring Academy of Gundishapur into the Arabic world. They patronized scholars such as Jabir ibn Hayyan and Jabril ibn Bukhtishu. They are also credited with the establishment of the first paper mill in Baghdad. The power of the Barmakids in those times is reflected in "The Book of One Thousand and One Nights"; the vizier Ja'far appears in several stories, as well as a tale that gave rise to the expression "Barmecide feast".

In the same manner as commercial patronage, those who attend a sporting event may be referred to as patrons, though the usage in much of the world is now considered archaic—with some notable exceptions. Those who attend the Masters Tournament, one of the four major championships of professional golf, are still traditionally referred to as "patrons," largely at the insistence of the Augusta National Golf Club. This insistence is occasionally made fun of by sportswriters and other media. In polo, a "patron" is a person who puts together a team by hiring one or more professionals. The rest of the team may be amateurs, often including the patron himself (or, increasingly, herself).

Also, people who attend hurling or Gaelic football games organised by the Gaelic Athletic Association are referred to as patrons.




</doc>
<doc id="752" url="https://en.wikipedia.org/wiki?curid=752" title="Art">
Art

Art is a diverse range of human activities in creating visual, auditory or performing artifacts (artworks), expressing the author's imaginative, conceptual ideas, or technical skill, intended to be appreciated for their beauty or emotional power. In their most general form these activities include the production of works of art, the criticism of art, the study of the history of art, and the aesthetic dissemination of art. 

The three classical branches of art are painting, sculpture and architecture. Music, theatre, film, dance, and other performing arts, as well as literature and other media such as interactive media, are included in a broader definition of the arts. Until the 17th century, "art" referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts.

Though the definition of what constitutes art is disputed and has changed over time, general descriptions mention an idea of imaginative or technical skill stemming from human agency and creation. The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics.

In the perspective of the history of art, artistic works have existed for almost as long as humankind: from early pre-historic art to contemporary art; however, some theorists feel that the typical concept of "artistic works" fits less well outside modern Western societies. One early sense of the definition of "art" is closely related to the older Latin meaning, which roughly translates to "skill" or "craft," as associated with words such as "artisan." English words derived from this meaning include "artifact", "artificial", "artifice", "medical arts", and "military arts". However, there are many other colloquial uses of the word, all with some relation to its etymology.
Over time, philosophers like Plato, Aristotle, Socrates and Kant, among others, questioned the meaning of art. Several dialogues in Plato tackle questions about art: Socrates says that poetry is inspired by the muses, and is not rational. He speaks approvingly of this, and other forms of divine madness (drunkenness, eroticism, and dreaming) in the "Phaedrus "(265a–c), and yet in the ""Republic"" wants to outlaw Homer's great poetic art, and laughter as well. In "Ion", Socrates gives no hint of the disapproval of Homer that he expresses in the "Republic". The dialogue "Ion" suggests that Homer's "Iliad" functioned in the ancient Greek world as the Bible does today in the modern Christian world: as divinely inspired literary art that can provide moral guidance, if only it can be properly interpreted.

With regards to the literary art and the musical arts, Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be mimetic or imitative art, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation—through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.

The more recent and specific sense of the word "art" as an abbreviation for "creative art" or "fine art" emerged in the early 17th century. Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or "finer" work of art.

Within this latter sense, the word "art" may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill. The creative arts ("art" as discipline) are a collection of disciplines which produce "artworks" ("art" as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience). Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects. For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression.

Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference. However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent.

The nature of art has been described by philosopher Richard Wollheim as "one of the most elusive of the traditional problems of human culture". Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as "mimesis" or representation. Art as mimesis has deep roots in the philosophy of Aristotle. Leo Tolstoy identified art as a use of indirect means to communicate from one person to another. Benedetto Croce and R. G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator. The theory of art as form has its roots in the philosophy of Kant, and was developed in the early twentieth century by Roger Fry and Clive Bell. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation. George Dickie has offered an institutional theory of art that defines a work of art as any artifact upon which a qualified person or persons acting on behalf of the social institution commonly referred to as "the art world" has conferred "the status of candidate for appreciation". Larry Shiner has described fine art as "not an essence or a fate but something we have made. Art as we have generally understood it is a European invention barely two hundred years old."

Art may be characterized in terms of mimesis (its representation of reality), narrative (storytelling), expression, communication of emotion, or other qualities. During the Romantic period, art came to be seen as "a special faculty of the human mind to be classified with religion and science".

The oldest documented forms of art are visual arts, which include creation of images or objects in fields including today painting, sculpture, printmaking, photography, and other visual media. Sculptures, cave paintings, rock paintings and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them. The oldest art objects in the world—a series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave. Containers that may have been used to hold paints have been found dating as far back as 100,000 years. Etched shells by "Homo erectus" from 430,000 and 540,000 years ago were discovered in 2014.

Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in its art. Because of the size and duration of these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions.

In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of subjects about Biblical and religious culture, and used styles that showed the higher glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless, a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe.

Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three-dimensional picture space.

In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture. Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance, while religious painting borrowed many conventions from sculpture and tended to bright contrasting colors with emphasis on outlines. China saw the flourishing of many art forms: jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and each one is traditionally named after the ruling dynasty. So, for example, Tang dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming dynasty paintings are busy and colorful, and focus on telling stories via setting and composition. Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century.

The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others.

The history of twentieth-century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc. cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art. Thus, Japanese woodblock prints (themselves influenced by Western Renaissance draftsmanship) had an immense influence on impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, in the 19th and 20th centuries the West has had huge impacts on Eastern art with originally western ideas like Communism and Post-Modernism exerting a powerful influence.

Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Theodor W. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist." Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with skepticism and irony. Furthermore, the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than of regional ones.

In "The Origin of the Work of Art", Martin Heidegger, a German philosopher and a seminal thinker, describes the essence of art in terms of the concepts of being and truth. He argues that art is not only a way of expressing the element of truth in a culture, but the means of creating it and providing a springboard from which "that which is" can be revealed. Works of art are not merely representations of the way things are, but actually produce a community's shared understanding. Each time a new artwork is added to any culture, the meaning of what it is to exist is inherently changed.

The creative arts are often divided into more specific categories, typically along perceptually distinguishable categories such as media, genre, styles, and form. Art form refers to the elements of art that are independent of its interpretation or significance. It covers the methods adopted by the artist and the physical composition of the artwork, primarily non-semantic aspects of the work (i.e., figurae), such as color, contour, dimension, medium, melody, space, texture, and value. Form may also include visual design principles, such as arrangement, balance, contrast, emphasis, harmony, proportion, proximity, and rhythm.

In general there are three schools of philosophy regarding art, focusing respectively on form, content, and context. Extreme Formalism is the view that all aesthetic properties of art are formal (that is, part of the art form). Philosophers almost universally reject this view and hold that the properties and aesthetics of art extend beyond materials, techniques, and form. Unfortunately, there is little consensus on terminology for these informal properties. Some authors refer to subject matter and content – i.e., denotations and connotations – while others prefer terms like meaning and significance.

Extreme Intentionalism holds that authorial intent plays a decisive role in the meaning of a work of art, conveying the content or essential main idea, while all other interpretations can be discarded. It defines the subject as the persons or idea represented, and the content as the artist's experience of that subject. For example, the composition of Napoleon I on his Imperial Throne is partly borrowed from the Statue of Zeus at Olympia. As evidenced by the title, the subject is Napoleon, and the content is Ingres's representation of Napoleon as "Emperor-God beyond time and space". Similarly to extreme formalism, philosophers typically reject extreme intentionalism, because art may have multiple ambiguous meanings and authorial intent may be unknowable and thus irrelevant. Its restrictive interpretation is "socially unhealthy, philosophically unreal, and politically unwise".

Finally, the developing theory of post-structuralism studies art's significance in a cultural context, such as the ideas, emotions, and reactions prompted by a work. The cultural context often reduces to the artist's techniques and intentions, in which case analysis proceeds along lines similar to formalism and intentionalism. However, in other cases historical and material conditions may predominate, such as religious and philosophical convictions, sociopolitical and economic structures, or even climate and geography. Art criticism continues to grow and develop alongside art.

Art can connote a sense of trained ability or mastery of a medium. Art can also simply refer to the developed and efficient use of a language to convey meaning with immediacy and or depth. Art can be defined as an act of expressing feelings, thoughts, and observations.

There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes.
A common view is that the "art", particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability, an originality in stylistic approach, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill. Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity. At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled.
A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's "Fountain" is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills. Tracey Emin's "My Bed", or Damien Hirst's "The Physical Impossibility of Death in the Mind of Someone Living" follow this example and also manipulate the mass media. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts. The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However, there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating "hands-on" works of art.

Art has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept. This does not imply that the purpose of Art is "vague", but that it has had many unique, different reasons for being created. Some of these functions of Art are provided in the following outline. The different purposes of art may be grouped according to those that are non-motivated, and those that are motivated (Lévi-Strauss).

The non-motivated purposes of art are those that are integral to being human, transcend the individual, or do not fulfill a specific external purpose. In this sense, Art, as creativity, is something humans must do by their very nature (i.e., no other species creates art), and is therefore beyond utility.
Motivated purposes of art refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) sell a product, or simply as a form of communication.

The functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.

Since ancient times, much of the finest art has represented a deliberate display of wealth or power, often achieved by using massive scale and expensive materials. Much art has been commissioned by political rulers or religious establishments, with more modest versions only available to the most wealthy in society.

Nevertheless, there have been many periods where art of very high quality was available, in terms of ownership, across large parts of society, above all in cheap media such as pottery, which persists in the ground, and perishable media such as textiles and wood. In many different cultures, the ceramics of indigenous peoples of the Americas are found in such a wide range of graves that they were clearly not restricted to a social elite, though other forms of art may have been. Reproductive methods such as moulds made mass-production easier, and were used to bring high-quality Ancient Roman pottery and Greek Tanagra figurines to a very wide market. Cylinder seals were both artistic and practical, and very widely used by what can be loosely called the middle class in the Ancient Near East. Once coins were widely used, these also became an art form that reached the widest range of society.

Another important innovation came in the 15th century in Europe, when printmaking began with small woodcuts, mostly religious, that were often very small and hand-colored, and affordable even by peasants who glued them to the walls of their homes. Printed books were initially very expensive, but fell steadily in price until by the 19th century even the poorest could afford some with printed illustrations. Popular prints of many different sorts have decorated homes and other places for centuries.

Public buildings and monuments, secular and religious, by their nature normally address the whole of society, and visitors as viewers, and display to the general public has long been an important factor in their design. Egyptian temples are typical in that the most largest and most lavish decoration was placed on the parts that could be seen by the general public, rather than the areas seen only by the priests. Many areas of royal palaces, castles and the houses of the social elite were often generally accessible, and large parts of the art collections of such people could often be seen, either by anybody, or by those able to pay a small price, or those wearing the correct clothes, regardless of who they were, as at the Palace of Versailles, where the appropriate extra accessories (silver shoe buckles and a sword) could be hired from shops outside.

Special arrangements were made to allow the public to see many royal or private collections placed in galleries, as with the Orleans Collection mostly housed in a wing of the Palais Royal in Paris, which could be visited for most of the 18th century. In Italy the art tourism of the Grand Tour became a major industry from the Renaissance onwards, and governments and cities made efforts to make their key works accessible. The British Royal Collection remains distinct, but large donations such as the Old Royal Library were made from it to the British Museum, established in 1753. The Uffizi in Florence opened entirely as a gallery in 1765, though this function had been gradually taking the building over from the original civil servants' offices for a long time before. The building now occupied by the Prado in Madrid was built before the French Revolution for the public display of parts of the royal art collection, and similar royal galleries open to the public existed in Vienna, Munich and other capitals. The opening of the Musée du Louvre during the French Revolution (in 1793) as a public museum for much of the former French royal collection certainly marked an important stage in the development of public access to art, transferring ownership to a republican state, but was a continuation of trends already well established.

Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. Museums in the United States tend to be gifts from the very rich to the masses. (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status.

There have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is "necessary to present something more than mere objects" said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was simply an idea, it could not be bought and sold. "Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art ... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form ... [have] endeavored to undermine the art object qua object."

In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. "With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors."

Art has long been controversial, that is to say disliked by some viewers, for a wide variety of reasons, though most pre-modern controversies are dimly recorded, or completely lost to a modern view. Iconoclasm is the destruction of art that is disliked for a variety of reasons, including religious ones. Aniconism is a general dislike of either all figurative images, or often just religious ones, and has been a thread in many major religions. It has been a crucial factor in the history of Islamic art, where depictions of Muhammad remain especially controversial. Much art has been disliked purely because it depicted or otherwise stood for unpopular rulers, parties or other groups. Artistic conventions have often been conservative and taken very seriously by art critics, though often much less so by a wider public. The iconographic content of art could cause controversy, as with late medieval depictions of the new motif of the Swoon of the Virgin in scenes of the Crucifixion of Jesus. The "Last Judgment" by Michelangelo was controversial for various reasons, including breaches of decorum through nudity and the Apollo-like pose of Christ.

The content of much formal art through history was dictated by the patron or commissioner rather than just the artist, but with the advent of Romanticism, and economic changes in the production of art, the artists' vision became the usual determinant of the content of his art, increasing the incidence of controversies, though often reducing their significance. Strong incentives for perceived originality and publicity also encouraged artists to court controversy. Théodore Géricault's "Raft of the Medusa" (c. 1820), was in part a political commentary on a recent event. Édouard Manet's "Le Déjeuner sur l'Herbe" (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world. John Singer Sargent's "Madame Pierre Gautreau (Madam X)" (1884), caused a controversy over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation.
The gradual abandonment of naturalism and the depiction of realistic representations of the visual appearance of subjects in the 19th and 20th centuries led to a rolling controversy lasting for over a century. In the twentieth century, Pablo Picasso's "Guernica" (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's "Interrogation III" (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's "Piss Christ" (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts.

Before Modernism, aesthetics in Western art was greatly concerned with achieving the appropriate balance between different aspects of realism or truth to nature and the ideal; ideas as to what the appropriate balance is have shifted to and fro over the centuries. This concern is largely absent in other traditions of art. The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature.

The definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches to assessing the aesthetic value of art: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans.

The arrival of Modernism in the late nineteenth century lead to a radical break in the conception of the function of art, and then again in the late twentieth century with the advent of postmodernism. Clement Greenberg's 1960 article "Modernist Painting" defines modern art as "the use of characteristic methods of a discipline to criticize the discipline itself". Greenberg originally applied this idea to the Abstract Expressionist movement and used it as a way to understand and justify flat (non-illusionistic) abstract painting:

Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond "high art" to all cultural image-making, including fashion images, comics, billboards and pornography.

Duchamp once proposed that art is any activity of any kind- everything. However, the way that only certain activities are classified today as art is a social construction. There is evidence that there may be an element of truth to this. "" is an art history book which examines the construction of the modern system of the arts i.e. Fine Art. Shiner finds evidence that the older system of the arts before our modern system (fine art) held art to be any skilled human activity i.e. Ancient Greek society did not possess the term art but techne. Techne can be understood neither as art or craft, the reason being that the distinctions of art and craft are historical products that came later on in human history. Techne included painting, sculpting and music but also; cooking, medicine, horsemanship, geometry, carpentry, prophecy, and farming etc.

Following Duchamp during the first half of the twentieth century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other. This resulted in the rise of the New Criticism school and debate concerning "the intentional fallacy". At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist.

In 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled "The Intentional Fallacy", in which they argued strongly against the relevance of an author's intention, or "intended meaning" in the analysis of a literary work. For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting.

In another essay, "The Affective Fallacy," which served as a kind of sister essay to "The Intentional Fallacy" Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text. This fallacy would later be repudiated by theorists from the reader-response school of literary theory. Ironically, one of the leading theorists from this school, Stanley Fish, was himself trained by New Critics. Fish criticizes Wimsatt and Beardsley in his essay "Literature in the Reader" (1970).

As summarized by Gaut and Livingston in their essay "The Creation of Art": "Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms's assumption that the artist's activities and experience were a privileged critical topic." These authors contend that: "Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art. So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work."

Gaut and Livingston define the intentionalists as distinct from formalists stating that: "Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works." They quote Richard Wollheim as stating that, "The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself."

The end of the 20th century fostered an extensive debate known as the linguistic turn controversy, or the "innocent eye debate", and generally referred to as the structuralism-poststructuralism debate in the philosophy of art. This debate discussed the encounter of the work of art as being determined by the relative extent to which the conceptual encounter with the work of art dominates over the perceptual encounter with the work of art.

Decisive for the linguistic turn debate in art history and the humanities were the works of yet another tradition, namely the structuralism of Ferdinand de Saussure and the ensuing movement of poststructuralism. In 1981, the artist Mark Tansey created a work of art titled "The Innocent Eye" as a criticism of the prevailing climate of disagreement in the philosophy of art during the closing decades of the 20th century. Influential theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain rhetorical tropes, in art history and historical discourse was explored by Hayden White. The fact that language is "not" a transparent medium of thought had been stressed by a very different form of philosophy of language which originated in the works of Johann Georg Hamann and Wilhelm von Humboldt. Ernst Gombrich and Nelson Goodman in his book "Languages of Art: An Approach to a Theory of Symbols" came to hold that the conceptual encounter with the work of art predominated exclusively over the perceptual and visual encounter with the work of art during the 1960s and 1970s. He was challenged on the basis of research done by the Nobel prize winning psychologist Roger Sperry who maintained that the human visual encounter was not limited to concepts represented in language alone (the linguistic turn) and that other forms of psychological representations of the work of art were equally defensible and demonstrable. Sperry's view eventually prevailed by the end of the 20th century with aesthetic philosophers such as Nick Zangwill strongly defending a return to moderate aesthetic formalism among other alternatives.

 Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art. Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's "Fountain", the movies, superlative imitations of banknotes, conceptual art, and video games. Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art" (Novitz, 1996). According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the "Daily Mail" criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work. In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood."

Anti-art is a label for art that intentionally challenges the established parameters and values of art; it is term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects. One of these, "Fountain" (1917), an ordinary urinal, has achieved considerable prominence and influence on art. Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art.

Architecture is often included as one of the visual arts; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential in a way that they usually are not in a painting, for example.

Somewhat in relation to the above, the word "art" is also used to apply judgments of value, as in such expressions as "that meal was a work of art" (the cook is an artist), or "the art of deception", (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity. Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered "art" is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly understood that what is not somehow aesthetically satisfying cannot be art. However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3rd of May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'.

The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of what is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the "zeitgeist". Art is often intended to appeal to and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art may be considered an exploration of the human condition; that is, what it is to be human.






</doc>
<doc id="329625" url="https://en.wikipedia.org/wiki?curid=329625" title="Mimesis">
Mimesis

Mimesis (; "mīmēsis", from μιμεῖσθαι "mīmeisthai", "to imitate", from "mimos", "imitator, actor") is a term used in literary criticism and philosophy that carries a wide range of meanings which include "imitatio", imitation, nonsensuous similarity, receptivity, representation, mimicry, the act of expression, the act of resembling, and the presentation of the self.

In ancient Greece, mimesis was an idea that governed the creation of works of art, in particular, with correspondence to the physical world understood as a model for beauty, truth, and the good. Plato contrasted mimesis, or imitation, with diegesis, or narrative. After Plato, the meaning of mimesis eventually shifted toward a specifically literary function in ancient Greek society, and its use has changed and been reinterpreted many times since.

One of the best-known modern studies of mimesis, understood as a form of realism in literature, is Erich Auerbach's "", which opens with a famous comparison between the way the world is represented in Homer's "Odyssey" and the way it appears in the "Bible". From these two seminal texts, the Odyssey being Western and the Bible having been written by a variety of Mid-Eastern writers, Auerbach builds the foundation for a unified theory of representation that spans the entire history of Western literature, including the Modernist novels being written at the time Auerbach began his study. In art history, "mimesis", "realism" and "naturalism" are used, often interchangeably, as terms for the accurate, even "illusionistic", representation of the visual appearance of things.

Mimesis has been theorised by thinkers as diverse as Plato, Aristotle, Philip Sidney, Samuel Taylor Coleridge, Adam Smith, Gabriel Tarde, Sigmund Freud, Walter Benjamin, Theodor Adorno, Erich Auerbach, Paul Ricœur, Luce Irigaray, Jacques Derrida, René Girard, Nikolas Kompridis, Philippe Lacoue-Labarthe, Michael Taussig, Merlin Donald, and Homi Bhabha.

Both Plato and Aristotle saw in mimesis the representation of nature, including human nature, as reflected in the dramas of the period. Plato wrote about mimesis in both "Ion" and "The Republic" (Books II, III, and X). In "Ion", he states that poetry is the art of divine madness, or inspiration. Because the poet is subject to this divine madness, instead of possessing "art" or "knowledge" – "techne" – of the subject (532c), the poet does not speak truth (as characterized by Plato's account of the Forms). As Plato has it, truth is only the concern of the philosopher. As culture in those days did not consist in the solitary reading of books, but in the listening to performances, the recitals of orators (and poets), or the acting out by classical actors of tragedy, Plato maintained in his critique that theatre was not sufficient in conveying the truth (540c). He was concerned that actors or orators were thus able to persuade an audience by rhetoric rather than by telling the truth (535b).

In Book II of "The Republic", Plato describes Socrates' dialogue with his pupils. Socrates warns we should not seriously regard poetry as being capable of attaining the truth and that we who listen to poetry should be on our guard against its seductions, since the poet has no place in our idea of God.

In developing this in Book X, Plato told of Socrates' metaphor of the three beds: one bed exists as an idea made by God (the Platonic ideal, or form); one is made by the carpenter, in imitation of God's idea; one is made by the artist in imitation of the carpenter's.

So the artist's bed is twice removed from the truth. Those who copy only touch on a small part of things as they really are, where a bed may appear differently from various points of view, looked at obliquely or directly, or differently again in a mirror. So painters or poets, though they may paint or describe a carpenter, or any other maker of things, know nothing of the carpenter's (the craftsman's) art, and though the better painters or poets they are, the more faithfully their works of art will resemble the reality of the carpenter making a bed, nonetheless the imitators will still not attain the truth (of God's creation).

The poets, beginning with Homer, far from improving and educating humanity, do not possess the knowledge of craftsmen and are mere imitators who copy again and again images of virtue and rhapsodise about them, but never reach the truth in the way the superior philosophers do.

Similar to Plato's writings about mimesis, Aristotle also defined mimesis as the perfection, and imitation of nature. Art is not only imitation but also the use of mathematical ideas and symmetry in the search for the perfect, the timeless, and contrasting being with becoming. Nature is full of change, decay, and cycles, but art can also search for what is everlasting and the first causes of natural phenomena. Aristotle wrote about the idea of four causes in nature. The first, the formal cause, is like a blueprint, or an immortal idea. The second cause is the material cause, or what a thing is made out of. The third cause is the efficient cause, that is, the process and the agent by which the thing is made. The fourth, the final cause, is the good, or the purpose and end of a thing, known as "telos".

Aristotle's "Poetics" is often referred to as the counterpart to this Platonic conception of poetry. "Poetics" is his treatise on the subject of mimesis. Aristotle was not against literature as such; he stated that human beings are mimetic beings, feeling an urge to create texts (art) that reflect and represent reality.

Aristotle considered it important that there be a certain distance between the work of art on the one hand and life on the other; we draw knowledge and consolation from tragedies only because they do not happen to us. Without this distance, tragedy could not give rise to catharsis. However, it is equally important that the text causes the audience to identify with the characters and the events in the text, and unless this identification occurs, it does not touch us as an audience. Aristotle holds that it is through "simulated representation", mimesis, that we respond to the acting on the stage which is conveying to us what the characters feel, so that we may empathise with them in this way through the mimetic form of dramatic roleplay. It is the task of the dramatist to produce the tragic enactment in order to accomplish this empathy by means of what is taking place on stage.

In short, catharsis can only be achieved if we see something that is both recognisable and distant. Aristotle argued that literature is more interesting as a means of learning than history, because history deals with specific facts that have happened, and which are contingent, whereas literature, although sometimes based on history, deals with events that could have taken place or ought to have taken place.

Aristotle thought of drama as being "an imitation of an action" and of tragedy as "falling from a higher to a lower estate" and so being removed to a less ideal situation in more "tragic" circumstances than before. He posited the characters in tragedy as being better than the average human being, and those of comedy as being worse.

Michael Davis, a translator and commentator of Aristotle writes:
It was also Plato and Aristotle who contrasted mimesis with diegesis (Greek διήγησις). Mimesis "shows", rather than "tells", by means of directly represented action that is enacted. Diegesis, however, is the "telling" of the story by a narrator; the author narrates action indirectly and describes what is in the characters' minds and emotions. The narrator may speak as a particular character or may be the "invisible narrator" or even the "all-knowing narrator" who speaks from above in the form of commenting on the action or the characters.

In Book III of his "Republic" (c. 373 BCE), Plato examines the style of poetry (the term includes comedy, tragedy, epic and lyric poetry): All types narrate events, he argues, but by differing means. He distinguishes between narration or report (diegesis) and imitation or representation (mimesis). Tragedy and comedy, he goes on to explain, are wholly imitative types; the dithyramb is wholly narrative; and their combination is found in epic poetry. When reporting or narrating, "the poet is speaking in his own person; he never leads us to suppose that he is any one else"; when imitating, the poet produces an "assimilation of himself to another, either by the use of voice or gesture". In dramatic texts, the poet never speaks directly; in narrative texts, the poet speaks as himself or herself.

In his "Poetics", Aristotle argues that kinds of poetry (the term includes drama, flute music, and lyre music for Aristotle) may be differentiated in three ways: according to their "medium", according to their "objects", and according to their "mode" or "manner" (section I); "For the medium being the same, and the objects the same, the poet may imitate by narration—in which case he can either take another personality, as Homer does, or speak in his own person, unchanged—or he may present all his characters as living and moving before us" (section III).

Though they conceive of mimesis in quite different ways, its relation with diegesis is identical in Plato's and Aristotle's formulations.

In ludology, mimesis is sometimes used to refer to the self-consistency of a represented world, and the availability of in-game rationalisations for elements of the gameplay. In this context, mimesis has an associated grade: highly self-consistent worlds that provide explanations for their puzzles and game mechanics are said to display a higher degree of mimesis. This usage can be traced back to the essay "Crimes Against Mimesis".

Dionysian imitatio is the influential literary method of imitation as formulated by Greek author Dionysius of Halicarnassus in the 1st century BCE, which conceived it as technique of rhetoric: emulating, adaptating, reworking and enriching a source text by an earlier author.

Dionysius' concept marked a significant depart from the concept of "mimesis" formulated by Aristotle's in the 4th century BCE, which was only concerned with "imitation of nature" instead of the "imitation of other authors". Latin orators and rhetoricians adopted the literary method of Dionysius' "imitatio" and discarded Aristotle's "mimesis".

Mimesis, or imitation, as he referred to it, was a crucial concept for Samuel Taylor Coleridge's theory of the imagination. Coleridge begins his thoughts on imitation and poetry from Plato, Aristotle, and Philip Sidney, adopting their concept of imitation of nature instead of other writers. His departure from the earlier thinkers lies in his arguing that art does not reveal a unity of essence through its ability to achieve sameness with nature. Coleridge claims:
Here, Coleridge opposes imitation to copying, the latter referring to William Wordsworth's notion that poetry should duplicate nature by capturing actual speech. Coleridge instead argues that the unity of essence is revealed precisely through different materialities and media. Imitation, therefore, reveals the sameness of processes in nature.

The Belgian feminist Luce Irigaray used the term to describe a form of resistance where women imperfectly imitate stereotypes about themselves in order to expose and undermine such stereotypes .

In "Mimesis and Alterity" (1993), the anthropologist Michael Taussig examines the way that people from one culture adopt another's nature and culture (the process of mimesis) at the same time as distancing themselves from it (the process of alterity). He describes how a legendary tribe, the "white Indians", or Cuna, have adopted in various representations figures and images reminiscent of the white people they encountered in the past (without acknowledging doing so).

Taussig, however, criticises anthropology for reducing yet another culture, that of the Cuna, for having been so impressed by the exotic technologies of the whites that they raised them to the status of gods. To Taussig this reductionism is suspect, and he argues this from both sides in his "Mimesis and Alterity" to see values in the anthropologists' perspective while simultaneously defending the independence of a lived culture from the perspective of anthropological reductionism.

In "Things Hidden Since the Foundation of the World" (1978), René Girard posits that human behavior is based upon mimesis, and that imitation can engender pointless conflict. Girard notes the productive potential of competition: "It is because of this unprecedented capacity to promote competition within limits that always remain socially, if not individually, acceptable that we have all the amazing achievements of the modern world," but states that competition stifles progress once it becomes an end in itself: "rivals are more apt to forget about whatever objects are the cause of the rivalry and instead become more fascinated with one another."




</doc>
<doc id="59096005" url="https://en.wikipedia.org/wiki?curid=59096005" title="Transformative arts">
Transformative arts

Transformative arts is the use of artistic activities, such as story-telling, painting, and music-making, to precipitate constructive individual and social change.

The individual changes effected through transformative arts are commonly cognitive and emotional. This results from the way participation in a creative process and pursuit of an artistic practice can promote a critical re-evaluation of previously held beliefs, accompanied by unfamiliar feelings, which alters perception of the world, oneself, and others.

The social changes effected through transformative arts occurs when this altered perception manifests in new ways of interacting with others.

Although engagement in artistic activities has been integral to the means by which individuals and communities have sought personal comfort, self-reflection, and group cohesion for thousands of years, the origin of transformative arts as a modern formal concept is commonly attributed to the work of John Dewey.

Dewey espoused four main ideas around which transformative arts pivot. Firstly, art is not an object but an experience, in which one or more persons participate. Secondly, every individual is potentially an artist by way of his or her capacity to participate in this experience, through any artistic activity. Thirdly, such participation inevitably precipitates some kind of transformative change to how the participants think, feel, and behave. Fourthly, art is therefore both psychological and social, transforming not only individual intrapersonal processes, but also interpersonal relationships.

Accordingly, transformative arts are facilitated by artists with the psychological purpose of promoting individual introspection, and with the social purpose of promoting inclusion, reciprocity, and justice.



</doc>
<doc id="429063" url="https://en.wikipedia.org/wiki?curid=429063" title="Information art">
Information art

Information art, which is also known as informatism or data art, is emerging artforms that are inspired by and principally incorporate data, computer science, information technology, artificial intelligence, and related data-driven fields. The information revolution has resulted in over-abundant data that are critical in a wide range of areas, from the Internet to healthcare systems. Related to conceptual art, electronic art and new media art, informatism considers this new technological, economical, and cultural paradigm shift, such that artworks may provide social commentaries, synthesize multiple disciplines, and develop new aesthetics. Realization of information art often take, although not necessarily, interdisciplinary and multidisciplinary approaches incorporating visual, audio, data analysis, performance, and others. Furthermore, physical and virtual installations involving informatism often provide human-computer interaction that generate artistic contents based on the processing of large amounts of data.

Information art has a long history as visualization of qualitative and quantitative data forms a foundation in science, technology, and governance. Information design and informational graphics, which has existed before computing and the Internet, are closely connected with this new emergent art movement. An early example of informatism the 1970 exhibition organized called "Information" at the Museum of Modern Art in New York City (curated by Kynaston McShine). This is the time when conceptual art has emerged as a leading tendency in the United States and internationally. At the same time arose the activities of Experiments in Art and Technology known as E.A.T.

Information art are manifested using a variety of data sources such as photographs, census data, video clips, search engine results, digital painting, network signals, and others. Often, such data are transformed, analyzed, and interpreted in order to convey concepts and develop aesthetics. When dealing with big data, artists may use statistics and machine learning to seek meaningful patterns that drive audio, visual, and other forms of representations. Recently, informatism is used in interactive and generative installations that are often dynamically linked with data and analytical pipelines.






</doc>
<doc id="8217939" url="https://en.wikipedia.org/wiki?curid=8217939" title="Theme (arts)">
Theme (arts)

In art, theme is usually about life, society or human nature, but can be any other subject. Themes are the fundamental and often universal ideas explored in a work. Themes are usually implied rather than explicitly stated. Deep thematic content is not required in a work, but the great majority of works have some kind of thematic content, not always intended by the author. Analysis of changes (or implied change) in dynamic characteristics of the work can provide insight into a particular theme.

A theme is not the same as the subject of a work. For example, the "subject" of "Star Wars" is "the battle for control of the galaxy between the Galactic Empire and the Rebel Alliance". The "themes" explored in the films might be "moral ambiguity" or "the conflict between technology and nature".

Themes differ from motifs in the visual arts in that themes are ideas conveyed by the visual experience as a whole, while motifs are elements of the content. In the same way, a literary story with repeated symbolism related to chess does not make the story's theme the similarity of life to chess. Themes arise from the interplay of the plot, the characters, and the attitude the author takes to them, and the same story can be given very different themes in the hands of different authors.



</doc>
<doc id="518965" url="https://en.wikipedia.org/wiki?curid=518965" title="Artistic freedom">
Artistic freedom

Artistic freedom (or "freedom of artistic expression") can be defined as "the freedom to imagine, create and distribute diverse cultural expressions free of governmental censorship, political interference or the pressures of non-state actors." Generally, artistic freedom describes the extent of independence artists obtain to create art freely. Moreover, artistic freedom concerns "the rights of citizens to access artistic expressions and take part in cultural life - and thus [represents] one of the key issues for democracy." The extent of freedom indispensable to create art freely differs regarding the existence or nonexistence of national instruments established to protect, to promote, to control or to censor artists and their creative expressions. This is why universal, regional and national legal provisions have been installed to guarantee the right to freedom of expression in general and of artistic expression in particular. In 2013, Ms Farida Shaheed, United Nations Special Rapporteur to the Human Rights Council, presented her "Report in the field of cultural rights: The right to freedom of expression and creativity" providing a comprehensive study of the status quo of, and specifically the limitations and challenges to, artistic freedom worldwide. In this study, artistic freedom "was put forward as a basic human right that went beyond the 'right to create' or the 'right to participate in cultural life'." It stresses the range of fundamental freedoms indispensable for artistic expression and creativity, e.g. the freedoms of movement and association. "The State of Artistic Freedom" is an integral report published by arts censorship monitor Freemuse on an annual basis.

Repeatedly, the terms "artistic freedom" and "freedom of artistic expressions" are used as synonyms. Their underlying concepts "art", "freedom" and "expression" comprise very vast fields of discussion: "Art is a very 'subtle' - sometimes also symbolic - form of expression, suffering from definition problems more than any other form." As a result, "[i]t is almost impossible to give a satisfying definition of the concept art. It is even more difficult to define the concepts artistic creativity and artistic expression." UNESCO's 2005 Convention on the Diversity of Cultural Expressions defines cultural expressions as "those expressions that result from the creativity of individuals, groups and societies, and that have cultural content" while the latter "refers to the symbolic meaning, artistic dimension and cultural values that originate from or express cultural identities." In the context of the freedom of (artistic) expressions, "[t]he word expression in the first instance refers to verbalisation of thoughts." Freedom of artistic expression "may mean that we have to tolerate some art that is offensive, insulting, outrageous, or just plain bad. But it is a small price to pay for the liberty and diversity that form the foundation of a free society." Officially, UNESCO defines artistic freedom as "the freedom to imagine, create and distribute diverse cultural expressions free of governmental censorship, political interference or the pressures of non-state actors. It includes the right of all citizens to have access to these works and is essential for the wellbeing of societies." UNESCO puts forth that "artistic freedom embodies a bundle of rights protected under international law." These include:


Legal frameworks to protect and promote artistic freedom reflect the conviction that "[c]ulture constitutes one process of, and space for, democratic debate. The freedom of artistic expression forms its backbone. There is compelling evidence that participation in culture also promotes democratic participation as well as empowerment and well-being of our citizens." Farida Shaheed wrote: "Artists may entertain people, but they also contribute to social debates, sometimes bringing counter-discourses and potential counterweights to existing power centres." Moreover, she emphasized that "the vitality of artistic creativity is necessary for the development of vibrant cultures and the functioning of democratic societies. Artistic expressions and creations are an integral part of cultural life,which entails contesting meanings and revisiting culturally inherited ideas and concepts." According to Freemuse, "[p]opulists and nationalists, who often portray human rights as a limitation on what they claim is the will of the majority, are on the rise globally. As this phenomenon rises, artists continue to play an important role in expressing alternative visions for society." This is why "artists are sometimes responsible for radical criticism." As a result, artistic expressions and artists are suffering censorship and violations worldwide. "Artists are among the first to be silenced by repressive regimes: the poets, playwrights and painters who challenge the status quo are often lone workers, and as such easy targets for an authoritarian state or violent oppressor. When their views fail to accord with the mainstream, the artist is also vulnerable to the censorship of the mob. That is why it is vital that artistic expression is protected."

Freemuse's 2016 report "Arts under threat" shows that "[i]t is not only governments violating the right to artistic freedom. 2016 saw a worrying amount of actions by non-state actors, ranging from militant extremists to peaceful community groups, against art and artists. In some incidences, authorities censored artists based on requests or the interference from civil society groups." Based on this development, "[m]ajor sources of international law across the board recognize freedom of artistic creativity explicitly, or implicitly, as an inherent element of the right to freedom of expression. In these instruments, the individual right to express ideas creatively is often irrevocably linked with the right to receive them." The growing importance of artistic freedom as a specific right is reflected by the introduction of the role of the UN Special Rapporteur in the field of culture in 2009, and other rapporteurs, notably the Special Rapporteur on freedom of expression. 

According to Farida Shaheed, the most explicit legal provisions protecting the right to the freedom indispensable for artistic expression and creativity are the following:



In September 2015, 57 UN Member States reaffirmed the right to freedom of expression including creative and artistic expression through a joint statement. Additionally, in 2015, the Carthage Declaration on the Protection of Artists in Vulnerable Situations was adopted in Tunis.

The following legal instruments do not specifically mention artistic freedom but rather understand it as a pillar of freedom of expression in general related to freedom of thought, conscience and religion. They aim to guarantee the right to freedom of expression or the right to participate in cultural life without specific reference to the arts. 


Artistic freedom first appeared as a distinct right in UNESCO's 1980 Recommendation concerning the Status of the Artist underlining "the essential role of art in the life and development of the individual and of society' and the duty of States to protect and defend artistic freedom." Although not a binding instrument, the Recommendation is an important reference in defining artists' rights across the spectrum worldwide. The 1980 Recommendation serves as a reference for policy development and as a basis for new formulations of cultural policies."Member States, recognizing the essential role of art in the life and development of the individual and of society, accordingly have a duty to protect, defend and assist artists and their freedom of creation. For this purpose, they should take all necessary steps to stimulate artistic creativity and the flowering of talent, in particular by adopting measures to secure greater freedom for artists, without which they cannot fulfill their mission, and to improve their status by acknowledging their right to enjoy the fruits of their work."

"Main article: Convention on the Protection and Promotion of the Diversity of Cultural Expressions"

The 2005 Convention on the Protection and Promotion of the Diversity of Cultural Expressions acknowledges that "the diversity of cultural expressions can only be promoted if human rights and fundamental freedoms are guaranteed." A guiding principle of the 2005 Convention is that "cultural diversity can be protected and promoted only if human rights and fundamental freedoms, such as freedom of expression, information and communication, as well as the ability of individuals to choose cultural expressions, are guaranteed." In this context, governance of culture refers to policies and measures governments establish to promote and to protect all forms of creativity and artistic expressions. The most recent UNESCO Convention in the field of culture and ratified by 146 Parties, it frames the formulation and implementation of different types of legislative, regulatory, institutional and financial interventions to promote the emergence of diverse cultural and creative industry sectors around the world. As a result, it aims to ensure participation in cultural life and to support access to diverse cultural expressions (film, music, performing arts, etc.). The progress and challenges in implementing the Convention is monitored through its Global Report Series Re|Shaping Cultural Policies.

Similar to the aforementioned universal instruments to protect artists and artistic freedom, "[i]n national constitutions (...), freedom of artistic creativity is often located within the strongly-protected right to freedom of expression." Certain countries also "recognize the freedom of artistic expression within the ambit of the right to science and culture." The following national legislative measures are listed in alphabetical order. The list is to be completed. 

Adopted on 23 May 2013 by "Direction générale des arts (DGA)", the decree "Décret portant statut de l'artiste au Burkina Faso" envisages improving the social protection and the living conditions of artists, particularly the social security of employed artists and freelancers, the return of social contributions of artists and the complement dispositive for mutual accountability.

In Canada, the Canadian Charter of Rights and Freedoms protects artistic expression.

In July 2016, France amended its legislation in order to extend it with the legal protection of artistic freedom, architecture and heritage. For the first time in international law, artistic expressions are established as public goods and the "dissemination of artistic creation is free". This implies not only that artists are free to create but also that the wider public has access to it. As a result, art and artistic expressions cannot be censored or simply excluded from exhibits and other events. 

Article 5 of the German Basic Law contains a special paragraph that connects the right to freedom of expression with the right to freely develop the arts and sciences."

On 19 June 2017, Mexico published its "Ley General de Cultura y Derechos Culturales" promising strong protection for artistic freedom and artists and cultural professionals, a provision specifically needed given the alarming conditions under which Mexican artists, journalists and cultural professionals currently work.

On 6 September 2018, the Spanish Congress of Deputies unanimously ratified a proposal assigned to elaborate a "Estatuto del Artista y del Profesional de la Cultura". Broadly, the decree aims to protect and promote artists with regard to taxation, their work security and legal protection. 

Article 1 (2) of the Swedish Fundamental Law explicitly includes the freedom of artistic creation as part of the key purposes of freedom of expression: "The purpose of freedom of expression under this Fundamental Law is to secure the free exchange of opinion, free and comprehensive information, and freedom of artistic creation."

On 20 June 2016, Togo adopted its "Statut de l'artiste". Its major objective is to acknowledge artists as individuals and their moral role in society, their contributions towards the intellectual sphere protected by copyright. It defines the rights and duties linked to artistic professions and aims to promote creativity and to protect artists socially.

Adopted in 2014, article 42 of the Tunisian Constitution states: "The right to culture is guaranteed. The freedom of creative expression is guaranteed. The State encourages cultural creativity and supports the strengthening of national culture, its diversity and renewal, in promoting the values of tolerance, rejection of violence, openness to different cultures and dialogue between civilizations."

In the U.S., the first amendment protects artistic expression. According to the Court, freedom of artistic creativity is an element of the respect for freedom of self-expression, one of the core values of the First Amendment. However, the U.S. Supreme Court has never considered artistic freedom as a distinct category akin to political or commercial speech: "it rather addresses the various forms of art in their relation to the First Amendment on a contextual basis."

The International Cities of Refuge Network (ICORN) explains the purpose of its existence with the following statement:"Writers and artists are especially vulnerable to censorship, harassment, imprisonment and even death, because of what they do. They represent the liberating gift of the human imagination and give voice to thoughts, ideas, debate and critique, disseminated to a wide audience. They also tend to be the first to speak out and resist when free speech is threatened."
Freemuse's report (2018) demonstrates that artistic freedom """is being shut down in every corner of the globe, including in the traditionally democratic West. According to Freemuse's 2016 report, the music industry is the main target of serious violations, and second to film in overall violations, including non-violent censorship. The most serious violations included the murder of Pakistani Qawwali singer Amjad Sabri and the killing of Burundi musician Pascal Treasury Nshimirimana. In 2019, Karima Bennoune, UN Special Rapporteur in the field of cultural rights, underlines that "the freedom of artistic expression and creativity of persons with disabilities, women or older persons" remains significantly restricted. She states that "many cultural rights actors have not incorporated a gender perspective into their work, while many women's rights advocates have not considered cultural rights issues." Referring to Freemuse's 2016 report, UNESCO stresses that "laws dealing with terrorism and state security, defamation, religion and 'traditional values' have been used to curb artistic and other forms of free expression."

Moreover, new digital technologies, including social media platforms, are challenging artistic freedom: "Art in the online and digital space continues to challenge authorities and corporations who are quick to react by closing down expression rather than using it as an opportunity to foster it." Social media and music streaming channels, like Instagram and SoundCloud are becoming the platforms on which artists publicly display and promote their work. However, they also bring with them threats to rights and freedoms. Online trolls often intimidate artists to withdraw their work. Additionally, growing digital surveillance has a corrosive effect on artistic freedom. Many platforms have established mechanisms, such as Instagram's guidelines on 'standards of behavior' whose formulations are very vague. This provides disproportionate power to individuals and organizations who use the platform's reporting processes to get individual artworks removed, and sometimes entire accounts blocked. In addition, the impact of algorithms on diversity of content is another area of concern: platforms display a plethora of cultural offerings, but also control not only sales but also communication and the recommendation algorithms (e.g. adapting offered content to the profile of each internet user). These algorithms finally serve to promote certain contents while oppressing others.

In conclusion, new digital technologies - while providing a platform for the distribution of artistic content - may interrupt the flow of ideas of artists and curtail their artistic freedom.

In the 10th Anniversary UN Report on Cultural Rights, Ole Reitov, former exectutive director of Freemuse, underscores the progressive fact that "artistic freedom is no longer a 'marginalized' issue in the 'world of freedom of expression'". Since Farida Shaheed's report and inspired by lobbying from arts and human rights NGOs, efforts to promote artistic freedom have multiplied across the entire United Nations system: "The UN Universal Periodic Review provides an opportunity for NGOs, among others, to make submissions on States' failures to meet human rights standards, including artistic freedom. New calls for a UN Action Plan on the Safety of Artists and Audiences (similar to the one for journalists) have been put forward." As UNESCO's Global Report "Re|shaping Cultural Policies" (2018) shows, the number and capacity of organizations monitoring artistic freedom is increasing. "In this domain as well, cities are taking valuable initiatives by providing safe havens for artists at risk." As the list above shows, "measures to support the economic and social rights of artists are appearing increasingly in national legislation, especially in Africa."
Despite the progress made and legal instruments established to promote and protect freedom of artistic expressions, "there is urgent need for monitoring and surveillance, essential if these freedoms are to become a permanent reality."

Karima Bennoune notes that the increasing number of reported attacks perpetrated by State and non-State actors against cultural professionals reflects the boosting capacity of monitoring artistic freedom. She states the UNESCO global reports monitoring the implementation of the 2005 Convention on the Protection and Promotion of Cultural Expressions have been "[o]f particular relevance". The reports provide a monitoring framework comprising four overarching goals to enhance cultural policies worldwide. One of these goals aims to "Promote Human Rights and Fundamental Freedoms" and encompasses artistic freedom as an "area of monitoring" incorporating core indicators to measure achievements regarding the rights and protection of artists. Additionally, the framework relates artistic freedom to the Sustainable Development Goal (SDG) 16 of the UN 2030 Agenda, which aims to "'Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels". Specifically, the SDG's target 16.10 aims to "ensure public access to information and protect fundamental freedoms, in accordance with national legislation and international agreements".

Additionally, there are many other initiatives advocating and monitoring artistic freedom. Alongside other organizations documenting violations against freedom of artistic expression (such as Arterial Network, Artists at Risk Connection, PEN International and the International Federation of Library Associations and Institutions), Freemuse is an independent international organization particularly monitoring the freedom of expression of musicians and composers worldwide. "Freemuse's reports collated from all over the world show that artists are increasingly facing censorship, persecution, incarceration or death, because of their work."

In order to monitor the actions taken to implement the 1980 Recommendation concerning the Status of the Artists, the Secretariat of the 2005 UNESCO Convention (see below) runs a global survey every four years gathering information from Members States, NGOs and INGOs and prepares a report, which is then submitted to the General Conference.




</doc>
<doc id="60394746" url="https://en.wikipedia.org/wiki?curid=60394746" title="George Harcourt (painter)">
George Harcourt (painter)

George Harcourt (1868-1947) was a Scottish painter.

From Dumbarton, Scotland, George Harcourt was born in 1868 and died in 1947. Harcourt was a portrait and figure painter, known for painting influential members of society. He studied at the Herkomer School of Art in Bushey from 1889-92 and later went on to become the head of school. He rose to become the President of the Royal Society of Portrait Painters in 1945 following a spell as vice-president (1934-45) and was also the Director of the Royal Academy. He first became an associate of the Royal Academy in 1919 (ARA) and was then elected as an Academician (RA) in 1926. In 1944 he was elected as a senior Academician (Senior RA) and became Director of Schools in 1927.

After studying art in Dumbarton, George Harcourt initially worked as an interior decorator of salons and luxury cabins at the local shipyard "Denny Brothers Shipbuilding". In 1888 he went to Bushey (north of London) and enrolled in the "Herkomer School of Art". The school, founded by the German-born painter Hubert von Herkomer (1849-1914) in 1883, enjoyed a very good reputation in England at the time and was known for its unconventional educational practice. As one of the best students, Harcourt later became a lecturer at this art school. Later, he taught at the prestigious London Slade School of Fine Art, in which his daughter Anne studied.

Over the years, Harcourt's style of painting changed from a Pre-Raphaelite influenced naturalism (here mainly genre paintings, more rarely landscapes) to an almost photorealistic portraiture. This perfection not only gave him a great reputation among colleagues, but also secured him a good income as a portraitist of the British upper class. 

He submitted to the Royal Academy of Arts his diploma thesis ("A Portrait of Miss Anne Harcourt"), this proved he had skills that went beyond the naive kitsch of his earlier years. 

In 1893 George Moore reviewed the artworks at the Royal Academy commenting in The New Review, London, on a portrait exhibited there by George Harcourt - "At the Window". 


Harcourt received in 1912 a gold medal at the International Exhibition in Amsterdam. In 1921 he submitted to the jury of the Royal Academy of Art, a portrait of his daughter Anne. He was admitted in 1926 as a full member of the Academy (RA). Up to this point he was an "Associate Royal Academician" (ARA), which he had held since 1919. He also briefly became Deputy Director of the Royal Academy Schools. In 1923 he got another gold medal in the exhibition at the Salon de Paris.

"The Exhibition of the Royal Academy of Arts" (1927)

"Memorial Exhibition of George Harcourt and Francis Hodge". Royal Institute of Painters in Water Colour (1949).

"Stand to your work". Hubert Herkomer and his students. Watford Museum, Watford 1983, . (Catalog of the exhibition in the Watford Museum "A passion for work: Sir Hubert von Herkomer 1849-1914", September 1983).



</doc>
<doc id="18958074" url="https://en.wikipedia.org/wiki?curid=18958074" title="Closure (business)">
Closure (business)

Closure is the term used to refer to the actions necessary when it is no longer necessary or possible for a business or other organization to continue to operate. Closure may be the result of a bankruptcy, where the organization lacks sufficient funds to continue operations, as a result of the proprietor of the business dying, as a result of a business being purchased by another organization (or a competitor) and shut down as superfluous, or because it is the non-surviving entity in a corporate merger. A closure may occur because the purpose for which the organization was created is no longer necessary.

While a closure is typically of a business or a non-profit organization, any entity which is created by human beings can be subject to a closure, from a single church to a whole religion, up to and including an entire country if, for some reason, it ceases to exist.

Closures are of two types, voluntary or involuntary. Voluntary closures of organizations are much rarer than involuntary ones, as, in the absence of some change making operations impossible or unnecessary, most operations will continue until something happens that causes a change requiring this situation.

The most common form of voluntary closure would be when those involved in an organization such as a social club, a band, or other non-profit organization decide to cease operating. Once the organization has paid any outstanding debts and completed any pending operations, closure may simply mean that the organization ceases to exist.

If an organization has debts that cannot be paid, it may be necessary to perform a liquidation of its assets. If there is anything left after the assets are converted to cash, in the case of a for-profit organization, the remainder is distributed to the stockholders; in the case of a non-profit, by law any remaining assets must be distributed to another non-profit.

If an organization has more debts than assets, it may have to declare bankruptcy. If the organization is viable, it may reorganizes itself as a result of the bankruptcy and continue operations. If it is not viable for the business to continue operating, then a closure occurs through a bankruptcy liquidation: its assets are liquidated, the creditors are paid from whatever assets could be liquidated, and the business ceases operations.

Possibly the largest "closure" in history (but more closely analogous to a demerger) was the split of the Soviet Union into its constituent countries. In comparison, the end of East Germany can be considered a merger rather than a closure as West Germany assumed all of the assets and liabilities of East Germany. The end of the Soviet Union was the equivalent of a closure through a bankruptcy liquidation, because while Russia assumed most of the assets and responsibilities of the former Soviet Union, it did not assume all of them. There have been issues over who is responsible for unpaid parking tickets accumulated by motor vehicles operated on behalf of diplomatic missions operated by the former Soviet Union in other countries, as Russia claims it is not responsible for them.

Several major business closures include the bankruptcy of the Penn Central railroad, the Enron scandals, and MCI Worldcom's bankruptcy and eventual merger into Verizon.


</doc>
<doc id="21041941" url="https://en.wikipedia.org/wiki?curid=21041941" title="Viability study">
Viability study

A Viability study is an in depth investigation of the profitability of the business idea to be converted into a business enterprise.

This type of report studies a situation (for example, a problem or opportunity) and the plan for doing something about it, then determines whether that plan is "feasible". This would involve determining whether it is technologically possible to achieve and whether it is practical in the current technological, economical and social scenario. The feasibility report does not provide a simple "Yes" or "No" answer, but is used in the analysis of a decision. It is not just a tool to provide a recommendation, it is also used to gather data and give reasoning behind the recommendation given, to be later used in evaluation.

This study is the most important especially to people who plan to start their own business.

This type of report examines either a stated need or a selection of choices, or in some cases both. The report is a collection of analysis and evaluation of the situation, and generally will examine the strengths and weaknesses, opportunities and threats in the situation, take them into account, and be combined with the feasibility report in order to give a recommendation. Sometimes a recommendation can be given to do nothing, if all options currently possible would prove unbeneficial. The recommendation report answers the question "Which option should we choose?" (or "Which are the best options?") by allowing a recommendation to be made. This can be linked into the analysis of the continuum of choice in the scenario.

This type of report provides an opinion or judgment rather that is based on the above two report, along with additional data available. It provides a studied opinion on the value or worth of something. This type of report compares the object of the analysis to a set of requirements (or criteria) and determines how well it meets those requirements. Generally, the evaluation report will feature an overall recommendation on the course of action to be taken.


</doc>
<doc id="18166327" url="https://en.wikipedia.org/wiki?curid=18166327" title="Business acumen">
Business acumen

Business acumen ("Business savvy" and "business sense" are often used as synonyms) is keenness and quickness in understanding and dealing with a "business situation" (risks and opportunities) in a manner that is likely to lead to a good outcome. Additionally, business acumen has emerged as a vehicle for improving financial performance and leadership development. Consequently, several different types of strategies have developed around improving business acumen.

In his 2012 work, "Seeing the Big Picture, Business Acumen to Build Your Credibility, Career, and Company", Kevin R. Cope put forward that an individual who possesses business acumen views the business with an "executive mentality" - they understand how the moving parts of a company work together to make it successful and how financial metrics like profit margin, cash flow, and stock price reflect how well each of those moving parts is doing its job. Cope proposes that an individual who has the following five abilities could be described as someone having a strong sense of business acumen: 

Raymond R. Reilly of the Ross School of Business at the University of Michigan and Gregory P. Reilly of the University of Connecticut document traits that individuals with business acumen possess: 
Thus, developing stronger business acumen means a more thoughtful analysis, clearer logic underlying business decisions, closer attention to key dimensions of implementation and operation, and more disciplined performance management.

Financial literacy is a comprehensive understanding of the drivers of growth, profitability, and cash flow; an organization's financial statements; key performance measures; and the implications of decisions on value creation. In a SHRM article entitled, "Business Acumen Involves More Than Numbers" Chris Berger, member of human resources at CTPartners, explains that business acumen starts with the ability to understand how a company makes decisions, and that leaders must be financially literate and be able to understand numbers on company financial statements. It entails the ability to take the knowledge of business fundamentals and use it to think strategically and then take appropriate action.

According to E. Ted Prince, "Financial literacy is almost never the need for senior managers and high potentials. Most already possess degrees in business, including MBAs, and many have also had experience in the business sides of their professional roles. The need for these managers is to understand how their actions and their behavior impact their financial decision-making and how this in turn affects financial outcomes at the unit and the corporate level." It's evident that an individual with business acumen has some level of financial understanding and knowledge - but someone who is financially literate doesn't necessarily possess strong business acumen.

Bob Selden, faculty member of Mobilizing People, a leadership development program based in Switzerland, observes a complementary relationship between business acumen and leadership. Selden states the importance of nurturing both the development of strategic skills and that of good leadership and management skills in order for business leaders to achieve effectiveness.

A study published in Human Resource Management International Digest titled, Business acumen: a critical concern of modern leadership development: Global trends accelerate the move away from traditional approaches, reveals why traditional leadership development approaches, which rely on personality and competency assessments as the scientific core of their approach, are failing. The paper demonstrates the importance of business acumen in leadership-development approaches and contends that business acumen will have an increasing impact on leadership development and HR agendas. Research into this relationship resulted in the creation of the Perth Leadership Outcome Model, which links financial outcomes to individual leadership traits.

In a study that interviewed 55 global business leaders, business acumen was cited as the most critical competency area for global leaders.

In their 2011 work, "The Leadership Pipeline", Ram Charan, Stephen Drotter, and James Noel study the process and criteria for selecting a group manager, and suggest that the process and criteria are similar for selecting a CEO. According to them an obvious criterium for selecting a leader is well-developed business acumen.

An organization full of high business acumen individuals can expect to see leaders with a heightened perspective that translates into an ability to inspire and excite the organization to achieve its potential.

Programs designed to improve an individual or group's business acumen have supported the recognition of the concept as a significant topic in the corporate world. Executive Development Associates' 2009/2010 survey of Chief Learning Officers, Senior Vice Presidents of Human Resources, and Heads of Executive and Leadership Development listed business acumen as the second most significant trend in executive development. A 2011 report explores the impact of business acumen training on an organization in terms of intangibles and more tangible expressions of value. The findings support the notion that business acumen is a learned skill - developed on the job by learning the required skills from knowledge mentors while working in different employment positions. They also suggest that the learning process ranges widely, from structured internal company training programs, to an individual's self-chosen moves from one position to another.

The combination of these reports and surveys indicate that business acumen is a learned skill of increasing importance within the corporate world. There are different types of business acumen development programs available:

A business simulation is another corporate development tool used to increase business acumen. Several companies offer business simulations as a way to educate mid-level managers and non-financial leaders within their organization on cash flow and financial-decision-making processes. Their forms can vary from computer simulations to boardgame-style simulations.

The advent of personal assessments for business acumen is based in the emerging theories of behavioral finance and attempts to correlate innate personality traits with positive financial outcomes. This method approaches business acumen not as entirely based in either knowledge or experience, but on the combination of these and other factors which comprise an individual's financial personality or "signature." The results from this research have been limited, but noteworthy.


</doc>
<doc id="26085407" url="https://en.wikipedia.org/wiki?curid=26085407" title="Business interaction networks">
Business interaction networks

Business interaction networks are networks that allow businesses and their communities of interest to collaborate and do business online securely via the Internet. 

Mary Johnston Turner first discussed the concept in a Network World opinion piece in August, 1995, and attributed the first advocacy for the concept to the now-defunct BBN Planet, the ISP division of BBN Technologies.


</doc>
<doc id="16416558" url="https://en.wikipedia.org/wiki?curid=16416558" title="Business idea">
Business idea

A business idea is a concept that can be used for financial gain that is usually centered on a product or service that can be offered for money. An idea is the base of the pyramid when it comes to the business as a whole.

The characteristics of a promising business idea are:

A business idea is often linked to its creator who needs to identify the business' value proposition in order to launch to market and establish competitive advantage.

For businesses this could mean: creating new ideas, new product development through research and development or improving existing services. Innovation can be the central focus of a business and this can help them to grow and become a market leader if they execute their ideas properly. Businesses that are focused on innovation are usually more efficient, cost effective and productive. Successful innovation should be built into the business strategy, where you can create a culture of innovation and drive forward creative problem solving.

These successful companies were built on sheer innovation and we can see how valuable they have become in the short time they have been around or have been focusing on innovation. When we compare Tesla's value to that of General Motors, we see that the market capitalization of General Motors is $53.98 billion today in which the company has been around since 1908 whereas Tesla was founded in 2003 and has achieved 50% of General Motors value within 12 years.

A unique selling point (USP) is the factor that makes a company or a product stand out from its competitors, whether it is through; pricing, quality, customer service or innovation.

Each successful company has a unique selling proposition (USP). A USP can be created through the element of being first to a market, for example Uber was the first company to allow for taxicab hailing via mobile app. Because Uber had reached this market first, it had a USP and therefore it received loyal customers. However; with fierce competition copying Uber's business model, Uber has had to develop its service through innovation. 

Business ideas that solve problems are fundamental to developing our world and companies such as Curemark are one of many who do this. Curemark is a biotech company founded by Dr Joan Fallon, who noticed that a lot of the children she treated were low on an enzyme for processing protein and since then she has quit her job and has built Curemark to solve this problem. Curemark has now raised $50 million and is on its way to solving a problem that truly exists.

Profitability is a business's ability to generate earnings compared to its costs over a certain period of time. This is possibly the most important aspect of any business idea in the long term, as this is what makes a business survive in order to keep having the impact that it has. Profitable ideas need a strong revenue stream against its costs and this tends to create the success of the business, however some companies defy this and make losses to begin with, yet are still exceptional business ideas that are worth billions.




</doc>
<doc id="22697384" url="https://en.wikipedia.org/wiki?curid=22697384" title="Business sector">
Business sector

In economics, the business sector or corporate sector - sometimes popularly called simply "business" - is "the part of the economy made up by companies". It is a subset of the domestic economy,
excluding the economic activities of general government, of private households, and of non-profit organizations serving individuals. An alternative analysis of economies, the three-sector theory, subdivides them into: 


In the United States the business sector accounted for about 78 percent of the value of gross domestic product (GDP) . Kuwait and Tuvalu each had business sectors accounting for less than 40% of GDP .





</doc>
<doc id="2886264" url="https://en.wikipedia.org/wiki?curid=2886264" title="Core business">
Core business

The core business of an organization is an idealized construct intended to express that organization's "main" or "essential" activity. 

The corporate trend in the mid-20th Century, of acquiring new enterprises and forming conglomerates, enabled corporations to reduce costs funds and similar investment vehicles, and sometimes the following of a popular trend among corporate management seeking to appeal to current and impress investors.

Core business process means that a business's success depends not only on how well each department performs its work, but also on how well the company manages to coordinate departmental activities to conduct the core business process, which is;

1. The market-sensing process
Meaning all activities in gathering marketing intelligence and acting on the information.

2. The new-offering realization process
Covering all activities in research, development and launching new quality offerings quickly and within budget.

3. The customer acquisition process
all the activities defining the target market and prospecting for new customers

4. The customer relationship management process
all the activities covering building deeper understanding, relationships and offerings to individual customers.

5. The fulfillment management process
all the activities in receiving and approving orders, shipping out on time and collecting payment.

To be successful, a business needs to look for competitive advantages beyond its own operations. The business needs to look at the competitiveness value chain of suppliers, distributors and customers. Many companies today have partnered with specific suppliers and distributors to create a superior value delivery network.

Kotler & Keller, Marketing management, 2009, p76

Core competency


</doc>
<doc id="2633364" url="https://en.wikipedia.org/wiki?curid=2633364" title="Registered office">
Registered office

A registered office is the official address of an incorporated company, association or any other legal entity. Generally it will form part of the public record and is required in most countries where the registered organization or legal entity is incorporated. A registered physical office address is required for incorporated organizations to receive official correspondence and formal notices from government departments, investors, banks, shareholders and the general public.

In the United Kingdom and many other common law countries, the registered office address does not have to be where the organization conducts its actual business or trade, and it is not unusual for law firms, accountants or incorporation agents to provide the official registered office address service. In the United Kingdom all statutory correspondence for an incorporated organization (e.g. formal notices, service of process, tax and government communications) is posted or hand delivered to the registered office address as recorded on the Companies House register. A registered physical office address is required for incorporated organizations to receive official correspondence and formal notices from government departments, investors, banks, shareholders and the general public.

Under regulations implemented in the UK on 1 October 2009, company directors may now also use a registered office address instead of their private home address for contact on the Companies House register.

The company's full registered name must be visible to the public on the premises. The Company's records previously had to be kept at the registered office and available for public inspection. Since 1 October 2009 it has been possible for companies to designate a single alternative inspection location (SAIL) as a place to keep their records which must be available for public inspection.

In many other countries the address with which a company is registered must be where its headquarters or seat is located, and this will often determine the subnational registry at which the company must be registered.



</doc>
<doc id="8613151" url="https://en.wikipedia.org/wiki?curid=8613151" title="Business economics">
Business economics

Business economics is a field in applied economics which uses economic theory and quantitative methods to analyze business enterprises and the factors contributing to the diversity of organizational structures and the relationships of firms with labour, capital and product markets. A professional focus of the journal "Business Economics" has been expressed as providing "practical information for people who apply economics in their jobs."

Business economics is an integral part of traditional economics and is an extension of economic concepts to the real business situations. It is an applied science in the sense of a tool of managerial decision-making and forward planning by management. In other words, business economics is concerned with the application of economic theory to business management. Business economics is based on microeconomics in two categories: positive and normative. 

Business economics is concerned with economic issues and problems related to business organization, management, and strategy. Issues and problems include: an explanation of why "corporate" firms emerge and exist; why they expand: horizontally, vertically and spacially; the role of entrepreneurs and entrepreneurship; the significance of organizational structure; the relationship of firms with employees, providers of capital, customers, and government; and interactions between firms and the business environment.

The term 'business economics' is used in a variety of ways. Sometimes it is used as synonymously with industrial economics/industrial organisation, managerial economics, and economics for business. Still, there may be substantial differences in the usage of 'economics for business' and 'managerial economics' with the latter used more narrowly.
One view of the distinctions between these would be that business economics is wider in its scope than industrial economics in that it would be concerned not only with "industry" but also businesses in the service sector. Economics for business looks at the major principles of economics but focuses on applying these economic principles to the real world of business. Managerial economics is the application of economic methods in the managerial decision-making process.

Many universities offer courses in business economics and offer a range of interpretations as to the meaning of the word. The Bachelor of Business Economics (BBE) Program at University of Delhi is designed to meet the growing need for an analytical and quantitative approach to problem solving in the changing corporate world by the application of the latest techniques evolved in the fields of economics and business. The Autonomous University of Barcelona (UAB), the Universidad Pública de Navarra (UPNa) and the University of the Balearic Islands (UIB) developed an official Master of Science in Management, Organization and Business Economics focused on management and business topics to train professionals in the study of organizations, on a conceptual and quantitative basis. To achieve this, advanced analysis tools are used from the fields of Neoclassical economics, New institutional economics, Statistics, Econometrics and Operations research. This focus is complemented with contributing ideas and theories to develop the necessary instruments to facilitate the management of sophisticated and complex organizations.

The program at Harvard University uses economic methods to analyze practical aspects of business, including business administration, management, and related fields of business economics. The University of Miami defines business economics as involving the study of how we use our resources for the production, distribution, and consumption of goods and services. This requires business economists to analyze social institutions, banks, the stock market, the government and their relationships with labor negotiations, taxes, international trade, and urban and environmental issues.

Courses at the University of Manchester interpret business economics to be concerned with the economic analysis of how businesses contribute to welfare of society rather than on the welfare of an individual or a business. This is done via an examination of the relationship between ownership, control and firm objectives; theories of the growth of the firm; the behavioural theory of the firm; theories of entrepreneurship; the factors that influence the structure, conduct and performance of business at the industry level.

Italian universities borrow their concept of business economics from the tradition of Gino Zappa, for example a standard course at the Politecnico di Milano involves studying corporate governance, accounting, investment analysis, budgeting and business strategy.

La Trobe University of Melbourne, Australia associates business economics with the process of demand, supply and equilibrium coordinating the behaviour of individuals and businesses in the market. Also, business economics extends to government policy, economic variables and international factors which influence business and competition.





</doc>
<doc id="47827092" url="https://en.wikipedia.org/wiki?curid=47827092" title="Business guru">
Business guru

A business guru or management guru is a leading authority on business practices and can be defined as 'a person with influential ideas or theories about business'. The earliest use of the term business guru can be tracked back to the 1960s being used in "Business Week". There are no existing qualifications that make someone a business guru. Anyone can become a business guru by making impact in a particular business field. It's also possible to claim to be a business guru at any time. It's not a title. The lists of people who have been accepted as business gurus have constantly changed over time. However, there are some people who have been accepted by a great majority as a business guru and also some organizations which have created their own lists of gurus. One English writer has described management gurus as "overwhelmingly a US phenomenon."

There is no definitive list of business gurus, but some writers have proposed "personal" lists. These lists are mostly created by organizations such as business magazines or management writers. There have been many business guru lists created through history.

A list consisting of people who are included in almost all of the lists created, collectively known as the "Famous Five", are: Frederick Winslow Taylor, Michael Porter, Alfred Sloan, Peter Drucker, and Douglas McGregor.

In 2001, "Harvard Business Review" asked the gurus to name their favorite gurus. The people named were Peter Drucker, James March and Herbert Simon.

Another list includes Peter Drucker, Michael Porter, and Tom Peters as the three leading gurus of our time.

There are also many gurus who have emerged and disappeared through history. For example, the Japanese were known for making improvements to the business world and bringing out gurus in the 1980s, which included Keniche Ohmae and Akio Morita. Then European gurus emerged, which included Yves Doz, Geert Hofstede, Manfred Kets De Vries and Charles Handy.

One management expert, Gary Hamel, says there have been "few genuine breakthroughs" since the work of Taylor and Max Weber. In his book, Hamel says that management is "stuck in a time warp." Similarly, even one of the authors of a book about management gurus warns that management theory is "not served well by fads," citing Enron as a "management fad for its supposed culture of innovation."



</doc>
<doc id="30781093" url="https://en.wikipedia.org/wiki?curid=30781093" title="Religion and business">
Religion and business

Religion and business have throughout history interacted in ways that relate to and affected one another, as well as influenced sociocultural evolution, political geographies, and labour laws.

Some areas, countries or cities have an economy based on religious tourism. Examples include Islamic Hajj tourism and Vatican tourism. The hotels and markets of important religious places are a source of income to the locals.

The boards or shines sometimes receive so much in donations that governments to take it under control for proper utilization of resources and management. The annual revenues of most of the religious places are not regulated.

Judaism outlines requirements of accurate weights and measurements in commerce, as well as prohibitions on monetary deception, verbal deception and misrepresentation.

Globally, halal products comprise a US$2 trillion industry.

As of 2003, the kosher industry had certified more than 100,000 products, which total approximately US$165 billion in sales annually.

United Kingdom labour law prohibits employer discrimination based on religion, belief, or any lack thereof.

In the United States, labor laws including Title VII of the Civil Rights Act of 1964 prohibit businesses from discriminating against employees based on the basis of religion. Business law is also at times applied to religious organizations, due to their status as incorporated entities.





</doc>
