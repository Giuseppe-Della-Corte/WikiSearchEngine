<doc id="45451212" url="https://en.wikipedia.org/wiki?curid=45451212" title="Two-factor theory of intelligence">
Two-factor theory of intelligence

Charles Spearman developed his two-factor theory of intelligence using factor analysis. His research not only led him to develop the concept of the "g factor" of general intelligence, but also the "s" factor of specific intellectual abilities. L. L. Thurstone, Howard Gardner, and Robert Sternberg also researched the structure of intelligence, and in analyzing their data, concluded that a single underlying factor was influencing the general intelligence of individuals. However, Spearman was criticized in 1916 by Godfrey Thomson, who claimed that the evidence was not as crucial as it seemed. Modern research is still expanding this theory by investigating Spearman's law of diminishing returns, and adding connected concepts to the research.

In 1904, Charles Spearman had developed a statistical procedure called factor analysis. In factor analysis, related variables are tested for correlation to each other, then the correlation of the related items are evaluated to find clusters or groups of the variables. Spearman tested how well people performed on various tasks relating to intelligence. Such tasks include: distinguishing pitch, perceiving weight and colors, directions, and mathematics. When analyzing the data he collected, Spearman noted that those that did well in one area also scored higher in other areas. With this data, Spearman concluded that there must be one central factor that influences our cognitive abilities. Spearman termed this general intelligence "g".

Due to the controversy of the structure of intelligence, other psychologists also published their relevant research. Other than Charles Spearman, three others developed a hypothesis regarding the structure of intelligence. L. L. Thurstone tested subjects on 56 different abilities; from his data he established seven primary mental abilities relating to intelligence. He categorized them as: spatial ability, numerical ability, word fluency, memory, perceptual speed, verbal comprehension, and inductive reasoning. Other researchers, interested in this new research study, analyzed Thurstone's data, discovering that those scored high in one category often did well in the others. This finding gives support that there is an underlying factor influencing them, namely "g".

Howard Gardner suggested in his theory of multiple intelligences that intelligence is formed out of multiple abilities. He recognized eight intelligences: linguistic, musical, spatial, intrapersonal, interpersonal, logical-mathematical, bodily-kinesthetic, and naturalist. He also considered the possibility of a ninth intelligent ability, existential intelligence. Gardner proposed that individuals who excelled in one ability would lack in another. Instead, his results showed that each of his eight intelligences correlate positively with each other. After further analysis, Gardner found that logic, spatial abilities, language, and mathematics are all linked in some way, giving support for an underlying "g" factor that is prominent in almost all intelligence in general.

Robert Sternberg agreed with Gardner that there were multiple intelligences, but he narrowed his scope to just three in his triarchic theory of intelligence: analytical, creative, and practical. He classified analytical intelligence as problem-solving skills in tests and academics. Creative intelligence is considered how people react adaptively in new situations, or create novel ideas. Practical intelligence is defined as the everyday logic used when multiple solutions or decisions are possible. When Sternberg analyzed his data the relationship between the three intelligences surprised him. The data resembled what the other psychologists had found. All three mental abilities correlated highly with one another, and evidence that one basic factor, "g", was the primary influence.

Not all psychologists agreed with Spearman and his general intelligence. In 1916, Godfrey Thomson wrote a paper criticizing Spearman's "g":The object of this paper is to show that the cases brought forward by Professor Spearman in favor of the existence of General Ability are by no means "crucial." They are it is true not inconsistent with the existence of such a common element but neither are they inconsistent with its non-existence. The essential point about Professor Spearman's hypothesis is the existence of this General Factor. Both he and his opponents are agreed that there are Specific Factors peculiar to individual tests, both he and his opponents agree that there are Group Factors which run through some but not all tests. The difference between them is that Professor Spearman says there is a further single factor which runs through all tests, and that by pooling a few tests the Group Factors can soon be eliminated and a point reached where all the correlations are due to the General Factor alone. (pp. 217)

Spearman originally came up with the term General Intelligence, or as he called it, "g", to measure intelligence in his Two Theory on Intelligence. Spearman first researched in an experiment with 24 children from a small village school measuring three intellectual measures, based on teachers rankings, to address intellectual and sensory as the two different sets of measure: School Cleverness, Common Sense A and Common Sense B. His results showed the average r between intellectual and sensory measures to be +.38, School Cleverness and Commonsense to be at +0.55, and the three tasks intercorrelated at +0.25. This data was looked at other populations including high school. Spearman proposed that intellectual and sensory measure be combined as assessment of general intelligence.

Spearman proposed that his two-factor theory has two components. The general intelligence, "g", influences the performance on all mental tasks, while another component influences abilities on a particular task. To explain the differences in performance on different tasks, Spearman hypothesized that this other component was specific to a certain aspect of intelligence. This second factor he named "s", for specific ability. Regarding "g", Spearman saw individuals as having more or less general intelligence, while "s" varied from person to person on a task. In 1999, behavior geneticist Robert Plomin described "g" by saying: ""g" is one of the most reliable and valid measures in the behavioral domain... and it predicts important social outcomes such as educational and occupational levels far better than any other trait."

To visualize "g", imagine a Venn diagram with four circles overlapping. In the middle of the overlapping circles, would be "g", which influences all the specific intelligences, while "s" is represented by the four circles. Though the specific number of "s" factors are unknown, a few have been relatively accepted: mechanical, spatial, logical, and arithmetical.

Rising interest in the debate on the structure of intelligence prompted Spearman to elaborate and argue for his hypothesis. He claimed that "g" was not made up of one single ability, but rather two genetically influenced, unique abilities working together. He called these abilities "eductive" and "reproductive". He suggested that future understanding of the interaction between these two different abilities would drastically change how individual differences and cognition are understood in psychology, possibly creating the basis for wisdom.

Many researches are currently using Spearman's form of intelligence testing in their current studies. Although not all of the studies are currently using Spearman's exact model for intelligence testing, they are adding some modern concepts to that study. Spearman described that there was a functional relationship between intelligence and Sensory Discriminatory Abilities. Recent research has determined that there is an overlap between Working Memory, General Discriminatory Abilities, and Fluid Intelligence. His work has been built on, expanded, and linked to many other factors related to intelligence.

Intelligence testing measuring the "g" factor has been studied recently to re-explore Spearman's law of diminishing returns. This study investigates how "g" test scores will most likely decrease as "g" increases. Research has been done to investigate if "g" scores are made up of scores from Differential Ability Scales, "s" factors, and how the law of diminishing returns compare to Spearman's Law of diminishing returns. With the use of linear and nonlinear Confirmatory Factor Analysis, it is showing that the nonlinear model best described the data. The nonlinear model suggests that as "g" increases, the "s" factor lowers the overall score and inaccurately represents general intelligence.

This theory is still greatly present in today's modern psychology. Researchers are examining this theory and recreating it in modern research. The "g" factor is still frequently studied in current research. For example, a study could use and be compared with various other similar intelligence measures. Scales such as the Wechsler Intelligence Scale for Children has been compared with Spearman's "g", which shows that there has a decrease in statistic significance.

Research has been adapted to incorporate modern psychological topics into Spearman's Two Factor Theory of Intelligence. Nature versus Nurture is one topic that has been cross studied with Spearman's "g" factor. Research shows that although environmental factors influence the "g" factor differently, it has been found that it is affected if influenced early in life, rather than adulthood where there is little to no impact. Genetic influence has been documented to greatly influence "g" factor on intelligence.



</doc>
<doc id="8082342" url="https://en.wikipedia.org/wiki?curid=8082342" title="Repetition priming">
Repetition priming

Repetition priming refers to improvements in a behavioural response when stimuli are repeatedly presented. The improvements can be measured in terms of "accuracy" or "reaction time", and can occur when the repeated stimuli are either identical or similar to previous stimuli. These improvements have been shown to be cumulative, so as the number of repetitions increases the responses get continually faster up to a maximum of around seven repetitions. These improvements are also found when the repeated items are changed slightly in terms of orientation, size and position. The size of the effect is also modulated by the length of time the item is presented for and the length time between the first and subsequent presentations of the repeated items.

Repetition priming can occur without a person being aware of either the repeats or the improvements in his/her response, so it is generally thought to involve implicit memory processes that are dissociable from explicit memory processes. This idea has support from findings that amnesic patients with damage to limbic and/or diencephalic structures show measurable repetition priming effects but have deficits on explicit measures of memory. However, some researchers suggest that implicit and explicit memory systems are not in fact separate. Repetition priming has also been associated with attentional processes, stimulus expectation and episodic memory.

Research into repetition priming has been used to investigate the nature of mechanisms underlying the behavioural effects of rapid learning. In utilizing measures of "repetition suppression", the putative neural correlate of repetition priming, and measuring changes in the neural response associated with changing the presented stimuli, researchers are attempting to index regions and their processing biases along perceptual, conceptual and response dimensions. This area of research is based on multiple measurement methods from single cell recordings to multi-regional measurements using functional magnetic resonance imaging (fMRI), electroencephalography (EEG) and magnetoencephalography (MEG). Transcranial magnetic stimulation (TMS) has also been used to temporarily 'lesion' (inactivate) specific regions and so get an indication of the necessity of those regions in processing specific dimensions of the presented stimuli. Much of this research has been focused on the visual domain, however auditory and olfactory processes have also been investigated.

Numerous models have been put forward to explain behavioural efficiencies that are gained with repeated presentations of the same or similar stimuli. These are outlined below.

In this model the attenuation of a neural response is hypothesised to be due to an overall reduction in the amplitude of a neuron's firing. Whether this reduction occurs across all neurons that responded to the initial stimulus or just the critical subset of those that initially responded maximally, is still unclear. However, evidence does suggest that a mechanism like this reduces redundant neural firing and enhances efficiencies in processing in the early visual cortex.

Along similar lines is the idea that repetition causes the neurons that are less relevant to the representation of the stimulus to stop firing when that stimulus is repeated. In this way the representation is supported by a gradually sparser response, resulting in an adaptive reduction in metabolic requirements and increased efficiencies in information transmission through the neural hierarchy. This could be the result of lateral inhibition within representational levels in a competitive Hebbian learning system, where strong connections get stronger and inhibit the weaker connections. Much of the evidence for this comes from primate studies of the inferotemporal cortex and single cell recordings with long training periods. However, decreases in firing rate over short-term training on repeated stimuli appear to be greatest in those cells that initially respond with the highest activation rate, in line with the fatigue model above.

The key concept in this model is that information travels faster through the network when the current stimulus representation overlaps with a previous representation, driven by more rapid onset of neural activation with repeated presentations. fMRI studies have been used in an attempt to measure these potential latency differences but the temporal resolution is not very precise and single cell recordings typically do not show shortened latencies to repeated stimuli. Another possible explanation of facilitation is synaptic potentiation within an attractor neural network model where repetition decreases the settling time as the attractor basin deepens and so increases the overall speed of processing.

When a stimulus is repeated top-down feedback modulates the neural response of earlier processing regions, with reduced neural activation and improved behavioural responses reflecting fulfilled expectations. The idea for this comes from predictive coding theories and Bayesian statistics and has some support in fMRI studies manipulating stimulus expectation. However, the results may also reflect the involvement of attention, which seems to have a modulatory effect on the extent of priming elicited.

This theory is based on the idea that because downstream neurons are sensitive to both the firing rates and the timing of those inputs, efficiencies in processing may be gained through synchronised activation. Evidence of synchronisation associated with repeated stimuli include phase locking found between two regions of the cat visual cortex while measuring spike synchronisation for trained compared with novel stimuli and suppressed firing and increased synchrony of spikes with repetition of odour puffs to locust's antennae. Evidence using EEG and MEG suggests that stimulus repetition in humans results in increased synchrony between distinct cortical regions, often the same regions that show reduced local neural activity (see repetition suppression below). In one study, the timing of this across-region synchronization predicted the amount of behavioral facilitation seen with repetition priming, suggestive of a close link between synchrony and behavior.

This theory suggests that repetition priming is a result of binding the initial stimulus directly to the response while bypassing the intervening layers of computation. The mechanism mediating this direct binding has not been clarified but several hypotheses have been put forward. One theory explains it as a race between automatic activation of a previous stimulus-response route and the reengagement of the "algorithmic" route and another theory suggests the operation of an "action-trigger" where repeated stimuli trigger the previous response through perceptual or conceptual associations with the original stimulus. In support of this theory is evidence of a response congruency effect, which would be expected from these stimulus-response bindings. The increased synchrony between regions discussed above could be a neural correlate of stimulus-response binding.

The phenomenon of "repetition suppression", a reduction in neural activity when stimuli are repeated, is thought to depend on processing overlaps between repeated items and is generally considered to be the neural correlate of repetition priming. As such it has been used extensively in research investigating the nature of representations across various levels of the visual processing hierarchy. In doing so researchers have found that repetition suppression appears to occur on multiple processing levels; dependent on the stimuli being processed and the processing level at which the experimental manipulation is directed; with reductions in neural activity to repeated stimuli occurring in regions involved with the initial processing of those features. However, care must be taken in interpreting the results of these studies as the relationship between repetition suppression and repetition priming has not been definitively established.

Although repetition priming is most often associated with neural attenuation for repeated presentation of stimuli, increases in neural responses have also been measured in a number of experimental contexts. For example, when performing mathematical calculations, when repeated stimuli are degraded, in studies involving a backward masking paradigm and when stimuli have no pre-existing associations or meaning.


</doc>
<doc id="34790424" url="https://en.wikipedia.org/wiki?curid=34790424" title="Viveza criolla">
Viveza criolla

Viveza criolla is a Spanish language phrase literally meaning "creole' cleverness" and may be translated as "creoles' cunning", describing a way of life in Chile, Argentina, Uruguay, Colombia and Venezuela, among other Latin American countries. It is a philosophy of progress along the line of least resistance and ignoring rules, a lack of sense of responsibility and consideration for others, and it extends to all social groups and throughout the whole country, although it predominates in Buenos Aires. Viveza criolla has been called "the principal cause of a moral, cultural, economic, social and political crisis". It is a similar concept to jeitinho brasileiro in Brazil.

Viveza criolla includes:




</doc>
<doc id="4744384" url="https://en.wikipedia.org/wiki?curid=4744384" title="Human nature">
Human nature

Human nature is a bundle of characteristics, including ways of thinking, feeling, and acting, which humans are said to have naturally. The term is often regarded as capturing what it is to be human, or the essence of humanity. The term is controversial because it is disputed whether or not such an essence exists. Arguments about human nature have been a mainstay of philosophy for centuries and the concept continues to provoke lively philosophical debate. The concept also continues to play a role in science, with neuroscientists, psychologists and social scientists sometimes claiming that their results have yielded insight into human nature. Human nature is traditionally contrasted with characteristics that vary among humans, such as characteristics associated with specific cultures. Debates about human nature are related to, although not the same as, debates about the comparative importance of genes and environment in development ("nature versus nurture").

The concept of nature as a standard by which to make judgments is traditionally said to have begun in Greek philosophy, at least as regards the Western and Middle Eastern languages and perspectives which are heavily influenced by it.

The teleological approach of Aristotle came to be dominant by late classical and medieval times. By this account, human nature really causes humans to become what they become, and so it exists somehow independently of individuals. This in turn has been understood as also showing a special connection between human nature and divinity. This approach understands human nature in terms of final and formal causes. In other words, nature itself (or a nature-creating divinity) has intentions and goals, similar somehow to human intentions and goals, and one of those goals is humanity living naturally. Such understandings of human nature see this nature as an "idea", or "form" of a human.

However, the existence of this invariable and metaphysical human nature is subject of much historical debate, continuing into modern times. Against this idea of a fixed human nature, the relative malleability of man has been argued especially strongly in recent centuries—firstly by early modernists such as Thomas Hobbes and Jean-Jacques Rousseau. In Rousseau's "Emile, or On Education", Rousseau wrote: "We do not know what our nature permits us to be". Since the early 19th century, thinkers such as Hegel, Marx, Kierkegaard, Nietzsche, Sartre, structuralists, and postmodernists have also sometimes argued against a fixed or innate human nature.

Charles Darwin's theory of evolution has changed the nature of the discussion, supporting the proposition that mankind's ancestors were not like mankind today. Still more recent scientific perspectives—such as behaviorism, determinism, and the chemical model within modern psychiatry and psychology—claim to be neutral regarding human nature. As in much of modern science, such disciplines seek to explain with little or no recourse to metaphysical causation. They can be offered to explain the origins of human nature and its underlying mechanisms, or to demonstrate capacities for change and diversity which would arguably violate the concept of a fixed human nature.

Philosophy in classical Greece is the ultimate origin of the Western conception of the nature of a thing. According to Aristotle, the philosophical study of human nature itself originated with Socrates, who turned philosophy from study of the heavens to study of the human things. Socrates is said to have studied the question of how a person should best live, but he left no written works. It is clear from the works of his students Plato and Xenophon, and also by what was said about him by Aristotle (Plato's student), that Socrates was a rationalist and believed that the best life and the life most suited to human nature involved reasoning. The Socratic school was the dominant surviving influence in philosophical discussion in the Middle Ages, amongst Islamic, Christian, and Jewish philosophers.

The human soul in the works of Plato and Aristotle has a divided nature, divided in a specifically human way. One part is specifically human and rational, and divided into a part which is rational on its own, and a spirited part which can understand reason. Other parts of the soul are home to desires or passions similar to those found in animals. In both Aristotle and Plato, spiritedness ("thumos") is distinguished from the other passions ("epithumiai"). The proper function of the "rational" was to rule the other parts of the soul, helped by spiritedness. By this account, using one's reason is the best way to live, and philosophers are the highest types of humans.

Aristotle—Plato's most famous student—made some of the most famous and influential statements about human nature. In his works, apart from using a similar scheme of a divided human soul, some clear statements about human nature are made:

For Aristotle, reason is not only what is most special about humanity compared to other animals, but it is also what we were meant to achieve at our best. Much of Aristotle's description of human nature is still influential today. However, the particular teleological idea that humans are "meant" or intended to be something has become much less popular in modern times.

For the Socratics, human nature, and all natures, are metaphysical concepts. Aristotle developed the standard presentation of this approach with his theory of four causes. Every living thing exhibits four aspects or "causes": matter, form, effect, and end. For example, an oak tree is made of plant cells (matter), grew from an acorn (effect), exhibits the nature of oak trees (form), and grows into a fully mature oak tree (end). Human nature is an example of a formal cause, according to Aristotle. Likewise, to become a fully actualized human being (including fully actualizing the mind) is our end. Aristotle ("Nicomachean Ethics," Book X) suggests that the human intellect (νούς) is "smallest in bulk" but the most significant part of the human psyche, and should be cultivated above all else. The cultivation of learning and intellectual growth of the philosopher, which is thereby also the happiest and least painful life.

Human nature is a central question in Chinese philosophy. From the Song dynasty, the theory of potential or innate goodness of human beings became dominant in Confucianism.

Mencius argues that human nature is good. He understood human nature as the innate tendency to an ideal state that's expected to be formed under the right conditions. Therefore, humans have the capacity to be good, even though they are not all good. 

According to Mencian theory, human nature contains four beginnings (端, "duan") of morality: a sense of compassion that develops into benevolence (仁, "ren"), a sense of shame and disdain that develops into righteousness (義, "yi"), a sense of respect and courtesy that develops into propriety (禮, "li"), and a sense of right and wrong that develops into wisdom (智, "zhi"). In Mencius' view, goodness is the result of the development of innate tendencies toward the virtues of benevolence, righteousness, wisdom, and propriety. These tendencies have an affective dimension, because they are manifested in moral emotions. These beginnings, however, do not only contain affective motivations but also intuitive judgments (such as what's right and wrong, deferential, respectful, or disdainful) for morality. Moral reflection (思, "si") upon the manifestations of the four beginnings leads to the development of virtues. It brings recognition that virtue takes precedence over satisfaction, but a lack of reflection inhibits moral development. In other words, humans have a constitution comprising emotional predispositions that direct them to goodness. 

Mencius addresses the question why the capacity for evil is not grounded in human nature. It is not the function of ears and eyes (i.e., sensory organs associated with sensual desires) but the function of the heart to reflect. People can be misled and led astray by their desires if they do not engage their ethical motivations. Therefore, if an individual becomes bad, it is not the result of his or her constitution. Mencius considers core virtues—benevolence, righteousness, propriety, and wisdom—as internal qualities that humans originally possess, so people can not attain full satisfaction by solely pursuits of self-interest due to their innate morality. Wong (2018) underscores that "while Mencius is often characterized superficially by his saying that human nature is good [..], he means [...] that it contains predispositions to feel and act in morally appropriate ways and to make intuitive normative judgments that can with the right nurturing conditions give human beings guidance as to the proper emphasis to be given to the desires of the senses [...]."

Xunzi understood human nature as the basic faculties, capacities, and desires that people have from birth. He argues that human nature is evil and that any goodness is the result of human activity. It is human nature to seek profit, because humans desire for sensory satisfaction. However, Xuzi states: "Now the nature of man is evil. It must depend on teachers and laws to become correct and achieve propriety and righteousness and then it becomes disciplined." He underscores that goodness comes from the traits and habits acquired through conscious actions, which he calls artifice (偽, "wei"). Therefore, morality is seen as a human artifice but not as a part of human nature.

Mencius sees ritual (i.e., the standard for how humans should treat and interact with each other) as an outward expression of the inherent moral sense in human nature. Xunzi agrees that humans have a moral sense, but he sees it as evidence for the badness in people. People namely desire what they lack, in which the human desire for goodness is an indication for the lack of goodness.

Human nature is one of the major foundations of Legalism in China. However, Legalists do not consider whether human goodness or badness is inborn as well as whether human beings possess the fundamental qualities associated with that nature.

Legalists see the overwhelming majority of human beings as selfish in nature. They hold the view that human nature is evil, in which individuals are driven by selfishness. Therefore, people are not expected to always behave morally. For instance, due to the corrupt nature of humans, Legalists did not trust that officials would carry out their duties in a fair and impartial manner. There is a perpetual political struggle, characterized by conflict among contending human actors and interests, where individuals are easily tempted due to their selfish nature at the expense of others.

According to Legalism, selfishness in human nature can not be eliminated or altered by education or self-cultivation. It dismisses the possibility that people can overcome their selfishness and consider the possibility that people can be driven by moral commitment to be exceptionally rare. Legalists do not see the individual morality of both the rulers or the ruled as an important concern in a political system. Instead, Legalist thinkers such as Han Fei emphasize clear and impersonal norms and standards (such as laws, regulations, and rules) as the basis to maintain order.

Legalists posit that human selfishness can be an asset rather than a threat to a state. Herein, individuals must be allowed to pursue their selfish interests in a manner that benefits rather than contradicts the needs of a state. Therefore, a political system that presupposes this human selfishness is the only viable system, wherein its actors (such as ministers and other officials) must be controlled and checked as they can not truly be trusted. Legalists view the usage of reward and punishment as effective political controls, because they consider that such aspects underlie human nature. According to the Legalist statesman Shang Yang, it is crucial to investigate the disposition of people in terms of rewards and penalties when a law is established. He further explains that a populace can not be driven to pursuits of agriculture or warfare if people consider these to be bitter or dangerous on the basis of calculations about their possible benefits, but people can be directed toward these pursuits through the application of positive and negative incentives.

In Christian theology, there are two ways of "conceiving human nature". The first is "spiritual, Biblical, and theistic", whereas the second is "natural, cosmical, and anti-theistic". The focus in this section is on the former. As William James put it in his study of human nature from a religious perspective, "religion" has a "department of human nature".

Various views of human nature have been held by theologians. However, there are some "basic assertions" in all "biblical anthropology".
The Bible contains no single "doctrine of human nature". Rather, it provides material for more philosophical descriptions of human nature. For example, Creation as found in the Book of Genesis provides a theory on human nature.

Catechism of the Catholic Church in chapter "Dignity of the human person" has article about man as image of God, vocation to beatitude, freedom, human acts, passions, moral conscience, virtues and sin.

As originally created, the Bible describes "two elements" in human nature: "the body and the breath or spirit of life breathed into it by God". By this was created a "living soul", that is, a "living person". According to Genesis 1:27, this living person was made in the "image of God". From the biblical perspective, "to be human is to bear the image of God".

Genesis does not elaborate the meaning of "the image of God", but scholars find suggestions. One is that being created in the image of God distinguishes human nature from that of the beasts. Another is that as God is "able to make decisions and rule" so humans made in God's image are "able to make decisions and rule". A third is that mankind possesses an inherent ability "to set goals" and move toward them. That God denoted creation as "good" suggests that Adam was "created in the image of God, in righteousness."

Adam was created with ability to make "right choices", but also with the ability to choose sin, by which he fell from righteousness into a state of "sin and depravity". Thus, according to the Bible, "humankind is not as God created it".

By Adam's fall into sin, "human nature" became "corrupt", although it retains the image of God. Both the Old Testament and the New Testament teach that "sin is universal". For example, Psalm 51:5 reads: "For behold I was conceived in iniquities; and in sins did my mother conceive me." Jesus taught that everyone is a "sinner naturally" because it is mankind's "nature and disposition to sin". Paul, in Romans 7:18, speaks of his "sinful nature".

Such a "recognition that there is something wrong with the moral nature of man is found in all religions". Augustine of Hippo coined a term for the assessment that all humans are born sinful: original sin. Original sin is "the tendency to sin innate in all human beings". The doctrine of original sin is held by the Catholic Church and most mainstream Protestant denominations, but rejected by the Eastern Orthodox Church, which holds the similar doctrine of ancestral fault.

"The corruption of original sin extends to every aspect of human nature": to "reason and will" as well as to "appetites and impulses". This condition is sometimes called "total depravity". Total depravity does not mean that humanity is as "thoroughly depraved" as it could become. Commenting on Romans 2:14, John Calvin writes that all people have "some notions of justice and rectitude ... which are implanted by nature" all people.

Adam embodied the "whole of human nature" so when Adam sinned "all of human nature sinned". The Old Testament does not explicitly link the "corruption of human nature" to Adam's sin. However, the "universality of sin" implies a link to Adam. In the New Testament, Paul concurs with the "universality of sin". He also makes explicit what the Old Testament implied: the link between humanity's "sinful nature" and Adam's sin In Romans 5:19, Paul writes, "through [Adam's] disobedience humanity became sinful". Paul also applied humanity's sinful nature to himself: "there is nothing good in my sinful nature."

The theological "doctrine of original sin" as an inherent element of human nature is not based only on the Bible. It is in part a "generalization from obvious facts" open to empirical observation.

A number of experts on human nature have described the manifestations of original (i.e., the innate tendency to) sin as empirical facts.

Empirical discussion questioning the genetic exclusivity of such an intrinsic badness proposition is presented by researchers Elliott Sober and David Sloan Wilson. In their book, "Unto Others: The Evolution and Psychology of Unselfish Behavior", they propose a theory of multilevel group selection in support of an inherent genetic "altruism" in opposition to the original sin exclusivity for human nature.

Liberal theologians in the early 20th century described human nature as "basically good", needing only "proper training and education". But the above examples document the return to a "more realistic view" of human nature "as basically sinful and self-centered". Human nature needs "to be regenerated ... to be able to live the unselfish life".

According to the Bible, "Adam's disobedience corrupted human nature" but God mercifully "regenerates". "Regeneration is a radical change" that involves a "renewal of our [human] nature". Thus, to counter original sin, Christianity purposes "a complete transformation of individuals" by Christ.

The goal of Christ's coming is that fallen humanity might be "conformed to or transformed into the image of Christ who is the perfect image of God", as in 2 Corinthians 4:4. The New Testament makes clear the "universal need" for regeneration. A sampling of biblical portrayals of regenerating human nature and the behavioral results follow.

One of the defining changes that occurred at the end of the Middle Ages was the end of the dominance of Aristotelian philosophy, and its replacement by a new approach to the study of nature, including human nature. In this approach, all attempts at conjecture about formal and final causes were rejected as useless speculation. Also, the term "law of nature" now applied to any regular and predictable pattern in nature, not literally a law made by a divine lawmaker, and, in the same way, "human nature" became not a special metaphysical cause, but simply whatever can be said to be typical tendencies of humans.

Although this new realism applied to the study of human life from the beginning—for example, in Machiavelli's works—the definitive argument for the final rejection of Aristotle was associated especially with Francis Bacon. Bacon sometimes wrote as if he accepted the traditional four causes ("It is a correct position that "true knowledge is knowledge by causes". And causes again are not improperly distributed into four kinds: the material, the formal, the efficient, and the final") but he adapted these terms and rejected one of the three: But of these the final cause rather corrupts than advances the sciences, except such as have to do with human action. The discovery of the formal is despaired of. The efficient and the material (as they are investigated and received, that is, as remote causes, without reference to the latent process leading to the form) are but slight and superficial, and contribute little, if anything, to true and active science.This line of thinking continued with René Descartes, whose new approach returned philosophy or science to its pre-Socratic focus upon non-human things. Thomas Hobbes, then Giambattista Vico, and David Hume all claimed to be the first to properly use a modern Baconian scientific approach to human things.

Hobbes famously followed Descartes in describing humanity as matter in motion, just like machines. He also very influentially described man's natural state (without science and artifice) as one where life would be "solitary, poor, nasty, brutish and short". Following him, John Locke's philosophy of empiricism also saw human nature as a "tabula rasa". In this view, the mind is at birth a "blank slate" without rules, so data are added, and rules for processing them are formed solely by our sensory experiences.

Jean-Jacques Rousseau pushed the approach of Hobbes to an extreme and criticized it at the same time. He was a contemporary and acquaintance of Hume, writing before the French Revolution and long before Darwin and Freud. He shocked Western civilization with his Second Discourse by proposing that humans had once been solitary animals, without reason or language or communities, and had developed these things due to accidents of pre-history. (This proposal was also less famously made by Giambattista Vico.) In other words, Rousseau argued that human nature was not only not fixed, but not even approximately fixed compared to what had been assumed before him. Humans are political, and rational, and have language now, but originally they had none of these things. This in turn implied that living under the management of human reason might not be a happy way to live at all, and perhaps there is no ideal way to live. Rousseau is also unusual in the extent to which he took the approach of Hobbes, asserting that primitive humans were not even naturally social. A civilized human is therefore not only imbalanced and unhappy because of the mismatch between civilized life and human nature, but unlike Hobbes, Rousseau also became well known for the suggestion that primitive humans had been happier, "noble savages".

Rousseau's conception of human nature has been seen as the origin of many intellectual and political developments of the 19th and 20th centuries. He was an important influence upon Kant, Hegel, and Marx, and the development of German idealism, historicism, and romanticism.

What human nature did entail, according to Rousseau and the other modernists of the 17th and 18th centuries, were animal-like passions that led humanity to develop language and reasoning, and more complex communities (or communities of any kind, according to Rousseau).

In contrast to Rousseau, David Hume was a critic of the oversimplifying and systematic approach of Hobbes, Rousseau, and some others whereby, for example, all human nature is assumed to be driven by variations of selfishness. Influenced by Hutcheson and Shaftesbury, he argued against oversimplification. On the one hand, he accepted that, for many political and economic subjects, people could be assumed to be driven by such simple selfishness, and he also wrote of some of the more social aspects of "human nature" as something which could be destroyed, for example if people did not associate in just societies. On the other hand, he rejected what he called the "paradox of the sceptics", saying that no politician could have invented words like honourable' and 'shameful,' 'lovely' and 'odious,' 'noble' and 'despicable, unless there was not some natural "original constitution of the mind".

Hume—like Rousseau—was controversial in his own time for his modernist approach, following the example of Bacon and Hobbes, of avoiding consideration of metaphysical explanations for any type of cause and effect. He was accused of being an atheist. He wrote: 

After Rousseau and Hume, the nature of philosophy and science changed, branching into different disciplines and approaches, and the study of human nature changed accordingly. Rousseau's proposal that human nature is malleable became a major influence upon international revolutionary movements of various kinds, while Hume's approach has been more typical in Anglo-Saxon countries, including the United States.

The concept of human nature is a source of ongoing debate in contemporary philosophy, specifically within philosophy of biology, a subfield of the philosophy of science. Prominent critics of the concept – David L. Hull, Michael Ghiselin, and David Buller; see also – argue that human nature is incompatible with modern evolutionary biology. Conversely, defenders of the concept argue that when defined in certain ways, human nature is both scientifically respectable and meaningful. Therefore, the value and usefulness of the concept depends essentially on how one construes it. This section summarizes the prominent construals of human nature and outlines the key arguments from philosophers on both sides of the debate.

Philosopher of science David L. Hull has influentially argued that there is no such thing as human nature. Hull's criticism is raised against philosophers who conceive human nature as a set of intrinsic phenotypic traits (or characters) that are universal among humans, unique to humans, and definitive of what it is to be a member of the biological species "Homo sapiens." In particular, Hull argues that such "essential sameness of human beings" is "temporary, contingent and relatively rare" in biology. He argues that variation, insofar as it is the result of evolution, is an essential feature of all biological species. Moreover, the type of variation which characterizes a certain species in a certain historical moment is "to a large extent accidental". He writes:Periodically a biological species might be characterized by one or more characters which are both universally distributed among and limited to the organisms belonging to that species, but such states of affairs are temporary, contingent and relatively rare.""Hull reasons that properties universally shared by all members of a certain species are usually also possessed by members of other species, whereas properties exclusively possessed by the members of a certain species are rarely possessed by "all" members of that species. For these reasons, Hull observes that, in contemporary evolutionary taxonomy, belonging to a particular species does not depend on the possession of any specific intrinsic properties. Rather, it depends on standing in the right kind of relations (relations of genealogy or interbreeding, depending on the precise species concept being used) to other members of the species. Consequently, there can be no "intrinsic" properties that define what it is to be a member of the species "Homo sapiens". Individual organisms, including humans, are part of a species by virtue of their relations with other members of the same species, not shared intrinsic properties. 

According to Hull, the moral significance of his argument lies in its impact on the biologically legitimate basis for the concept of "human rights". While its has long been argued that there is a sound basis for "human rights" in the idea that all human beings are essentially the same, should Hull's criticism work, such a basis – at least on a biological level – would disappear. Nevertheless, Hull does not perceive this to be a fundamental for human rights, because people can choose to continue respecting human rights even without sharing the same human nature.

Several contemporary philosophers have attempted to defend the notion of human nature against charges that it is incompatible with modern evolutionary biology by proposing alternative interpretations. They claim that the concept of human nature continues to bear relevance in the fields of neuroscience and biology. Many have proposed non-essentialist notions. Others have argued that, even if Darwinism has shown that any attempt to base species membership on ""intrinsic" essential properties" is untenable, essences can still be "relational" – this would be consistent with the interbreeding, ecological, and phylogenetic species concepts, which are accepted by modern evolutionary biology. These attempts aim to make Darwinism compatible with a certain conception of human nature which is stable across time.

Philosopher of science Edouard Machery has proposed that the above criticisms only apply to a specific definition (or "notion") of human nature, and not to "human nature in general". He distinguishes between two different notions:


Machery clarifies that, to count as being "a result of evolution", a property must have an ultimate explanation in Ernst Mayr's sense. It must be possible to explain the trait as the product of evolutionary processes. Importantly, properties can count as part of human nature in the nomological sense even if they are not universal among humans and not unique to humans. In other words, nomological properties need not be necessary nor sufficient for being human. Instead, it is enough that these properties are shared by most humans, as a result of the evolution of their species – they "need to be typical". Therefore, human nature in the nomological sense does not define what it is to be a member of the species "Homo sapiens". Examples of properties that count as parts of human nature on the nomological definition include: being bipedal, having the capacity to speak, having a tendency towards biparental investment in children, having fear reactions to unexpected noises. Finally, since they are the product of evolution, properties belonging to the nomological notion of human nature are not fixed, but they can change over time.

Machery agrees with biologists and others philosophers of biology that the essentialist notion of human nature is incompatible with modern evolutionary biology: we cannot explain membership in the human species by means of a definition or a set of properties. However, he maintains that this does not mean humans have no nature, because we can accept the nomological notion which is not a definitional notion. Therefore, we should think of human nature as the many properties humans have in common as a result of evolution.

Machery also highlights potential drawbacks of the nomological account. One is that the nomological notion is a watered-down notion that cannot perform many of the roles that the concept of human nature is expected to perform in science and philosophy. The properties endowed upon humans by the nomological account do not distinguish humans from other animals or define what it is to be human. Machery pre-empts this objection by claiming that the nomological concept of human nature still fulfils many roles. He highlights the importance of a conception which picks out what humans share in common which can be used to make scientific, psychological generalizations about human-beings. One advantage of such a conception is that it gives an idea of the traits displayed by the majority of human beings which can be explained in evolutionary terms.

Another potential drawback is that the nomological account of human nature threatens to lead to the absurd conclusion that all properties of humans are parts of human nature. According to the nomological account, a trait is only part of human nature if it is a result of evolution. However, there is a sense in which all human traits are results of evolution. For example, the belief that water is wet is shared by all humans. However, this belief is only possible because we have, for example, evolved a sense of touch. It is difficult to separate traits which are the result of evolution and those which are not. Machery claims the distinction between proximate and ultimate explanation can do the work here: only some human traits can be given an ultimate explanation, he argues.

According to the philosopher Richard Samuels the account of human nature is expected to fulfill the five following roles:


Samuels objects that Machery's nomological account fails to deliver on the causal explanatory function, because it claims that superficial and co-varying properties "are" the essence of human nature. Thus, human nature cannot be the underlying "cause" of these properties and accordingly cannot fulfill its causal explanatory role.

Philosopher Grant Ramsey also rejects Machery's nomological account. For him, defining human nature with respect to only universal traits fails to capture many important human characteristics. Ramsey quotes the anthropologist Clifford Geertz, who claims that "the notion that unless a cultural phenomenon is empirically universal it cannot reflect anything about the nature of man is about as logical as the notion that because sickle-cell anemia is, fortunately, not universal, it cannot tell us anything about human genetic processes. It is not whether phenomena are empirically common that is critical in science...but whether they can be made to reveal the enduring natural processes that underly them". Following Geertz, Ramsey holds that the study of human nature should not rely exclusively on universal or near-universal traits. There are many idiosyncratic and particular traits of scientific interest. Machery's account of human nature cannot give an account to such differences between men and women as the nomological account only picks out the common features within a species. In this light, the female menstrual cycle which is a biologically an essential and useful feature cannot be included in a nomological account of human nature.

Ramsey also objects that Machery uncritically adopts the innate-acquired dichotomy, distinguishing between human properties due to enculturation and those due to evolution. Ramsey objects that human properties do not just fall in one of the two categories, writing that "any organismic property is going to be due to both heritable features of the organism as well as the particular environmental features the organism happens to encounter during its life."

Richard Samuels (in his article "Science and Human Nature") proposes a causal essentialist view that "human nature should be identified with a suite of mechanisms, processes, and structures that causally explain many of the more superficial properties and regularities reliably associated with humanity". This view is "causal" because the mechanisms causally explain the superficial properties reliably associated with humanity by referencing the underlying causal structures the properties belong to. For example, it is true that the belief that water is wet is shared by all humans yet it is not in itself a significant aspect of human nature. Instead, the psychological process that lead us to assign the word "wetness" to water is a universal trait shared by all human beings. In this respect, the superficial belief that water is wet reveals an important causal psychological process which is widely shared by most human beings. The explanation is also "essentialist" because there is a core set of empirically discoverable cognitive mechanism that count as part of the human nature. According to Samuels, his view avoids the standard biological objections to human nature essentialism.

Samuels argues that the theoretical roles of human nature includes: organizing role, descriptive functions, causal explanatory functions, taxonomic functions, and invariances.

In comparison with traditional essentialist view, the "causal essentialist" view does not accomplish the taxonomic role of human nature (the role of defining what it is to be human). He claims however, that no conception could achieve this, as the fulfillment of the role would not survive evolutionary biologists’ objections (articulated above by in "Criticisms of the concept of human nature"). In comparison with Machery’s nomological conception, Samuels wants to restore the causal-explanatory function of human nature. He defines the essence of human nature as causal mechanisms and not as surface-level properties. For instance, on this view, linguistic behaviour is not part of human nature, but the cognitive mechanisms underpinning linguistic behaviour might count as part of human nature.

Grant Ramsey proposes an alternative account of human nature, which he names the "life-history trait cluster" account. This view stems from the recognition that the combination of a specific genetic constitution with a specific environment is not sufficient to determine how a life will go, i.e., whether one is rich, poor, dies old, dies young, etc. Many ‘life histories’ are possible for a given individual, each populated by a great number of traits. Ramsey defines his conception of human nature in reference to the “pattern of trait clusters within the totality of extant possible life-histories”. In other words, there are certain life histories, i.e., possible routes one's life can take, for example: being rich, being a PhD student, or getting ill. Ramsey underlines the patterns behind these possible routes by delving into the causes of these life histories. For example, one can make the following claim: “Humans sweat when they get exhausted" or one can also propose neurological claims such as “Humans secrete Adrenaline when they are in flight-fight mode.” This approach enables Ramsey to go beyond the superficial appearances and understand the similarities/differences between individuals in a deeper level which refers to the causal mechanisms (processes, structures and constraints etc.) which lie beneath them. Once we list all the possible life-histories of an individual, we can find these causal patterns and add them together to form the basis of individual nature.

Ramsey's next argumentative manoeuvre is to point out that traits are not randomly scattered across potential life histories; there are patterns. “These patterns” he states “provide the basis for the notion of individual and human nature”. While one’s ‘individual nature’ consists of the pattern of trait clusters distributed across that individual’s set of possible life histories, Human Nature, Ramsey defines as “the pattern of trait clusters within the totality of extant human possible life histories”. Thus, if we were to combine all possible life histories of all individuals in existence we would have access to the trait distribution patterns that constitute human nature.

Trait patterns, on Ramsey's account, can be captured in the form of conditional statements, such as "if female, you develop ovaries" or "if male, you develop testes". These statements will not be true of all humans. Ramsey contends that these statements capture part of human nature if they have a good balance of "pervasiveness" (many people satisfy the antecedent of the conditional statement), and "robustness" (many people who satisfy the antecedent go on to satisfy the consequent).

The contemporary debate between so-called “bioconservatives” and “transhumanists” is directly related to the concept of human nature: transhumanists argue that "current human nature is improvable through the use of applied science and other rational methods". Bioconservatives believe that the costs outweigh the benefits: in particular, they present their position as a defense of human nature which, according to them, is threatened by human enhancement technologies. Although this debate is mainly of an ethical kind, it is deeply rooted in the different interpretations of human nature, human freedom, and human dignity (which, according to bioconservatives, is specific to human beings, while transhumanists think that it can be possessed also by posthumans). As explained by Allen Buchanan, the literature against human enhancement is characterized by two main concerns: that "enhancement may alter or destroy human nature" and that "if enhancement alters or destroys human nature, this will undercut our ability to ascertain the good", as "the good is determined by our nature".

Bioconservatives include Jürgen Habermas, Leon Kass, Francis Fukuyama, and Bill McKibben. Some of the reasons why they oppose (certain forms of) human enhancement technology are to be found in the worry that such technology would be “dehumanizing” (as they would undermine the human dignity intrinsically built in our human nature). For instance, they fear that becoming “posthumans” could pose a threat to “ordinary” humans or be harmful to posthumans themselves. 

Jürgen Habermas makes the argument against the specific case of genetic modification of unborn children by their parents, referred to as “eugenic programming” by Habermas. His argument is two-folded: The most immediate threat is on the “ethical freedom” of programmed individuals, and the subsequent threat is on the viability of liberal democracy. Reasoning of the former can be formulated as the following: Genetic programming of desirable traits, capabilities and dispositions puts restrictions on a person’s freedom to choose a life of his own, to be the "sole author" of his existence. A genetically-programmed child may feel "alienated" from his identity, which is now irreversibly co-written by human agents other than himself. This feeling of alienation, resulted from“contingency of a life’s beginning that is not at [one’s] disposal,” makes it difficult for genetically-modified persons to perceive themselves as moral agents who can make ethical judgement freely and independently - that is, without any substantial or definitive interference from another agent. Habermas proposes a second threat - the undermining power of genetic programming on the viability of democracy. The basis of liberal democracy, Habermas rightfully claims, is the symmetrical and independent mutual recognition among free, equal and autonomous persons. Genetic programming jeopardizes this condition by irreversibly subjecting children to permanent dependence on their parents, thus depriving them of their "perceived" ability to be full citizens of the legal community. This fundamental modification to human relationship erodes the foundation of liberal democracy and puts its viability in danger.

The most famous proponent of transhumanism, on the other hand, is Oxford Swedish philosopher Nick Bostrom. According to Bostrom, "human enhancement technologies should be made widely available", as they would offer enormous potential for improving the lives of human beings, without "dehumanizing" them: for instance, improving their intellectual and physical capacities, or protecting them from suffering, illnesses, aging, and physical and cognitive shortcomings. In response to bioconservatives, transhumanists argue that expanding a person's "capability set" would increase her freedom of choice, rather than reducing it.

Allen Buchanan has questioned the relevance of the concept of human nature to this debate. In "Human Nature and Enhancement", he argued that good but also bad characteristics are part of human nature, and that changing the "bad" ones does not necessarily imply that the "good" ones will be affected. Moreover, Buchanan argued that the way we evaluate the good is independent of human nature: in fact, we can "make coherent judgements about the defective aspects of human nature, and if those defects were readied this need not affect our ability to judge what is good". Buchanan's conclusion is that the debate on enhancement of human beings would be more fruitful if it was conducted without appealing to the concept of human nature.

Tim Lewens presented a similar position: since the only notions of human nature which are compatible with biology offer "no ethical guidance in debates over enhancement", we should set the concept of human nature aside when debating about enhancement. On the other hand, "folk", neo-Aristotelian conceptions of human nature seem to have normative implications, but they have no basis in scientific research. Grant Ramsey replied to these claims, arguing that his "life-history trait cluster" account allows the concept of human nature "to inform questions of human enhancement".

Appeals to nature often fall foul of the naturalistic fallacy, whereby certain capacities or traits are considered morally 'good' in virtue of their "naturalness." The fallacy was initially introduced by G E Moore in 1903, who challenged philosopher's attempts to define "good" reductively, in terms of natural properties (such as "desirable"). Reliance on 'the natural' as a justification for resisting enhancement is criticised on several grounds by transhumanists, against the bioconservative motivation to preserve or protect 'human nature'.

For example, Nick Bostrom asserts "had Mother Nature been a real parent, she would have been in jail for child abuse and murder" thus not worthy of unqualified protection.

Similarly, Arthur Caplan opposes naturalistic objections to life extension enhancements, by claiming that:

"The explanation of why ageing occurs has many of the attributes of a stochastic or chance phenomenon. And this makes ageing unnatural and in no way an intrinsic part of human nature. As such, there is no reason why it is intrinsically wrong to try to reverse or cure ageing."


</doc>
<doc id="96909" url="https://en.wikipedia.org/wiki?curid=96909" title="Chinese fire drill">
Chinese fire drill

"Chinese fire drill" is a slang term for a situation that is chaotic or confusing, possibly due to poor or misunderstood instructions.

The term goes back to the early 1900s, and is alleged to have originated when a ship run by British officers and a Chinese crew practiced a fire drill for a fire in the engine room. The bucket brigade drew water from the starboard side, took it to the engine room, and poured it onto the 'fire'. To prevent flooding, a separate crew hauled the accumulated water from the engine room, up to the main deck and heaved the water over the port side. The drill had previously gone according to plan until the orders became confused in translation. The bucket brigade began to draw the water from the starboard side, run over to the port side and then throw the water overboard, bypassing the engine room completely. Additionally, the term is documented to have been used in the U.S. Marine Corps during World War II, where it was often expressed in the phrase "as screwed up as a Chinese fire drill". It was also commonly used by Americans during the Korean War and the Vietnam War.

Historians trace Westerners' use of the word Chinese to denote "confusion" and "incomprehensibility" to the earliest contacts between Europeans and Chinese people in the 1600s, and attribute it to Europeans' inability to understand China's radically different culture and world view. In his 1989 "Dictionary of Invective", British editor Hugh Rawson lists 16 phrases that use the word Chinese to denote "incompetence, fraud and disorganization".

Other examples of such use include:

The term is also used to describe a U.S. collegiate prank (also known as red-light green-light) performed by a vehicle's occupants when stopped at a traffic light, especially when there is a need to swap drivers or fetch something from the trunk. Before the light changes to green, each occupant gets out, runs around the vehicle, and gets back inside.

The prank varies regionally. In North Carolina for example, it does not need to be performed strictly at a red light. A cul-de-sac is often used.

A Chinese Firedrill is the name of a solo project by Armored Saint and Fates Warning bassist Joey Vera, which to date has released a single album "Circles" in 2007. The name comes from the different musical foundations in each song, almost giving the impression that, like a "Chinese firedrill", it is "chaotic or confusing."



</doc>
<doc id="25992931" url="https://en.wikipedia.org/wiki?curid=25992931" title="Attention seeking">
Attention seeking

Attention seeking behavior is to act in a way that is likely to elicit attention, usually to elicit validation from others. People are thought to engage in both positive and negative attention seeking behavior independent of the actual benefit or harm to health. Most behavior that is motivated by attention seeking is considered to be driven by self-consciousness and thus an externalization of personality rather than internal and self-motivated behavior. This type of influence on behavior can result in a potential loss of a person's sense of agency, personality disorder and the behavior associated with these conditions.

Enjoying the attention of others is socially acceptable in some situations. In some instances, however, the need for attention can lead to new difficulties and may highlight underlying, preexisting ones. However, as a tactical method, it is often used in combat, theatre (upstaging) and it is fundamental to marketing. One strategy used to counter various types of attention-seeking behavior is planned ignoring.

If as a child, the person did not receive much attention from their parents or their peers then they may grow up feeling neglected. Those feelings will then be the main drive behind the person's attention-seeking behavior. Children of abusive parents and parents who are always absent may feel overlooked, and so the child may grow up becoming an attention-seeking adult.

Sometimes adults seek attention because of jealousy. When someone finds themselves threatened by another person who takes all the attention, they may respond with attention-seeking behavior.

Lack of self-esteem can be another cause for attention-seeking behavior. Some people think that they are overlooked and so they think that the only solution to restore their balance is to bring back the lost attention. The attention they will get in this case will provide them with reassurance and will help them think that they are worthy.

Narcissists are also attention seekers. They consider this attention a good source of narcissistic supply and so they strive to get it.


Planned ignoring is a strategy where a person gives no outward sign of recognizing a behavior, such as no eye contact, no verbal response and no physical response to the person seeking attention. The desired consequence of attention-seeking behavior is receiving attention in some form (positive or negative) from another person; when attention-seeking behavior no longer attracts attention, it may eventually cease.




</doc>
<doc id="48722353" url="https://en.wikipedia.org/wiki?curid=48722353" title="Token resistance">
Token resistance

Token resistance is a term, commonly referred to in the seduction community, denoting a rejection of advances, almost always of the sexual kind, with intention of actually engaging in the activity that was initially rejected.

Studies have refuted the stereotype that most women—and only women—engage in token resistance to sex, with results indicating that only a very small fraction of women and men have ever engaged in token resistance.

Token Resistance is reported by women and men. there's results for men and women when they say "no" to something in a sexual context, it means no. 


</doc>
<doc id="48909822" url="https://en.wikipedia.org/wiki?curid=48909822" title="Guiltive">
Guiltive

The guiltive is a term introduced by John Haiman for the speaker attitude whereby the speaker overtly presents him- or herself as generous or indifferent but actually means the opposite of what he or she is saying, with the intention of making the addressee feel guilty.

The guiltive is similar to sarcasm: in both, the speaker's ostensible message is accompanied by a derived metamessage "This message is bogus." In sarcasm this is overtly marked by the speaker (for example, using intonation or caricatured formality), whereas in the guiltive it is instead "left to be supplied by the addressee, who is thereby made to feel like a worm." The fact that the speaker still sounds sincere (albeit known not to be) suggests an affinity with polite language. But unlike politeness, the purpose of which is to avoid aggression, the guiltive is a form of passive-aggressiveness intended to make the listener feel bad.

The name "guiltive" is formed with the "-ive" suffix, which is commonly used for the names of grammatical moods. But as with sarcasm, no language has been found to have grammaticalized it.



</doc>
<doc id="9703" url="https://en.wikipedia.org/wiki?curid=9703" title="Evolutionary psychology">
Evolutionary psychology

Evolutionary psychology is a theoretical approach in the social and natural sciences that examines psychological structure from a modern evolutionary perspective. It seeks to identify which human psychological traits are evolved adaptations – that is, the functional products of natural selection or sexual selection in human evolution. Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and immune system, is common in evolutionary biology. Some evolutionary psychologists apply the same thinking to psychology, arguing that the modularity of mind is similar to that of the body and with different modular adaptations serving different functions. Evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments.

Evolutionary psychology is not simply a subdiscipline of psychology but its evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology in the same way evolutionary biology has for biology.

Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. There have been studies of human social behaviour related to infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment, with impressive findings.

The theories and findings of evolutionary psychology have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature.

Criticism of evolutionary psychology involves questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.

Evolutionary psychology is an approach that views human nature as the product of a universal set of evolved psychological adaptations to recurring problems in the ancestral environment. Proponents suggest that it seeks to integrate psychology into the other natural sciences, rooting it in the organizing theory of biology (evolutionary theory), and thus understanding psychology as a branch of biology. Anthropologist John Tooby and psychologist Leda Cosmides note:

Evolutionary psychology is the long-forestalled scientific attempt to assemble out of the disjointed, fragmentary, and mutually contradictory human disciplines a single, logically integrated research framework for the psychological, social, and behavioral sciences – a framework
that not only incorporates the evolutionary sciences on a full and equal basis, but that systematically works out all of the revisions in existing belief and research practice that such a synthesis requires.

Just as human physiology and evolutionary physiology have worked to identify physical adaptations of the body that represent "human physiological nature," the purpose of evolutionary psychology is to identify evolved emotional and cognitive adaptations that represent "human psychological nature." According to Steven Pinker, it is "not a single theory but a large set of hypotheses" and a term that "has also come to refer to a particular way of applying evolutionary theory to the mind, with an emphasis on adaptation, gene-level selection, and modularity." Evolutionary psychology adopts an understanding of the mind that is based on the computational theory of mind. It describes mental processes as computational operations, so that, for example, a fear response is described as arising from a neurological computation that inputs the perceptional data, e.g. a visual image of a spider, and outputs the appropriate reaction, e.g. fear of possibly dangerous animals. Under this view, any Domain-general learning is impossible because of the Combinatorial explosion. This implies Domain-specific learning. Evolutionary Psychology specifies the domain as the problems of survival and reproduction.

While philosophers have generally considered the human mind to include broad faculties, such as reason and lust, evolutionary psychologists describe evolved psychological mechanisms as narrowly focused to deal with specific issues, such as catching cheaters or choosing mates. The discipline views the human brain as comprising many functional mechanisms, called "psychological adaptations" or evolved cognitive mechanisms or "cognitive modules", designed by the process of natural selection. Examples include language-acquisition modules, incest-avoidance mechanisms, cheater-detection mechanisms, intelligence and sex-specific mating preferences, foraging mechanisms, alliance-tracking mechanisms, agent-detection mechanisms, and others. Some mechanisms, termed "domain-specific", deal with recurrent adaptive problems over the course of human evolutionary history. "Domain-general" mechanisms, on the other hand, are proposed to deal with evolutionary novelty.

Evolutionary psychology has roots in cognitive psychology and evolutionary biology but also draws on behavioral ecology, artificial intelligence, genetics, ethology, anthropology, archaeology, biology, and zoology. It is closely linked to sociobiology, but there are key differences between them including the emphasis on "domain-specific" rather than "domain-general" mechanisms, the relevance of measures of current fitness, the importance of mismatch theory, and psychology rather than behavior. Most of what is now labeled as sociobiological research is now confined to the field of behavioral ecology.

Nikolaas Tinbergen's four categories of questions can help to clarify the distinctions between several different, but complementary, types of explanations. Evolutionary psychology focuses primarily on the "why?" questions, while traditional psychology focuses on the "how?" questions.

Evolutionary psychology is founded on several core premises.


Evolutionary psychology has its historical roots in Charles Darwin's theory of natural selection. In "The Origin of Species", Darwin predicted that psychology would develop an evolutionary basis:

Two of his later books were devoted to the study of animal emotions and psychology; "The Descent of Man, and Selection in Relation to Sex" in 1871 and "The Expression of the Emotions in Man and Animals" in 1872. Darwin's work inspired William James's functionalist approach to psychology. Darwin's theories of evolution, adaptation, and natural selection have provided insight into why brains function the way they do.

The content of evolutionary psychology has derived from, on the one hand, the biological sciences (especially evolutionary theory as it relates to ancient human environments, the study of paleoanthropology and animal behavior) and, on the other, the human sciences, especially psychology.

Evolutionary biology as an academic discipline emerged with the modern synthesis in the 1930s and 1940s. In the 1930s the study of animal behavior (ethology) emerged with the work of the Dutch biologist Nikolaas Tinbergen and the Austrian biologists Konrad Lorenz and Karl von Frisch.

W.D. Hamilton's (1964) papers on inclusive fitness and Robert Trivers's (1972) theories on reciprocity and parental investment helped to establish evolutionary thinking in psychology and the other social sciences. In 1975, Edward O. Wilson combined evolutionary theory with studies of animal and social behavior, building on the works of Lorenz and Tinbergen, in his book "".

In the 1970s, two major branches developed from ethology. Firstly, the study of animal "social" behavior (including humans) generated sociobiology, defined by its pre-eminent proponent Edward O. Wilson in 1975 as "the systematic study of the biological basis of all social behavior" and in 1978 as "the extension of population biology and evolutionary theory to social organization." Secondly, there was behavioral ecology which placed less emphasis on "social" behavior; it focused on the ecological and evolutionary basis of animal and human behavior.

In the 1970s and 1980s university departments began to include the term "evolutionary biology" in their titles. The modern era of evolutionary psychology was ushered in, in particular, by Donald Symons' 1979 book "The Evolution of Human Sexuality" and Leda Cosmides and John Tooby's 1992 book "The Adapted Mind".

From psychology there are the primary streams of developmental, social and cognitive psychology. Establishing some measure of the relative influence of genetics and environment on behavior has been at the core of behavioral genetics and its variants, notably studies at the molecular level that examine the relationship between genes, neurotransmitters and behavior. Dual inheritance theory (DIT), developed in the late 1970s and early 1980s, has a slightly different perspective by trying to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. DIT is seen by some as a "middle-ground" between views that emphasize human universals versus those that emphasize cultural variation.

The theories on which evolutionary psychology is based originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.

Evolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively high maximal potential reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively low maximal potential reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, and he relied on group selection to explain the evolution of altruistic (self-sacrificing) behavior. But group selection was considered a weak explanation, because in any group the less altruistic individuals will be more likely to survive, and the group will become less self-sacrificing as a whole.

In 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a gene-centered view of evolution. Hamilton noted that genes can increase the replication of copies of themselves into the next generation by influencing the organism's social traits in such a way that (statistically) results in helping the survival and reproduction of other copies of the same genes (most simply, identical copies in the organism's close relatives). According to Hamilton's rule, self-sacrificing behaviors (and the genes influencing them) can evolve if they typically help the organism's close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how altruism can evolve. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories help to explain the development of altruistic behavior, and account for hostility toward cheaters (individuals that take advantage of others' altruism).

Several mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring, while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers–Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture, and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).

Evolutionary psychology is based on the hypothesis that, just like hearts, lungs, livers, kidneys, and immune systems, cognition has functional structure that has a genetic basis, and therefore has evolved by natural selection. Like other organs and tissues, this functional structure should be universally shared amongst a species, and should solve important problems of survival and reproduction.

Evolutionary psychologists seek to understand psychological mechanisms by understanding the survival and reproductive functions they might have served over the course of evolutionary history. These might include abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, cooperate with others and follow leaders. Consistent with the theory of natural selection, evolutionary psychology sees humans as often in conflict with others, including mates and relatives. For instance, a mother may wish to wean her offspring from breastfeeding earlier than does her infant, which frees up the mother to invest in additional offspring. Evolutionary psychology also recognizes the role of kin selection and reciprocity in evolving prosocial traits such as altruism. Like chimpanzees and bonobos, humans have subtle and flexible social instincts, allowing them to form extended families, lifelong friendships, and political alliances. In studies testing theoretical predictions, evolutionary psychologists have made modest findings on topics such as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price and parental investment.

Proponents of evolutionary psychology in the 1990s made some explorations in historical events, but the response from historical experts was highly negative and there has been little effort to continue that line of research. Historian Lynn Hunt says that the historians complained that the researchers:

Hunt states that, "the few attempts to build up a subfield of psychohistory collapsed under the weight of its presuppositions." She concludes that as of 2014 the "'iron curtain' between historians and psychology...remains standing."

Not all traits of organisms are evolutionary adaptations. As noted in the table below, traits may also be exaptations, byproducts of adaptations (sometimes called "spandrels"), or random variation between individuals.

Psychological adaptations are hypothesized to be innate or relatively easy to learn, and to manifest in cultures worldwide. For example, the ability of toddlers to learn a language with virtually no training is likely to be a psychological adaptation. On the other hand, ancestral humans did not read or write, thus today, learning to read and write require extensive training, and presumably represent byproducts of cognitive processing that use psychological adaptations designed for other functions. However, variations in manifest behavior can result from universal mechanisms interacting with different local environments. For example, Caucasians who move from a northern climate to the equator will have darker skin. The mechanisms regulating their pigmentation do not change; rather the input to those mechanisms change, resulting in different output.

One of the tasks of evolutionary psychology is to identify which psychological traits are likely to be adaptations, byproducts or random variation. George C. Williams suggested that an "adaptation is a special and onerous concept that should only be used where it is really necessary." As noted by Williams and others, adaptations can be identified by their improbable complexity, species universality, and adaptive functionality.

A question that may be asked about an adaptation is whether it is generally obligate (relatively robust in the face of typical environmental variation) or facultative (sensitive to typical environmental variation). The sweet taste of sugar and the pain of hitting one's knee against concrete are the result of fairly obligate psychological adaptations; typical environmental variability during development does not much affect their operation. By contrast, facultative adaptations are somewhat like "if-then" statements. For example, adult attachment style seems particularly sensitive to early childhood experiences. As adults, the propensity to develop close, trusting bonds with others is dependent on whether early childhood caregivers could be trusted to provide reliable assistance and attention. The adaptation for skin to tan is conditional to exposure to sunlight; this is an example of another facultative adaptation. When a psychological adaptation is facultative, evolutionary psychologists concern themselves with how developmental and environmental inputs influence the expression of the adaptation.

Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Cultural universals include behaviors related to language, cognition, social roles, gender roles, and technology. Evolved psychological adaptations (such as the ability to learn a language) interact with cultural inputs to produce specific behaviors (e.g., the specific language learned). Basic gender differences, such as greater eagerness for sex among men and greater coyness among women, are explained as sexually dimorphic psychological adaptations that reflect the different reproductive strategies of males and females. Evolutionary psychologists contrast their approach to what they term the "standard social science model," according to which the mind is a general-purpose cognition device shaped almost entirely by culture.

Evolutionary psychology argues that to properly understand the functions of the brain, one must understand the properties of the environment in which the brain evolved. That environment is often referred to as the "environment of evolutionary adaptedness".

The idea of an "environment of evolutionary adaptedness" was first explored as a part of attachment theory by John Bowlby. This is the environment to which a particular evolved mechanism is adapted. More specifically, the environment of evolutionary adaptedness is defined as the set of historically recurring selection pressures that formed a given adaptation, as well as those aspects of the environment that were necessary for the proper development and functioning of the adaptation.

Humans, comprising the genus "Homo", appeared between 1.5 and 2.5 million years ago, a time that roughly coincides with the start of the Pleistocene 2.6 million years ago. Because the Pleistocene ended a mere 12,000 years ago, most human adaptations either newly evolved during the Pleistocene, or were maintained by stabilizing selection during the Pleistocene. Evolutionary psychology therefore proposes that the majority of human psychological mechanisms are adapted to reproductive problems frequently encountered in Pleistocene environments. In broad terms, these problems include those of growth, development, differentiation, maintenance, mating, parenting, and social relationships.

The environment of evolutionary adaptedness is significantly different from modern society. The ancestors of modern humans lived in smaller groups, had more cohesive cultures, and had more stable and rich contexts for identity and meaning. Researchers look to existing hunter-gatherer societies for clues as to how hunter-gatherers lived in the environment of evolutionary adaptedness. Unfortunately, the few surviving hunter-gatherer societies are different from each other, and they have been pushed out of the best land and into harsh environments, so it is not clear how closely they reflect ancestral culture. However, all around the world small-band hunter-gatherers offer a similar developmental system for the young ("hunter-gatherer childhood model," Konner, 2005; 
evolved developmental niche" or "evolved nest;" Narvaez et al., 2013). The characteristics of the niche are largely the same as for social mammals, who evolved over 30 million years ago: soothing perinatal experience, several years of on-request breastfeeding, nearly constant affection or physical proximity, responsiveness to need (mitigating offspring distress), self-directed play, and for humans, multiple responsive caregivers. Initial studies show the importance of these components in early life for positive child outcomes.

Evolutionary psychologists sometimes look to chimpanzees, bonobos, and other great apes for insight into human ancestral behavior. Christopher Ryan and Cacilda Jethá argue that evolutionary psychologists have overemphasized the similarity of humans and chimps, which are more violent, while underestimating the similarity of humans and bonobos, which are more peaceful.

Since an organism's adaptations were suited to its ancestral environment, a new and different environment can create a mismatch. Because humans are mostly adapted to Pleistocene environments, psychological mechanisms sometimes exhibit "mismatches" to the modern environment. One example is the fact that although about 10,000 people are killed with guns in the US annually, whereas spiders and snakes kill only a handful, people nonetheless learn to fear spiders and snakes about as easily as they do a pointed gun, and more easily than an unpointed gun, rabbits or flowers. A potential explanation is that spiders and snakes were a threat to human ancestors throughout the Pleistocene, whereas guns (and rabbits and flowers) were not. There is thus a mismatch between humans' evolved fear-learning psychology and the modern environment.

This mismatch also shows up in the phenomena of the supernormal stimulus, a stimulus that elicits a response more strongly than the stimulus for which the response evolved. The term was coined by Niko Tinbergen to refer to non-human animal behavior, but psychologist Deirdre Barrett said that supernormal stimulation governs the behavior of humans as powerfully as that of other animals. She explained junk food as an exaggerated stimulus to cravings for salt, sugar, and fats, and she says that television is an exaggeration of social cues of laughter, smiling faces and attention-grabbing action. Magazine centerfolds and double cheeseburgers pull instincts intended for an environment of evolutionary adaptedness where breast development was a sign of health, youth and fertility in a prospective mate, and fat was a rare and vital nutrient. The psychologist Mark van Vugt recently argued that modern organizational leadership is a mismatch. His argument is that humans are not adapted to work in large, anonymous bureaucratic structures with formal hierarchies. The human mind still responds to personalized, charismatic leadership primarily in the context of informal, egalitarian settings. Hence the dissatisfaction and alienation that many employees experience. Salaries, bonuses and other privileges exploit instincts for relative status, which attract particularly males to senior executive positions.

Evolutionary theory is heuristic in that it may generate hypotheses that might not be developed from other theoretical approaches. One of the major goals of adaptationist research is to identify which organismic traits are likely to be adaptations, and which are byproducts or random variations. As noted earlier, adaptations are expected to show evidence of complexity, functionality, and species universality, while byproducts or random variation will not. In addition, adaptations are expected to manifest as proximate mechanisms that interact with the environment in either a generally obligate or facultative fashion (see above). Evolutionary psychologists are also interested in identifying these proximate mechanisms (sometimes termed "mental mechanisms" or "psychological adaptations") and what type of information they take as input, how they process that information, and their outputs. Evolutionary developmental psychology, or "evo-devo," focuses on how adaptations may be activated at certain developmental times (e.g., losing baby teeth, adolescence, etc.) or how events during the development of an individual may alter life history trajectories.

Evolutionary psychologists use several strategies to develop and test hypotheses about whether a psychological trait is likely to be an evolved adaptation. Buss (2011) notes that these methods include:

Evolutionary psychologists also use various sources of data for testing, including experiments, archaeological records, data from hunter-gatherer societies, observational studies, neuroscience data, self-reports and surveys, public records, and human products.
Recently, additional methods and tools have been introduced based on fictional scenarios, mathematical models, and multi-agent computer simulations.

Foundational areas of research in evolutionary psychology can be divided into broad categories of adaptive problems that arise from the theory of evolution itself: survival, mating, parenting, family and kinship, interactions with non-kin, and cultural evolution.

Problems of survival are clear targets for the evolution of physical and psychological adaptations. Major problems the ancestors of present-day humans faced included food selection and acquisition; territory selection and physical shelter; and avoiding predators and other environmental threats.

Consciousness meets George Williams' criteria of species universality, complexity, and functionality, and it is a trait that apparently increases fitness.

In his paper "Evolution of consciousness," John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness. In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social "and" natural environments by providing an energy-saving "neutral" gear in an otherwise energy-expensive motor output machine. Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars. Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought. Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches. Consistent with this hypothesis, Gordon Gallup found that chimps and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.

The concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behavior involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviors are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so. Evolutionary psychology approaches self-deception as an adaptation that can improve one's results in social exchanges.

Sleep may have evolved to conserve energy when activity would be less fruitful or more dangerous, such as at night, and especially during the winter season.

Many experts, such as Jerry Fodor, write that the purpose of perception is knowledge, but evolutionary psychologists hold that its primary purpose is to guide action. For example, they say, depth perception seems to have evolved not to help us know the distances to other objects but rather to help us move around in space. Evolutionary psychologists say that animals from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge.

Building and maintaining sense organs is metabolically expensive, so these organs evolve only when they improve an organism's fitness. More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources, so the senses must provide exceptional benefits to fitness. Perception accurately mirrors the world; animals get useful, accurate information through their senses.

Scientists who study perception and sensation have long understood the human senses as adaptations to their surrounding worlds. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects. Larger animals naturally make lower-pitched sounds as a consequence of their size. The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds. Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the environment of evolutionary adaptedness. For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the environment of evolutionary adaptedness, so present day humans have an intrinsic hunger for salty and sweet tastes. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.

Evolutionary psychologists contend that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks. For example, people with damage to a particular part of the brain suffer from the specific defect of not being able to recognize faces (prosopagnosia). Evolutionary psychology suggests that this indicates a so-called face-reading module.

In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations. Facultative adaptations express themselves differently depending on input from the environment. Sometimes the input comes during development and helps shape that development. For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation. Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods. The input can also come during daily tasks, helping the organism cope with changing environmental conditions. For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships. Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water. Learning capacities sometimes demonstrate differences between the sexes. In many animal species, for example, males can solve spatial problem faster and more accurately than females, due to the effects of male hormones during development. The same might be true of humans.

Motivations direct and energize behavior, while emotions provide the affective component to motivation, positive or negative. In the early 1970s, Paul Ekman and colleagues began a line of research which suggests that many emotions are universal. He found evidence that humans share at least five basic emotions: fear, sadness, happiness, anger, and disgust. Social emotions evidently evolved to motivate social behaviors that were adaptive in the environment of evolutionary adaptedness. For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared. Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.
Motivation has a neurobiological basis in the reward system of the brain. Recently, it has been suggested that reward systems may evolve in such a way that there may be an inherent or unavoidable trade-off in the motivational system for activities of short versus long duration.

Cognition refers to internal representations of the world and internal information processing. From an evolutionary psychology perspective, cognition is not "general purpose," but uses heuristics, or strategies, that generally increase the likelihood of solving problems that the ancestors of present-day humans routinely faced. For example, present day humans are far more likely to solve logic problems that involve detecting cheating (a common problem given humans' social nature) than the same logic problem put in purely abstract terms. Since the ancestors of present-day humans did not encounter truly random events, present day humans may be cognitively predisposed to incorrectly identify patterns in random sequences. "Gamblers' Fallacy" is one example of this. Gamblers may falsely believe that they have hit a "lucky streak" even when each outcome is actually random and independent of previous trials. Most people believe that if a fair coin has been flipped 9 times and Heads appears each time, that on the tenth flip, there is a greater than 50% chance of getting Tails. Humans find it far easier to make diagnoses or predictions using frequency data than when the same information is presented as probabilities or percentages, presumably because the ancestors of present-day humans lived in relatively small tribes (usually with fewer than 150 people) where frequency information was more readily available.

Evolutionary psychology is primarily interested in finding commonalities between people, or basic human psychological nature. From an evolutionary perspective, the fact that people have fundamental differences in personality traits initially presents something of a puzzle. (Note: The field of behavioral genetics is concerned with statistically partitioning differences between people into genetic and environmental sources of variance. However, understanding the concept of heritability can be tricky – heritability refers only to the differences between people, never the degree to which the traits of an individual are due to environmental or genetic factors, since traits are always a complex interweaving of both.)

Personality traits are conceptualized by evolutionary psychologists as due to normal variation around an optimum, due to frequency-dependent selection (behavioral polymorphisms), or as facultative adaptations. Like variability in height, some personality traits may simply reflect inter-individual variability around a general optimum. Or, personality traits may represent different genetically predisposed "behavioral morphs" – alternate behavioral strategies that depend on the frequency of competing behavioral strategies in the population. For example, if most of the population is generally trusting and gullible, the behavioral morph of being a "cheater" (or, in the extreme case, a sociopath) may be advantageous. Finally, like many other psychological adaptations, personality traits may be facultative – sensitive to typical variations in the social environment, especially during early development. For example, later born children are more likely than first borns to be rebellious, less conscientious and more open to new experiences, which may be advantageous to them given their particular niche in family structure. It is important to note that shared environmental influences do play a role in personality and are not always of less importance than genetic factors. However, shared environmental influences often decrease to near zero after adolescence but do not completely disappear.

According to Steven Pinker, who builds on the work by Noam Chomsky, the universal human ability to learn to talk between the ages of 1 – 4, basically without training, suggests that language acquisition is a distinctly human psychological adaptation (see, in particular, Pinker's "The Language Instinct"). Pinker and Bloom (1990) argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop.

Pinker follows Chomsky in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction. Chomsky himself does not believe language to have evolved as an adaptation, but suggests that it likely evolved as a byproduct of some other adaptation, a so-called spandrel. But Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.

Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gopnik identified a dominant gene that causes language impairment in the KE family of Britain. This gene turned out to be a mutation of the FOXP2 gene. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. However, the once-popular idea that FOXP2 is a 'grammar gene' or that it triggered the emergence of language in "Homo sapiens" is now widely discredited.

Currently several competing theories about the evolutionary origin of language coexist, none of them having achieved a general consensus. Researchers of language acquisition in primates and humans such as Michael Tomasello and Talmy Givón, argue that the innatist framework has understated the role of imitation in learning and that it is not at all necessary to posit the existence of an innate grammar module to explain human language acquisition. Tomasello argues that studies of how children and primates actually acquire communicative skills suggests that humans learn complex behavior through experience, so that instead of a module specifically dedicated to language acquisition, language is acquired by the same cognitive mechanisms that are used to acquire all other kinds of socially transmitted behavior.

On the issue of whether language is best seen as having evolved as an adaptation or as a spandrel, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptionist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading. He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in "The Symbolic Species" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.

If the theory that language could have evolved as a single adaptation is accepted, the question becomes which of its many functions has been the basis of adaptation. Several evolutionary hypotheses have been posited: that language evolved for the purpose of social grooming, that it evolved as a way to show mating potential or that it evolved to form social contracts. Evolutionary psychologists recognize that these theories are all speculative and that much more evidence is required to understand how language might have been selectively adapted.

Given that sexual reproduction is the means by which genes are propagated into future generations, sexual selection plays a large role in human evolution. Human mating, then, is of interest to evolutionary psychologists who aim to investigate evolved mechanisms to attract and secure mates. Several lines of research have stemmed from this interest, such as studies of mate selection mate poaching, mate retention, mating preferences and conflict between the sexes.

In 1972 Robert Trivers published an influential paper on sex differences that is now referred to as parental investment theory. The size differences of gametes (anisogamy) is the fundamental, defining difference between males (small gametes – sperm) and females (large gametes – ova). Trivers noted that anisogamy typically results in different levels of parental investment between the sexes, with females initially investing more. Trivers proposed that this difference in parental investment leads to the sexual selection of different reproductive strategies between the sexes and to sexual conflict. For example, he suggested that the sex that invests less in offspring will generally compete for access to the higher-investing sex to increase their inclusive fitness (also see Bateman's principle<ref name="doi10.1038/hdy.1948.21"></ref>). Trivers posited that differential parental investment led to the evolution of sexual dimorphisms in mate choice, intra- and inter- sexual reproductive competition, and courtship displays. In mammals, including humans, females make a much larger parental investment than males (i.e. gestation followed by childbirth and lactation). Parental investment theory is a branch of life history theory.

Buss and Schmitt's (1993) "Sexual Strategies Theory" proposed that, due to differential parental investment, humans have evolved sexually dimorphic adaptations related to "sexual accessibility, fertility assessment, commitment seeking and avoidance, immediate and enduring resource procurement, paternity certainty, assessment of mate value, and parental investment." Their "Strategic Interference Theory" suggested that conflict between the sexes occurs when the preferred reproductive strategies of one sex interfere with those of the other sex, resulting in the activation of emotional responses such as anger or jealousy.

Women are generally more selective when choosing mates, especially under long term mating conditions. However, under some circumstances, short term mating can provide benefits to women as well, such as fertility insurance, trading up to better genes, reducing risk of inbreeding, and insurance protection of her offspring.

Due to male paternity insecurity, sex differences have been found in the domains of sexual jealousy. Females generally react more adversely to emotional infidelity and males will react more to sexual infidelity. This particular pattern is predicted because the costs involved in mating for each sex are distinct. Women, on average, should prefer a mate who can offer resources (e.g., financial, commitment), thus, a woman risks losing such resources with a mate who commits emotional infidelity. Men, on the other hand, are never certain of the genetic paternity of their children because they do not bear the offspring themselves ("paternity insecurity"). This suggests that for men sexual infidelity would generally be more aversive than emotional infidelity because investing resources in another man's offspring does not lead to propagation of their own genes.

Another interesting line of research is that which examines women's mate preferences across the ovulatory cycle. The theoretical underpinning of this research is that ancestral women would have evolved mechanisms to select mates with certain traits depending on their hormonal status. Known as the ovulatory shift hypothesis, the theory posits that, during the ovulatory phase of a woman's cycle (approximately days 10–15 of a woman's cycle), a woman who mated with a male with high genetic quality would have been more likely, on average, to produce and rear a healthy offspring than a woman who mated with a male with low genetic quality. These putative preferences are predicted to be especially apparent for short-term mating domains because a potential male mate would only be offering genes to a potential offspring. This hypothesis allows researchers to examine whether women select mates who have characteristics that indicate high genetic quality during the high fertility phase of their ovulatory cycles. Indeed, studies have shown that women's preferences vary across the ovulatory cycle. In particular, Haselton and Miller (2006) showed that highly fertile women prefer creative but poor men as short-term mates. Creativity may be a proxy for good genes. Research by Gangestad et al. (2004) indicates that highly fertile women prefer men who display social presence and intrasexual competition; these traits may act as cues that would help women predict which men may have, or would be able to acquire, resources.

Reproduction is always costly for women, and can also be for men. Individuals are limited in the degree to which they can devote time and resources to producing and raising their young, and such expenditure may also be detrimental to their future condition, survival and further reproductive output.
Parental investment is any parental expenditure (time, energy etc.) that benefits one offspring at a cost to parents' ability to invest in other components of fitness (Clutton-Brock 1991: 9; Trivers 1972). Components of fitness (Beatty 1992) include the well-being of existing offspring, parents' future reproduction, and inclusive fitness through aid to kin (Hamilton, 1964). Parental investment theory is a branch of life history theory.

Robert Trivers' theory of parental investment predicts that the sex making the largest investment in lactation, nurturing and protecting offspring will be more discriminating in mating and that the sex that invests less in offspring will compete for access to the higher investing sex (see Bateman's principle). Sex differences in parental effort are important in determining the strength of sexual selection.

The benefits of parental investment to the offspring are large and are associated with the effects on condition, growth, survival and ultimately, on reproductive success of the offspring. However, these benefits can come at the cost of parent's ability to reproduce in the future e.g. through the increased risk of injury when defending offspring against predators, the loss of mating opportunities whilst rearing offspring and an increase in the time to the next reproduction. Overall, parents are selected to maximize the difference between the benefits and the costs, and parental care will likely evolve when the benefits exceed the costs.

The Cinderella effect is an alleged high incidence of stepchildren being physically, emotionally or sexually abused, neglected, murdered, or otherwise mistreated at the hands of their stepparents at significantly higher rates than their genetic counterparts. It takes its name from the fairy tale character Cinderella, who in the story was cruelly mistreated by her stepmother and stepsisters. Daly and Wilson (1996) noted: "Evolutionary thinking led to the discovery of the most important risk factor for child homicide – the presence of a stepparent. Parental efforts and investments are valuable resources, and selection favors those parental psyches that allocate effort effectively to promote fitness. The adaptive problems that challenge parental decision making include both the accurate identification of one's offspring and the allocation of one's resources among them with sensitivity to their needs and abilities to convert parental investment into fitness increments…. Stepchildren were seldom or never so valuable to one's expected fitness as one's own offspring would be, and those parental psyches that were easily parasitized by just any appealing youngster must always have incurred a selective disadvantage"(Daly & Wilson, 1996, pp. 64–65). However, they note that not all stepparents will "want" to abuse their partner's children, or that genetic parenthood is any insurance against abuse. They see step parental care as primarily "mating effort" towards the genetic parent.

Inclusive fitness is the sum of an organism's classical fitness (how many of its own offspring it produces and supports) and the number of equivalents of its own offspring it can add to the population by supporting others. The first component is called classical fitness by Hamilton (1964).

From the gene's point of view, evolutionary success ultimately depends on leaving behind the maximum number of copies of itself in the population. Until 1964, it was generally believed that genes only achieved this by causing the individual to leave the maximum number of viable offspring. However, in 1964 W. D. Hamilton proved mathematically that, because close relatives of an organism share some identical genes, a gene can also increase its evolutionary success by promoting the reproduction and survival of these related or otherwise similar individuals. Hamilton concluded that this leads natural selection to favor organisms that would behave in ways that maximize their inclusive fitness. It is also true that natural selection favors behavior that maximizes personal fitness.

Hamilton's rule describes mathematically whether or not a gene for altruistic behavior will spread in a population:
where

The concept serves to explain how natural selection can perpetuate altruism. If there is an "altruism gene" (or complex of genes) that influences an organism's behavior to be helpful and protective of relatives and their offspring, this behavior also increases the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. Altruists may also have some way to recognize altruistic behavior in unrelated individuals and be inclined to support them. As Dawkins points out in "The Selfish Gene" (Chapter 6) and "The Extended Phenotype", this must be distinguished from the green-beard effect.

Although it is generally true that humans tend to be more altruistic toward their kin than toward non-kin, the relevant proximate mechanisms that mediate this cooperation have been debated (see kin recognition), with some arguing that kin status is determined primarily via social and cultural factors (such as co-residence, maternal association of sibs, etc.), while others have argued that kin recognition can also be mediated by biological factors such as facial resemblance and immunogenetic similarity of the major histocompatibility complex (MHC). For a discussion of the interaction of these social and biological kin recognition factors see Lieberman, Tooby, and Cosmides (2007) (PDF).

Whatever the proximate mechanisms of kin recognition there is substantial evidence that humans act generally more altruistically to close genetic kin compared to genetic non-kin.

Although interactions with non-kin are generally less altruistic compared to those with kin, cooperation can be maintained with non-kin via mutually beneficial reciprocity as was proposed by Robert Trivers. If there are repeated encounters between the same two players in an evolutionary game in which each of them can choose either to "cooperate" or "defect," then a strategy of mutual cooperation may be favored even if it pays each player, in the short term, to defect when the other cooperates. Direct reciprocity can lead to the evolution of cooperation only if the probability, w, of another encounter between the same two individuals exceeds the cost-to-benefit ratio of the altruistic act:

Reciprocity can also be indirect if information about previous interactions is shared. Reputation allows evolution of cooperation by indirect reciprocity. Natural selection favors strategies that base the decision to help on the reputation of the recipient: studies show that people who are more helpful are more likely to receive help. The calculations of indirect reciprocity are complicated and only a tiny fraction of this universe has been uncovered, but again a simple rule has emerged. Indirect reciprocity can only promote cooperation if the probability, q, of knowing someone’s reputation exceeds the cost-to-benefit ratio of the altruistic act:

One important problem with this explanation is that individuals may be able to evolve the capacity to obscure their reputation, reducing the probability, q, that it will be known.

Trivers argues that friendship and various social emotions evolved in order to manage reciprocity. Liking and disliking, he says, evolved to help present day humans' ancestors form coalitions with others who reciprocated and to exclude those who did not reciprocate. Moral indignation may have evolved to prevent one's altruism from being exploited by cheaters, and gratitude may have motivated present day humans' ancestors to reciprocate appropriately after benefiting from others' altruism. Likewise, present day humans feel guilty when they fail to reciprocate. These social motivations match what evolutionary psychologists expect to see in adaptations that evolved to maximize the benefits and minimize the drawbacks of reciprocity.

Evolutionary psychologists say that humans have psychological adaptations that evolved specifically to help us identify nonreciprocators, commonly referred to as "cheaters." In 1993, Robert Frank and his associates found that participants in a prisoner's dilemma scenario were often able to predict whether their partners would "cheat," based on a half-hour of unstructured social interaction. In a 1996 experiment, for example, Linda Mealey and her colleagues found that people were better at remembering the faces of people when those faces were associated with stories about those individuals cheating (such as embezzling money from a church).

Humans may have an evolved set of psychological adaptations that predispose them to be more cooperative than otherwise would be expected with members of their tribal in-group, and, more nasty to members of tribal out groups. These adaptations may have been a consequence of tribal warfare. Humans may also have predispositions for "altruistic punishment" – to punish in-group members who violate in-group rules, even when this altruistic behavior cannot be justified in terms of helping those you are related to (kin selection), cooperating with those who you will interact with again (direct reciprocity), or cooperating to better your reputation with others (indirect reciprocity).

Though evolutionary psychology has traditionally focused on individual-level behaviors, determined by species-typical psychological adaptations, considerable work has been done on how these adaptations shape and, ultimately govern, culture (Tooby and Cosmides, 1989). Tooby and Cosmides (1989) argued that the mind consists of many domain-specific psychological adaptations, some of which may constrain what cultural material is learned or taught. As opposed to a domain-general cultural acquisition program, where an individual passively receives culturally-transmitted material from the group, Tooby and Cosmides (1989), among others, argue that: "the psyche evolved to generate adaptive rather than repetitive behavior, and hence critically analyzes the behavior of those surrounding it in highly structured and patterned ways, to be used as a rich (but by no means the only) source of information out of which to construct a 'private culture' or individually tailored adaptive system; in consequence, this system may or may not mirror the behavior of others in any given respect." (Tooby and Cosmides 1989).

According to Paul Baltes, the benefits granted by evolutionary selection decrease with age. Natural selection has not eliminated many harmful conditions and nonadaptive characteristics that appear among older adults, such as Alzheimer disease. If it were a disease that killed 20-year-olds instead of 70-year-olds this may have been a disease that natural selection could have eliminated ages ago. Thus, unaided by evolutionary pressures against nonadaptive conditions, modern humans suffer the aches, pains, and infirmities of aging and as the benefits of evolutionary selection decrease with age, the need for modern technological mediums against non-adaptive conditions increases.

As humans are a highly social species, there are many adaptive problems associated with navigating the social world (e.g., maintaining allies, managing status hierarchies, interacting with outgroup members, coordinating social activities, collective decision-making). Researchers in the emerging field of evolutionary social psychology have made many discoveries pertaining to topics traditionally studied by social psychologists, including person perception, social cognition, attitudes, altruism, emotions, group dynamics, leadership, motivation, prejudice, intergroup relations, and cross-cultural differences.

When endeavouring to solve a problem humans at an early age show determination while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.

Adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies between physiological and psychological dysfunctions, as noted in the table below. Prominent theorists and evolutionary psychiatrists include Michael T. McGuire, Anthony Stevens, and Randolph M. Nesse. They, and others, suggest that mental disorders are due to the interactive effects of both nature and nurture, and often have multiple contributing causes.

Evolutionary psychologists have suggested that schizophrenia and bipolar disorder may reflect a side-effect of genes with fitness benefits, such as increased creativity. (Some individuals with bipolar disorder are especially creative during their manic phases and the close relatives of people with schizophrenia have been found to be more likely to have creative professions.) A 1994 report by the American Psychiatry Association found that people suffered from schizophrenia at roughly the same rate in Western and non-Western cultures, and in industrialized and pastoral societies, suggesting that schizophrenia is not a disease of civilization nor an arbitrary social invention. Sociopathy may represent an evolutionarily stable strategy, by which a small number of people who cheat on social contracts benefit in a society consisting mostly of non-sociopaths. Mild depression may be an adaptive response to withdraw from, and re-evaluate, situations that have led to disadvantageous outcomes (the "analytical rumination hypothesis") (see Evolutionary approaches to depression).

Some of these speculations have yet to be developed into fully testable hypotheses, and a great deal of research is required to confirm their validity.

Evolutionary psychology has been applied to explain criminal or otherwise immoral behavior as being adaptive or related to adaptive behaviors. Males are generally more aggressive than females, who are more selective of their partners because of the far greater effort they have to contribute to pregnancy and child-rearing. Males being more aggressive is hypothesized to stem from the more intense reproductive competition faced by them. Males of low status may be especially vulnerable to being childless. It may have been evolutionary advantageous to engage in highly risky and violently aggressive behavior to increase their status and therefore reproductive success. This may explain why males are generally involved in more crimes, and why low status and being unmarried is associated with criminality. Furthermore, competition over females is argued to have been particularly intensive in late adolescence and young adulthood, which is theorized to explain why crime rates are particularly high during this period.

Many conflicts that result in harm and death involve status, reputation, and seemingly trivial insults. Steven Pinker in his book "The Blank Slate" argues that in non-state societies without a police it was very important to have a credible deterrence against aggression. Therefore, it was important to be perceived as having a credible reputation for retaliation, resulting in humans to develop instincts for revenge as well as for protecting reputation ("honor"). Pinker argues that the development of the state and the police have dramatically reduced the level of violence compared to the ancestral environment. Whenever the state breaks down, which can be very locally such as in poor areas of a city, humans again organize in groups for protection and aggression and concepts such as violent revenge and protecting honor again become extremely important.

Rape is theorized to be a reproductive strategy that facilitates the propagation of the rapist's progeny. Such a strategy may be adopted by men who otherwise are unlikely to be appealing to women and therefore cannot form legitimate relationships, or by high status men on socially vulnerable women who are unlikely to retaliate to increase their reproductive success even further. The sociobiological theories of rape are highly controversial, as traditional theories typically do not consider rape to be a behavioral adaptation, and objections to this theory are made on ethical, religious, political, as well as scientific grounds.

Adaptationist perspectives on religious belief suggest that, like all behavior, religious behaviors are a product of the human brain. As with all other organ functions, cognition's functional structure has been argued to have a genetic foundation, and is therefore subject to the effects of natural selection and sexual selection. Like other organs and tissues, this functional structure should be universally shared amongst humans and should have solved important problems of survival and reproduction in ancestral environments. However, evolutionary psychologists remain divided on whether religious belief is more likely a consequence of evolved psychological adaptations, or a byproduct of other cognitive adaptations.

Coalitional psychology is an approach to explain political behaviors between different coalitions and the conditionality of these behaviors in evolutionary psychological perspective. This approach assumes that since human beings appeared on the earth, they have evolved to live in groups instead of living as individuals to achieve benefits such as more mating opportunities and increased status. Human beings thus naturally think and act in a way that manages and negotiates group dynamics.

Coalitional psychology posits five hypotheses on how these psychological adaptations operate:

Critics of evolutionary psychology accuse it of promoting genetic determinism, panadaptionism (the idea that all behaviors and anatomical features are adaptations), unfalsifiable hypotheses, distal or ultimate explanations of behavior when proximate explanations are superior, and malevolent political or moral ideas.

Critics have argued that evolutionary psychology might be used to justify existing social hierarchies and reactionary policies. It has also been suggested by critics that evolutionary psychologists' theories and interpretations of empirical data rely heavily on ideological assumptions about race and gender.

In response to such criticism, evolutionary psychologists often caution against committing the naturalistic fallacy – the assumption that "what is natural" is necessarily a moral good. However, their caution against committing the naturalistic fallacy has been criticized as means to stifle legitimate ethical discussions.

Some criticisms of evolutionary psychology point at contradictions between different aspects of adaptive scenarios posited by evolutionary psychology. One example is the evolutionary psychology model of extended social groups selecting for modern human brains, a contradiction being that the synaptic function of modern human brains require high amounts of many specific essential nutrients so that such a transition to higher requirements of the same essential nutrients being shared by all individuals in a population would decrease the possibility of forming large groups due to bottleneck foods with rare essential nutrients capping group sizes. It is mentioned that some insects have societies with different ranks for each individual and that monkeys remain socially functioning after removal of most of the brain as additional arguments against big brains promoting social networking. The model of males as both providers and protectors is criticized for the impossibility of being in two places at once, the male cannot both protect his family at home and be out hunting at the same time. In the case of the claim that a provider male could buy protection service for his family from other males by bartering food that he had hunted, critics point at the fact that the most valuable food (the food that contained the rarest essential nutrients) would be different in different ecologies and as such vegetable in some geographical areas and animal in others, making it impossible for hunting styles relying on physical strength or risk taking to be universally of similar value in bartered food and instead making it inevitable that in some parts of Africa, food gathered with no need for major physical strength would be the most valuable to barter for protection. A contradiction between evolutionary psychology's claim of men needing to be more sexually visual than women for fast speed of assessing women's fertility than women needed to be able to assess the male's genes and its claim of male sexual jealousy guarding against infidelity is also pointed at, as it would be pointless for a male to be fast to assess female fertility if he needed to assess the risk of there being a jealous male mate and in that case his chances of defeating him before mating anyway (pointlessness of assessing one necessary condition faster than another necessary condition can possibly be assessed).

Evolutionary psychology has been entangled in the larger philosophical and social science controversies related to the debate on nature versus nurture. Evolutionary psychologists typically contrast evolutionary psychology with what they call the standard social science model (SSSM). They characterize the SSSM as the "blank slate", "relativist", "social constructionist", and "cultural determinist" perspective that they say dominated the social sciences throughout the 20th century and assumed that the mind was shaped almost entirely by culture.

Critics have argued that evolutionary psychologists created a false dichotomy between their own view and the caricature of the SSSM. Other critics regard the SSSM as a rhetorical device or a straw man and suggest that the scientists whom evolutionary psychologists associate with the SSSM did not believe that the mind was a blank state devoid of any natural predispositions.

Some critics view evolutionary psychology as a form of genetic reductionism and genetic determinism, a common critique being that evolutionary psychology does not address the complexity of individual development and experience and fails to explain the influence of genes on behavior in individual cases. Evolutionary psychologists respond that they are working within a nature-nurture interactionist framework that acknowledges that many psychological adaptations are facultative (sensitive to environmental variations during individual development). The discipline is generally not focused on proximate analyses of behavior, but rather its focus is on the study of distal/ultimate causality (the evolution of psychological adaptations). The field of behavioral genetics is focused on the study of the proximate influence of genes on behavior.

A frequent critique of the discipline is that the hypotheses of evolutionary psychology are frequently arbitrary and difficult or impossible to adequately test, thus questioning its status as an actual scientific discipline, for example because many current traits probably evolved to serve different functions than they do now. While evolutionary psychology hypotheses are difficult to test, evolutionary psychologists assert that it is not impossible. Part of the critique of the scientific base of evolutionary psychology includes a critique of the concept of the Environment of Evolutionary Adaptation. Some critics have argued that researchers know so little about the environment in which "Homo sapiens" evolved that explaining specific traits as an adaption to that environment becomes highly speculative. Evolutionary psychologists respond that they do know many things about this environment, including the facts that present day humans' ancestors were hunter-gatherers, that they generally lived in small tribes, etc.

Evolutionary psychologists generally presume that, like the body, the mind is made up of many evolved modular adaptations, although there is some disagreement within the discipline regarding the degree of general plasticity, or "generality," of some modules. It has been suggested that modularity evolves because, compared to non-modular networks, it would have conferred an advantage in terms of fitness and because connection costs are lower.

In contrast, some academics argue that it is unnecessary to posit the existence of highly domain specific modules, and, suggest that the neural anatomy of the brain supports a model based on more domain general faculties and processes. Moreover, empirical support for the domain-specific theory stems almost entirely from performance on variations of the Wason selection task which is extremely limited in scope as it only tests one subtype of deductive reasoning.

Cecilia Heyes argues that the picture presented by some evolutionary psychology of the human mind as a collection of cognitive instincts - organs of thought shaped by genetic evolution over very long time periods - does not fit research results. She posits instead that humans have cognitive gadgets - 'special-purpose organs of thought' built in the course of development through social interaction. These are products of cultural rather than genetic evolution, and may develop and change much more quickly and flexibly than cognitive instincts.

Evolutionary psychologists have addressed many of their critics (see, for example, books by Segerstråle (2000), "Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond," Barkow (2005), "Missing the Revolution: Darwinism for Social Scientists," and Alcock (2001), "The Triumph of Sociobiology"). Among their rebuttals are that some criticisms are straw men, are based on an incorrect nature versus nurture dichotomy, are based on misunderstandings of the discipline, etc. Robert Kurzban suggested that "...critics of the field, when they err, are not slightly missing the mark. Their confusion is deep and profound. It’s not like they are marksmen who can’t quite hit the center of the target; they’re holding the gun backwards."







</doc>
<doc id="12082283" url="https://en.wikipedia.org/wiki?curid=12082283" title="Human vestigiality">
Human vestigiality

In the context of human evolution, human vestigiality involves those traits (such as organs or behaviors) occurring in humans that have lost all or most of their original function through evolution. Although structures called "vestigial" often appear functionless, a vestigial structure may retain lesser functions or develop minor new ones. In some cases, structures once identified as vestigial simply had an unrecognized function.

The examples of human vestigiality are numerous, including the anatomical (such as the human tailbone, wisdom teeth, and inside corner of the eye), the behavioral (goose bumps and palmar grasp reflex), and molecular (pseudogenes). Many human characteristics are also vestigial in other primates and related animals.
Charles Darwin listed a number of putative human vestigial features, which he termed rudimentary, in "The Descent of Man" (1890). These included the muscles of the ear; wisdom teeth; the appendix; the tail bone; body hair; and the semilunar fold, in the corner of the eye. Darwin also commented on the sporadic nature of many vestigial features, particularly musculature. Making reference to the work of the anatomist William Turner, Darwin highlighted a number of sporadic muscles which he identified as vestigial remnants of the panniculus carnosus, particularly the sternalis muscle.

In 1893, Robert Wiedersheim published "The Structure of Man", a book on human anatomy and its relevance to man's evolutionary history. This book contained a list of 86 human organs that he considered vestigial, or as Wiedersheim himself explained: "Organs having become wholly or in part functionless, some appearing in the Embryo alone, others present during Life constantly or inconstantly. For the greater part Organs which may be rightly termed Vestigial." His list of supposedly vestigial organs included many of the examples on this page as well as others then mistakenly believed to be purely vestigial, such as the pineal gland, the thymus gland, and the pituitary gland. Some of these organs that had lost their obvious, original functions later turned out to have retained functions that had gone unrecognized before the discovery of hormones or many of the functions and tissues of the immune system. Examples included:

Historically, there was a trend not only to dismiss the vermiform appendix as being uselessly vestigial, but an anatomical hazard, a liability to dangerous inflammation. As late as the mid-20th century, many reputable authorities conceded it no beneficial function. This was a view supported, or perhaps inspired, by Darwin himself in the 1874 edition of his book "The Descent of Man, and Selection in Relation to Sex". The organ's patent liability to appendicitis and its poorly understood role left the appendix open to blame for a number of possibly unrelated conditions. For example, in 1916, a surgeon claimed that removal of the appendix had cured several cases of trifacial neuralgia and other nerve pain about the head and face, even though he stated that the evidence for appendicitis in those patients was inconclusive. The discovery of hormones and hormonal principles, notably by Bayliss and Starling, argued against these views, but in the early twentieth century, there remained a great deal of fundamental research to be done on the functions of large parts of the digestive tract. In 1916, an author found it necessary to argue against the idea that the colon had no important function and that "the ultimate disappearance of the appendix is a coordinate action and not necessarily associated with such frequent inflammations as we are witnessing in the human".

There had been a long history of doubt about such dismissive views. Around 1920, the prominent surgeon Kenelm Hutchinson Digby documented previous observations, going back more than thirty years, that suggested lymphatic tissues, such as the tonsils and appendix, may have substantial immunological functions.

In modern humans, the appendix is a vestige of a redundant organ that in ancestral species had digestive functions, much as it still does in extant species in which intestinal flora hydrolyze cellulose and similar indigestible plant materials. Some herbivorous animals, such as rabbits, have a terminal vermiform appendix and cecum that apparently bear patches of tissue with immune functions and may also be important in maintaining the composition of intestinal flora. It does not however seem to have much digestive function, if any, and is not present in all herbivores, even those with large caeca. As shown in the accompanying pictures however, the human appendix typically is about comparable to that of the rabbit's in size, though the caecum is reduced to a single bulge where the ileum empties into the colon. Some carnivorous animals may have appendices too, but seldom have more than vestigial caeca. In line with the possibility of vestigial organs developing new functions, some research suggests that the appendix may guard against the loss of symbiotic bacteria that aid in digestion, though that is unlikely to be a novel function, given the presence of vermiform appendices in many herbivores.
Intestinal bacterial populations entrenched in the appendix may support quick re-establishment of the flora of the large intestine after an illness, poisoning, or after an antibiotic treatment depletes or otherwise causes harmful changes to the bacterial population of the colon.
A 2013 study, however, refutes the idea of an inverse relationship between cecum size and appendix size and presence. It is widely present in euarchontoglires (a superorder of mammals that includes rodents and primates) and has also evolved independently in the diprotodont marsupials, monotremes, and is highly diverse in size and shape which could suggest it is not vestigial. Researchers deduce that the appendix has the ability to protect good bacteria in the gut. That way, when the gut is affected by a bout of diarrhea or other illness that cleans out the intestines, the good bacteria in the appendix can repopulate the digestive system and keep the person healthy.

The coccyx, or tailbone, is the remnant of a lost tail. All mammals have a tail at some point in their development; in humans, it is present for a period of 4 weeks, during stages 14 to 22 of human embryogenesis. This tail is most prominent in human embryos 31–35 days old. The tailbone, located at the end of the spine, has lost its original function in assisting balance and mobility, though it still serves some secondary functions, such as being an attachment point for muscles, which explains why it has not degraded further. The coccyx serves as an attachment site for tendons, ligaments, and muscles. It also functions as an insertion point of some of the muscles of the pelvic floor.
In rare cases, congenital defect results in a short tail-like structure being present at birth. Twenty-three cases of human babies born with such a structure have been reported in the medical literature since 1884. In rare cases such as these, the spine and skull were determined to be entirely normal. The only abnormality was that of a tail approximately twelve centimeters long. These tails were able to be surgically removed, and the individuals have resumed normal lives.

Wisdom teeth are vestigial third molars that human ancestors used to help in grinding down plant tissue. The common postulation is that the skulls of human ancestors had larger jaws with more teeth, which were possibly used to help chew down foliage to compensate for a lack of ability to efficiently digest the cellulose that makes up a plant cell wall. As human diets changed, smaller jaws were naturally selected, yet the third molars, or "wisdom teeth", still commonly develop in human mouths. Currently, wisdom teeth have become useless and even harmful to the extent where surgical procedures are often performed to remove them.

Agenesis (failure to develop) of wisdom teeth in human populations ranges from zero in Tasmanian Aboriginals to nearly 100% in indigenous Mexicans. The difference is related to the PAX9 gene (and perhaps other genes).

In some animals, the vomeronasal organ (VNO) is part of a second, completely separate sense of smell, known as the accessory olfactory system. Many studies have been performed to find if there is an actual presence of a VNO in adult human beings. Trotier et al. estimated that around 92% of their subjects who had not had septal surgery had at least one intact VNO. Kjaer and Fisher Hansen, on the other hand, stated that the VNO structure disappeared during fetal development as it does for some primates. However, Smith and Bhatnagar (2000) asserted that Kjaer and Fisher Hansen simply missed the structure in older fetuses. Won (2000) found evidence of a VNO in 13 of his 22 cadavers (59.1%) and in 22 of his 78 living patients (28.2%). Given these findings, some scientists have argued that there is a VNO in adult human beings. However, most investigators have sought to identify the opening of the vomeronasal organ in humans, rather than identify the tubular epithelial structure itself. Thus it has been argued that such studies, employing macroscopic observational methods, have sometimes missed or even misidentified the vomeronasal organ.

Among studies that use microanatomical methods, there is no reported evidence that human beings have active sensory neurons like those in working vomeronasal systems of other animals. Furthermore, there is no evidence to date that suggests there are nerve and axon connections between any existing sensory receptor cells that may be in the adult human VNO and the brain. Likewise, there is no evidence for any accessory olfactory bulb in adult human beings, and the key genes involved in VNO function in other mammals have become pseudogenes in human beings. Therefore, while the presence of a structure in adult human beings is debated, a review of the scientific literature by Tristram Wyatt concluded, "most in the field ... are sceptical about the likelihood of a functional VNO in adult human beings on current evidence."

The ears of a macaque monkey and most other monkeys have far more developed muscles than those of humans, and therefore have the capability to move their ears to better hear potential threats. Humans and other primates such as the orangutan and chimpanzee however have ear muscles that are minimally developed and non-functional, yet still large enough to be identifiable. A muscle attached to the ear that cannot move the ear, for whatever reason, can no longer be said to have any biological function. In humans there is variability in these muscles, such that some people are able to move their ears in various directions, and it can be possible for others to gain such movement by repeated trials. In such primates, the inability to move the ear is compensated mainly by the ability to turn the head on a horizontal plane, an ability which is not common to most monkeys—a function once provided by one structure is now replaced by another.

The outer structure of the ear also shows some vestigial features, such as the node or point on the helix of the ear known as Darwin's tubercle which is found in around 10% of the population.

The plica semilunaris is a small fold of tissue on the inside corner of the eye. It is the vestigial remnant of the nictitating membrane, an organ that is fully functional in some other species of mammals. Its associated muscles are also vestigial. Only one species of primate, the Calabar angwantibo, is known to have a functioning nictitating membrane.

The orbitalis muscle is a vestigial or rudimentary nonstriated muscle (smooth muscle) of the eye that crosses from the infraorbital groove and sphenomaxillary fissure and is intimately united with the periosteum of the orbit. It was described by Johannes Peter Müller and is often called Müller's muscle. The muscle forms an important part of the lateral orbital wall in some animals, but in humans it is not known to have any significant function.

In the internal genitalia of each human sex, there are some residual organs of mesonephric and paramesonephric ducts during embryonic development:

Human vestigial structures also include leftover embryological remnants that once served a function during development, such as the belly button, and analogous structures between biological sexes. For example, men are also born with two nipples, which are not known to serve a function compared to women. In regards to genitourinary development, both internal and external genitalia of male and female fetuses have the ability to fully or partially form their analogous phenotype of the opposite biological sex if exposed to a lack/overabundance of androgens or the SRY gene during fetal development. Examples of vestigial remnants of genitourinary development include the hymen, which is a membrane that surrounds or partially covers the external vaginal opening that derives from the sinus tubercle during fetal development and is homologous to the male seminal colliculus. Some researchers have hypothesized that the persistence of the hymen may be to provide temporary protection from infection, as it separates the vaginal lumen from the urogenital sinus cavity during development. Other examples include the glans penis and the clitoris, the labia minora and the ventral penis, and the ovarian follicles and the seminiferous tubules.

In modern times, there is controversy regarding whether the foreskin is a vital or vestigial structure. In 1949, British physician Douglas Gairdner noted that the foreskin plays an important protective role in newborns. He wrote, "It is often stated that the prepuce is a vestigial structure devoid of function ... However, it seems to be no accident that during the years when the child is incontinent the glans is completely clothed by the prepuce, for, deprived of this protection, the glans becomes susceptible to injury from contact with sodden clothes or napkin." During the physical act of sex, the foreskin reduces friction, which can reduce the need for additional sources of lubrication. "Some medical researchers, however, claim circumcised men enjoy sex just fine and that, in view of recent research on HIV transmission, the foreskin causes more trouble than it's worth." The area of the outer forskin measures between 7 and 100 cm, and the inner foreskin measures between 18 and 68 cm, which is a wide range. Regarding vestigial structures, Charles Darwin wrote, "An organ, when rendered useless, may well be variable, for its variations cannot be checked by natural selection." In the March 2017 publication of "The Global Health Journal: Science and Practice", Morris and Krieger wrote, "The variability in foreskin size is consistent with the foreskin being a vestigial structure." Charles Darwin speculated that the sensitivity of the foreskin to fine touch might have served as an "early warning system" in our naked ancestors while it protected the glans from the intrusion of biting insects and parasites.

A number of muscles in the human body are thought to be vestigial, either by virtue of being greatly reduced in size compared to homologous muscles in other species, by having become principally tendonous, or by being highly variable in their frequency within or between populations.

The occipitalis minor is a muscle in the back of the head which normally joins to the auricular muscles of the ear. This muscle is very sporadic in frequency—always present in Malays, in 56% of Africans, 50% of Japanese, 36% of Europeans, and is nonexistent in the Khoikhoi people of southwestern Africa and in Melanesians. Other small muscles in the head associated with the occipital region and the post-auricular muscle complex are often variable in their frequency.

The platysma, a quadrangular (four sides) muscle in a sheet-like configuration, is a vestigial remnant of the panniculous carnosus of animals. In horses, it is the muscle that allows it to flick a fly off its back.

In many non-human mammals, the upper lip and sinus area is associated with whiskers or vibrissae which serve a sensory function. In humans, these whiskers do not exist but there are still sporadic cases where elements of the associated vibrissal capsular muscles or sinus hair muscles can be found. Based on histological studies of the upper lips of 20 cadavers, Tamatsu et al. found that structures resembling such muscles were present in 35% (7/20) of their specimens.

The palmaris longus muscle is seen as a small tendon between the flexor carpi radialis and the flexor carpi ulnaris, although it is not always present. The muscle is absent in about 14% of the population, however this varies greatly with ethnicity. It is believed that this muscle actively participated in the arboreal locomotion of primates, but currently has no function, because it does not provide more grip strength. One study has shown the prevalence of palmaris longus agenesis in 500 Indian patients to be 17.2% (8% bilateral and 9.2% unilateral). The palmaris is a popular source of tendon material for grafts and this has prompted studies which have shown the absence of the palmaris does not have any appreciable effect on grip strength.

The levator claviculae muscle in the posterior triangle of the neck is a supernumerary muscle present in only 2–3% of all people but nearly always present in most mammalian species, including gibbons and orangutans.

The pyramidalis muscle of the abdomen is a small and triangular muscle, anterior to the rectus abdominis, and contained in the rectus sheath. It is absent in 20% of humans and when absent, the lower end of the rectus then becomes proportionately increased in size. Anatomical studies suggest that the forces generated by the pyramidalis muscles are relatively small.

The latissimus dorsi muscle of the back has several sporadic variations. One particular variant is the existence of the dorsoepitrochlearis or latissimocondyloideus muscle which is a muscle passing from the tendon of the latissimus dorsi to the long head of the triceps brachii. It is notable due to its well developed character in other apes and monkeys, where it is an important climbing muscle, namely the dorsoepitrochlearis brachii. This muscle is found in ≈5% of humans.

The plantaris muscle is composed of a thin muscle belly and a long thin tendon. The muscle belly is approximately long, and is absent in 7–10% of the human population. It has some weak functionality in moving the knee and ankle but is generally considered redundant and is often used as a source of tendon for grafts. The long, thin tendon of the plantaris is humorously called "the freshman's nerve", as it is often mistaken for a nerve by first-year medical students.

Another intriguing example of human vestigiality occurs in the tongue, specifically the chondroglossus muscle. In a morphological study of 100 Japanese cadavers, it was found that 86% of fibers identified were solid and bundled in the appropriate way to facilitate speech and mastication. The other 14% of fibers were short, thin and sparse – nearly useless, and thus concluded to be of vestigial origin.

Extra nipples or breasts sometimes appear along the mammary lines of humans, appearing as a remnant of mammalian ancestors who possessed more than two nipples or breasts.

Humans also bear some vestigial behaviors and reflexes. For example, the formation of goose bumps in humans under stress is a vestigial reflex; a possible function in human evolutionary ancestors was to raise the body's hair, making the ancestor appear larger and scaring off predators. Raising the hair is also used to trap an extra layer of air, keeping an animal warm. Due to the diminished amount of hair in humans, the reflex formation of goose bumps when cold is also vestigial.

The palmar grasp reflex is supported to be a vestigial behavior in human infants. When placing a finger or object to the palm of an infant, it will securely grasp it. This grasp is found to be rather strong. Some infants—37% according to a 1932 study—are able to support their own weight from a rod, although there is no way they can cling to their mother. The grasp is also evident in the feet too. When a baby is sitting down, its prehensile feet assume a curled-in posture, similar to that observed in an adult chimp. An ancestral primate would have had sufficient body hair to which an infant could cling unlike modern humans, thus allowing its mother to escape from danger, such as climbing up a tree in the presence of a predator without having to occupy her hands holding her baby.

It has been proposed that the hiccup is an evolutionary remnant of earlier amphibian respiration. Amphibians such as tadpoles gulp air and water across their gills via a rather simple motor reflex akin to mammalian hiccuping. The motor pathways that enable hiccuping form early during fetal development, before the motor pathways that enable normal lung ventilation form. Thus, according to recapitulation theory, the hiccup is evolutionarily antecedent to modern lung respiration. Additionally, they point out that hiccups and amphibian gulping are inhibited by elevated CO and may be stopped by GABA receptor agonists, illustrating a possible shared physiology and evolutionary heritage. These proposals may explain why premature infants spend 2.5% of their time hiccuping, possibly gulping like amphibians, as their lungs are not yet fully formed. Fetal intrauterine hiccups are of two types. The physiological type occurs before 28 weeks after conception and tend to last five to ten minutes. These hiccups are part of fetal development and are associated with the myelination of the phrenic nerve, which primarily controls the thoracic diaphragm. The phylogeny hypothesis explains how the hiccup reflex might have evolved, and if there is not an explanation, it may explain hiccups as an evolutionary remnant, held-over from our amphibious ancestors.
This hypothesis has been questioned because of the existence of the afferent loop of the reflex, the fact that it does not explain the reason for glottic closure, and because the very short contraction of the hiccup is unlikely to have a significant strengthening effect on the slow-twitch muscles of respiration.

There are also vestigial molecular structures in humans, which are no longer in use but may indicate common ancestry with other species. One example of this is L-gulonolactone oxidase, a gene that is functional in most other mammals and produces an enzyme that synthesizes vitamin C. In humans and other members of the suborder Haplorrhini, a mutation disabled the gene and made it unable to produce the enzyme. However, the remains of the gene are still present in the human genome as a vestigial genetic sequence called a pseudogene.



</doc>
<doc id="293526" url="https://en.wikipedia.org/wiki?curid=293526" title="Dual loyalty">
Dual loyalty

In politics, dual loyalty is loyalty to two separate interests that potentially conflict with each other, leading to a conflict of interest.

While nearly all examples of alleged "dual loyalty" are considered highly controversial, these examples point to the inherent difficulty in distinguishing between what constitutes a "danger" of dual loyalty – i.e., that there exists a pair of "misaligned" interests – versus what might be more simply a pair of "partially aligned" or even, according to the party being accused, a pair of "fully aligned" interests. For example, immigrants who still have feelings of loyalty to their country of origin will often insist that their two (or more) loyalties do not conflict. As Stanley A. Renshon at the Center for Immigration Studies notes,
Some scholars refer to a growing trend of transnationalism and suggest that as societies become more heterogeneous and multi-cultural, the term "dual loyalty" increasingly becomes a meaningless bromide. According to the theory of transnationalism, migration (as well as other factors including improved global communication) produces new forms of identity that transcend traditional notions of physical and cultural space. Nina Glick Schiller, Linda Basch, and Cristina Blanc-Szanton define a process by which immigrants "link together" their country of origin and their country of settlement.

The transnationalist view is that "dual loyalty" is a potentially "positive" expression of multi-culturalism, and can contribute to the diversity and strength of civil society. While this view is popular in many academic circles, others are skeptical of this idea. As one paper describes it,
Beyond its usage in particular instances, the term "dual loyalty" versus "transnationalism" continues to be the subject of much debate. As one academic writes:
Other historical examples of actual or perceived "dual loyalty" include the following:


</doc>
<doc id="1150894" url="https://en.wikipedia.org/wiki?curid=1150894" title="Victim blaming">
Victim blaming

Victim blaming occurs when the victim of a crime or any wrongful act is held entirely or partially at fault for the harm that befell them. The study of victimology seeks to mitigate the perception of victims as responsible. There is a greater tendency to blame victims of rape than victims of robbery if victims and perpetrators know each other.

Psychologist William Ryan coined the phrase "blaming the victim" in his 1971 book of that title. In the book, Ryan described victim blaming as an ideology used to justify racism and social injustice against black people in the United States. Ryan wrote the book to refute Daniel Patrick Moynihan's 1965 work "The Negro Family: The Case for National Action" (usually simply referred to as the Moynihan Report).

Moynihan had concluded that three centuries of oppression of black people, and in particular with what he calls the uniquely cruel structure of American slavery as opposed to its Latin American counterparts, had created a long series of chaotic disruptions within the black family structure which, at the time of the report, manifested itself in high rates of unwed births, absent fathers, and single mother households in black families. Moynihan then correlated these familial outcomes, which he considered undesirable, to the relatively poorer rates of employment, educational achievement, and financial success found among the black population. Moynihan advocated the implementation of government programs designed to strengthen the black nuclear family.

Ryan objected that Moynihan then located the proximate cause of the plight of black Americans in the prevalence of a family structure in which the father was often sporadically, if at all, present, and the mother was often dependent on government aid to feed, clothe, and provide medical care for her children. Ryan's critique cast the Moynihan theories as attempts to divert responsibility for poverty from social structural factors to the behaviors and cultural patterns of the poor.

Although Ryan popularized the phrase, other scholars had identified the phenomenon of victim blaming.
In 1947 Theodor W. Adorno defined what would be later called "blaming the victim," as "one of the most sinister features of the Fascist character".
Shortly thereafter Adorno and three other professors at the University of California, Berkeley formulated their influential and highly debated "F-scale" (F for fascist), published in "The Authoritarian Personality" (1950), which included among the fascist traits of the scale the "contempt for everything discriminated against or weak." A typical expression of victim blaming is the "asking for it" idiom, e.g. "she was asking for it" said of a victim of violence or sexual assault.

Secondary victimization is the re-traumatization of the sexual assault, abuse, or rape victim through the responses of individuals and institutions. Types of secondary victimization include victim blaming, disbelieving the victim's story, minimizing the severity of the attack, and inappropriate post-assault treatment by medical personnel or other organizations. Secondary victimization is especially common in cases of drug-facilitated, acquaintance, military sexual trauma and statutory rape.

Sexual assault victims experience stigmatization based on rape myths. A female rape victim is especially stigmatized in patrilineal cultures with strong customs and taboos regarding sex and sexuality. For example, a society may view a female rape victim (especially one who was previously a virgin) as "damaged". Victims in these cultures may suffer isolation, physical and psychological abuse, slut-shaming, public humiliation rituals, be disowned by friends and family, be prohibited from marrying, be divorced if already married, or even be killed. However, even in many developed countries, including some sectors of United States society, misogyny remains culturally ingrained.

One example of a sexist allegation against female victims of sexual assault is that "wearing provocative clothing stimulates sexual aggression in men" who believe that women wearing body-revealing clothes are "actively trying to seduce" a sexual partner. Such accusations against victims stem from the assumption that sexually revealing clothing conveys consent for sexual actions, irrespective of willful verbal consent. Research has yet to prove that attire is a significant causal factor in determining who is assaulted.

Victim blaming is also exemplified when a victim of sexual assault is found at fault for "performing actions which reduce their ability to resist or refuse consent", such as consuming alcohol. Victim advocacy groups and medical professionals are educating young adults on the definition of consent, and the importance of refraining from victim blaming. Most institutions have adopted the concept of affirmative consent and that refraining from sexual activity while under the influence is the safest choice.

In efforts to discredit alleged sexual assault victims in court, a defense attorney may "delve into an accuser's personal history", a common practice that also has the purposeful effect of making the victim so uncomfortable they choose not to proceed. This attack on character, especially one pointing out promiscuity, makes the argument that women who lead "high risk" lifestyles (promiscuity, drug use) are not real victims of rape.

Findings on Rape Myth Acceptance have supported feminist claims that sexism is at the root of female rape victim blaming.

A 2009 study in the Journal of Interpersonal Violence of male victims of sexual assault concludes that male rape victim blaming is usually done so because of social constructs of masculinity. Some effects of these kind of rape cases include a loss of masculinity, confusion about their sexual orientation, and a sense of failure in behaving as men should.

Victims of an unwanted sexual encounter usually develop psychological problems such as depression or sexual violence specific PTSD known as rape trauma syndrome.

An ideal victim is one who is afforded the status of victimhood due to unavoidable circumstances that put the individual at a disadvantage. One can apply this theory to any crime including and especially sexual assault. Nils Christie, a Norwegian criminology professor, has been theorizing about the concept of the ideal victim since the 1980s. In his research he gives two examples, one of an old woman who is attacked on her way home from visiting her family and the other of a man who is attacked at a bar by someone he knew. He describes the old woman as an ideal victim because she could not avoid being in the location that she was, she did not know her attacker, and she could not fight off her attacker. The man, however, could have avoided being at a bar, knew his attacker, and should have been able to fight off his attacker, being younger and a man.

When applying the ideal victim theory to sexual assault victims, often judicial proceedings define an ideal victim as one who resists their attacker and exercises caution in risky situations, despite law reforms to extinguish these fallacious requirements. When victims are not ideal they are at risk for being blamed for their attack because they are not considered real victims of rape. Because they do not fit the criteria being laid out in the rape law, they cannot be considered real victims and thereby their attacker will not be prosecuted.

A victim who is not considered an ideal, or real victim, is one who leads a "high risk" lifestyle, partaking in drugs or alcohol, or is perceived as promiscuous. A victim who intimately knows their attacker is also not considered an ideal victim. Examples of a sexual assault victim who is not ideal is a prostitute because they lead a high risk lifestyle. The perception is that these behaviors discount the credibility of a sexual assault victim's claim or that the behaviors and associations create the mistaken assumption of consent. Some of or all of the blame of the assault is then placed on these victims, and so they are not worthy of having their case presented in court. These perceptions persist in court rulings despite a shift in laws favoring affirmative consent- meaning that the participants in a sexual activity give a verbal affirmation rather than one participant who neither answers negatively nor positively. In other words, affirmative consent is yes means yes and no means no.

In addition to an ideal victim, there must be an ideal perpetrator for a crime to be considered ideal. The ideal attacker does not know their victim and is a completely non-sympathetic figure- one who is considered sub-human, an individual lacking morals. An attacker that knows their victim is not considered an ideal attacker, nor is someone who seems morally ordinary. Cases of intimate partner violence are not considered ideal because the victim knows their attacker. Husbands and wives are not ideal victims or perpetrators because they are intimately familiar with each other.

Many different cultures across the globe have formulated different degrees of victim blaming for different scenarios such as rape, hate crimes, and domestic abuse. Victim blaming is common around the world, especially in cultures where it is socially acceptable and advised to treat certain groups of people as lesser. For example, in Somalia victims of sexual abuse consistently endure social ostracization and harassment. One specific example is the kidnapping and rape of 14-year old Fatima: when the police arrived, both Fatima and her rapist were arrested. While they did not detain the offender for long, the officers held Fatima captive for a month and a prison guard continually raped her during that time.

In February 2016, the organisations International Alert and UNICEF published a study revealing that girls and women released from captivity by Nigeria's insurgency group Boko Haram often face rejection by their communities and families. Their children born of sexual violence faced even more discrimination.

Acid attacks on South Asian women, when people throw acid on women in an attempt to punish them for their perceived wrongdoings, are another example of victim-blaming. For instance, in New Delhi in 2005, a group of men threw acid on a 16-year-old girl because they believed she provoked the advances of a man. In Chinese culture, victim blaming is often associated with the crime of rape, as women are expected to resist rape using physical force. Thus, if rape occurs, it is considered to be at least partly the woman’s fault and her virtue is inevitably called into question.
In Western culture victim blaming has been largely recognized as a problematic way to view a situation, however this does not exempt Westerners from being guilty of the action. A recent example of Western victim blaming would be a civil trial held in 2013 where the Los Angeles School District blamed a 14-year-old girl for the sexual abuse she endured from her middle school teacher. The District's lawyer argued that the minor was responsible for the prevention of the abuse, putting the entire fault on the victim and exempting the perpetrator of any responsibility. Despite his efforts to convince the court that the victim must be blamed, the ruling stated that no minor student that has been sexually assaulted by his or her teacher is responsible for the prevention of that sexual assault.

Roy Baumeister, a social and personality psychologist, argued that blaming the victim is not necessarily always fallacious. He argued that showing the victim's possible role in an altercation may be contrary to typical explanations of violence and cruelty, which incorporate the trope of the innocent victim. According to Baumeister, in the classic telling of "the myth of pure evil," the innocent, well-meaning victims are going about their business when they are suddenly assaulted by wicked, malicious evildoers. Baumeister describes the situation as a possible distortion by both the perpetrator and the victim; the perpetrator may minimize the offense while the victim maximizes it, and so accounts of the incident shouldn't be immediately taken as objective truths.

In context, Baumeister refers to the common behavior of the aggressor seeing themselves as more of the "victim" than the abused, justifying a horrific act by way of their "moral complexity". This usually stems from an "excessive sensitivity" to insults, which he finds as a consistent pattern in abusive husbands. Essentially, the abuse the perpetrator administers is generally excessive, in comparison to the act/acts that they claim as to have provoked them.

Some scholars make the argument that some of the attitudes that are described as victim blaming and the victimologies that are said to counteract them are both extreme and similar to each other, an example of the horseshoe theory. For instance, they argue that the claim that "women wearing provocative clothing cause rape" is as demeaning to men as it is to women as depicting men as incapable of controlling their sexual desire is misandrist and denies men full agency, while also arguing that the generalization that women do not lie about rape (or any generalization about women not doing some things because of their gender) is misogynist by its implicit assumption that women act by simple default action modes which is incompatible with full agency. These scholars argue that it is important to impartially assess the evidence in each criminal trial individually and that any generalization based on statistics would change the situation from one where the control of evidence makes false reporting difficult to one where lack of individual control of the alleged crime makes it easier to file false reports and that statistics collected in the former situation would not be possible to apply to the latter situation. While the scholars make a distinction between actual victim blaming and rule by law that they consider to be falsely lumped with victim blaming in radical feminist rhetorics, they also advocate more protection from ad hominem questions to alleged victims about past life history and that the questions should focus on what is relevant for the specific alleged crime. They also cite examples that they consider to be cases of the horseshoe theory applied to the question of victim blaming. This includes cases in which psychologists who have testified on behalf of the prosecution in trials in which breast size have been used as a measure of female age when classifying pornographic cartoons as child pornography and been praised by feminists for it, and later the same psychologists have used the same psychological arguments when testifying on behalf of the defense in statutory rape cases and getting the defendant acquitted by claiming that the victim's breasts looked like those of an adult woman (considered by these scholars to be victim blaming based on appearance) and been praised by men's rights groups for it. It also includes the possibility that biopsychiatric models that consider sexual criminality hereditary and that are advocated by some feminists may blame victims of incest abuse for being genetically related to their abusers and thereby dissuading them from reporting abuse.

Other analysts of victim blaming discourse who neither support most of the phenomena that are described as victim blaming nor most of the measures that are marketed as countermeasures against such point at the existence of other ways of discovering and punishing crimes with victims besides the victim reporting the crime. Not only are there police patrols and possible eyewitnesses, but these analysts also argue that neighbors can overhear and report crimes that take place within the house such as domestic violence. For that reason along with the possibility of many witnesses turning up over time if the crime is ongoing long term as domestic abuse is generally said to be which would make some of the witnesses likely to be considered believable, analysts of this camp of thought argue that the main problem that prevent crimes from being successfully prosecuted is offender profiling that disbelieve the capacity and/or probability of many criminals to commit the crime, rather than disbelief or blaming of victim reports. These analysts cite international comparisons that show that the percentage of male on female cases in the statistics of successfully prosecuted domestic violence is not higher in countries that apply gender feminist theories about patriarchal structures than in countries that apply supposedly antifeminist evolutionary psychology profiling of sex differences in aggressiveness, impulse control and empathy, arguing that the criminal justice system prioritizing cases in which they believe the suspect most likely to be guilty makes evolutionary psychology at least as responsible as gender feminism for leaving domestic violence cases with female offenders undiscovered no matter if the victim is male or female. The analysts argue that many problems that are often attributed to victim blaming are instead due to offender profiling, and suggest randomized investigations instead of psychological profiling of suspected offenders.

A myth holds that Jews went passively "like sheep to the slaughter" during the Holocaust, which is considered by many writers, including Emil Fackenheim, to be a form of victim blaming. Secondary antisemitism is a type of antisemitism caused by non-Jewish Europeans' attempts to shift blame for the Holocaust onto the Jews, often summed up by the claim that "The Germans will never forgive the Jews for Auschwitz."

Leigh Leigh, born Leigh Rennea Mears, was a 14-year-old girl from Fern Bay, Australia, who was murdered on 3 November 1989. While attending a 16-year-old boy's birthday party at Stockton Beach, Leigh was assaulted by a group of boys after she returned distressed from a sexual encounter on the beach that a reviewing judge later called non-consensual. After being kicked and spat on by the group, Leigh left the party. Her naked body was found in the sand dunes nearby the following morning, with severe genital damage and a crushed skull. Leigh's murder received considerable attention in the media. Initially focusing on her sexual assault and murder, media attention later concentrated more on the lack of parental supervision and the drugs and alcohol at the party, and on Leigh's sexuality. The media coverage of the murder has been cited as an example of victim blaming.

In a case that became infamous in 2011, an 11-year-old female rape victim who suffered repeated gang rapes in Cleveland, Texas, was accused by a defense attorney of being a seductress who lured men to their doom. "Like the spider and the fly. Wasn't she saying, 'Come into my parlor', said the spider to the fly?", he asked a witness. The "New York Times" ran an article uncritically reporting on the way many in the community blamed the victim, for which the newspaper later apologized.

In a case that attracted worldwide coverage, when a woman was raped and killed in Delhi in December 2012, some Indian government officials and political leaders blamed the victim for various things, mostly based on conjecture. Many of the people involved later apologized.

In recent years, the issue of victim blaming has gained notoriety and become widely recognized in the media, particularly in the context of feminism, as women have often been blamed for behaving in ways that encourage harassment. 

In 2016, in the wake of New Year's Eve sexual assaults in Germany, the mayor of Cologne Henriette Reker came under heavy criticism, as her response appeared to blame the victims. She called for women to follow a "code of conduct," including staying at an "arm's length" from strangers. By the evening of 5 January, "#einearmlänge" ("an arm's length") became one of Germany's top-trending hashtags on Twitter. Reker called a crisis meeting with the police in response to the incidents. Reker called it "completely improper" to link the perpetrators to refugees.

Coverage of the 2016 Murder of Ashley Ann Olsen, an American murdered in Italy during a sexual encounter with a Senegalese immigrant, focused on the victim blaming in cross-cultural encounters.

In August 2017, the hashtag #AintNoCinderella took over the media in response to a national instance of victim-blaming occurring in India. After Varnika Kundu was stalked and harassed by two men on her way home late at night, BJP Vice President Ramveer Bhatti addressed the viral story with a claim that Kundu was somehow at fault for being out late by herself. He essentially blamed the woman for an incident of which she was merely a victim; and social media users took to Twitter and Instagram to challenge this idea that women should not be out late at night, and if they do, they are somehow "asking for it". Hundreds of women shared photos of themselves staying out past midnight, dressing boldly, and behaving in (harmless) ways that tend to be condemned in old-fashioned, anti-feminist ideology in order to make the statement: "I am not a child. I am not someone's property. I am not a seventeeth-century fantastical damsel in distress. I am a woman".





</doc>
<doc id="4095924" url="https://en.wikipedia.org/wiki?curid=4095924" title="Value (ethics)">
Value (ethics)

In ethics, value denotes the degree of importance of some thing or action, with the aim of determining what actions are best to do or what way is best to live (normative ethics), or to describe the significance of different actions. Value systems are proscriptive and prescriptive beliefs; they affect ethical behavior of a person or are the basis of their intentional activities. Often primary values are strong and secondary values are suitable for changes. What makes an action valuable may in turn depend on the ethical values of the objects it increases, decreases or alters. An object with "ethic value" may be termed an "ethic or philosophic good" (noun sense).

Values can be defined as broad preferences concerning appropriate courses of actions or outcomes. As such, values reflect a person's sense of right and wrong or what "ought" to be. "Equal rights for all", "Excellence deserves admiration", and "People should be treated with respect and dignity" are representatives of values. Values tend to influence attitudes and behavior and these types include ethical/moral values, doctrinal/ideological (religious, political) values, social values, and aesthetic values. It is debated whether some values that are not clearly physiologically determined, such as altruism, are intrinsic, and whether some, such as acquisitiveness, should be classified as vices or virtues.

Ethical value may be regarded as a study under ethics, which, in turn, may be grouped as philosophy. Similarly, "ethical value" may be regarded as a subgroup of a broader field of philosophic value sometimes referred to as axiology. Ethical value denotes something's degree of importance, with the aim of determining what action or life is best to do, or at least attempt to describe the value of different actions. 

The study of ethical value is also included in value theory. In addition, values have been studied in various disciplines: anthropology, behavioral economics, business ethics, corporate governance, moral philosophy, political sciences, social psychology, sociology and theology.

"Ethical value" is sometimes used synonymously with goodness. However, goodness has many other meanings and may be regarded as more ambiguous.

Personal values exist in relation to cultural values, either in agreement with or divergence from prevailing norms. A culture is a social system that shares a set of common values, in which such values permit social expectations and collective understandings of the good, beautiful and constructive. Without normative personal values, there would be no cultural reference against which to measure the virtue of individual values and so cultural identity would disintegrate.

Personal values provide an internal reference for what is good, beneficial, important, useful, beautiful, desirable and constructive. Values are one of the factors that generate behaviour and influence the choices made by an individual.

Values may help common human problems for survival by comparative rankings of value, the results of which provide answers to questions of why people do what they do and in what order they choose to do them. Moral, religious, and personal values, when held rigidly, may also give rise to conflicts that result from a clash between differing world views.

Over time the public expression of personal values that groups of people find important in their day-to-day lives, lay the foundations of law, custom and tradition. Recent research has thereby stressed the "implicit nature of value communication". Consumer behavior research proposes there are six internal values and three external values. They are known as List of Values (LOV) in management studies. They are self respect, warm relationships, sense of accomplishment, self-fulfillment, fun and enjoyment, excitement, sense of belonging, being well respected, and security. From a functional aspect these values are categorized into three and they are interpersonal relationship area, personal factors, and non-personal factors. From an ethnocentric perspective, it could be assumed that a same set of values will not reflect equally between two groups of people from two countries. Though the core values are related, the processing of values can differ based on the cultural identity of an individual.

Individual cultures emphasize values which their members broadly share. Values of a society can often be identified by examining the level of honor and respect received by various groups and ideas. In the United States of America, for example, top-level professional athletes receive more respect (measured in terms of monetary payment) than university professors.

Values clarification differs from cognitive moral education:

Values relate to the norms of a culture, but they are more global and intellectual than norms. Norms provide rules for behavior in specific situations, while values identify what should be judged as good or evil. While norms are standards, patterns, rules and guides of expected behavior, values are abstract concepts of what is important and worthwhile. Flying the national flag on a holiday is a norm, but it reflects the value of patriotism. Wearing dark clothing and appearing solemn are normative behaviors to manifest respect at a funeral. Different cultures represent values differently and to different levels of emphasis. "Over the last three decades, traditional-age college students have shown an increased interest in personal well-being and a decreased interest in the welfare of others." Values seemed to have changed, affecting the beliefs, and attitudes of the students.

Members take part in a culture even if each member's personal values do not entirely agree with some of the normative values sanctioned in that culture. This reflects an individual's ability to synthesize and extract aspects valuable to them from the multiple subcultures they belong to.

If a group member expresses a value that seriously conflicts with the group's norms, the group's authority may carry out various ways of encouraging conformity or stigmatizing the non-conforming behavior of that member. For example, imprisonment can result from conflict with social norms that the state has established as law.

Furthermore, institutions in the global economy can genuinely respect values which are of three kinds based on a "triangle of coherence". In the first instance, a value may come to expression within the World Trade Organization (WTO), as well as (in the second instance) within the United Nations – particularly in the Educational, Scientific and Cultural Organization (UNESCO) – providing a framework for global legitimacy through accountability. In the third instance, the expertise of member-driven international organizations and civil society depends on the incorporation of flexibility in the rules, to preserve the expression of identity in a globalized world..

Nonetheless, in warlike economic competition, differing views may contradict each other, particularly in the field of culture. Thus audiences in Europe may regard a movie as an artistic creation and grant it benefits from special treatment, while audiences in the United States may see it as mere entertainment, whatever its artistic merits. EU policies based on the notion of "cultural exception" can become juxtaposed with the policy of "cultural specificity" on the liberal Anglo-Saxon side. Indeed, international law traditionally treats films as property and the content of television programs as a service. Consequently, cultural interventionist policies can find themselves opposed to the Anglo-Saxon liberal position, causing failures in international negotiations.

Values are generally received through cultural means, especially diffusion and transmission or socialization from parents to children. Parents in different cultures have different values. For example, parents in a hunter–gatherer society or surviving through subsistence agriculture value practical survival skills from a young age. Many such cultures begin teaching babies to use sharp tools, including knives, before their first birthdays. Italian parents value social and emotional abilities and having an even temperament. Spanish parents want their children to be sociable. Swedish parents value security and happiness. Dutch parents value independence, long attention spans, and predictable schedules. American parents are unusual for strongly valuing intellectual ability, especially in a narrow "book learning" sense. The Kipsigis people of Kenya value children who are not only smart, but who employ that intelligence in a responsible and helpful way, which they call "ng'om". Luos of Kenya value education and pride which they call "nyadhi".

Factors that influence the development of cultural values are summarized below.

The Inglehart–Welzel cultural map of the world is a two-dimensional cultural map showing the cultural values of the countries of the world along two dimensions: The "traditional versus secular-rational values" reflect the transition from a religious understanding of the world to a dominance of science and bureaucracy. The second dimension named "survival values versus self-expression values" represents the transition from industrial society to post-industrial society.

Cultures can be distinguished as tight and loose in relation to how much they adhere to social norms and tolerates deviance. Tight cultures are more restrictive, with stricter disciplinary measures for norm violations while loose cultures have weaker social norms and a higher tolerance for deviant behavior. A history of threats, such as natural disasters, high population density, or vulnerability to infectious diseases, is associated with greater tightness. It has been suggested that tightness allows cultures to coordinate more effectively to survive threats. 

Studies in evolutionary psychology have led to similar findings. The so-called regality theory finds that war and other perceived collective dangers have a profound influence on both the psychology of individuals and on the social structure and cultural values. A dangerous environment leads to a hierarchical, authoritarian, and warlike culture, while a safe and peaceful environment fosters an egalitarian and tolerant culture.

Relative values differ between people, and on a larger scale, between people of different cultures. On the other hand, there are theories of the existence of "absolute values", which can also be termed "noumenal values" (and not to be confused with mathematical absolute value). An absolute value can be described as philosophically absolute and independent of individual and cultural views, as well as independent of whether it is known or apprehended or not. Ludwig Wittgenstein was pessimistic towards the idea that an elucidation would ever happen regarding the absolute values of actions or objects; ""we can speak as much as we want about "life" and "its meaning," and believe that what we say is important. But these are no more than expressions and can never be facts, resulting from a tendency of the mind and not the heart or the will".

Philosophic value may be split into "instrumental value" and "intrinsic values". An instrumental value is worth having as a means towards getting something else that is good (e.g., a radio is instrumentally good in order to hear music). An intrinsically valuable thing is worth for itself, not as a means to something else. It is giving value intrinsic and extrinsic properties.

An "ethic good" with "instrumental value" may be termed an ethic mean, and an "ethic good" with "intrinsic value" may be termed an end-in-itself. An object may be both a mean and end-in-itself.

Intrinsic and instrumental goods are not mutually exclusive categories. Some objects are both good in themselves, and also good for getting other objects that are good. "Understanding science" may be such a good, being both worthwhile in and of itself, and as a means of achieving other goods. In these cases, the sum of instrumental (specifically the all instrumental value) and intrinsic value of an object may be used when putting that object in value systems, which is a set of consistent values and measures.

The "intensity" of philosophic value is the degree it is generated or carried out, and may be regarded as the prevalence of the good, the object having the value.

It should not be confused with the amount of value per object, although the latter may vary too, e.g. because of instrumental value conditionality. For example, taking a fictional life-stance of accepting waffle-eating as being the end-in-itself, the intensity may be the speed that waffles are eaten, and is zero when no waffles are eaten, e.g. if no waffles are present. Still, each waffle that had been present would still have value, no matter if it was being eaten or not, independent on intensity.

"Instrumental value conditionality" in this case could be exampled by every waffle not present, making them less valued by being far away rather than easily accessible.

In many life stances it is the product of value and intensity that is ultimately desirable, i.e. not only to generate value, but to generate it in large degree. Maximizing lifestances have the highest possible intensity as an imperative.

There may be a distinction between positive and negative philosophic or ethic value. While positive ethic value generally correlates with something that is pursued or maximized, negative ethic value correlates with something that is avoided or minimized.

Negative value may be both intrinsic negative value and/or instrumental negative value.

A "protected value" (also sacred value) is one that an individual is unwilling to trade off no matter what the benefits of doing so may be. For example, some people may be unwilling to kill another person, even if it means saving many others individuals. Protected values tend to be "intrinsically good", and most people can in fact imagine a scenario when trading off their most precious values would be necessary. If such trade-offs happen between two competing protected values such as killing a person and defending your family they are called "tragic trade-offs."

Protected values have been found to be play a role in protracted conflicts (e.g., the Israeli-Palestinian conflict) because they can hinder businesslike (<nowiki>"utilitarian"</nowiki>) negotiations. A series of experimental studies directed by Scott Atran and Ángel Gómez among combatants on the ISIS frontline in Iraq and with ordinary citizens in Western Europe suggest that commitment to sacred values motivate the most "devoted actors" to make the costliest sacrifices, including willingness to fight and die, as well as a readiness to forsake close kin and comrades for those values if necessary. From the perspective of utilitarianism, protected values are biases when they prevent utility from being maximized across individuals.

According to Jonathan Baron and Mark Spranca, protected values arise from norms as described in theories of deontological ethics (the latter often being referred to in context with Immanuel Kant). The protectedness implies that people are concerned with their participation in transactions rather than just the consequences of it.

A "value system" is a set of consistent values used for the purpose of ethical or ideological integrity.

As a member of a society, group or community, an individual can hold both a personal value system and a communal value system at the same time. In this case, the two value systems (one personal and one communal) are externally consistent provided they bear no contradictions or situational exceptions between them.

A value system in its own right is internally consistent when

Conversely, a value system by itself is internally inconsistent if:

Abstract exceptions serve to reinforce the ranking of values. Their definitions are generalized enough to be relevant to any and all situations. Situational exceptions, on the other hand, are ad hoc and pertain only to specific situations. The presence of a type of exception determines one of two more kinds of value systems:

The difference between these two types of systems can be seen when people state that they hold one value system yet in practice deviate from it, thus holding a different value system. For example, a religion lists an absolute set of values while the practice of that religion may include exceptions.

Implicit exceptions bring about a third type of value system called a formal value system. Whether idealized or realized, this type contains an implicit exception associated with each value: "as long as no higher-priority value is violated". For instance, a person might feel that lying is wrong. Since preserving a life is probably more highly valued than adhering to the principle that lying is wrong, lying to save someone’s life is acceptable. Perhaps too simplistic in practice, such a hierarchical structure may warrant explicit exceptions.

Although sharing a set of common values, like hockey is better than baseball or ice cream is better than fruit, two different parties might not rank those values equally. Also, two parties might disagree as to certain actions are right or wrong, both in theory and in practice, and find themselves in an ideological or physical conflict. Ethonomics, the discipline of rigorously examining and comparing value systems, enables us to understand politics and motivations more fully in order to resolve conflicts.

An example conflict would be a value system based on individualism pitted against a value system based on collectivism. A rational value system organized to resolve the conflict between two such value systems might take the form below. Note that added exceptions can become recursive and often convoluted.

Philosophical value is distinguished from economic value, since it is independent on some other desired condition or commodity. The economic value of an object may rise when the exchangeable desired condition or commodity, e.g. money, become high in supply, and vice versa when supply of money becomes low.

Nevertheless, economic value may be regarded as a result of philosophical value. In the subjective theory of value, the personal philosophic value a person puts in possessing something is reflected in what economic value this person puts on it. The limit where a person considers to purchase something may be regarded as the point where the "personal philosophic value" of possessing something exceeds the personal philosophic value of what is given up in exchange for it, e.g. money. In this light, everything can be said to have a "personal economic value" in contrast to its "societal economic value."




</doc>
<doc id="1523424" url="https://en.wikipedia.org/wiki?curid=1523424" title="The Family of Man">
The Family of Man

The Family of Man was an ambitious photography exhibition curated by Edward Steichen, the director of the Museum of Modern Art's (MoMA) Department of Photography. It was first shown in 1955 from January 24 to May 8 at the New York MoMA, then toured the world for eight years to record-breaking audience numbers. Commenting on its appeal, Steichen said the people "looked at the pictures, and the people in the pictures looked back at them. They recognized each other."

According to Steichen, the exhibition represented the "culmination of his career."
The physical collection is archived and displayed at Clervaux Castle in Luxembourg (Edward Steichen's home country; he was born there in 1879 in Bivange). It was first presented there in 1994 after restoration of the prints.

In 2003 the "Family of Man" photographic collection was added to UNESCO's Memory of the World Register in recognition of its historical value.

As part of the Museum of Modern Art's International Program, the exhibition "The Family of Man" toured the world, making stops in thirty-seven countries on six continents. More than 9 million people viewed the exhibit, which is still in excess of the audience for any photographic exhibition since. The photographs included in the exhibition, which is still on display, focus on the commonalities that bind people and cultures around the world and the exhibition itself served as an expression of humanism in the decade following World War II.

The recently-formed United States Information Agency was instrumental in touring the photographs throughout the world in five different versions for seven years, under the auspices of the Museum of Modern Art International Program. Notably, it was not shown in Franco's Spain, in Vietnam, nor in China.
Copy 5: Following a bilateral agreement between the USA and USSR, in 1959 the American National Exhibition was to be held in Moscow and the Russians were to have the use of New York City's Coliseum. This Moscow trade fair at Sokolniki Park was the scene of Soviet Premier Nikita Khrushchev and United States Vice President Richard Nixon's 'Kitchen Debate' over the relative merits of communism and capitalism. "The Family of Man" was a late inclusion not originally envisaged in MoMA's itinerary. With a grant to the Museum of $15,000 (less than half of what it requested) and funding from the plastics industry for the radical pre-fabricated translucent pavilion design to house it, a fifth copy of the show was salvaged from what was left from Beruit and Scandinavia showings, augmented with new prints. In Moscow, in the context of a trade show 'supermarket' meant to demonstrate lavish consumerism, and a multimedia display assembled by Charles Eames, the collection's overtones of peace and human brotherhood symbolized a lifting of the overhanging danger of an atomic war for Soviet citizens in the midst of the Cold War. This meaning seemed to be grasped especially by Russian students and intellectuals. Recognising the importance of the Moscow exhibition as "the high spot of the project" Steichen attended its opening and made copious photographs of the event.

The original prints from Copy 3 exhibited in the permanent collection at Clervaux Castle in Luxembourg have been restored twice, once in the 1990s and more comprehensively during a closure of the museum in the years 2010-2013.

The physical installation and layout of the Family of Man exhibition aimed to enable the visitor to read this as a photo-essay about human development and cycles of life affirming a common human identity and destiny against the contemporary Cold War threats of nuclear war.

Architect Paul Rudolph designed a series of temporary walls among the existing structural columns guiding visitors past the images, the effect of which he described as "telling a story", encouraging them to pause at those which attracted their attention. HIs layout was accommodated as closely as possible, using his display features, in the international venues which varied considerably from the original space at MoMA.

Open spaces within the layout required viewers to make their own decisions about their passage through the exhibition, and to gather to discuss it. The layout and placement of prints and their variation in size encouraged the bodily participation of the audience, who would have to bend to examine a small print displayed below eye level and then to step back to view a mural image, and to negotiate both narrow and expansive spaces.

The prints range in size from 24 x 36 cm to 300 x 400 cm and were made, in the case of the contemporary images, by assistant Jack Jackson, from the negative supplied to Steichen by each photographer. Also included were copies of historical images including a Matthew Brady civil war documentation and a Lewis Carroll portrait. Blown-up, often mural scale images, angled, floated or curved, some inset into other floor-to-ceiling prints, even displayed on the ceiling (a canted view of a silhouetted axeman and tree), on posts like finger-boards (in the final room), and the floor (for a Ring o' Roses series), were grouped together according to diverse themes. Repeated prints of Eugene Harris' portrait of a Peruvian flute-player forming a coda, or acting as 'Pied Piper' to the audience in the opinion of some reviewers, and according to Steichen himself, expressing "a little bit of mischief, but much sweetness—that's the song of life." Lighting intensities varied throughout the series of ten rooms in order to set the mood.

The exhibition opened with an entrance archway papered with a blow-up of a crowd in London by Pat English framing Wyn Bullock's Chinese landscape of sunlight on water into which was inset an image of a truncated nude of a pregnant woman in an evocation of creation myths. Subjects then ranged in sequence from lovers, to childbirth, to household, and careers, then to death and on a topical portentous note, the hydrogen bomb (an image from LIFE magazine of the test detonation "Mike," Operation Ivy, Enewetak Atoll, October 31, 1952) which was the only full-colour image; a room-filling backlit 1.8m x 2.43m Eastman transparency, replaced for the travelling version of the show with a different view of the same explosion in black and white.

Finally, full cycle, visitors returned once more to children in a room in which the last picture was W. Eugene Smith's iconic 1946 "A Walk to Paradise Garden". As the centrepiece of the exhibition a hanging sculptural installation of photographs including Vito Fiorenza's Sicilian family group and Carl Mydans' of a Japanese family (both from nations which were recent enemies of the Allies in WW2), another from Bechuanaland by Nat Farbman and a rural family of the United States by Nina Leen, encouraged circulation to view double-sided prints and invited reflection on the universal nature of the family beyond cultural differences.

Photos were chosen according to their capacity to communicate a story, or a feeling, that contributed to the overarching narrative. Each grouping of images builds upon the next, creating an intricate story of human life. The design of the exhibition built on trade displays and Steichen's 1945 "Power In The Pacific" exhibition which was designed by George Kidder Smith for MoMA, Steichen's commissioning of Herbert Bayer for the presentation of his curatorship of other exhibitions and his own long history of initiation of innovative exhibits dating back to his association with Gallery 291 early in the century. In 1963 Steichen elaborated on the special opportunities offered by the exhibition format;

In the cinema and television, the image is revealed at a pace set by the director. In the exhibition gallery, the visitor sets his own pace. He can go forward and then retreat or hurry along according to his own impulse and mood as these are stimulated by the exhibition. In the creation of such an exhibition, resources are brought into play that are not available elsewhere. The contrast in scale of images, the shifting of focal points, the intriguing perspective of long- and short- range visibility with the images to come being glimpsed beyond the images at hand —all these permit the spectator an active participation that no other form of visual communication can give.

The enlarged prints by the multiple photographers were displayed without explanatory captions, and instead were intermingled with quotations by, among others, James Joyce, Thomas Paine, Lillian Smith, and William Shakespeare, chosen by photographer and social activist Dorothy Norman. Carl Sandburg, Steichen's brother-in-law, 1951 recipient of the Pulitzer Prize for Poetry and known for his biography of Abraham Lincoln, wrote an accompanying poetic commentary also displayed as text panels throughout the exhibition and included in the publication, of which the following are a sample;

Jerry Mason (1914–1991) contemporaneously edited and published a complimentary book of the exhibition through Ridge Press, formed for the purpose in 1955 in partnership with Fred Sammis. The book, which has never been out of print, was designed by Leo Lionni (May 5, 1910 – October 11, 1999) and reproduced in a variety of formats (most popularly a soft-cover volume) in the 1950s, and reprinted in large format for its 40th anniversary, and in its various editions has sold more than four million copies. Most images from the exhibition were reproduced with an introduction by Carl Sandburg, whose prologue reads, in part:

The first cry of a baby in Chicago, or Zamboango, in Amsterdam or Rangoon, has the same pitch and key, each saying, "I am! I have come through! I belong! I am a member of the Family. Many the babies and grownup here from photographs made in sixty-eight nations round our planet Earth. You travel and see what the camera saw. The wonder of human mind, heart wit and instinct is here. You might catch yourself saying, 'I'm not a stranger here.' 

However, an omission from the book, highly significant and contrary to Steichen's stated pacifist aim, was the image of a hydrogen bomb test explosion; audiences of the time were highly sensitive to the threat of universal nuclear annihilation. In place of the huge colour transparency to which a space was devoted in the MoMA exhibition, and the black-and-white mural print that toured countries other than Japan, only this quotation of Bertrand Russell's anti-nuclear warning, in white type on a black page, appears in the book;
[...] The best authorities are unanimous in saying that a war with hydrogen bombs is quite likely to put an end to the human race [...] There will be universal death — sudden for only a minority, but for the majority a slow torture of disease and disintegration.

For most purchasers, this was their first encounter with a book that gave priority to the photographic image over text.

In 2015, to mark the sixtieth anniversary of the inaugural exhibition, MoMA reissued the book as a hardcover edition, with the original jacket design from 1955 (albeit without the signature of designer Leo Lionni) and duotone printing from new copies of all of the photographs.

Steichen's stated objective was to draw attention, visually, to the universality of human experience and the role of photography in its documentation. The exhibition brought together 503 photos from 68 countries, the work of 273 photographers (163 of whom were Americans) which, with 70 European photographers, means that the ensemble represents a primarily Western viewpoint. That forty were women photographers can in some part be attributed to Joan Miller's contribution to the selection.

Dorothea Lange assisted her friend Edward Steichen in recruiting photographers using her FSA and "Life" connections who in turn promoted the project to their colleagues. In 1953 she circulated a letter; "A Summons to Photographers All Over the World," calling on them to;

show Man to Man across the world. Here we hope to reveal by visual images Man's dreams and aspirations, his strength, his despair under evil. If photography can bring these things to life, this exhibition will be created in a spirit of passionate and devoted faith in Man. Nothing short of that will do.
The letter then listed topics that photographs might cover and these categories are reflected in the show's final arrangement. Lange's work features in the exhibition.

Steichen and his team drew heavily on "Life" archives for the photographs used in the final exhibition, seventy-five by Abigail Solomon-Godeau's count, more than 20% of the total (111 out of 503), while some were obtained from other magazines; "Vogue" was represented by nine, "Fortune" (7), "Argosy" (seven, all by Homer Page), "Ladies Home Journal" (4); "Popular Photography" (3), and others "Seventeen", "Glamour, Harper's Bazaar, Time," the British "Picture Post" and the French "Du", by one. From picture agencies American, Soviet, European and international, which also supplied the above magazines, came about 13% of the content, with Magnum represented by 43 of the pictures, Rapho with thirteen, Black Star with ten, Pix with seven, Sovfoto, which had three and Brackman with four, with around half a dozen other agencies represented by one photo.

Steichen travelled internationally to collect images, through 11 European countries including France, Switzerland, Austria and Germany. In total, Steichen procured 300 images from European photographers, many from the humanist group, which were first shown in the "Post-War European Photography" exhibition at the Museum of Modern Art in 1953. Due to the incorporation of this body of work into the 1955 "The Family of Man" exhibition, "Post-War European Photography" is thought of as a preview to "The Family of Man". The international tour of the definitive 1955 exhibition was sponsored by the now defunct United States Information Agency, whose aim was to counter Cold War propaganda by creating a better world image of American policies and values.

Though most photographers were represented by a single picture, some had several included; Robert Doisneau, Homer Page, Helen Levitt, Manuel Álvarez Bravo, Bill Brandt, Édouard Boubat, Harry Callahan (with two), Nat Farbman (five of Bechuanaland, and more from "Life"), Robert Frank (four), Bert Hardy and Robert Harrington (three). Steichen himself supplied five photos, while his assistant Wayne Miller had thirteen chosen; by far the greatest number.

The following lists all participating photographers (see original 1955 MoMA checklist):

Photography, said Steichen, "communicates equally to everybody throughout the world. It is the only universal language we have, the only one requiring no translation." When the exhibition opened most reviewers loved the show, embracing the idea of this 'universal language', and lauding Steichen as a sort of author and the exhibition as a text or essay. Photographer Barbara Morgan, in Aperture, connected this concept with the show's universalising theme;

In comprehending the show the individual himself is also enlarged, for these photographs are not photographs only — they are also phantom images of our co-citizens; this woman into whose photographic eyes I now look is perhaps today weeding her family rice paddy, or boiling a fish in coconut milk. Can you look at the polygamist family group and imagine the different norms that make them live happily in their society which is so unlike — yet like — our own? Empathy with these hundreds of human beings truly expands our sense of values.

Roland Barthes however was quick to criticise the exhibition as being an example of his concept of myth - the dramatization of an ideological message. In his book Mythologies, published in France a year after the exhibition in Paris in 1956, Barthes declared it to be a product of "conventional humanism," a collection of photographs in which everyone lives and "dies in the same way everywhere ." "Just showing pictures of people being born and dying tells us, literally, nothing."

Many other noteworthy reactions, both positive and negative, have been proffered in social/cultural studies and as part of artistic and historical texts. The earliest critics of the show were, ironically, photographers, who felt that Steichen had downplayed individual talent and discouraged the public from accepting photography as art. The show was the subject of an entire issue of Aperture; "The Controversial 'Family of Man'" Walker Evans disdained its "human familyhood [and] bogus heartfeeling" Phoebe Lou Adams complained that "If Mr. Steichen's well-intentioned spell doesn't work, it can only be because he has been so intent on [Mankind's] physical similarities that...he has utterly forgotten that a family quarrel can be as fierce as any other kind."

Some critics complained that Steichen merely transposed the magazine photo-essay from page to museum wall; in 1955 Rollie McKenna likened the experience to a ride through a funhouse, while Russell Lynes in 1973 wrote that Family of Man "was a vast photo-essay, a literary formula basically, with much of the emotional and visual quality provided by sheer bigness of the blow-ups and its rather sententious message sharpened by juxtaposition of opposites — wheat fields and landscapes of boulders, peasants and patricians, a sort of 'look at all these nice folks in all these strange places who belong to this family.'" Jacob Deschin, photography critic for The New York Times, wrote, "the show is essentially a picture story to support a concept and an editorial achievement rather than an exhibition of photography."

From an optic of struggle, echoing Barthes, Susan Sontag in On Photography accused Steichen of sentimentalism and oversimplification: ' ... they wished, in the 1950s, to be consoled and distracted by a sentimental humanism. ... Steichen's choice of photographs assumes a human condition or a human nature shared by everybody." Directly quoting Barthes, without acknowledgement, she continues; "By purporting to show that individuals are born, work, laugh, and die everywhere in the same way, "The Family of Man" denies the determining weight of history - of genuine and historically embedded differences, injustices, and conflicts.'

Others attacked the show as an attempt to paper over problems of race and class, including Christopher Phillips, John Berger, and Abigail Solomon-Godeau, who in her 2004 essay, while describing herself as among "those who intellectually came of age as postmodernists, poststructuralists, feminists, Marxists, antihumanists, or, for that matter, atheists, this little essay of Barthes's efficiently demonstrated the problem — indeed the bad faith — of sentimental humanism", concedes that "as photography exhibitions go, it is perhaps the ultimate "bad object" for progressives or critical theorists", but "good to think with.". Many of these critics, including Solomon-Godeau who openly admits it, it should be noted, had not viewed the exhibition but were working from the published catalogue which notably excludes the image of the atomic explosion.

While "The Family of Man" was being exhibited there at its last venue in 1959 several pictures were torn down in Moscow by the Nigerian student Theophilus Neokonkwo. An Associated Press report of the time suggests that his actions were in a protest at colonialist attitudes to black races

Conversely, other critics defended the exhibition, referring to the political and cultural environment in which it was staged. Among these were Fred Turner, Eric J. Sandeen, Blake Stimson and Walter L. Hixson. Most recently, a compilation of essays by contemporary critics supported by newly translated writings contemporary to the exhibition's appearances collected and edited by Gerd Hurm, Anke Reitz and Shamoon Zamir presents a revised reading of Steichen's motivations and audience reactions, and a reassessment of the validity of Roland Barthes' influential criticism in "La grande famille des hommes" in his "Mythologies".

In the years since "The Family of Man", several exhibitions stemmed from projects directly inspired by Steichen's work and others were presented in opposition to it. Still others were alternative projects offering new thoughts on the themes and motifs presented in 1955. These serve to represent artists', photographers' and curators' responses to the exhibition beside those of the cultural critics, and to track the evolution of reactions as societies and their self-images change.

Following "The Family of Man" by 10 years, the 1965 "Weltausstellung der Fotographie" (World Exhibition of Photography) was based on an idea by Karl Pawek and, supported by the German magazine "Stern", toured the world. It presented 555 photographs by 264 authors from 30 countries, outweighing the numbers in Steichen's exhibition. In the preface to the catalogue entitled 'Die humane Kamera' ('The human camera'), Heinrich Boll wrote: "There are moments in which the meaning of a landscape and its breath become felt in a photograph. The portrayed person becomes familiar or a historical moment happens in front of the lens; a child in uniform, women who search the battlefield for their dead. They are moments in which crying is more than private as it becomes the crying of mankind. Secrets are not revealed, the secret about human existence becomes visible."

The exhibition, wrote Pawek, 'would like to keep alive the spirit of Edward Steichen's wonderful ideas and of his memorable collection, "The Family of Man".'. His exhibition posed the question 'Who is Man?' in 42 topics. It focussed on issues that were sublimated in "The Family of Man" by the idea of universal brotherhood between men and women of different races and cultures. Racism, which in Steichen's show was represented by a lynching scene (replaced in the European showings by an enlargement of the famous picture of the Nürnberg trial), is confronted in the "Weltausstellung der Fotographie" section VIII Das Missverständnis mit der Rasse (Misunderstandings about Race) by the black man in the photograph by Gordon Parks who seems to view from his window two scenes of attacks on black people (photographed by Charles Moore). Another photograph by Henri Leighton shows two children walking together in public holding hands, one black, one white. Though reference to the content of the older exhibition in the new is evident, the unifying idealism of The Family of Man is here replaced with a much more fragmented and sociological one. The exhibition met with rejection by the press and functionaries in the photographic profession in Germany and Switzerland, and was described by Fritz Kempe, board member of a prominent photo company, as "tasty fodder to stimulate the aggressive instincts of semi—intellectual young men.". Nevertheless, it went on to tour 261 art museums in 36 countries and was visited by 3,500,000 people.

In 1968, a second "Weltausstellung der Fotographie" (2nd World Exhibition of Photography) was devoted to images of women with 522 photographs from 85 countries by 236 photographers, of whom barely 10% were female (compared to 21% for "The Family of Man"), though there is evidence of the effect of feminist consciousness in images of men in domestic environments cleaning, cooking and tending babies. In his introduction, Karl Pawek writes: "I had approached the first exhibition with my entire theological, philosophical and sociological equipment. 'What is Man'?; the question had to awaken ideological ideas. [...] I also operated from a philosophical point of view when presenting the[se] photos. As far as woman was concerned, the theme of the second exhibition, I knew nothing. There I was, without any philosophy about woman. Perhaps woman is not a philosophical theme. Perhaps there is only mankind, and woman is something unique and special? Thus I could only hold on to what was concrete in the pictures."

UNESCO named 1977 The Year of Children and in response the book "The Family of Children" was dedicated to Steichen by editor Jerry Mason, and imitated the original catalogue in its layout, in the use of quotations and in the colours used on the cover. As for Steichen's show there was a call-out for imagery but 300,000 entries were received compared to the 4 million at the MoMA show, resulting in a selection of 377 photos by 218 participants from 70 countries.

Independent curator Marvin Heiferman's "The Family of Man 1955·1984" was a floor to ceiling collage of over 850 images and texts from magazines. newspapers and the art world shown in 1984 at PSI, The Institute for Art and Urban Resources Inc. (now MoMA PS1) Long Island City N.Y. Abigail Solomon-Godeau described it as a reexamination of the themes of the 1955 show and critique of Steichen's arrangement of them into a 'spectacle';

...a grab bag of imagery and publicity ranging from baby food and sanitary napkin boxes to hard-core pornography, from detergent boxes to fashion photography, a cornucopia of consumer culture much of which, in one way or another, could be seen to engage the same themes purveyed in "The Family of Man". In a certain sense, Heifferman's [sic] riposte to Steichen's show made the useful connection between the spectacle of the exhibition and the spectacle of the commodity, suggesting that both must be understood within the framing context of late capitalism.

In 1990 the second Rotterdam Biennale lead exhibition was "Oppositions: We are the world, you are the third world - Commitment and cultural identity in contemporary photography from Japan, Canada. Brazil, the Soviet Union and the Netherlands" The cover of the catalogue imitates the layout and colour of the original but replaces the famous image of the little flute player by Eugene Harris with six images, four photographs of young women from different cultural backgrounds and two excerpts from paintings. In the exhibit scenes of a endangered ecology and the threat to cultural identity in the global village predominate, but there are intimations that nature and love may prevail, despite everything artificial that surrounds it, notably so in family life.

In 1992 the American critic and photographer Larry Fink published a collection of photographs under the heading of "New Relations. The Family of Man Revisited" in the Photography Center Quarterly. His approach updated Steichen's vision by integrating aspects of human existence which Steichen had omitted both because of his wish for coherence and of his innermost convictions. Fink provides only the following commentary: "Rather than a fawn pretence to anthropological/sociologic analysis of the events depicted; rather than categorise and choose democratically for social relevance.| took the path of least resistance and most reward. I simply selected quality images with the belief that the path of strong visual energies would visit equal strong social presences". He concludes:

The show is a compendium of visual hints. It is not an answer or even a full question, but cognitive clues...

In September/October 1996 the NGBK (Neue Gesellschaft fur Bildende kunst Berlin- New Society for the Visual Arts Berlin) in the context of 'Haus der Kulturen der Welt (HKW)' (House of World Cultures Berlin) conceived and organised the project "family, nation, tribe, community: SHIFT" with direct reference to the historical MoMA exhibition. In the catalogue, five authors; Ezra Stoller, Max Kozloff, Torsten Neuendorff, Bettina Allamoda and Jean Back analyse and comment on the historical model and twenty-two artists offer individual approaches around the following themes: Universalism/Separatism, Family/Anti-family, Individualisation, Common Strategies, Differences. The works are predominantly from artist photographers rather than photojournalists; Bettina Allamoda, Aziz + Cucher, Los Carpinteros, Alfredo Jaar, Mike Kelley, Edward and Nancy Reddin Kienholz, Lovett/Codagnone, Loring McAlpin, Christian Philipp Müller, Anna Petrie, Martha Rosler, Lisa Schmitz, STURTEVANT, Mitra Tabizian and Andy Golding, Wolfgang Tillmans, Danny Tisdale, Lincoln Tobler, and David Wojnarowicz reflect major contemporary issues: identity, the information crisis, the illusion of leisure, and ethics. In his introduction to the exhibition, Frank Wagner writes that Steichen had offered a vision of an harmonious, neat and highly structured world which, in reality, was complex, often unintelligible and even contradictory, but by contrast, this Berlin exhibition highlights 'first' and 'third' world tensions and is eager to concentrate on a variety of attitudes.

The following year Enrico Lunghi directed the exhibition "The 90s: A Family of Man?: images of mankind in contemporary art", held 02.10.–30.11.1997 in Luxembourg, Steichen's birthplace and by then the repository of the archive of a full version of his "The Family of Man." Aside from their understanding of Steichen's efforts to present commonalities amongst the human race, curators Paul di Felice and Pierre Stiwer interpret Steichen's show as an effort to make content of Museum of Modern Art accessible to the public in an era when it was regarded as the elitist supporter of 'incomprehensible' abstract art. They point to their predecessor's success in having his show embraced by a record audience and emphasise that dissenting voices of criticism were heard only amongst 'intellectuals'. However, Steichen's success, they caution, was to manipulate the message of his selected imagery; 'After all,' they write, 'wasn't he the artistic director of Vogue and Vanity Fair ... ?'. They proclaim their desire to retain the exhibiting artists' 'autonomy' while not posing their work as the antithesis of Steichen's concept, but to respect, and echo, its arrangement while 'raising questions' as indicated by the question mark in their quotation of the original title. The exhibition and catalogue 'quote' from Steichen, setting pages of the book of his exhibition with their quotations around groupings of images (in monochrome) beside the works of contemporary artists (predominantly in colour) collected in themes used in the original, though the correlation fails for some contemporary ideas, which digital imaging, installation and montage works effectively convey. The thirty-five artists include Christian Boltanski, Nan Goldin, Inez van Lamsweerde, Orlan and Wolfgang Tillmans.

The Photographic Society of America (PSA) drew on their archives to stage "Reconsidering The Family of Man" during April and May 2012. They based the display on the concept of Steichen's original exhibit but concentrated on his sub-theme of the passage from birth to death. From the close to 5,000 photographs in the PSA collection, a selection of 50 was made for their show. One work in common with the original exhibition was Ansel Adams' "Mount Williamson from Manzanar" which in "The Family of Man" was presented at mural scale, while the PSA used a vintage,11 "x 14" Adams print from their collection, displaying it wilh a first edition copy of "The Family of Man" publication opened to a double-page spread of Adams photograph.

As part of the 2015-2016 France-Korea year, curators of the Centre national des arts plastiques (Cnap) and the Fonds Régional d’Art Contemporain of Aquitaine (Frac Aquitaine), Pascal Beausse (Cnap), Claire Jacquet (Frac Aquitaine), and Magali Nachtergael, Assistant Professor at the Sorbonne, collaborated to produce the exhibition "The Family of the Invisibles" at the Seoul Museum of Art (SeMA) and the Ilwoo Space in Seoul, from 5 April to 29 May 2016. The show was devoted to invisible and minority figures, their demands for identity, and the possibility of reconfiguring a politics of representation to the ideal of giving a place to each member of the human community as represented in more than 200 emblematic photographs. The works from the 1930s to 2016, drawn from the Cnap and Frac Aquitaine collections were selected on the principle of Roland Barthes' deconstruction identified by the curators in his "Mythologies" and in "Camera Lucida", the latter being treated as a visual manifesto for minorities. The exhibition was presented in the Seoul Museum of Art in four sections, culminating in provocative contemporary photography including the 2009 series of deceased migrants wrapped in cloth in "Les Proscrits" ('The Outcasts') by Mathieu Pernot, and Sophie Calle’s 1986 "Les Aveugles" in which she photographed those things that her blind subjects described as the most beautiful. The “Prologue” of the exhibition at the Ilwoo Space, provided a critical and historical counterpoint. Texts by Pascal Beausse, Jacqueline Guittard, Claire Jacquet and Magali Nachtergael, Suejin Shin (Ilwoo Foundation) and Kyung-hwan Yeo (SeMA) were presented in a catalogue.
"The Family Of No Man: Re-visioning the world through non-male eyes", held July 2–8, 2018 in Arles brought together responses to an open call by Cosmos Arles Books, a satellite space of the Rencontres d'Arles, by 494 female and inter-gender artists from all around the world, in a revisitation of Edward Steichen’s original. Works were displayed in interactive installations outdoors and indoors, and uploaded to an online platform as they were received.

The permanent installation of the exhibition today at Chateau Clervaux in Luxembourg follows the layout of the inaugural exhibition at MoMA in order to recreate the original viewing experience, though necessarily it is adapted to the unique space of two floors of the restored Castle. Since the 2013 restoration it now incorporates a library (that includes some of the catalogues of the sequel exhibitions above) and contextualises "The Family of Man" with historical material and interpretation.





</doc>
<doc id="2430449" url="https://en.wikipedia.org/wiki?curid=2430449" title="Critical positivity ratio">
Critical positivity ratio

The critical positivity ratio (also known as the Losada ratio or the Losada line) is a largely discredited concept in positive psychology positing an exact ratio of positive to negative emotions which distinguishes "flourishing" people from "languishing" people. The ratio was proposed by Marcial Losada and psychologist Barbara Fredrickson, who identified a ratio of positive to negative affect of exactly 2.9013 as separating flourishing from languishing individuals in a 2005 paper in "American Psychologist". The concept of a critical positivity ratio was widely embraced by both academic psychologists and the lay public; Fredrickson and Losada's paper was cited nearly 1,000 times, and Fredrickson wrote a popular book expounding the concept of "the 3-to-1 ratio that will change your life". Fredrickson wrote: "Just as zero degrees Celsius is a special number in thermodynamics, the 3-to-1 positivity ratio may well be a magic number in human psychology."

The first critical evaluation of the mathematical modeling behind the critical positivity ratio was published by a group of Finnish researchers Luoma, Hämäläinen and Saarinen in 2008. The authors noted that "Only very limited explanations are given about the modeling process and the meaning and interpretation of its parameters. Thus, the reasoning behind the model equations remains unclear to the reader." Moreover, "the model also produces strange and previously unreported behavior under certain conditions. Thus, the predictive validity of the model also becomes problematic." Later in 2013, the critical positivity ratio aroused the skepticism of Nick Brown, a graduate student in applied positive psychology, who felt that the paper's mathematical claims underlying the critical positivity ratio were fundamentally flawed. Brown collaborated with physicist Alan Sokal and psychologist Harris Friedman on a re-analysis of the paper's data. They argued that Fredrickson and Losada's paper contained "numerous fundamental conceptual and mathematical errors", as did Losada's earlier work on positive psychology, which completely invalidated their claims. In their response, Hämäläinen and colleagues argued that there were no fundamental errors in the mathematics itself, but the problems relate to the interpretation and justification of the use of the model. Losada declined to respond to the criticism, indicating that he was too busy running his consulting business. Fredrickson wrote a response in which she conceded that the mathematical aspects of the critical positivity ratio were "questionable" and that she had "neither the expertise nor the insight" to defend them, but she maintained that the empirical evidence was solid. Brown and colleagues, whose response was published the next year, maintain that there is no evidence for the critical positivity ratio.

In response, "American Psychologist" formally retracted the mathematical modeling elements of Fredrickson & Losada's paper, including the specific critical positivity ratio of 2.9013, as invalid. The problems with the paper went unnoticed for years despite the widespread publicity surrounding the critical positivity ratio, contributed to a perception that social psychology as a field lacked scientific soundness and rigorous critical thinking. Sokal later stated: "The main claim made by Fredrickson and Losada is so implausible on its face that some red flags ought to have been raised."

Building on research by Barbara Fredrickson indicating that individuals with a higher ratio of positive to negative emotions tend to have more successful life outcomes, and on studies by Marcial Losada applying differential equations from fluid dynamics to human emotions, Fredrickson and Losada used nonlinear dynamics modelling (based on Lorenz systems) to argue that the ideal positivity/negativity ratio lies between 2.9013 and 11.6346. They argued that those with ratios within this range will "flourish", whereas those with values outside it will "languish". They claimed that their model predicted cut-off points for the maximum and minimum positivity ratios within which one should observe qualitative changes in an individual's level of flourishing.

Losada's article was critiqued by Andrés Navas in a Note to the French website of the CNRS "Images des Mathématiques". The whole theory of the critical positivity ratio was strongly critiqued by Nicholas Brown, Alan Sokal, and Harris Friedman, in a 2013 article published in "American Psychologist". Brown et al. argue that Losada's conclusions in previous papers using modelling from fluid dynamics, and those in his paper co-authored with Fredrickson, are not only based on poorly reported experiments – they argue that it is difficult to draw any conclusions from some previous studies by Losada because critical details are omitted, and "interpretations of results are made with little or no justification" (p. 5) – but are based on elementary errors in the use of differential equations.

Among the severe flaws claimed by Brown et al. in the positivity-ratio theory and its presentation were:

Brown et al. state that one can:only marvel at the astonishing coincidence that human emotions should turn out to be governed by exactly the same set of equations that were derived in a celebrated article several decades ago as a deliberately simplified model of convection in fluids, and whose solutions happen to have visually appealing properties. An alternative explanation – and, frankly, the one that appears most plausible to us – is that the entire process of "derivation" of the Lorenz equations has been contrived to demonstrate an imagined fit between some rather limited empirical data and the scientifically impressive world of nonlinear dynamics. (p. 8)They "urge future researchers to exercise caution in the use of advanced mathematical tools, such as nonlinear dynamics" (p. 1).

Fredrickson responded to the critique by agreeing that Losada's mathematical modelling is "questionable" and does not show that there are precise values of the ratio, but also arguing that the evidence for the benefits of a high positivity/negativity ratio is solid. Fredrickson noted that Losada declined to respond to the criticism. Sokal was openly critical about this partial retraction, and in 2014, "American Psychologist" published their response to Fredrikson's retraction, where they emphatically argue that there is no evidence for the critical positivity ratio whatsoever. Responding to comments on their original critique, they conclude the entire affair by lamenting that instead of replacing the "unbridled romanticism" of humanist psychology with a rigorous evidence-based psychology, as Seligman and Csikszentmihalyi promised in their founding manifesto of positive psychology, the widespread acceptance of the critical positivity ratio shows, in their view, that positive psychology has betrayed this promise: "That the sin is now romantic scientism rather than pure romanticism is not, in our view, a great advance."



</doc>
<doc id="50797822" url="https://en.wikipedia.org/wiki?curid=50797822" title="Inclusive fitness in humans">
Inclusive fitness in humans

Inclusive fitness in humans is the application of inclusive fitness theory to human social behaviour, relationships and cooperation.

Inclusive fitness theory (and the related kin selection theory) are general theories in evolutionary biology that propose a method to understand the evolution of social behaviours in organisms. While various ideas related to these theories have been influential in the study of the social behaviour of non-human organisms, their application to human behaviour has been debated.

Inclusive fitness theory is broadly understood to describe a "statistical" criterion by which social traits can "evolve" to become widespread in a population of organisms. However, beyond this some scientists have interpreted the theory to make predictions about how the "expression" of social behavior is mediated in both humans and other animals – typically that genetic relatedness determines the expression of social behaviour. Other biologists and anthropologists maintain that beyond its statistical evolutionary relevance the theory does not necessarily imply that genetic relatedness "per se" determines the "expression" of social behavior in organisms. Instead, the expression of social behavior may be mediated by correlated conditions, such as shared location, shared rearing environment, familiarity or other contextual cues which correlate with shared genetic relatedness, thus meeting the statistical evolutionary criteria without being deterministic. While the former position still attracts controversy, the latter position has a better empirical fit with anthropological data about human kinship practices, and is accepted by cultural anthropologists.

Applying evolutionary biology perspectives to humans and human society has often resulted in periods of controversy and debate, due to their apparent incompatibility with alternative perspectives about humanity. Examples of early controversies include the reactions to "On the Origin of Species", and the Scopes Monkey Trial. Examples of later controversies more directly connected with inclusive fitness theory and its use in sociobiology include physical confrontations at meetings of the Sociobiology Study Group and more often intellectual arguments such as Sahlins' 1976 book "The use and abuse of biology", Lewontin et al.'s 1984 "Not in Our Genes", and Kitcher's 1985 "Vaulting Ambition:Sociobiology and the Quest for Human Nature". Some of these later arguments were produced by other scientists, including biologists and anthropologists, against Wilson's 1975 book "", which was influenced by (though not necessarily endorsed by) Hamilton's work on inclusive fitness theory.

A key debate in applying inclusive fitness theory to humans has been between biologists and anthropologists around the extent to which human kinship relationships (considered to be a large component of human solidarity and altruistic activity and practice) are necessarily based on or influenced by genetic relationships or blood-ties ('consanguinity'). The position of most social anthropologists is summarized by Sahlins (1976), that for humans "the categories of 'near' and 'distant' [kin] vary independently of consanguinal distance and that these categories organize actual social practice" (p. 112). Biologists wishing to apply the theory to humans directly disagree, arguing that "the categories of 'near' and 'distant' do not 'vary independently of consanguinal distance', not in any society on earth." (Daly et al. 1997, p282).

This disagreement is central because of the way the association between blood ties/genetic relationships and altruism are conceptualized by many biologists. It is frequently understood by biologists that inclusive fitness theory makes predictions about "how behaviour is mediated" in both humans and other animals. For example, a recent experiment conducted on humans by the evolutionary psychologist Robin Dunbar and colleagues was, as they understood it, designed "to test the prediction that altruistic behaviour is mediated by Hamilton's rule" (inclusive fitness theory) and more specifically that "If participants follow Hamilton's rule, investment (time for which the [altruistic] position was held) should increase with the recipient's relatedness to the participant. In effect, we tested whether investment flows differentially down channels of relatedness." From their results, they concluded that "human altruistic behaviour is mediated by Hamilton's rule ... humans behave in such a way as to maximize inclusive fitness: they are more willing to benefit closer relatives than more distantly related individuals." (Madsen et al. 2007). This position continues to be rejected by social anthropologists as being incompatible with the large amount of ethnographic data on kinship and altruism that their discipline has collected over many decades, that demonstrates that in many human cultures, kinship relationships (accompanied by altruism) do not necessarily map closely onto genetic relationships.

Whilst the above understanding of inclusive fitness theory as necessarily making predictions about how human kinship and altruism is mediated is common amongst evolutionary psychologists, other biologists and anthropologists have argued that it is at best a limited (and at worst a mistaken) understanding of inclusive fitness theory. These scientists argue that the theory is better understood as simply describing an "evolutionary" criterion for the emergence of altruistic behaviour, which is explicitly statistical in character, not as predictive of "proximate" or "mediating" mechanisms of altruistic behaviour, which may not necessarily be determined by genetic relatedness (or blood ties) "per se". These alternative non-deterministic and non-reductionist understandings of inclusive fitness theory and human behavior have been argued to be compatible with anthropologists' decades of data on human kinship, and compatible with anthropologists' perspectives on human kinship. This position (e.g. nurture kinship) has been largely accepted by social anthropologists, whilst the former position (still held by evolutionary psychologists, see above) remains rejected by social anthropologists.

Inclusive fitness theory, first proposed by Bill Hamilton in the early 1960s, proposes a selective criterion for the potential evolution of social traits in organisms, where social behavior that is costly to an individual organism's survival and reproduction could nevertheless emerge under certain conditions. The key condition relates to the statistical likelihood that significant benefits of a social trait or behavior accrue to (the survival and reproduction of) other organisms whom also carry the social trait. Inclusive fitness theory is a general treatment of the statistical probabilities of social traits accruing to any other organisms likely to propagate a copy of the same social trait. Kin selection theory treats the narrower but more straightforward case of the benefits accruing to close genetic relatives (or what biologists call 'kin') who may also carry and propagate the trait. Under conditions where the social trait sufficiently correlates (or more properly, "regresses") with other likely bearers, a net overall increase in reproduction of the social trait in future generations can result.

The concept serves to explain how natural selection can perpetuate altruism. If there is an "altruism gene" (or complex of genes or heritable factors) that influence an organism's behavior in such a way that is helpful and protective of relatives and their offspring, this behavior can also increase the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. In formal terms, if such a complex of genes arises, Hamilton's rule (rb>c) specifies the selective criteria (in terms of relatedness (r), cost (c) benefit (b)) for such a trait to increase in frequency in the population (see Inclusive fitness for more details). Hamilton noted that inclusive fitness theory does not by itself predict that a given species will necessarily evolve such altruistic behaviors, since an opportunity or context for interaction between individuals is a more primary and necessary requirement in order for any social interaction to occur in the first place. As Hamilton put it, "Altruistic or selfish acts are only possible when a suitable social object is available. In this sense behaviours are conditional from the start." (Hamilton 1987, 420). In other words, whilst inclusive fitness theory specifies a set of necessary criteria for the evolution of certain altruistic traits, it does not specify a sufficient condition for their evolution in any given species, since the typical ecology, demographics and life pattern of the species must also allow for social interactions between individuals to occur, before any potential elaboration of social traits can evolve in regard to those interactions.

The initial presentation of inclusive fitness theory (in the mid 1960s, see "The Genetical Evolution of Social Behaviour") focused on making the general mathematical case for the possibility of social evolution. However, since many field biologists mainly use theory as a guide to their observations and analysis of empirical phenomena, Hamilton also speculated about possible proximate behavioural mechanisms that might be observable in organisms whereby a social trait could effectively achieve this necessary statistical correlation between its likely bearers:

Hamilton here was suggesting two broad proximate mechanisms by which social traits might meet the criterion of correlation specified by the theory:

Kin recognition (active discrimination): if a social trait enables an organism to distinguish between different degrees of genetic relatedness when interacting in a mixed population, and to discriminate (positively) in performing social behaviours on the basis of detecting genetic relatedness, then the average relatedness of the recipients of altruism could be high enough to meet the criterion. In another section of the same paper (page 54) Hamilton considered whether 'supergenes' that identify copies of themselves in others might evolve to give more accurate information about genetic relatedness. He later (1987, see below) considered this to be wrong-headed and withdrew the suggestion.

Viscous populations (spatial cues): even indiscriminate altruism may achieve the correlation in 'viscous' populations where individuals have low rates of dispersal or short distances of dispersal from their home range (their location of birth). Here, social partners are typically genealogically closely related, and so altruism can flourish even in the absence of kin recognition and kin discrimination faculties – spatial proximity and circumstantial cues provide the necessary correlation.

These two alternative suggestions had important effects on how field biologists understood the theory and what they looked for in the behavior of organisms. Within a few years biologists were looking for evidence that 'kin recognition' mechanisms might occur in organisms, assuming this was a necessary prediction of inclusive fitness theory, leading to a sub-field of 'kin recognition' research.

A common source of confusion around inclusive fitness theory is that Hamilton's early analysis included some inaccuracies, that, "although corrected by him in later publications", are often not fully understood by other researchers who attempt to apply inclusive fitness to understanding organisms' behaviour. For example, Hamilton had initially suggested that the statistical correlation in his formulation could be understood by a correlation coefficient of genetic relatedness, but quickly accepted George Price's correction that a general regression coefficient was the more relevant metric, and together they published corrections in 1970. A related confusion is the connection between inclusive fitness and multi-level selection, which are often incorrectly assumed to be mutually exclusive theories. The regression coefficient helps to clarify this connection:

Hamilton also later modified his thinking about likely mediating mechanisms whereby social traits achieve the necessary correlation with genetic relatedness. Specifically he corrected his earlier speculations that an innate ability (and 'supergenes') to recognise actual genetic relatedness was a likely mediating mechanism for kin altruism:

The point about inbreeding avoidance is significant, since the whole genome of sexual organisms benefits from avoiding close inbreeding; there is a different selection pressure at play compared to the selection pressure on social traits (see Kin recognition for more information).

Since Hamiton's 1964 speculations about active discrimination mechanisms (above), other theorists such as Richard Dawkins had clarified that there would be negative selection pressure against mechanisms for genes to recognize copies of themselves in other individuals and discriminate socially between them on this basis. Dawkins used his 'Green beard' thought experiment, where a gene for social behaviour is imagined also to cause a distinctive phenotype that can be recognised by other carriers of the gene. Due to conflicting genetic similarity in the rest of the genome, there would be selection pressure for green-beard altruistic sacrifices to be suppressed via meitoic drive.

Hamilton's later clarifications often go unnoticed, and because of the long-standing assumption that kin selection requires innate powers of kin recognition, some theorists have later tried to clarify the position:

The assumption that 'kin selection requires kin discrimination' has obscured the more parsimonious possibility that spatial-cue-based mediation of social cooperation based on limited dispersal and shared developmental context are commonly found in many organisms that have been studied, including in social mammal species. As Hamilton pointed out, "Altruistic or selfish acts are only possible when a suitable social object is available. In this sense behaviours are conditional from the start" (Hamilton 1987, see above section). Since a reliable context of interaction between social actors is always a necessary condition for social traits to emerge, a reliable context of interaction is necessarily present to be leveraged by context-dependent cues to mediate social behaviours. Focus on mediating mechanisms of limited dispersal and reliable developmental context has allowed significant progress in applying kin selection and inclusive fitness theory to a wide variety of species, including humans, on the basis of cue-based mediation of social bonding and social behaviours (see below).

In mammals, as well as in other species, ecological niche and demographic conditions strongly shape typical contexts of interaction between individuals, including the frequency and circumstances surrounding the interactions between genetic relatives. Although mammals exist in a wide variety of ecological conditions and varying demographic arrangements, certain contexts of interaction between genetic relatives are nevertheless reliable enough for selection to act upon. New born mammals are often immobile and always totally dependent (socially dependent if you will) on their carer(s) for nursing with nutrient rich milk and for protection. This fundamental social dependence is a fact of life for all mammals, including humans. These conditions lead to a reliable spatial context in which there is a statistical association of replica genes between a reproductive female and her infant offspring (and has been evolutionary typical) for most mammal species. Beyond this natal context, extended possibilities for frequent interaction between related individuals are more variable and depend on group living vs. solitary living, mating patterns, duration of pre-maturity development, dispersal patterns, and more. For example, in group living primates with females remaining in their natal group for their entire lives, there will be lifelong opportunities for interactions between female individuals related through their mothers and grandmothers etc. These conditions also thus provide a spatial-context for cue-based mechanisms to mediate social behaviours.

In addition to the above examples, a wide variety of evidence from mammal species supports the finding that shared context and familiarity mediate social bonding, rather than genetic relatedness "per se". Cross-fostering studies (placing unrelated young in a shared developmental environment) strongly demonstrate that unrelated individuals bond and cooperate just as would normal littermates. The evidence therefore demonstrates that bonding and cooperation are mediated by proximity, shared context and familiarity, not via active recognition of genetic relatedness. This is problematic for those biologists who wish to claim that inclusive fitness theory predicts that social cooperation is "mediated" via genetic relatedness, rather than understanding the theory simply to state that social traits can "evolve" under conditions where there is "statistical" association of genetically related organisms. The former position sees the expression of cooperative behaviour as more or less deterministically caused by genetic relatedness, where the latter position does not. The distinction between cooperation mediated by shared context, and cooperation mediated by genetic relatedness "per se", has significant implications for whether inclusive fitness theory can be seen as compatible with the anthropological evidence on human social patterns or not. The "shared context" perspective is largely compatible, the "genetic relatedness" perspective is not (see below).

The debate about how to interpret the implications of Inclusive fitness theory for human social cooperation has paralleled some of the key misunderstandings outlined above. Initially, evolutionary biologists interested in humans wrongly assumed that in the human case, 'kin selection requires kin discrimination' along with their colleagues studying other species (see West et al., above). In other words, many biologists assumed that strong social bonds accompanied by altruism and cooperation in human societies (long studied by the anthropological field of kinship) were necessarily built upon recognizing genetic relatedness (or 'blood ties'). This seemed to fit well with historical research in anthropology originating in the nineteenth century (see history of kinship) that often assumed that human kinship was built upon a recognition of shared blood ties.

However, independently of the emergence of inclusive fitness theory, from 1960s onwards many anthropologists themselves had reexamined the balance of findings in their own ethnographic data and had begun to reject the notion that human kinship is 'caused by' blood ties (see Kinship). Anthropologists have gathered very extensive ethnographic data on human social patterns and behaviour over a century or more, from a wide spectrum of different cultural groups. The data demonstrates that many cultures do not consider 'blood ties' (in the genealogical sense) to underlie their close social relationships and kinship bonds. Instead social bonds are often considered to be based on location-based shared circumstances including living together (co-residence), sleeping close together, working together, sharing food (commensality) and other forms of shared life together. Comparative anthropologists have shown that these aspects of shared circumstances are a significant component of what influences kinship in most human cultures, notwithstanding whether or not 'blood ties' are necessarily present (see Nurture kinship, below).

Although blood ties (and genetic relatedness) often correlate with kinship, just as in the case of mammals (above section), evidence from human societies suggests that it is not the genetic relatedness "per se" that is the mediating mechanism of social bonding and cooperation, instead it is the shared context (albeit typically consisting of genetic relatives) and the familiarity that arises from it, that mediate the social bonds. This implies that genetic relatedness is not the "determining mechanism" nor required for the formation of social bonds in kinship groups, or for the expression of altruism in humans, even if statistical correlations of genetic relatedness are an "evolutionary criterion" for the emergence of such social traits in biological organisms over evolutionary timescales. Understanding this distinction between the "statistical role of genetic relatedness in the evolution of social traits" and yet its "lack of necessary determining role in mediating mechanisms of social bonding and the expression of altruism" is key to inclusive fitness theory's proper application to human social behaviour (as well as to other mammals).

Compatible with biologists' emphasis on familiarity and shared context mediating social bonds, the concept of nurture kinship in the anthropological study of human social relationships highlights the extent to which such relationships are brought into being through the performance of various acts of sharing, acts of care and performance of nurture between individuals who live in close proximity. Additionally the concept highlights ethnographic findings that, in a wide swath of human societies, people understand, conceptualize and symbolize their relationships predominantly in terms of giving, receiving and sharing nurture. The concept stands in contrast to the earlier anthropological concepts of human kinship relations being fundamentally based on "blood ties", some other form of shared substance, or a proxy for these (as in fictive kinship), and the accompanying notion that people universally understand their social relationships predominantly in these terms.

The nurture kinship perspective on the ontology of social ties, and how people conceptualize them, has become stronger in the wake of David M. Schneider's influential "Critique of the Study of Kinship" and Holland's subsequent "Social Bonding and Nurture Kinship: Compatibility between Cultural and Biological approaches", demonstrating that as well as the ethnographic record, "biological theory and evidence" also more strongly support the "nurture" perspective than the "blood" perspective. Both Schneider and Holland argue that the earlier "blood" theory of kinship derived from an unwarranted extension of symbols and values from anthropologists' own cultures (see ethnocentrism).


</doc>
<doc id="43868891" url="https://en.wikipedia.org/wiki?curid=43868891" title="Social Bonding and Nurture Kinship">
Social Bonding and Nurture Kinship

Social Bonding and Nurture Kinship: Compatibility between Cultural and Biological Approaches is a book on human kinship and social behavior by Maximilian Holland, published in 2012. The work synthesizes the perspectives of evolutionary biology, psychology and sociocultural anthropology towards understanding human social bonding and cooperative behavior. It presents a theoretical treatment that many consider to have resolved longstanding questions about the proper place of genetic (or 'blood') connections in human kinship and social relations, and a synthesis that "should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology". The book has been called "A landmark in the field of evolutionary biology" which "gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior", "places genetic determinism in the correct perspective" and serves as "a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries."

The aim of the book is to show that "properly interpreted, cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible regarding processes of social bonding in humans." Holland's position is based on demonstrating that the dominant biological theory of social behavior (inclusive fitness theory) is typically misunderstood to predict that genetic ties are necessary for the "expression" of social behaviors, whereas in fact the theory only implicates genetic associations as necessary for the "evolution" of social behaviors. Whilst rigorous evolutionary biologists have long understood the distinction between these levels of analysis (see Tinbergen's four questions), past attempts to apply inclusive fitness theory to humans have often overlooked the distinction between "evolution" and "expression".

Beyond its central argument, the broader philosophical implications of Holland's work are considered by commentators to be that it both "helps to untangle a long-standing disciplinary muddle" and "clarifies the relationship between biological and sociocultural approaches to human kinship." It is claimed that the book "demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs" and as a consequence, delivers "a convincing, solid and informed blow to the residual genetic determinism that still influences the interpretation of social behaviour."

The book's form consists of a cumulative argument (using a wide range of supporting evidence) made over nine chapters, with each chapter ending in a brief retrospective summary, and the final chapter containing a recapitulation and summary of the whole, and drawing some wider conclusions.

Holland begins by tracing transitions in the history of anthropological theories of social behavior and kinship, noting the varying importance with which 'blood ties' have been understood to be a necessary element of human kinship and social relations. He suggests that whilst the mounting ethnographic evidence has led to a move away from the 'blood kinship' concept in recent decades, many sociocultural anthropologists still query the connection between kinship and blood, reproduction or some other apparently biological functions. Meanwhile, many biologists, biological anthropologists and evolutionary psychologists have persisted in viewing human kinship and cooperative behavior as necessarily associated with genetic relationships and 'blood ties'. The current situation has been characterized as "a clash between incommensurate paradigms, holding as they may, completely incompatible ideas about human nature." Holland argues that a clear resolution to these questions is still outstanding, and would therefore be of value. In closing the introduction, Holland writes; "The approach is not reductive. The claim is rather that a thorough investigation of the ‘biological facts’ can be useful mainly though allowing a change in focus... away from confusion about the place of genealogy in social ties, and onto a reformulated baseline, built around varied "processual aspects of social bonding.""

The book reviews the background and key elements of Hamilton's inclusive fitness theory from the 1960s onwards, setting out its significant conceptual and heuristic value. Holland notes that Hamilton acknowledged that his earliest and most widely known account (1964) contained technical inaccuracies. He also notes Hamilton's early speculations about possible proximate mechanisms of the expression of social behavior ("supergenes" as a possible alternative to "behaviour-evoking-situations") contained errors that have nevertheless remained very influential in popular accounts. Specifically, the supergenes notion (sometimes called the "Green-beard effect") - that organisms may evolve genes that are able to identify identical copies in others and preferentially direct social behaviours towards them - was theoretically clarified and withdrawn by Hamilton in 1987. However, in the intervening years, the notion that supergenes (or more often, simply individual organisms) have evolved "to identify genetic relatives and preferentially cooperate with them" took hold, and became the way many biologists came to understand the theory. This persisted, despite Hamilton's 1987 correction. In Holland's view it is the pervasiveness of this longstanding but erroneous perspective, and the suppression of the alternative 'behaviour-evoking-situations' perspective regarding social expression mechanisms, that is largely responsible for the ongoing clash between biological and sociocultural approaches to human kinship.

Holland shows that, in the 1970s and 80s, the first wave of attempts (known as "human sociobiology" or "Darwinian anthropology") to apply inclusive fitness theory to human social behavior relied on, and further reinforced, this same misinterpretation (above section) about the theory's predictions and the proximate mechanisms of social behavior. Holland also shows that this period of research was burdened with many misplaced assumptions about "universal" attributes of the human sexes, sexuality and gender roles, apparently projected from the "specific cultural values" of the researchers themselves. Holland also shows that, following the perceived failures of this early wave, and particularly its methodological agnosticism regarding proximate mechanisms of social behavior, the evolutionary psychology school grew up in its place. Although this latter school typically avoided engaging with the ethnographic data on human kinship, Holland argues that in the few cases where it did so, it repeated the misinterpretation of inclusive fitness theory that characterized the first wave. Holland also notes that Kitcher, in his critique of the sociobiological position, suggested that perhaps the expression of social behaviors in humans might quite simply be based on cues of context and familiarity, rather than genetic relatedness "per se".

Chapters four and five investigate further the theory and evidence surrounding the "proximate mechanisms" of social behavior; specifically the question of whether social behaviors are expressed by organisms via "behaviour-evoking-situations" or via direct detection of actual genetic relatedness. Related questions have been the domain of kin recognition theory. Holland notes that the name 'kin recognition' itself suggests some expectation that a positive identification of genetic relatedness is a prediction of inclusive fitness theory, and is thus expected. Similar points have been made by others; "many behavioural ecologists seem to implicitly assume that specialised mechanisms allowing individuals to distinguish their kin from non-kin must have evolved." Again, the possibility that "behaviour-evoking-situations" might be the more parsimonious mechanism of the expression of social behavior, and fully compatible with inclusive fitness theory, has often been underemphasized. However, Holland's review of the evidence notes that field studies in this area quickly established that "behaviour-evoking-situations" do in fact overwhelmingly mediate social behaviours in those species studied, and that, particularly in mammal species, social bonding and familiarity formed in early developmental contexts (e.g. in burrows or nesting sites) are a common mediating mechanism for social behaviors, independently of genetic relatedness "per se". On the basis of the preceding theoretical analysis and review of evidence, at the end of chapter five, Holland argues that;
It is entirely erroneous, both in reference to theory and in reference to the evidence, to claim or suggest that 'the facts of biology' support the claim that organisms have evolved to cooperate with genetic relatives "per se".

Having argued for the above position on the lack of necessity for genetic relatedness "per se" to mediate social bonding and behavior, Holland suggests that "The further question then is; can we uncover in any greater detail how familiarity and other context-dependent cues operate?". To discover the extent to which the variety of human kinship behaviors may nevertheless be compatible with this (less deterministic) interpretation of biological theory of social behavior, Holland suggests that a survey of primates' most fundamental social patterns may give clues, especially those of species most closely connected with humans. The variety of primate mating systems, group-membership ('philopatry') patterns, and life-cycle patterns are reviewed. Holland finds that;

Like other mammals, Catarrhini primate demographics are strongly influenced by ecological conditions, particularly density and distribution of food sources... Cohesive social groups and delayed natal dispersal mean that maternally related individuals, including maternal siblings, face a statistically reliable context of interaction in all Catarrhini primates. This reliable context of interaction with maternally related individuals is extended amongst those species with female philopatry (especially Cercopithecinae).

As with other social mammals, evidence suggest that the reliability of 'behaviour-evoking-situations' this social context provides has shaped the mechanisms of proximate expression of social bonding and behavior;

Adoption of infants by females (and sometimes males) demonstrates that care-giving and bonding to infants is not mediated by positive powers of discrimination. From the infant's perspective, it will bond with any responsive carer. If not necessarily the actual mother, in natural conditions this will often be a maternal relative (particularly an older sibling), but the context is primary, not the actual relatedness. Similarly, social bonding and social behaviours between maternal siblings (and occasionally between other maternal relatives) is context-driven in primates, and mediated via the care-giver.

Holland also notes how Bowlby and colleagues' attachment theory was strongly informed by primate bonding patterns and mechanisms, and that in Bowlby's later writing the then emerging inclusive fitness theory was explicitly linked to. 
[Bowlby's] work demonstrated that social attachments form on the basis of provision of care, and responsiveness to elicitations for care. The social context of living together and the familiarity this brings, provides the circumstance within which social bonds can form...

On the basis of combining more recent primate research with the findings of attachment theory, Holland proposes that "In attempting to define more specific forms of "the giving of care and nurture" which may mediate social bonding we [find] that provision of food is likely to play a part, as well as the more intangible provision of warmth and comfort, and a safe base for sleeping."

Holland claims that, while biological theory of social behavior is not deterministic in respect of genetic relatedness vis-a-vis the formation of social bonds and expression of social behaviors, evidence does point to compatibility between a non-reductive interpretation of the theory and how such bonds and behaviors operate in social mammals, primates and in humans. In the final part of the book, Holland explores the extent to which this perspective is also compatible with sociocultural anthropology's ethnographic accounts of human kinship and social behavior, both occasional accounts from the past, as well as more contemporary accounts that have explicitly eschewed the earlier 'blood ties' assumption. Holland finds that;

Many contemporary accounts focus on social bonds formed in childhood and the importance of the performance of acts of care, including food provision, in mediating these bonds. In all cases it is this performance of care which is considered the overriding factor in
mediating social bonds, notwithstanding 'blood ties'. In short, there is strong compatibility between the perspectives on social bonding that emerge from a proper account of biological theory and those documented by ethnographers.

Holland's concluding chapter gives a summary of his fundamental position;

A crucial implication of this argument taken as a whole is that "the expression of the kinds of social behaviours treated by inclusive fitness theory does not require genetic relatedness." Sociobiology and evolutionary psychology's claims that biological science predicts that organisms "will" direct social behaviour towards relatives are thus both "theoretically" and "empirically" erroneous. Such claims and their supporting arguments also give a highly misleading and reductive account of basic biological theory. Properly interpreted, "cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible" regarding processes of social bonding in humans. Most of all, this requires a focus on the circumstances and processes which lead to social bonding.

The book notes that, as an outcome of the analysis, Schneider's sociocultural perspective on human kinship is vindicated;

Do the biological facts have some priority or are they but one of the conditions, like ecology, economy, demography, etc., to which kinship systems must adapt? Take note: if the latter is the case, then kinship must be as much rooted in these other conditions as in the biological facts.

The author supplies several examples of the insight that Schneider's broad approach can provide. The book closes with an example of a clash of cultural perspectives on kinship and family norms, and makes the suggestion that;

Constructing from narrow cultural particulars (Euro-American or otherwise) an essentialised model of 'human nature' does not constitute science; it is closer to cultural colonialism. In any analysis intended to shed light on proposed universals of the human condition, reflexivity is essential, and cultural and biological approaches both surely necessary.

Kinship theorist and member of the national academy of science, Robin Fox wrote of the work:

An excellent and constructive discussion of matters in kinship and its cultural and biological components, handsomely reconciling what have been held to be incompatible positions.Max Holland gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior. If he had been in the debate in the 1980s then a lot of subsequent confusion could have been avoided"

Irwin Bernstein, distinguished research professor in the university of Georgia's "Behavioral and Brain Sciences Program" made the following comment on Holland's book:

Max Holland has demonstrated extraordinarily thorough scholarship in his exhaustive review of the often contentious discussions of kinship. He has produced a balanced synthesis melding the two approaches exemplified in the biological and sociocultural behavioral positions. His work in reconciling opposing views clearly demonstrates the value of interdisciplinary approaches. This should be the definitive word on the subject.

Philip Kitcher, John Dewey Professor of Philosophy, and James R. Barker Professorship of Contemporary Civilization at Columbia University, past president of the American Philosophical Association and inaugural winner of the Prometheus Prize, stated of the book:

Max Holland has provided a wide-ranging and deeply-probing analysis of the influence of genetic relatedness and social context on human kinship. He argues that while genetic relatedness may play a role in the evolution of social behavior, it does not determine the forms of such behavior. His discussion is exemplary for its thoroughness, and should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology.

Kirk Endicott, professor emeritus of anthropology at the university of Dartmouth, wrote that Holland's book was:

A brilliant discussion of the relationship between kinship and social bonding as understood in evolutionary biology and in sociocultural anthropology. Among other contributions, it debunks the common misconception that biological evolution involves individual organisms actively pursuing the goal of increasing the numbers of their genes in successive generations, the measure of their so-called ‘individual inclusive fitness’. Holland demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs.

Janet Carsten, kinship theorist and professor of anthropology at the university of Edinburgh stated that:

This book is a scholarly attempt to get beyond the often sterile oppositions between evolutionary and culturalist approaches to kinship. In bringing together two sides of the debate, it constitutes a valuable contribution to kinship studies.

In a review for the journal Critique of Anthropology, Nicholas Malone concluded that:

Lucid and effective... Holland has produced a significant work of scholarship that will be of interest to a wide swath of the anthropological community.

Commenting on the book for the journal Social Analysis, Anni Kajanus found that:

Holland has done an excellent and thorough job in reviewing the disciplinary and interdisciplinary histories of approaches to kinship and social bonds in anthropology, biology, and psychology. Most importantly, he clarifies the different levels of analysis when looking at human behavior in real time and in the evolutionary time frame. This makes the book essential reading for anyone who acknowledges that human relatedness and social bonds are shaped by the
evolved dispositions of our species, their development through the life-course of an individual, and our specific cultural-historical environments... Holland’s book goes a long way toward clarifying and therefore advancing these theoretical debates

An in-depth review of the book by primatologist Augusto Vitale, in the journal "Folia Primatologica", found that:

Stuart Semple, evolutionary anthropologist, reviewing the book in the journal "Acta Ethologica" stated that:

As someone who teaches behavioural ecology to biologists, and primate biology to social and biological anthropologists, I will be strongly recommending this book to all of my advanced undergraduates, masters and PhD students, as well as to my colleagues. Not only does it help to resolve debates that have run for many years, but it is also an outstanding example of what can be achieved by immersing oneself in literature from different fields, while retaining an intellectual openness and exercising incisive analysis. Many of us talk enthusiastically about inter- and multi-disciplinarity, but often this is not much more than lip service. This book is a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries. There should be more texts like this.

In addition to praise for the book's significance, the Folia Primatologica review noted that the book is at times too dense and requires close reading;
The argument here and there becomes too detailed and tortuous, but it is absolutely captivating... [Colleagues] who are less used to extremely detailed theoretical reasoning, will find it difficult at the beginning...



</doc>
<doc id="50161687" url="https://en.wikipedia.org/wiki?curid=50161687" title="Functional Ensemble of Temperament">
Functional Ensemble of Temperament

Functional Ensemble of Temperament (FET) is a neurochemical model suggesting specific functional roles of main neurotransmitter systems in the regulation of behaviour.

We use medications to adjust a release of brain neurotransmitters in cases of depression, anxiety disorder, schizophrenia and other mental disorders because an imbalance within neurotransmitter systems can emerge as consistent characteristics in behaviour compromising people’s lives. All people have a weaker form of such imbalance in at least one of such neurotransmitter systems that make each of us distinct from one another. The impact of this weak imbalance in neurochemistry can be seen in the consistent features of behaviour in healthy people (temperament). In this sense temperament (as neuro-chemically-based individual differences) and mental illness represents varying degrees along the same continuum of neurotransmitter imbalance in neurophysiological systems of behavioural regulation 
In fact, multiple temperament traits (such as Impulsivity, sensation seeking, neuroticism, endurance, plasticity, sociability or extraversion) have been linked to brain neurotransmitters and hormone systems.

By the end of the 20th century, it became clear that the human brain operates with more than a dozen neurotransmitters and a large number of neuropeptides and hormones. The relationships between these different chemical systems are complex as some of them suppress and some of them induce each other’s release during neuronal exchanges. This complexity of relationships devalues the old approach of assigning “inhibitory vs. excitatory” roles to neurotransmitters: the same neurotransmitters can be either inhibitory or excitatory depending on what system they interact with. It became clear that an impressive diversity of neurotransmitters and their receptors is necessary to meet a wide range of behavioural situations, but the links between temperament traits and specific neurotransmitters are still a matter of research. Several attempts were made to assign specific (single) neurotransmitters to specific (single) traits. For example, dopamine was proposed to be a neurotransmitter of the trait of Extraversion, noradrenaline was linked to anxiety, and serotonin was thought to be a neurotransmitter of an inhibition system. These assignments of neurotransmitter functions appeared to be an oversimplification when confronted by the evidence of much more diverse functionality. 
Research groups led by Petra Netter in Germany, Lars Farde in Karolinska Institute in Sweden and Trevor Robbins in Cambridge, UK had most extensive studies of the links between temperament/personality traits or dynamical properties of behavior and groups of neurotransmitters 

The architecture of the Functional Ensemble of Temperament (FET) was developed by Trofimova as the Compact version of the Structure of Temperament Questionnaire (STQ-77) in 1997-2007. This model inherits the activity-specific approach to the structure of temperament proposed by Rusalov in mid-1980s. According to this approach, the traits of temperament (and behavioural regulation) related to motor-physical, social-verbal and mental aspects of activities are based on different neurophysiological systems and should be assessed separately (so you can see a separation of traits into 3 rows related to these 3 types of activities). The final STQ-77/FET model considers 12 systems (and temperament traits): 9 systems (and traits) regulating the formal functional aspects of behaviour (energetic, dynamic and orientational, each assessed in three domains (intellectual, physical and social-verbal) together with 3 systems related to emotionality (Neuroticism, Impulsivity and a disposition of Satisfaction (formerly called Self-Confidence) (see Figure).

The differences between Trofimova’s and Rusalov’s models of temperament (and the structures of their versions of the STQ) are: 

In 2007-2013 this STQ-77 model of temperament was reviewed and compared to the main findings in neurophysiology, neurochemistry, clinical psychology and kinesiology resulting in the neurochemical FET model offered by Irina Trofimova, McMaster University. Trevor Robbins, Cambridge University who collaborated with Trofimova on this project in 2014-2016 suggested a revision of the part of the FET neurochemical hypothesis related to the trait of Intellectual Endurance (sustained attention). This neurochemical component of the FET hypothesis was upgraded in 2015 by underlying a key role of acetylcholine and noradrenalin in sustained attention.
In February 2018, by the suggestion of Dr Marina Kolbeneva (Institute of Psychology, Russian Academy of Sciences) the scale Self-Confidence was renamed as the scale of dispositional Satisfaction.

The final STQ-77/FET model considers 12 systems (and temperament traits): nine systems (and traits) regulating the formal functional aspects of behaviour (energetic, dynamic and orientational) each assessed in three domains (intellectual, physical and social-verbal) together with three systems related to emotionality (Neuroticism, Impulsivity and Satisfaction (Self-Confidence)) (see Figure). The FET hypothesis suggests that the nine non-emotionality traits are regulated by the monoamines (MA) (noradrenalin, dopamine, serotonin), acetylcholine and neuropeptide systems, whereas the three emotionality-related traits emerge as a dysregulation of opioid receptors systems that have direct control over MA systems. Importantly, the FET model suggests that there is no one-to-one correspondence between the neurotransmitter systems underlying temperament traits (or mental disorders) but instead specific ensemble relationships between these systems emerge as temperament traits.
The FET hypothesis is based only on the strongest consensus points in the research studying the role of neurotransmitter in behavioural regulation and the components of temperament; it doesn’t list more controversial links between these multiple systems.



</doc>
<doc id="7707370" url="https://en.wikipedia.org/wiki?curid=7707370" title="Action assembly theory">
Action assembly theory

Action assembly theory (AAT) is a communication theory that emphasizes psychological and social influences on human action. The goal is to examine and describe the links between the cognition and behavior – how an individual's thoughts get transformed into action. It was developed by John Greene.

Action assembly theory describes the production of behavior in two essential processes: 

The idea of procedural record is at the center of action assembly. These records contain information about action, outcomes, and situations. These records are locked in the individual’s memory where it remembers past behaviors for the future.

A procedural record is at the center of action assembly. It is a personal nugget of truth about past behavior stockpiled for future use, part of an individual's memory system in which information about how to execute various behaviors is stored. Procedural records contain information about action, outcomes, and situations; for example, traveling at excessive speed (action) in a zone that specifies low speed limit (situation)can result in the issuance of a ticket (outcome).

Procedural records have different levels of strength. Some are mere scratches that barely leave a trace in the mind, while others are well-worn into long-term memory.

A central aspect of the action assembly theory is specifying the processes that link procedural records to behavioral representations. The activation process is the process used to select particular procedural records. For example, if a parent disciplined a child for stealing, all procedural records relevant to this goal and situation would be activated. In turn, if a common disciplinary tactic was to take away toys and play items, a procedural record of that would be activated quickly.

It is also important to consider the process of assembly, which organizes records into a behavioral representation. For example, assembly is considered a top-down process that begins with general strategy and goes to more specific ideas about communicating the specific message.

Action assembly theory has been useful for topics such as speech onset latency and hesitations during speaking. These concepts are assumed to be indicators of cognitive processing. Another use is the study of planning – individuals who plan more effectively are more fluent than those do not, because planning reduces the cognitive load at the time of message production.

When an interaction situation has multiple goals, the theory finds increased demands on an individual's information processing capacity. Assembly of goals may be difficult because a specific goals may be incompatible with behaviors associated with the other goals. In turn, multiple goal messages involve more speech hesitations and latencies. The utilization of Action Assembly theory can provide a clear opportunity to plan or assemble goals more careful to mitigate the effect.

When an interaction situation has multiple goals, the theory finds increased demands on an individual’s information processing capacity. Assembly of goals may be difficult because a specific goal may be incompatible with behaviors associated with each other goals. In turn, multiple goal messages involve more speech hesitation. The utilization of AAT can provide a clear opportunity to plan or assemble goals more careful to ease the effect.



</doc>
<doc id="13413800" url="https://en.wikipedia.org/wiki?curid=13413800" title="Autoplastic adaptation">
Autoplastic adaptation

Autoplastic adaptation (from the Greek word auto) is a form of adaptation where the subject attempts to change itself when faced with a difficult situation.

The concept of autoplastic adaptation was developed by Sigmund Freud, Sándor Ferenczi, and Franz Alexander. They proposed that when an individual was presented with a stressful situation, he could react in one of two ways: 

'Hysterical individuals appear to be turned inward. Their symptoms, instead of presenting actions directed outward (alloplastic activities), are mere internal innervations (autoplastic activities)'.

Freud, with 'his single-minded Lamarckianism', speculated that behind 'Lamarck's idea of "need"' was the 'power of unconscious ideas over one's own body, of which we see remnants in hysteria, in short, "the omnipotence of thought"'.

As a result, among his immediate followers, 'Insight into this regressive nature of the phenomenon of conversion may be taken as a starting-point for speculation about the archaic origin of the capacity for autoplastic conversion...according to which evolution took place through the autoplastic adaptation of the body to the demands of the environment'.

'Cross-cultural helpers have debated what has been called the autoplastic/alloplastic dilemma: how much should clients be encouraged to adapt to a given situation and how much...to change? Most Western helping modalities have a strong autoplastic bias; clients are encouraged to abandon traditional beliefs...to fit into a dominant society's mainstream'.

The analytic relationship is sometimes seen in similar terms: 'the two practitioners in treatment are engaged in an unending struggle between changing the other and effecting internal change..."autoplastic" and "alloplastic"'.



</doc>
<doc id="13413780" url="https://en.wikipedia.org/wiki?curid=13413780" title="Alloplastic adaptation">
Alloplastic adaptation

Alloplastic adaptation (from the Greek word "allos") is a form of adaptation where the subject attempts to change the environment when faced with a difficult situation. Criminality, mental illness, and activism can all be classified as categories of alloplastic adaptation.

The concept of alloplastic adaptation was developed by Sigmund Freud, Sándor Ferenczi, and Franz Alexander. They proposed that when an individual was presented with a stressful situation, he could react in one of two ways: 

'These terms are possibly due to Ferenczi, who used them in a paper on "The Phenomenon of Hysterical Materialization" (1919,24). But he there appears to attribute them to Freud' (who may have used them previously in private correspondence or conversation). Ferenczi linked 'the purely "autoplastic" tricks of the hysteric...[to] the bodily performances of "artists" and actors'.

Freud's only public use of the terms was in his paper "The Loss of Reality in Neurosis and Psychosis" (1924), where he points out that 'expedient, normal behaviour leads to work being carried out on the external world; it does not stop, as in psychosis, at effecting internal changes. It is no longer "autoplastic" but "alloplastic" '.

A few years later, in his paper on "The Neurotic Character" (1930), Alexander described 'a type of neurosis in which...the patient's entire life consists of actions not adapted to reality but rather aimed at relieving unconscious tensions'. Alexander considered that 'neurotic characters of this type are more easily accessible to psychoanalysis than patients with symptom neuroses...[due] to the fact that in the latter the patient has regressed from alloplasticity to autoplasticity; after successful analysis he must pluck up courage to take action in real life'. 

Otto Fenichel however took issue with Alexander on this point, maintaining that 'The pseudo-alloplastic attitude of the neurotic character cannot be changed into a healthy alloplastic one except by first being transformed, for a time, into a neurotic autoplastic attitude, which can then be treated like an ordinary symptom neurosis'.

Alloplasticity has also been used to describe humanity's cultural "evolution". Man's 'evolution by culture...is through alloplastic experiment with objects outside his own body...Unlike autoplastic experiments, alloplastic ones are both replicable and reversible'. 

In particular, 'advanced technological societies...are generally characterized by "alloplastic" relations with the environment, involving the manipulation of the environment itself'.



</doc>
<doc id="13552978" url="https://en.wikipedia.org/wiki?curid=13552978" title="Behavioral confirmation">
Behavioral confirmation

Behavioral confirmation is a type of self-fulfilling prophecy whereby people's social expectations lead them to behave in ways that cause others to confirm their expectations. The phenomenon of belief creating reality is known by several names in literature: self-fulfilling prophecy, expectancy confirmation, and behavioral confirmation, which was first coined by social psychologist Mark Snyder in 1984. Snyder preferred this term because it emphasizes that it is the target's actual behavior that confirms the perceiver's beliefs.

Preconceived beliefs and expectations are used by human beings when they interact with others, as guides to action. Their actions may then guide the interacting partner to behave in a way that confirms the individual's initial beliefs. The self-fulfilling prophecy is essentially the idea that beliefs and expectations can and do create their own reality. Sociologist Robert K. Merton defined a self-fulfilling prophecy as, in the beginning, a false definition of the situation evoking a new behavior which makes the originally false conception come true.

Self-fulfilling prophecy focuses on the behavior of the perceiver in electing expected behavior from the target, whereas behavioral confirmation focuses on the role of the target's behavior in confirming the perceiver's beliefs.

Research has shown that a person (referred to as a perceiver) who possesses beliefs about another person (referred to as a target) will often act on these beliefs in ways that lead the target to actually behave in ways that confirm the perceiver's original beliefs.

In one demonstration of behavioral confirmation in social interaction, Snyder and colleagues had previously unacquainted male and female partners get acquainted through a telephone-like intercom system. The male participants were referred to as the perceivers, and the female participants were referred to as the targets. Prior to their conversations, the experimenter gave the male participants a Polaroid picture and led them to believe that it depicted their female partners. The male participants were unaware that, in fact, the pictures were not of their partners. The experimenter gave the perceivers pictures which portrayed either physically attractive or physically unattractive women in order to activate the perceiver's stereotypes that they may possess concerning attractive and unattractive people. The perceiver-target dyads engaged in a 10-minute, unstructured conversation, which was initiated by the perceivers. Individuals, identified as the raters, listened in on only the targets' contributions to the conversations and rated their impressions of the targets. Results showed that targets whose partners believed them to be physically attractive came to behave in a more sociable, warm, and outgoing manner than targets whose partners believed them to be physically unattractive. Consequently, targets behaviorally confirmed the perceivers' beliefs, thus turning the perceivers' beliefs into self-fulfilling prophecies. The study also supported and displayed the physical attractiveness stereotype.

These findings suggest that human beings, who are the targets of many perceivers in everyday life, may routinely act in ways which are consistent not with their own attitudes, beliefs, or feelings; but rather with the perceptions and stereotypes which others hold of them and their attributes. This seems to suggest that the power of others' beliefs over one's behaviour is extremely strong.

Snyder proposed a four-step sequence in which behavioral confirmation occurs:

The perceiver and the target have a common goal of getting acquainted with one other, and they do so in different functions. Behavioral confirmation occurs from the combination of a perceiver who is acting in the service of the knowledge function and a target whose behaviors serve an adjustive function.

The perceiver uses knowledge motivations in order to get a stable and predictable view of those whom one interacts, eliciting behavioral confirmation. Perceivers use knowledge-oriented strategies, which occur when perceivers view their interactions with targets as opportunities to find out about their targets' personality and to check their impressions of targets, leading perceivers to ask belief-confirming questions. The perceiver asks the target questions in order to form stable and predictable impressions of their partner, and perceivers tend to confidently assume that possession of even the limited information gathered about the other person gives them the ability to predict that that person's future will be consistent with the impressions gathered.

When the target is motivated by adjustive functions, they are motivated to try to get along with their partners and to have a smooth and pleasant conversation with the perceiver. The adjustive function motivates the targets to reciprocate perceivers' overtures and thereby to behaviorally confirm perceivers' erroneous beliefs. Without the adjustive function, this may lead to behavioral disconfirmation.


The principle objection to the idea of behavioral confirmation is that the laboratory situations that are used in the research often do not map onto real-world social interaction easily. In addition, it is argued that behavioral disconfirmation is just as likely to develop out of expectancies as are self-fulfilling expectations. A strong criticism by Lee Jussim is the allegation that, in all previous behavioral confirmation studies, the participants have been falsely misled about the targets' characteristics; however, in real life, people's expectations are generally correct. To combat such critique, behavioral confirmation has adapted to introduce a non-conscious element. Even though there are clearly pitfalls to the phenomenon, it has continuously been studied over the past few decades, highlighting its importance in psychology.


</doc>
<doc id="398255" url="https://en.wikipedia.org/wiki?curid=398255" title="Sportsmanship">
Sportsmanship

Sportsmanship is an aspiration or ethos that a sport or activity will be enjoyed for its own sake, with proper consideration for fairness, ethics, respect, and a sense of fellowship with one's competitors. A "sore loser" refers to one who does not take defeat well, whereas a "good sport" means being a "good winner" as well as being a "good loser" (someone who shows courtesy towards another in a sports game).

Sportsmanship can be conceptualized as an enduring and relatively stable characteristic or disposition such that individuals differ in the way they are generally expected to behave in sports situations. Sportsmanship mainly refers to virtues such as fairness, self-control, courage, and persistence, and has been associated with interpersonal concepts of treating others and being treated fairly, maintaining self-control if dealing with others, and respect for both authority and opponents. Sportsmanship is also looked at as being the way one reacts to a sport/game/player.

The four elements of sportsmanship are often shown being good form, the will to win, equity and fairness. All four elements are critical and a balance must be found among all four for true sportsmanship to be illustrated. These elements may also cause conflict, as a person may desire to win more than play in equity and fairness and thus resulting in a clash within the aspects of sportsmanship. This will cause problems as the person believes they are being a good sportsman, but they are defeating the purpose of this idea as they are ignoring two key components of being sportsman like. When athletes become too self-centred, the idea of sportsmanship is dismissed.

Today's sporting culture, in particular the base of elite sport, places great importance on the idea of competition and winning and thus sportsmanship takes a back seat as a result. In most, if not all sports, sportsmen at the elite level make the standards on sportsmanship and no matter whether they like it or not, they are seen as leaders and role models in society.

Since every sport is rule driven, the most common offence of bad sportsmanship is the act of cheating or breaking the rules to gain an unfair advantage this is called unsportsmanlike conduct. A competitor who exhibits poor sportsmanship after losing a game or contest is often called a "sore loser", while a competitor who exhibits poor sportsmanship after winning is typically called a "bad winner". Sore loser behavior includes blaming others for the loss, not accepting responsibility for personal actions that contributed to the defeat, reacting to the loss in an immature or improper fashion, making excuses for the defeat, and citing unfavorable conditions or other petty issues as reasons for the defeat. A bad winner acts in a shallow fashion after his or her victory, such as by gloating about his or her win, rubbing the win in the face(s) of the opponent(s), and lowering the opponent(s)'s self-esteem by constantly reminding the opponent(s) of "poor" performance in comparison (even if the opponent(s) competed well). Not showing respect to the other team is considered to being a bad sportsman and could lead to demoralising effects; as Leslie Howe describes: "If a pitcher in baseball decides to pitch not to his maximum ability suggest that the batter is not at an adequate level, [it] could lead to the batter to have low self-confidence or worth."

There are six different categories relating to sportsmanship: the elements of sports, the elements of sportsmanship, clarifications, conflicts, balance and irreducibility. All six of these characterize a person with good sportsmanship. Even though there is some affinity between some of the categories, they are distinct elements. "In essence, play has for its directed and immediate end joy, pleasure, and delights and which is dominated by a spirit of moderation and generosity. Athletics, on the other hand, is essentially a competitive activity, which has for its end victory in the contest and which is characterized of dedication, sacrifice and intensity." (Feelezz, 1896, pp. 3) Hence, the virtues of a player are radically different from the virtues of an athlete. (Feelezz, 1896, pp. 3). When talking about misunderstanding sportsmanship, Rudd and Stoll (2013) provide an example from 1995, a U.S. high school athletic league banned the post-game handshake that was a part of sports such as football and basketball. The handshaking was banned because of fights that were ensuing after the handshake.(pp. 41) Most players are influenced by the leaders around them such as coaches and older players, "if there are coaches and administrators who don't understand sportsmanship, then what about the players?"

There are various ways that sportsmanship is practiced in different sports. Being a good sport often includes treating others as you would also like to be treated, cheer for good plays (even if it is made by the opposition), accept responsibility for your mistakes, and keep your perspective. An example of treating others how you would like to be treated would include being respectful and polite to other team members and the opposition because in return you would also like to be treated the same way. Cheer for good plays could include if in netball a player of the opposition made a good lead for the ball, which then resulted in a goal, everyone would either clap or make a supportive comment to acknowledge that what the player did was very well done. To accept responsibility for your mistakes will entail not placing the blame on other people.

Some popular examples of good sportsmanship include shaking hands, help an opponent who may have fallen over, encourage everyone, cheer, clap or hi-five, and be respectful to everyone including teammates, the opposition, parents and officials. Most importantly it is often encouraged and said regarding sportsmanship that "It's not whether you win or lose, it's how you play the game."

Sportsmanship can be manifested in different ways depending on the game itself or the culture of the group.

Sportsmanship can be affected by a few contributing factors such as the players' values and attitudes towards the sport and also the professional role models that are shown to the public. Role models in sport are expected to act in a moral and respectful way. When elite sporting role models do not encourage sportsmanship this can also encourage people in society to act in similar ways to the athletes that they look up to and idolize. For example, if an individual looked up to an athlete who was drinking excessively, they may see this as acceptable behavior. The direct correlation between sportsmanship and leadership is also considered to be another contributing factor. Having a positive environment in your sporting team will therefore create good sportsmanship from the individuals. Having a positive leadership by the captains, coaches and supporters would then encourage a positive sporting environment.




</doc>
<doc id="51289352" url="https://en.wikipedia.org/wiki?curid=51289352" title="Widowhood effect">
Widowhood effect

The widowhood effect is the increase in the probability of a person dying a relatively short time after their long-time spouse has died. The pattern indicates a sharp increase in risk of death for the widower, particularly but not exclusively, in the three months closest thereafter the death of the spouse. This process of losing a spouse and dying shortly after has also been called "dying of a broken heart". Becoming a widow is often a very detrimental and life changing time in a spouse's life, that forces them to go through changes that they may not have anticipated to make for a significant amount of time. Responses of grief and bereavement due to the loss of a spouse increases vulnerability to psychological and physical illnesses.

Psychologically, losing a long-term spouse can cause symptoms such as depression, anxiety, and feelings of guilt. Physical illness may also occur as the body becomes more vulnerable to emotional and environmental stressors. There are many factors that may be affected when one becomes a widow. A widow (or widower if referring to a male who lost a spouse) tends to have a decline in health regulation. Higher prevalence in mortality rates are noted among bereaved spouses during the first six months of bereavement compared to the last six months of bereavement. The most crucial are said to be the first three months during grief processing. Grieving spouses are more vulnerable during these few months not only health wise but socially and physically. During this early period of bereavement spouses tend to have less interest in their health as well as physical appearance caring less about continuing with medications or adapting healthy behaviors such as eating healthy or exercising. Also, they are likelier to practice risky behaviors and commit suicide. Women on the other hand, are more likely to look for social support such as friends, family, or support groups regarding the matter.

This effect appears to be far more prevalent in older married couples than in younger married couples. As a result, studies that have been conducted in regards to this phenomenon since the early 2000s have revolved primarily around observations of older widows. Through the many studies that have been conducted over the years, it has been found that the widowhood effect affects the mortality rates of people with varying levels of severity depending on their genders and religions. It is far more frequent in more seasoned, long-term, elder couples than in recently married couples. Since the topic has only been recently studied within this last decade, and due to the prevalence of older couples being affected, most widows in similar studies are typically over the age of 50

Although there have not been many studies, the phenomenon is one of the best documented examples of the effect of social relations on health. Through the numerous studies that have been directed throughout the years, it has been found that the widowhood impact influences the death rates of individuals with changing levels of stringencies relying upon their sexual orientations and religions. There are many theories as to what causes this to occur. There are many factors and theories about the widowhood effect, but in general, a study on a large population sample has suggested rates of death nearly double during the first three months after loss of a spouse, and quickly taper thereafter.

Both men and women respond differently to the death of their spouse. In general, men tend to be more vulnerable to the widowhood effect. Men are affected more socially than women. Women tend to maintain social relationships and friendships outside of marriage, so when the wife dies first, men tend to lose out on these social relationships and support groups and they tend to isolate themselves. Women maintain their friendships and relationships and lean on them for support after their spouse dies.

The majority of peer-reviewed articles suggest that it is indeed men that are more frequently at risk of succumbing to the widowhood effect. A collaborative study by Gary R. Lee et al. (2001) surveyed 1686 married and widowed people age 65 or older using data from 1988 National Survey of Families and Households. The study took into account the psychological well-being of each person on a Center for Epidemiologic Studies— Depression scale (CES-D), and found that men were further depressed by the loss of their spouse than women were. Men were also more likely to die soon after the death of their spouse compared to their female counterparts. The researchers hypothesized that this was because older married men had a higher baseline happiness than their spouses, so they had more happiness to lose. Men in this study were also less likely to be avid church goers, despised chores, and were not as adept at helping their adult children. The lack of social behavior and general activity may contribute to the widowhood effect's influence on male mortality.

Experimenters involved in the study regarding: The effects of widowhood on physical and mental health, health behaviors, and health outcomes, took a deeper look into the overall health differences between recent widowers and those who are currently involved in a marriage. Mental health alterations include those of depression and socially extracting yourself were the most common for women who have become widowed in the past year or so. Although some of these women were shown to extract them selves from social settings, the study also supported an alternate hypothesis that women experiencing the effects of widowhood are more likely to engage coping mechanisms such as support groups, fellow widowers, and overall strong support systems such as close friends and family.

In a study done by Stahl and Shultz, they found that the death of a man’s spouse affected his physical activity. Men’s physical activity increases immediately following the death of a spouse, and the authors note that this may perhaps be a coping mechanism to alleviate depressive symptoms. In the year following the death of a spouse, men are more likely to experience a fluctuation in physical activity during the transition into widowhood, if they did not die. Men in the youngest age group of this study (55– 65 years) were at the highest risk of mortality after the death of their spouse. Elwert and Christakis found that within the first month of bereavement after widowhood morality is largest and doesn’t decline sharply until the sixth month of widowhood for white males, compared to only the third month for white females.

Nicholas A. Christakis of Harvard and Felix Elwert of the University of Wisconsin analyzed nine years’ worth of data gathered from almost 373,189 elderly married couples in the U.S. Their findings showed an 18% increase in “all-cause mortality” for men whose wives died first; for women, the risk is 16%. Although the percentage of males was higher than females, it is not a significant difference to state that men are more affected than women. It is hard to say whether males or females suffer from the widowhood effect more, however it is clear that it is higher in couples that are older.

A 2009 study by Ernest L. Abel and Michael L. Kruger compared the likelihood of death for Catholic as compared to Jewish widows, based on the graves of Jewish and Catholic couples in the Midwest. The data suggested that the widowhood effect was stronger in Jewish than in Catholic couples. Catholic women lived 11 years after the death of their spouse while Jewish women lived 9.5 years after the deaths of their husbands. Similarly, the Jewish men lived 5 years after the death of the wives while the Catholic men lived about 8 years after the death of their wives.

Experiencing the loss of a spouse often changes many components of the widows’ life. The recently widowed individual is forced to change their everyday routine, which often puts immense stress on the recent widower/widow. Research has found that surviving spouses tend to experience significant weight loss after the deaths of their mates. It has been theorized that these changes in weight are the result of differences in dietary intake before and after the death of a spouse. Danit R. Shahar et al. (2001) surveyed 116 older individuals in order to track their weight and eating habits over the course of their longitudinal study. Half of the participants were widowed and the other half were nonwidowed. The study found that the widowed subjects were more likely to eat meals alone than the married individuals. The diets of the widowed subjects consisted of more commercial foods than their counterparts, but they also lost a significant amount of weight compared to the married group. Danit R. Shahar et al. (2001) hypothesized that this weight loss was the result of the widowed participants not finding as much enjoyment in eating as the once did. This lack of fulfillment during meals was correlated to a lack of companionship while eating. Widowed subjects had less of an appetite and as a result lost weight over the course of the study.

The death of a spouse can have a major impact on a person’s mental health. Each individual may respond to their spouse’s death differently. After the death of a spouse many widows begin to take more prescription medications for mental health issues. The mental health effects of men and women also differ. Men may become more depressed in widowhood compared to women because men may not have as strong of a support group. Married men also report a higher rate of happiness in their marriage and the death of their spouse could drastically alter this happiness. Women may have an easier time adapting to widowhood and be more willing to seek mental help while men tend to be less social and did not like to do chores, go to church, or help their children. Men and women both show greater rates of depression after the death of a spouse but the rates of depression in men tends to be higher than in women.

Recently, there has been a phenomenon discussed and researched dubbed the takotsubo condition, also referred to as the broken heart syndrome. Takotsubo has been discussed in contexts surrounding great physical and emotional stress, such as circumstance in which someone has been widowed, and emotional stress has long been associated with myocardial infarction. In their research, Brenn and Ytterstad saw an increase in death of women 55–64 years old due to heart disease in the first week of widowhood than married women 55–64 years old (2016). Although takotsubo is not considered to be the direct cause of death at this time, it is an observed phenomenon.

Elderly widows experience changes in their social lives prior to and following the deaths of their spouses. A study conducted by Rebecca L. Utz et al. (2002) revealed that elderly persons experiencing widowhood spent more time with family and friends than nonwidowed counterparts, based on the lifestyle changes that occur in elderly couples. Although widowed subjects were more likely to socialize with family and friends, they were no more likely to visit church or volunteer than the intact couples. This study also found that healthy spouses were reclusive while their significant other was on their deathbed, but due to a network of family and friends; the surviving spouse entered society being more social than had been prior to the death of their husband or wife. Elderly widows were more or less involved socially depending on the amount of support they had from family and friends. It has been noted that widows who have a close and supportive social network can counteract the effects of widowhood by remaining active in their social group. The loss of a spouse affects almost every domain of life, and as a consequence has a significant impact on wellbeing: psychological, social, physical, practical, and economic. With all of these aspects of a widowed individuals being affected maintaining a sense of normality is important to help avoid depression like symptoms. Social support, as well as creating new lasting relationships through social interaction can help the process of bereavement go smoother for widow effected individuals.

A 2015 study conducted by Rosato, O’Reilly and Wright revealed that there is a significant difference in urban-rural variation in the social environment as well as in health outcomes. There is evidence that social support from family and friends have better health outcomes on mortality rates. Investigations showed that the race of the partner influences widowhood effect; whites in endogamous marriages had greater mortality risks that were not obvious among blacks, which the authors concluded was due to a high level of family support for elderly among black families versuswhite families. Moreover, the study also found differences in urban and rural areas around the world. They found that elderly married couples in the US suffered significant mortality risks compared to those in Ireland where older people living in more rural areas receive more social support from their families, and they live with their children, while in the US elderly people live in care homes. As a result, mortality rates are greater in urban areas and less in rural areas.

In a study done by Elwert and Christakis, they found that there was no widowhood effect found in endogamously married black men or women (2016). Deducing this finding, they proposed that this might be because blacks are able to extend their marital survival advantage into widowhood. This is likely because blacks are prone to have kin nearer to help take care of them, they may be more self-sufficient than their white counterparts, and there is greater religious participation in blacks that may help them with spiritual comfort. Whites were found to have “a large and enduring widowhood effect” because there is no reparation to make up for the survival advantages that marriage gave them, even if they have been widowed for years.

The widowhood effect appears to have a higher impact in rural and intermediate areas compared to urban areas. One factor may be that there are greater distances to primary care services in rural areas, and this increases mortality due to discouraged health checkups (Wright, 2015). It is known that the size of family and social network coincides with physical functioning; the bigger the social group one belongs to the better they can physically function. Residential areas near green areas are associated with an increase in physical activity and lowered mortality (Wright, 2015). Researchers measured peak flow to show the increases or decreases in physical functioning, and the results suggest that married subjects have a higher peak flow compared to those divorced or widowed (Clouston, 2014).

It was suggested that the widowhood effect was a mere coincidence resulting from the selection of mates with similar health risk. In a recent study by Paul J. Boyle, Feng, Z., & Raab, G. M. (2011), it was concluded that the increased mortality rate of widows is caused by the death of their spouse. Researchers in the study used data from the Scottish Longitudinal Study to compare the ratios of death in widowed males and females. The male and female subjects were categorized into different groups dependent on the manner in which their spouse died. The results provided evidence that suggest a causal relationship between mortality rate and widowhood.

In April 2016, the American Heart Association published an article regarding phenomenon referred to as "broken heart syndrome". This particular syndrome seems to occur when a person experiences an overwhelming amount of stress in their life in a short period of time. The cases mentioned involved both positive events like winning the lottery as well as negative events like experiencing the death of a spouse. Though broken heart syndrome has been misdiagnosed as a heart attack, the differences between the two phenomena are clear. Heart attacks are the result of a blockage of arteries, but broken heart syndrome is the result of a hormone induced enlargement of a portion of the heart. The enlarged region of the heart is less effective in regards to pumping blood, and the normal sized regions of the heart are forced to work harder as a result.

 Widowhood effect is important to be aware of because of the affects that it can have on the body; mentally, physically, emotionally. Although there is no correct coping mechanism when losing a partner, it is important to find preventative ways to fight this from occurring because of the depressive symptoms and state of mental health that are a result from a partner passing away. Being aware of widowhood effect and acknowledging the feelings, such as grieving over that individual can be a way of coping with their current situation. Individuals that result to safe methods with grieving such as counseling or finding different hobbies can decrease their chances of experiencing widowhood effect.

There is no universally accepted “proper” method or way to grieve or adjust to life after loss; it varies among individuals, influenced by their cultural and social practices, personality, and the circumstances surrounding the death. Even though grief processing varies, there are ways to reduce the effects of widowhood. Since a spouse is often one’s primary source of social interactions, maintaining and establishing social bonds is a crucial aspect in determining the outcome of a widowed individual’s bereavement (DeSpelder & Strickland, 2015). Social participation may be utilized “as an active coping strategy” as discovered in a study by Rebecca Utz and colleagues (DeSpelder & Strickland, 2015). Additionally, the loss of a spouse means the loss of a partner; the loss of a supportive presence with whom responsibility was shared amongst one another. Studies have shown that for couples who followed traditional gender roles, adjustment to life after their loss was often more difficult than for couples who did not (DeSpelder & Strickland, 2015). Undertaking the responsibilities previously considered the ‘job’ of one’s deceased partner is often hard to handle on top of processing the loss; therefore, the utilization of organizations built to support and help widowed individuals may also prove to be helpful in reducing grief. The Widowed Persons Service (WPS) and their parent organization, the American Association of Retired Persons (AARP), are two organizations which provide aid specifically to widowed individuals (DeSpelder & Strickland, 2015). Effective and safe methods of grief processing are important for all individuals dealing with loss; however, the utilization of organizations may prove to be beneficial for those who need a little more help.


</doc>
<doc id="33912" url="https://en.wikipedia.org/wiki?curid=33912" title="Working memory">
Working memory

Working memory is a cognitive system with a limited capacity that is responsible for temporarily holding information available for processing. Working memory is important for reasoning and the guidance of decision-making and behavior. Working memory is often used synonymously with short-term memory, but some theorists consider the two forms of memory distinct, assuming that working memory allows for the manipulation of stored information, whereas short-term memory only refers to the short-term storage of information. Working memory is a theoretical concept central to cognitive psychology, neuropsychology, and neuroscience.

The term "working memory" was coined by Miller, Galanter, and Pribram, and was used in the 1960s in the context of theories that likened the mind to a computer. In 1968, Atkinson and Shiffrin used the term to describe their "short-term store". What we now call working memory was formerly referred to variously as a "short-term store" or short-term memory, primary memory, immediate memory, operant memory, and provisional memory. Short-term memory is the ability to remember information over a brief period (in the order of seconds). Most theorists today use the concept of working memory to replace or include the older concept of short-term memory, marking a stronger emphasis on the notion of manipulating information rather than mere maintenance.

The earliest mention of experiments on the neural basis of working memory can be traced back to more than 100 years ago, when Hitzig and Ferrier described ablation experiments of the prefrontal cortex (PFC); they concluded that the frontal cortex was important for cognitive rather than sensory processes. In 1935 and 1936, Carlyle Jacobsen and colleagues were the first to show the deleterious effect of prefrontal ablation on delayed response.

Numerous models have been proposed for how working memory functions, both anatomically and cognitively. Of those, the two that have been most influential are summarized below.

In 1974, Baddeley and Hitch introduced the multicomponent model of working memory. The theory proposed a model containing three components: the central executive, the phonological loop, and the visuospatial sketchpad with the central executive functioning as a control center of sorts, directing info between the phonological and visuospatial components. The central executive is responsible "inter alia" for directing attention to relevant information, suppressing irrelevant information and inappropriate actions, and coordinating cognitive processes when more than one task is simultaneously performed. A "central executive" is responsible for supervising the integration of information and for coordinating "slave systems" that are responsible for the short-term maintenance of information. One slave system, the phonological loop (PL), stores phonological information (that is, the sound of language) and prevents its decay by continuously refreshing it in a rehearsal loop. It can, for example, maintain a seven-digit telephone number for as long as one repeats the number to oneself again and again. The other slave system, the visuospatial sketchpad, stores visual and spatial information. It can be used, for example, for constructing and manipulating visual images and for representing mental maps. The sketchpad can be further broken down into a visual subsystem (dealing with such phenomena as shape, colour, and texture), and a spatial subsystem (dealing with location).

In 2000, Baddeley extended the model by adding a fourth component, the episodic buffer, which holds representations that integrate phonological, visual, and spatial information, and possibly information not covered by the slave systems (e.g., semantic information, musical information). The episodic buffer is also the link between working memory and long-term memory. The component is episodic because it is assumed to bind information into a unitary episodic representation. The episodic buffer resembles Tulving's concept of episodic memory, but it differs in that the episodic buffer is a temporary store.

Anders Ericsson and Walter Kintsch have introduced the notion of "long-term working memory", which they define as a set of "retrieval structures" in long-term memory that enable seamless access to the information relevant for everyday tasks. In this way, parts of long-term memory effectively function as working memory. In a similar vein, Cowan does not regard working memory as a separate system from long-term memory. Representations in working memory are a subset of representations in long-term memory. Working memory is organized into two embedded levels. The first consists of long-term memory representations that are activated. There can be many of these—there is theoretically no limit to the activation of representations in long-term memory. The second level is called the focus of attention. The focus is regarded as having a limited capacity and holds up to four of the activated representations.

Oberauer has extended Cowan's model by adding a third component, a more narrow focus of attention that holds only one chunk at a time. The one-element focus is embedded in the four-element focus and serves to select a single chunk for processing. For example, four digits can be held in mind at the same time in Cowan's "focus of attention". When the individual wishes to perform a process on each of these digits—for example, adding the number two to each digit—separate processing is required for each digit since most individuals cannot perform several mathematical processes in parallel. Oberauer's attentional component selects one of the digits for processing and then shifts the attentional focus to the next digit, continuing until all digits have been processed.

Working memory is generally considered to have limited capacity. An early quantification of the capacity limit associated with short-term memory was the "magical number seven" suggested by Miller in 1956. He claimed that the information-processing capacity of young adults is around seven elements, which he called "chunks", regardless of whether the elements are digits, letters, words, or other units. Later research revealed this number depends on the category of chunks used (e.g., span may be around seven for digits, six for letters, and five for words), and even on features of the chunks within a category. For instance, span is lower for long than short words. In general, memory span for verbal contents (digits, letters, words, etc.) depends on the phonological complexity of the content (i.e., the number of phonemes, the number of syllables), and on the lexical status of the contents (whether the contents are words known to the person or not). Several other factors affect a person's measured span, and therefore it is difficult to pin down the capacity of short-term or working memory to a number of chunks. Nonetheless, Cowan proposed that working memory has a capacity of about four chunks in young adults (and fewer in children and old adults).

Whereas most adults can repeat about seven digits in correct order, some individuals have shown impressive enlargements of their digit span—up to 80 digits. This feat is possible by extensive training on an encoding strategy by which the digits in a list are grouped (usually in groups of three to five) and these groups are encoded as a single unit (a chunk). For this to succeed, participants must be able to recognize the groups as some known string of digits. One person studied by Ericsson and his colleagues, for example, used an extensive knowledge of racing times from the history of sports in the process of coding chunks: several such chunks could then be combined into a higher-order chunk, forming a hierarchy of chunks. In this way, only some chunks at the highest level of the hierarchy must be retained in working memory, and for retrieval the chunks are unpacked. That is, the chunks in working memory act as retrieval cues that point to the digits they contain. Practicing memory skills such as these does not expand working memory capacity proper: it is the capacity to transfer (and retrieve) information from long-term memory that is improved, according to Ericsson and Kintsch (1995; see also Gobet & Simon, 2000).

Working memory capacity can be tested by a variety of tasks. A commonly used measure is a dual-task paradigm, combining a memory span measure with a concurrent processing task, sometimes referred to as "complex span". Daneman and Carpenter invented the first version of this kind of task, the "reading span", in 1980. Subjects read a number of sentences (usually between two and six) and tried to remember the last word of each sentence. At the end of the list of sentences, they repeated back the words in their correct order. Other tasks that do not have this dual-task nature have also been shown to be good measures of working memory capacity. Whereas Daneman and Carpenter believed that the combination of "storage" (maintenance) and processing is needed to measure working memory capacity, we know now that the capacity of working memory can be measured with short-term memory tasks that have no additional processing component. Conversely, working memory capacity can also be measured with certain processing tasks that don't involve maintenance of information. The question of what features a task must have to qualify as a good measure of working memory capacity is a topic of ongoing research.

Measures of working-memory capacity are strongly related to performance in other complex cognitive tasks, such as reading comprehension, problem solving, and with measures of intelligence quotient.

Some researchers have argued that working-memory capacity reflects the efficiency of executive functions, most notably the ability to maintain multiple task-relevant representations in the face of distracting irrelevant information; and that such tasks seem to reflect individual differences in the ability to focus and maintain attention, particularly when other events are serving to capture attention. Both working memory and executive functions rely strongly, though not exclusively, on frontal brain areas.

Other researchers have argued that the capacity of working memory is better characterized as the ability to mentally form relations between elements, or to grasp relations in given information. This idea has been advanced, among others, by Graeme Halford, who illustrated it by our limited ability to understand statistical interactions between variables. These authors asked people to compare written statements about the relations between several variables to graphs illustrating the same or a different relation, as in the following sentence: "If the cake is from France, then it has more sugar if it is made with chocolate than if it is made with cream, but if the cake is from Italy, then it has more sugar if it is made with cream than if it is made of chocolate". This statement describes a relation between three variables (country, ingredient, and amount of sugar), which is the maximum most individuals can understand. The capacity limit apparent here is obviously not a memory limit (all relevant information can be seen continuously) but a limit to how many relationships are discerned simultaneously.

There are several hypotheses about the nature of the capacity limit. One is that a limited pool of cognitive resources needed to keep representations active and thereby available for processing, and for carrying out processes. Another hypothesis is that memory traces in working memory decay within a few seconds, unless refreshed through rehearsal, and because the speed of rehearsal is limited, we can maintain only a limited amount of information. Yet another idea is that representations held in working memory interfere with each other.

The assumption that the contents of short-term or working memory decay over time, unless decay is prevented by rehearsal, goes back to the early days of experimental research on short-term memory. It is also an important assumption in the multi-component theory of working memory. The most elaborate decay-based theory of working memory to date is the "time-based resource sharing model". This theory assumes that representations in working memory decay unless they are refreshed. Refreshing them requires an attentional mechanism that is also needed for any concurrent processing task. When there are small time intervals in which the processing task does not require attention, this time can be used to refresh memory traces. The theory therefore predicts that the amount of forgetting depends on the temporal density of attentional demands of the processing task—this density is called "cognitive load". The cognitive load depends on two variables, the rate at which the processing task requires individual steps to be carried out, and the duration of each step. For example, if the processing task consists of adding digits, then having to add another digit every half second places a higher cognitive load on the system than having to add another digit every two seconds. In a series of experiments, Barrouillet and colleagues have shown that memory for lists of letters depends neither on the number of processing steps nor the total time of processing but on cognitive load.

Resource theories assume that the capacity of working memory is a limited resource that must be shared between all representations that need to be maintained in working memory simultaneously. Some resource theorists also assume that maintenance and concurrent processing share the same resource; this can explain why maintenance is typically impaired by a concurrent processing demand. Resource theories have been very successful in explaining data from tests of working memory for simple visual features, such as colors or orientations of bars. An ongoing debate is whether the resource is a continuous quantity that can be subdivided among any number of items in working memory, or whether it consists of a small number of discrete "slots", each of which can be assigned to one memory item, so that only a limited number of about 3 items can be maintained in working memory at all.

Several forms of interference have been discussed by theorists. One of the oldest ideas is that new items simply replace older ones in working memory. Another form of interference is retrieval competition. For example, when the task is to remember a list of 7 words in their order, we need to start recall with the first word. While trying to retrieve the first word, the second word, which is represented in proximity, is accidentally retrieved as well, and the two compete for being recalled. Errors in serial recall tasks are often confusions of neighboring items on a memory list (so-called transpositions), showing that retrieval competition plays a role in limiting our ability to recall lists in order, and probably also in other working memory tasks. A third form of interference is the distortion of representations by superposition: When multiple representations are added on top of each other, each of them is blurred by the presence of all the others. A fourth form of interference assumed by some authors is feature overwriting. The idea is that each word, digit, or other item in working memory is represented as a bundle of features, and when two items share some features, one of them steals the features from the other. The more items are held in working memory, and the more their features overlap, the more each of them will be degraded by the loss of some features.

None of these hypotheses can explain the experimental data entirely. The resource hypothesis, for example, was meant to explain the trade-off between maintenance and processing: The more information must be maintained in working memory, the slower and more error prone concurrent processes become, and with a higher demand on concurrent processing memory suffers. This trade-off has been investigated by tasks like the reading-span task described above. It has been found that the amount of trade-off depends on the similarity of the information to be remembered and the information to be processed. For example, remembering numbers while processing spatial information, or remembering spatial information while processing numbers, impair each other much less than when material of the same kind must be remembered and processed. Also, remembering words and processing digits, or remembering digits and processing words, is easier than remembering and processing materials of the same category. These findings are also difficult to explain for the decay hypothesis, because decay of memory representations should depend only on how long the processing task delays rehearsal or recall, not on the content of the processing task. A further problem for the decay hypothesis comes from experiments in which the recall of a list of letters was delayed, either by instructing participants to recall at a slower pace, or by instructing them to say an irrelevant word once or three times in between recall of each letter. Delaying recall had virtually no effect on recall accuracy. The interference theory seems to fare best with explaining why the similarity between memory contents and the contents of concurrent processing tasks affects how much they impair each other. More similar materials are more likely to be confused, leading to retrieval competition.

The capacity of working memory increases gradually over childhood and declines gradually in old age.

Measures of performance on tests of working memory increase continuously between early childhood and adolescence, while the structure of correlations between different tests remains largely constant. Starting with work in the Neo-Piagetian tradition, theorists have argued that the growth of working-memory capacity is a major driving force of cognitive development. This hypothesis has received substantial empirical support from studies showing that the capacity of working memory is a strong predictor of cognitive abilities in childhood. Particularly strong evidence for a role of working memory for development comes from a longitudinal study showing that working-memory capacity at one age predicts reasoning ability at a later age. Studies in the Neo-Piagetian tradition have added to this picture by analyzing the complexity of cognitive tasks in terms of the number of items or relations that have to be considered simultaneously for a solution. Across a broad range of tasks, children manage task versions of the same level of complexity at about the same age, consistent with the view that working memory capacity limits the complexity they can handle at a given age. Although neuroscience studies support the notion that children rely on prefrontal cortex for performing various working memory tasks, an fMRI meta-analysis on children compared to adults performing the n back task revealed lack of consistent prefrontal cortex activation in children, while posterior regions including the insular cortex and cerebellum remain intact. 

Working memory is among the cognitive functions most sensitive to decline in old age. Several explanations have been offered for this decline in psychology. One is the processing speed theory of cognitive aging by Tim Salthouse. Drawing on the finding of general slowing of cognitive processes as people grow older, Salthouse argues that slower processing leaves more time for working-memory contents to decay, thus reducing effective capacity. However, the decline of working-memory capacity cannot be entirely attributed to slowing because capacity declines more in old age than speed. Another proposal is the inhibition hypothesis advanced by Lynn Hasher and Rose Zacks. This theory assumes a general deficit in old age in the ability to inhibit irrelevant, or no-longer relevant, information. Therefore, working memory tends to be cluttered with irrelevant contents that reduce the effective capacity for relevant content. The assumption of an inhibition deficit in old age has received much empirical support but, so far, it is not clear whether the decline in inhibitory ability fully explains the decline of working-memory capacity. An explanation on the neural level of the decline of working memory and other cognitive functions in old age has been proposed by West. He argued that working memory depends to a large degree on the pre-frontal cortex, which deteriorates more than other brain regions as we grow old. Age related decline in working memory can be briefly reversed using low intensity transcranial stimulation, synchronizing rhythms in bilateral frontal and left temporal lobe areas.

Torkel Klingberg was the first to investigate whether intensive training of working memory has beneficial effects on other cognitive functions. His pioneering study suggested that working memory can be improved by training in ADHD patients through computerized programs. This study has found that a period of working memory training increases a range of cognitive abilities and increases IQ test scores. Another study of the same group has shown that, after training, measured brain activity related to working memory increased in the prefrontal cortex, an area that many researchers have associated with working memory functions. It has been shown in one study that working memory training increases the density of prefrontal and parietal dopamine receptors (specifically, DRD1) in test persons. However, subsequent work with the same training program has failed to replicate the beneficial effects of training on cognitive performance. A meta-analytic summary of research with Klingberg's training program up to 2011 shows that this training has at best a negligible effect on tests of intelligence and of attention

In another influential study, training with a working memory task (the dual n-back task) has improved performance on a fluid intelligence test in healthy young adults. The improvement of fluid intelligence by training with the n-back task was replicated in 2010, but two studies published in 2012 failed to reproduce the effect. The combined evidence from about 30 experimental studies on the effectiveness of working-memory training has been evaluated by several meta-analyses. The authors of these meta-analyses disagree in their conclusions as to whether or not working-memory training improves intelligence. Yet, these meta-analyses agree in their estimate of the size of the effect of working-memory training: If there is such an effect, it is likely to be small.

The first insights into the neuronal and neurotransmitter basis of working memory came from animal research. The work of Jacobsen and Fulton in the 1930s first showed that lesions to the PFC impaired spatial working memory performance in monkeys. The later work of Joaquin Fuster recorded the electrical activity of neurons in the PFC of monkeys while they were doing a delayed matching task. In that task, the monkey sees how the experimenter places a bit of food under one of two identical-looking cups. A shutter is then lowered for a variable delay period, screening off the cups from the monkey's view. After the delay, the shutter opens and the monkey is allowed to retrieve the food from under the cups. Successful retrieval in the first attempt – something the animal can achieve after some training on the task – requires holding the location of the food in memory over the delay period. Fuster found neurons in the PFC that fired mostly during the delay period, suggesting that they were involved in representing the food location while it was invisible. Later research has shown similar delay-active neurons also in the posterior parietal cortex, the thalamus, the caudate, and the globus pallidus. The work of Goldman-Rakic and others showed that principal sulcal, dorsolateral PFC interconnects with all of these brain regions, and that neuronal microcircuits within PFC are able to maintain information in working memory through recurrent excitatory glutamate networks of pyramidal cells that continue to fire throughout the delay period. These circuits are tuned by lateral inhibition from GABAergic interneurons. The neuromodulatory arousal systems markedly alter PFC working memory function; for example, either too little or too much dopamine or norepinephrine impairs PFC network firing and working memory performance.

The research described above on persistent firing of certain neurons in the delay period of working memory tasks shows that the brain has a mechanism of keeping representations active without external input. Keeping representations active, however, is not enough if the task demands maintaining more than one chunk of information. In addition, the components and features of each chunk must be bound together to prevent them from being mixed up. For example, if a red triangle and a green square must be remembered at the same time, one must make sure that "red" is bound to "triangle" and "green" is bound to "square". One way of establishing such bindings is by having the neurons that represent features of the same chunk fire in synchrony, and those that represent features belonging to different chunks fire out of sync. In the example, neurons representing redness would fire in synchrony with neurons representing the triangular shape, but out of sync with those representing the square shape. So far, there is no direct evidence that working memory uses this binding mechanism, and other mechanisms have been proposed as well. It has been speculated that synchronous firing of neurons involved in working memory oscillate with frequencies in the theta band (4 to 8 Hz). Indeed, the power of theta frequency in the EEG increases with working memory load, and oscillations in the theta band measured over different parts of the skull become more coordinated when the person tries to remember the binding between two components of information.

Localization of brain functions in humans has become much easier with the advent of brain imaging methods (PET and fMRI). This research has confirmed that areas in the PFC are involved in working memory functions. During the 1990s much debate has centered on the different functions of the ventrolateral (i.e., lower areas) and the dorsolateral (higher) areas of the PFC. A human lesion study provides additional evidence for the role of the dorsolateral prefrontal cortex in working memory. One view was that the dorsolateral areas are responsible for spatial working memory and the ventrolateral areas for non-spatial working memory. Another view proposed a functional distinction, arguing that ventrolateral areas are mostly involved in pure maintenance of information, whereas dorsolateral areas are more involved in tasks requiring some processing of the memorized material. The debate is not entirely resolved but most of the evidence supports the functional distinction.

Brain imaging has also revealed that working memory functions are not limited to the PFC. A review of numerous studies shows areas of activation during working memory tasks scattered over a large part of the cortex. There is a tendency for spatial tasks to recruit more right-hemisphere areas, and for verbal and object working memory to recruit more left-hemisphere areas. The activation during verbal working memory tasks can be broken down into one component reflecting maintenance, in the left posterior parietal cortex, and a component reflecting subvocal rehearsal, in the left frontal cortex (Broca's area, known to be involved in speech production).

There is an emerging consensus that most working memory tasks recruit a network of PFC and parietal areas. A study has shown that during a working memory task the connectivity between these areas increases. Another study has demonstrated that these areas are necessary for working memory, and not simply activated accidentally during working memory tasks, by temporarily blocking them through transcranial magnetic stimulation (TMS), thereby producing an impairment in task performance.

A current debate concerns the function of these brain areas. The PFC has been found to be active in a variety of tasks that require executive functions. This has led some researchers to argue that the role of PFC in working memory is in controlling attention, selecting strategies, and manipulating information in working memory, but not in maintenance of information. The maintenance function is attributed to more posterior areas of the brain, including the parietal cortex. Other authors interpret the activity in parietal cortex as reflecting executive functions, because the same area is also activated in other tasks requiring attention but not memory.

A 2003 meta-analysis of 60 neuroimaging studies found left frontal cortex was involved in low-task demand verbal working memory and right frontal cortex for spatial working memory. Brodmann's areas (BAs) 6, 8, and 9, in the superior frontal cortex was involved when working memory must be continuously updated and when memory for temporal order had to be maintained. Right Brodmann 10 and 47 in the ventral frontal cortex were involved more frequently with demand for manipulation such as dual-task requirements or mental operations, and Brodmann 7 in the posterior parietal cortex was also involved in all types of executive function.

Working memory has been suggested to involve two processes with different neuroanatomical locations in the frontal and parietal lobes. First, a selection operation that retrieves the most relevant item, and second an updating operation that changes the focus of attention made upon it. Updating the attentional focus has been found to involve the transient activation in the caudal superior frontal sulcus and posterior parietal cortex, while increasing demands on selection selectively changes activation in the rostral superior frontal sulcus and posterior cingulate/precuneus.

Articulating the differential function of brain regions involved in working memory is dependent on tasks able to distinguish these functions. Most brain imaging studies of working memory have used recognition tasks such as delayed recognition of one or several stimuli, or the n-back task, in which each new stimulus in a long series must be compared to the one presented n steps back in the series. The advantage of recognition tasks is that they require minimal movement (just pressing one of two keys), making fixation of the head in the scanner easier. Experimental research and research on individual differences in working memory, however, has used largely recall tasks (e.g., the reading span task, see below). It is not clear to what degree recognition and recall tasks reflect the same processes and the same capacity limitations.

Brain imaging studies have been conducted with the reading span task or related tasks. Increased activation during these tasks was found in the PFC and, in several studies, also in the anterior cingulate cortex (ACC). People performing better on the task showed larger increase of activation in these areas, and their activation was correlated more over time, suggesting that their neural activity in these two areas was better coordinated, possibly due to stronger connectivity.

One approach to model the neurophysiology and the functioning of working memory is the prefrontal cortex basal ganglia working memory (PBWM).

Working memory is impaired by acute and chronic psychological stress. This phenomenon was first discovered in animal studies by Arnsten and colleagues, who have shown that stress-induced catecholamine release in PFC rapidly decreases PFC neuronal firing and impairs working memory performance through feedforward, intracellular signaling pathways. Exposure to chronic stress leads to more profound working memory deficits and additional architectural changes in PFC, including dendritic atrophy and spine loss, which can be prevented by inhibition of protein kinase C signaling. fMRI research has extended this research to humans, and confirms that reduced working memory caused by acute stress links to reduced activation of the PFC, and stress increased levels of catecholamines. Imaging studies of medical students undergoing stressful exams have also shown weakened PFC functional connectivity, consistent with the animal studies. The marked effects of stress on PFC structure and function may help to explain how stress can cause or exacerbate mental illness.
The more stress in one's life, the lower the efficiency of working memory in performing simple cognitive tasks. Students who performed exercises that reduced the intrusion of negative thoughts showed an increase in their working memory capacity. Mood states (positive or negative) can have an influence on the neurotransmitter dopamine, which in turn can affect problem solving.

Alcohol abuse can result in brain damage which impairs working memory. Alcohol has an effect on the blood-oxygen-level-dependent (BOLD) response. The BOLD response correlates increased blood oxygenation with brain activity, which makes this response a useful tool for measuring neuronal activity. The BOLD response affects regions of the brain such as the basal ganglia and thalamus when performing a working memory task. Adolescents who start drinking at a young age show a decreased BOLD response in these brain regions. Alcohol dependent young women in particular exhibit less of a BOLD response in parietal and frontal cortices when performing a spatial working memory task. Binge drinking, specifically, can also affect one's performance on working memory tasks, particularly visual working memory. Additionally, there seems to be a gender difference in regards to how alcohol affects working memory. While women perform better on verbal working memory tasks after consuming alcohol compared to men, they appear to perform worse on spatial working memory tasks as indicated by less brain activity. Finally, age seems to be an additional factor. Older adults are more susceptible than others to the effects of alcohol on working memory.

Individual differences in working-memory capacity are to some extent heritable; that is, about half of the variation between individuals is related to differences in their genes. The genetic component of variability of working-memory capacity is largely shared with that of fluid intelligence.

Little is known about which genes are related to the functioning of working memory. Within the theoretical framework of the multi-component model, one candidate gene has been proposed, namely ROBO1 for the hypothetical phonological loop component of working memory.

Working memory capacity is correlated with learning outcomes in literacy and numeracy. Initial evidence for this relation comes from the correlation between working-memory capacity and reading comprehension, as first observed by Daneman and Carpenter (1980) and confirmed in a later meta-analytic review of several studies. Subsequent work found that working memory performance in primary school children accurately predicted performance in mathematical problem solving. One longitudinal study showed that a child's working memory at 5 years old is a better predictor of academic success than IQ.

In a large-scale screening study, one in ten children in mainstream classrooms were identified with working memory deficits. The majority of them performed very poorly in academic achievements, independent of their IQ. Similarly, working memory deficits have been identified in national curriculum low-achievers as young as seven years of age. Without appropriate intervention, these children lag behind their peers. A recent study of 37 school-age children with significant learning disabilities has shown that working memory capacity at baseline measurement, but not IQ, predicts learning outcomes two years later. This suggests that working memory impairments are associated with low learning outcomes and constitute a high risk factor for educational underachievement for children. In children with learning disabilities such as dyslexia, ADHD, and developmental coordination disorder, a similar pattern is evident.

There is some evidence that optimal working memory performance links to the neural ability to focus attention on task-relevant information and to ignore distractions, and that practice-related improvement in working memory is due to increasing these abilities. One line of research suggests a link between the working memory capacities of a person and their ability to control the orientation of attention to stimuli in the environment. Such control enables people to attend to information important for their current goals, and to ignore goal-irrelevant stimuli that tend to capture their attention due to their sensory saliency (such as an ambulance siren). The direction of attention according to one's goals is assumed to rely on "top-down" signals from the pre-frontal cortex (PFC) that biases processing in posterior cortical areas. Capture of attention by salient stimuli is assumed to be driven by "bottom-up" signals from subcortical structures and the primary sensory cortices. The ability to override "bottom-up" capture of attention differs between individuals, and this difference has been found to correlate with their performance in a working-memory test for visual information. Another study, however, found no correlation between the ability to override attentional capture and measures of more general working-memory capacity.

An impairment of working memory functioning is normally seen in several neural disorders:

ADHD: Several authors have proposed that symptoms of ADHD arise from a primary deficit in a specific executive function (EF) domain such as working memory, response inhibition or a more general weakness in executive control. A meta-analytical review cites several studies that found significant lower group results for ADHD in spatial and verbal working memory tasks, and in several other EF tasks. However, the authors concluded that EF weaknesses neither are necessary nor sufficient to cause all cases of ADHD.

Several neurotransmitters, such as dopamine and glutamate may be both involved in ADHD and working memory. Both are associated with the frontal brain, self-direction and self-regulation, but cause–effect have not been confirmed, so it is unclear whether working memory dysfunction leads to ADHD, or ADHD distractibility leads to poor functionality of working memory, or if there is some other connection.

Parkinson's disease: Patients with Parkinson's show signs of a reduced verbal function of working memory. They wanted to find if the reduction is due to a lack of ability to focus on relevant tasks, or a low amount of memory capacity. Twenty-one patients with Parkinson's were tested in comparison to the control group of 28 participants of the same age.The researchers found that both hypotheses were the reason working memory function is reduced which did not fully agree with their hypothesis that it is either one or the other.

Alzheimer's disease: As Alzheimer's disease becomes more serious, less working memory functions. There is one study that focuses on the neural connections and fluidity of working memory in mice brains. Half of the mice were given an injection that is similar to Alzheimer's effects, and the other half were not. Then they were expected to go through a maze that is a task to test working memory. The study help answer questions about how Alzheimer's can deteriorate the working memory and ultimately obliterate memory functions.

Huntington's disease: A group of researchers hosted a study that researched the function and connectivity of working memory over a 30-month longitudinal experiment. It found that there were certain places in the brain where most connectivity was decreased in pre-Huntington diseased patients, in comparison to the control group that remained consistently functional.




</doc>
<doc id="4409451" url="https://en.wikipedia.org/wiki?curid=4409451" title="Staring">
Staring

Staring is a prolonged gaze or fixed look. In staring, one object or person is the continual focus of visual interest, for an amount of time. Staring can be interpreted as being either hostile, or the result of intense concentration or affection. Staring behaviour can be considered a form of aggression, or an invasion of an individual's privacy. If eye contact is reciprocated, mutual staring can take the form of a battle of wills, or even a game where the loser is the person who looks away first – a staring contest.

To some extent, the meaning of a person’s staring behaviour depends upon the attributions made by the observer. Staring often occurs accidentally, when someone appears to be staring into space they may well be lost in thought, or stupefied, or simply unable to see.

Staring conceptually also implies confronting the inevitable – ‘staring death in the face’, or ‘staring into the abyss’. Group staring evokes and emphasises paranoia; such as the archetypal stranger walking into a saloon in a Western to be greeted by the stares of all the regulars. The fear of being stared at is called Scopophobia.

Children have to be socialised into learning acceptable staring behaviour. This is often difficult because children have different sensitivities to self-esteem. Staring is also sometimes used as a technique of flirting with an object of affection. However, being stared at, especially for a prolonged amount of time or very frequently by one person in particular, can cause discomfort to those subjected to it. 

Jean-Paul Sartre discusses "The look" in Being and Nothingness, in which the appearance of someone else creates a situation in which a person's subjectivity is transformed without their consent.

The act of staring implies a visual focus, where the subject of the gaze is objectified. This has been the subject of psychoanalytical studies on the nature of scopophilia, with a subsequent development in some aspects of feminist thought (see Gaze, film, photography and voyeurism).
Paradoxically, the notion of staring also implicates the looker in constructing themselves as a subject. Sartre was interested in the individual experiencing shame only when they perceive that their shameful act is being witnessed by another. (see The look)

A staring contest is a game in which two people stare into each other's eyes and attempt to maintain eye contact for a longer period than their opponent can. The game ends when one participant looks away or smiles.

A popular variation of the game exists in which the participants not only attempt to maintain eye contact, but also must resist the urge to blink, creating a physical challenge as well as a psychological one. Most other variations revolve chiefly around either of these two core objectives, with some allowing the aggressive use of distracting actions to force an opponent into defeat, while others prohibit virtually any action but staring.

Another commonly accepted ruleset is the 'ambush' ruleset, where one participant begins the contest without the opponent initially being aware of it. As soon as eye contact is made, the staring contest has begun, and proceeds according to regular conventions. The contest is allowed to pass without the opponent being aware they were involved.

Staring contests ('Stare-out') were featured as an animation in the first series of surreal BBC television comedy sketch show "Big Train" (aired in 1998). The animation satirised televised sporting events coverage and its over-excited commentary, inspired by events such as the World Chess Championship, boxing and the football World Cup. The sketches are set during the World Stare-out Championship Finals, a staring match which is described as a global event broadcast all over the world.


</doc>
<doc id="21559940" url="https://en.wikipedia.org/wiki?curid=21559940" title="Animal spirits (Keynes)">
Animal spirits (Keynes)

Animal spirits is the term John Maynard Keynes used in his 1936 book "The General Theory of Employment, Interest and Money" to describe the instincts, proclivities and emotions that ostensibly influence and guide human behavior, and which can be measured in terms of, for example, consumer confidence. It has since been argued that trust is also included in or produced by "animal spirits".

The original passage by Keynes reads:

The first use of the notion animal spirits is described by Descartes, Newton and other scientists on how the notion for the vitality of the body is used.

These animal spirits are of an æthereal nature. In one of his letters about light, Newton wrote that the animated spirits very easily live in "the brain, nerves, and muscles, may become a convenient vessel to hold so subtil a spirit." The spirit Newton is talking about here are the animated spirits that have an ethereal nature. It relates to life in the body. Later it became a concept that acquired a psychological content but was always thought of in connection with the life processes of the body. Therefore, retained a lower overall animal status.

William Safire explored origins of the phrase in his 2009 article "On Language: 'Animal Spirits'":
Thomas Hobbes used the phrase "animal spirits" to refer to passive emotions and instincts, as well as natural functions like breathing.

In social science, Karl Marx refers to "animal spirits" in the 1887 English translation of Capital, Volume 1. Marx speaks of the animal spirits of the workers, which he believes a capitalist can impel by encouraging social interaction and competition within her factory or depress by adopting assembly-line work whereby the worker repeats a single task.

"Animal spirits" was a euphemistic late-Victorian and Edwardian phrase used by English public school boys such as P. G. Wodehouse (born two years before the Etonian Keynes) who attended Dulwich College. Wodehouse and Arthur Conan Doyle were popular authors for public school boys in England before the Great War. Doyle himself used the phrase "animal spirits" in 1883, the year of Keynes's birth.

Two examples of Wodehouse's use of the phrase are in the 1909 book "Mike" (later republished in two parts as "Mike at Wrykyn" and "Mike and Psmith"). "Animal spirits" denoted an adolescent attitude to authority that resulted in energetically and deliberately acting on advice, opinion, or exhortation to the point of stretching the letter of any regulations involved to the limit. The aim was to maximise short-term disruption of what was considered to be 'normal' behaviour. Restoring equilibrium subsequently required a firm sanction from those in authority and possibly also a re-casting of the regulation to prevent repeats of the actions undertaken. The slang term of the era for this was 'ragging'. 

Wodehouse uses antithesis in the latter example to make comedy out of Mr. Downing's astonishment; surely nobody could be "less" susceptible to "animal spirits" than the suave, debonaire Psmith? "Psmith In The City" (1910) was based on Wodehouse's own experiences in the 'square mile' and the theme is implicitly elaborated on in the financial environment of the "New Asiatic Bank".

John Coates of Cambridge University supports the popular English Edwardian public school intuition that qualities such as dynamism and leadership coexist with less constructive traits such as recklessness, heedlessness, and in-caution. Coates attributes this to fluctuations in hormonal balances; abnormally high levels of testosterone may create individual success but also collective excessive aggression, overconfidence, and herd behaviour, while too much cortisol can promote irrational pessimism and risk aversion. The author's remedy for this is to shift the employment balance in finance towards women and older men and monitor traders' biology.

Recent research shows that the term 'animal spirits' was used in the works of a psychologist that Keynes had studied in 1905 and also suggests that Keynes implicitly drew upon an evolutionary understanding of human instinct.

In 2009, economists Akerlof and Shiller advised in addition that:




</doc>
<doc id="28066742" url="https://en.wikipedia.org/wiki?curid=28066742" title="Neanderthal behavior">
Neanderthal behavior

Almost everything about Neanderthal behaviour is controversial. From their physiology, Neanderthals are presumed to have been omnivores, but animal protein formed the majority of their dietary protein, showing them to have been apex predators and not scavengers. Some studies suggest they cooked vegetables.

The quality of stone tools at archaeological sites suggests Neanderthals were good at "expert" cognition, a form of observational learning and practice acquired through apprenticeship that relies heavily on long-term procedural memory. Neanderthal toolmaking changed little over hundreds of thousands of years. The lack of innovation was said to imply they may have had a reduced capacity for thinking by analogy and less working memory. The researchers further speculated that Neanderthal behaviour would probably seem neophobic, dogmatic and xenophobic to modern humans. A 2018 open access paper discussed, in light of recent developments in the fields of paleogenetics and paleoanthropology, whether or not Neanderthals were rational. The authors' argument focuses on the genetic evidence that supports interbreeding with "Homo sapiens," language acquisition (including the FOXP2 gene), archaeological signs of cultural development and potential for cumulative cultural evolution

Few Neanderthals lived past 35.

It is not known whether Neanderthals were anatomically capable of speech and whether they actually spoke. A once-widely believed theory that the Neanderthal vocal tract was different from that of living humans and so probably could not speak is now discredited. The only bone in the vocal tract is the hyoid but is so fragile that no Neanderthal hyoid was found until 1983, when excavators discovered a well-preserved one on Neanderthal Kebara 2, Israel. It was largely similar to that of living humans. Although the original excavators claimed that the similarity of this bone with that of living humans implied Neanderthals were anatomically capable of speech, it is not possible to reconstruct the vocal tract with information supplied by the hyoid. In particular, it does not allow to determine whether the larynx of its owner was in a low-lying position, a feature considered important in producing speech.

A 2013 study on the Kebara hyoid used X-ray microtomography and finite element analysis to conclude that the Neanderthal hyoid showed microscopic features more similar to a modern human's hyoid than to a chimpanzee hyoid. To the authors, that suggested the Neanderthal hyoid was used similarly to that in living humans, that is, to produce speech. Yet, because the authors did not compare the microscopic structure of the Kebara 2 hyoid with that of speech-hindered living humans, the result is not yet conclusive.

Although some researchers believe Neanderthal tool-making is too complex for them not to have had language, toolmaking experiments of Levallois technology, the most common Neanderthal toolmaking technique, have found that living humans can learn it in silence.

Neanderthals had the same DNA-coding region of the FOXP2 gene as living humans, but are different in one position of the gene's regulatory regions, and the extent of FOXP2 expression might hence have been different in Neanderthals. Although the gene appears necessary for language—living humans who don't have the normal human version of the gene have serious language difficulties—it is not necessarily sufficient. It is not known whether FOXP2 evolved for or in conjunction with language, nor whether there are other language-related genes that Neanderthals may or may not have had. Similarly, the size and functionality of the Neanderthal Broca's and Wernicke's areas, used for speech generation in modern humans, is debated.

In 1998, researchers suggested Neanderthals had a hypoglossal canal at least as large as humans, suggesting they had part of the neurological requirements for language. The canal carries the hypoglossal nerve, which controls the muscles of the tongue, necessary to produce language. However, a Berkeley research team showed no correlation between canal size and speech, as a number of extant non-human primates and fossilized australopithecines have larger hypoglossal canals.

The morphology of the outer and middle ear of "Homo heidelbergensis", the Neanderthal's ancestor, suggests they had an auditory sensitivity similar to modern humans and different from chimpanzees.

Neanderthal and early anatomically modern human archaeological sites show a more simple toolkit than those found in Upper Paleolithic sites, produced by modern humans after about 50,000 BP. In both early anatomically modern humans and Neanderthals, there is little innovation in the toolkit.

Tools produced by Middle Palaeolithic humans in Eurasia (both Neanderthals and early modern humans) are known as Mousterian. These were often produced using soft hammer percussion, with hammers made of materials like bones, antlers, and wood, rather than hard hammer percussion, using stone hammers. A result of this is that their bone industry was relatively simple. They routinely made stone implements. Neanderthal tools consisted of stone-flakes and task-specific hand axes, many of which were sharp.

There is evidence for violence among Neanderthals. The 40,000-year-old Neanderthal skull of St. Césaire has a healed fracture in its cranial vault likely caused by something sharp, suggesting interpersonal violence. The wound healed and the Neanderthal survived.

Whether they had projectile weapons is controversial. They seem to have had wooden spears, but it is unclear whether they were used as projectiles or as thrusting spears. Wood implements rarely survive, but several 320,000-year-old wooden spears about 2-metres in length were found near Schöningen, northern Germany, and are believed to be the product of the older "Homo heidelbergensis" species.

Neanderthals used fire on occasion, but it is not certain whether they were able to produce it. They may have used Pyrolusite (manganese dioxide) to accelerate the combustion of wood. "With archaeological evidence for fire places and the conversion of the manganese dioxide to powder, [it has been argued] that Neanderthals at Pech-de-l’Azé I used manganese dioxide in fire-making and produced fire on demand." MnO lowers the combustion temperature of wood from 350 degrees Celsius to 250 degrees Celsius and is common in Neanderthal archaeological sites.

Neanderthals produced birch tar through the dry distillation of birch bark.

Pendants and other jewelry showing traces of ochre dye and of deliberate grooving have also been found in one single stratigraphically disturbed Neanderthal archaeological layer, but whether these items were ever in the hands of Neanderthals or were mixed into their archaeological layers from overlying modern human ones is debated.

No claim of a deliberate Neanderthal burial is universally accepted. An interpretation of pre-Neanderthal Shanidar IV as having been ritually buried with flowers has been seriously questioned, and to Paul B. Pettitt, convincingly eliminated: "A recent examination of the microfauna from the strata into which the grave was cut suggests that the pollen was deposited by the burrowing
rodent "Meriones tersicus" (Persian jird), which is common in the Shanidar microfauna and whose burrowing activity can be observed today".

Traces of fossilized plants have been extracted from Neanderthal teeth found in Belgium and Iraq suggesting they mostly consumed plants. Nonetheless, preliminary studies indicated that Neanderthals obtained protein in their diet from animal sources. Evidence based on isotope studies shows that at least some Neanderthals may have eaten meat. 

Neanderthals hunted large animals, such as the mammoth. However, they are believed to have practiced cannibalism or ritual defleshing. This hypothesis was formulated after researchers found marks on Neanderthal bones similar to the bones of a dead deer butchered by Neanderthals.

Neanderthal bones from various sites (Combe-Grenal and Abri Moula in France, Krapina in Croatia and Grotta Guattari in Italy) have all been cited as bearing cut marks made by stone tools. However, the results of technological tests have revealed varied causes.

Re-evaluation of these marks using high-powered microscopes, comparisons to contemporary butchered animal remains, and recent ethnographic cases of excarnation mortuary practises have shown that perhaps this was a case of ritual defleshing.

Evidence of cannibalism includes:

Evidence indicating cannibalism would not distinguish Neanderthals from modern humans, which are known to have practiced cannibalism or mortuary defleshing (e.g., the sky burial of Tibet).

Upon Higham et al.'s (2010) publication of new radiocarbon dates shedding doubt on the association of Châtelperronian beads with Neanderthals, Paul Mellars wrote that “the single most impressive and hitherto widely cited pillar of evidence for the presence of complex ‘symbolic’ behavior among the late Neanderthal populations in Europe has now effectively collapsed”. This conclusion, however, is controversial, and others such as Jean-Jacques Hublin and colleagues have re-dated more material and used proteomic evidence to restate the challenged association with Neanderthal.

There exists a very large number of other claims of Neanderthal art, adornment, and structures. These are often taken literally by the media as showing Neanderthals were capable of symbolic thought, or "mental equals" to anatomically modern humans. As evidence of symbolism, none of them are widely accepted, although the same is true for Middle Palaeolithic anatomically modern humans. Among many others:




</doc>
<doc id="19111605" url="https://en.wikipedia.org/wiki?curid=19111605" title="1% rule (Internet culture)">
1% rule (Internet culture)

In Internet culture, the 1% rule is a rule of thumb pertaining to participation in an internet community, stating that only 1% of the users of a website actively create new content, while the other 99% of the participants only lurk. Variants include the "1–9–90 rule" (sometimes "90–9–1 principle" or the "89:10:1 ratio"), which states that in a collaborative website such as a wiki, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content.

Similar rules are known in information science, such as the 80/20 rule known as the Pareto principle, that 20 percent of a group will produce 80 percent of the activity, however the activity may need to be defined.

The 1% rule states that the number of people who create content on the Internet represents approximately 1% of the people who view that content. For example, for every person who posts on a forum, generally about 99 other people view that forum but do not post. The term was coined by authors and bloggers Ben McConnell and Jackie Huba, although earlier references to the same concept did not use this name.

The terms "lurk" and "lurking", in reference to online activity, are used to refer to online observation without engaging others in the community, and were first used by veteran print journalist, P. Tomi Austin, circa 1990, when her presence was noticed by other users in chat rooms, who queried her reasons for not engaging in chat. There were repeated inquiries about her identity and her refusal to engage in chat. The etiquette was, apparently, to greet other users upon entry into the chat rooms/sites. At the time, (then in her 30s, surfing among users averaging in their teens and 20s) she was only identified as "Bilbo", she explained that she was a mature, but computer-literate, user and novice to chat, and preferred to "lurk", or was "lurking" to familiarize herself with the chat culture, etiquette, and the sites to which she had logged on. In some instances, she needed to explain her coinage of the term "lurking", as the term was new to the online community, but others quickly understood her meaning. To her knowledge, the terms had not been used prior to that period, and there appears to be no earlier dated reference to the coinage.

A 2005 study of radical Jihadist forums found 87% of users had never posted on the forums, 13% had posted at least once, 5% had posted 50 or more times, and only 1% had posted 500 or more times. 

A 2014 peer-reviewed paper entitled "The 1% Rule in Four Digital Health Social Networks: An Observational Study" empirically examined the 1% rule in health oriented online forums. The paper concluded that the 1% rule was consistent across the four support groups, with a handful of "Superusers" generating the vast majority of content. A study later that year, from a separate group of researchers, replicated the 2014 van Mierlo study in an online forum for depression. Results indicated that the distribution frequency of the 1% rule fit followed Zipf's Law, which is a specific type of a power law. 

The "90–9–1" version of this rule states that for websites where users can both create and edit content, 1% of people create content, 9% edit or modify that content, and 90% view the content without contributing.

The actual percentage is likely to vary depending upon the subject matter. For example, if a forum requires content submissions as a condition of entry, the percentage of people who participate will probably be significantly higher than one percent, but the content producers will still be a minority of users. This is validated in a study conducted by Michael Wu, who uses economics techniques to analyze the participation inequality across hundreds of communities segmented by industry, audience type, and community focus.

The 1% rule is often misunderstood to apply to the Internet in general, but it applies more specifically to any given Internet community. It is for this reason that one can see evidence for the 1% principle on many websites, but aggregated together one can see a different distribution. This latter distribution is still unknown and likely to shift, but various researchers and pundits have speculated on how to characterize the sum total of participation. Research in late 2012 suggested that only 23% of the population (rather than 90 percent) could properly be classified as lurkers, while 17% of the population could be classified as intense contributors of content. Several years prior, results were reported on a sample of students from Chicago where 60 percent of the sample created content in some form.

A similar concept was introduced by Will Hill of AT&T Laboratories and later cited by Jakob Nielsen; this was the earliest known reference to the term "participation inequality" in an online context. The term regained public attention in 2006 when it was used in a strictly quantitative context within a blog entry on the topic of marketing.




</doc>
<doc id="900695" url="https://en.wikipedia.org/wiki?curid=900695" title="Gold digging">
Gold digging

Gold digging is a type of transactional relationship in which people, especially women, engage in relationships for money rather than love. When it turns into marriage, it is a type of marriage of convenience.

Peggy Hopkins Joyce was in the 1920s considered the perfect example of a gold digger, with some claims existing that the term was even coined to describe her.

A popular association between chorus girls and gold diggers was established in 1919 by "The Gold Diggers" play, association which was also present in the subsequent film four years later, "The Gold Diggers".

In 1920s and 1930s American cinema the "gold digger" was the type of "femme fatale" that gradually replaced the "vamp". The character type would be featured, for example, in "How to Marry a Millionaire", a 1953 film starring Marilyn Monroe, alongside Schatze Page and Loco Dempsey.

In the analysis of rap music it has been theorized that the "gold digger script" is one of a few prevalent sexual scripts present for young African American women.

Kanye West's "Gold Digger" references gold digging.




</doc>
<doc id="47228422" url="https://en.wikipedia.org/wiki?curid=47228422" title="User behavior analytics">
User behavior analytics

User behavior analytics (UBA) as defined by Gartner is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats. Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.

The problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that "Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too." 

Developments in UBA technology led Gartner to evolve the category to user and entity behavior analytics ("UEBA"). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, servers, data, or anything with an IP address. It moves beyond the fraud-oriented UBA focus to a broader one encompassing "malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP." The addition of "entity" reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. "When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats."

Particularly in the computer security market, there are many vendors for UEBA applications. They can be "differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based)." 
According to the 2015 market guide released by Gartner, "the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased." The report further projected, "Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems."



</doc>
<doc id="358677" url="https://en.wikipedia.org/wiki?curid=358677" title="Obedience (human behavior)">
Obedience (human behavior)

Obedience, in human behavior, is a form of "social influence in which a person yields to explicit instructions or orders from an authority figure". Obedience is generally distinguished from compliance, which is behavior influenced by peers, and from conformity, which is behavior intended to match that of the majority. Depending on context, obedience can be seen as moral, immoral, or amoral.

Humans have been shown to be obedient in the presence of perceived legitimate authority figures, as shown by the Milgram experiment in the 1960s, which was carried out by Stanley Milgram to find out how the Nazis managed to get ordinary people to take part in the mass murders of the Holocaust. The experiment showed that obedience to authority was the norm, not the exception. Regarding obedience, Milgram said that "Obedience is as basic an element in the structure of social life as one can point to. Some system of authority is a requirement of all communal living, and it is only the man dwelling in isolation who is not forced to respond, through defiance or submission, to the commands of others." A similar conclusion was reached in the Stanford prison experiment.

Although other fields have studied obedience, social psychology has been primarily responsible for the advancement of research on obedience. It has been studied experimentally in several different ways.

In one classical study, Stanley Milgram (as part of the Milgram experiment) created a highly controversial yet often replicated study. Like many other experiments in psychology, Milgram's setup involved deception of the participants. In the experiment, subjects were told they were going to take part in a study of the effects of punishment on learning. In reality, the experiment focuses on people's willingness to obey malevolent authority. Each subject served as a teacher of associations between arbitrary pairs of words. After meeting the "teacher" at the beginning of the experiment, the "learner" (an accomplice of the experimenter) sat in another room and could be heard, but not seen. Teachers were told to give the "learner" electric shocks of increasing severity for each wrong answer. If subjects questioned the procedure, the "researcher" (again, an accomplice of Milgram) would encourage them to continue. Subjects were told to ignore the agonized screams of the learner, his desire to be untied and stop the experiment, and his pleas that his life was at risk and that he suffered from a heart condition. The experiment, the "researcher" insisted, had to go on. The dependent variable in this experiment was the voltage amount of shocks administered.

The other classical study on obedience was conducted at Stanford University during the 1970s. Phillip Zimbardo was the main psychologist responsible for the experiment. In the Stanford Prison Experiment, college age students were put into a pseudo prison environment in order to study the impacts of "social forces" on participants behavior. Unlike the Milgram study in which each participant underwent the same experimental conditions, here using random assignment half the participants were prison guards and the other half were prisoners. The experimental setting was made to physically resemble a prison while simultaneously inducing "a psychological state of imprisonment".

The Milgram study found that most participants would obey orders even when obedience posed severe harm to others. With encouragement from a perceived authority figure, about two-thirds of the participants were willing to administer the highest level of shock to the learner. This result was surprising to Milgram because he thought that "subjects have learned from childhood that it is a fundamental breach of moral conduct to hurt another person against his will". Milgram attempted to explain how ordinary people were capable of performing potentially lethal acts against other human beings by suggesting that participants may have entered into an agentic state, where they allowed the authority figure to take responsibility for their own actions.

Zimbardo obtained similar results as the guards in the study obeyed orders and turned aggressive. Prisoners likewise were hostile to and resented their guards. The cruelty of the "guards" and the consequent stress of the "prisoners," forced Zimbardo to terminate the experiment prematurely, after 6 days.

The previous two studies greatly influenced how modern psychologists think about obedience. Milgram's study in particular generated a large response from the psychology community. In a modern study, Jerry Burger replicated Milgram's method with a few alterations. Burger's method was identical to Milgram's except when the shocks reached 150 volts, participants decided whether or not they wanted to continue and then the experiment ended (base condition). To ensure the safety of the participants, Burger added a two-step screening process; this was to rule out any participants that may react negatively to the experiment. In the modeled refusal condition, two confederates were used, where one confederate acted as the learner and the other was the teacher. The teacher stopped after going up to 90 volts, and the participant was asked to continue where the confederate left off. This methodology was considered more ethical because many of the adverse psychological effects seen in previous studies' participants occurred after moving past 150 volts. Additionally, since Milgram's study only used men, Burger tried to determine if there were differences between genders in his study and randomly assigned equal numbers of men and women to the experimental conditions.

Using data from his previous study, Burger probed participant's thoughts about obedience. Participants' comments from the previous study were coded for the number of times they mentioned "personal responsibility and the learner's well being". The number of prods the participants used in the first experiment were also measured.

Another study that used a partial replication of Milgram's work changed the experimental setting. In one of the Utrecht University studies on obedience, participants were instructed to make a confederate who was taking an employment test feel uncomfortable. Participants were told to make all of the instructed stress remarks to the confederate that ultimately made him fail in the experimental condition, but in the control condition they were not told to make stressful remarks. The dependent measurements were whether or not the participant made all of the stress remarks (measuring absolute obedience) and the number of stress remarks (relative obedience).

Following the Utrecht studies, another study used the stress remarks method to see how long participants would obey authority. The dependent measures for this experiment were the number of stress remarks made and a separate measure of personality designed to measure individual differences.

Burger's first study had results similar to the ones found in Milgram's previous study. The rates of obedience were very similar to those found in the Milgram study, showing that participants' tendency to obey has not declined over time. Additionally, Burger found that both genders exhibited similar behavior, suggesting that obedience will occur in participants independent of gender.
In Burger's follow-up study, he found that participants that worried about the well being of the learner were more hesitant to continue the study. He also found that the more the experimenter prodded the participant to continue, the more likely they were to stop the experiment.
The Utrecht University study also replicated Milgram's results. They found that although participants indicated they did not enjoy the task, over 90% of them completed the experiment.
The Bocchiaro and Zimbardo study had similar levels of obedience compared to the Milgram and Utrecht studies. They also found that participants would either stop the experiment at the first sign of the learner's pleas or would continue until the end of the experiment (called "the foot in the door scenario").
In addition to the above studies, additional research using participants from different cultures (including Spain, Australia, and Jordan) also found participants to be obedient.

One of the major assumptions of obedience research is that the effect is caused only by the experimental conditions, and Thomas Blass' research contests this point, as in some cases participant factors involving personality could potentially influence the results.
In one of Blass' reviews on obedience, he found that participant's personalities can impact how they respond to authority, as people that were high in authoritarian submission were more likely to obey. He replicated this finding in his own research, as in one of his experiments, he found that when watching portions of the original Milgram studies on film, participants placed less responsibility on those punishing the learner when they scored high on measures of authoritarianism.

In addition to personality factors, participants who are resistant to obeying authority had high levels of social intelligence.

Obedience can also be studied outside of the Milgram paradigm in fields such as economics or political science. One economics study that compared obedience to a tax authority in the lab versus at home found that participants were much more likely to pay participation tax when confronted in the lab. This finding implies that even outside of experimental settings, people will forgo potential financial gain to obey authority.

Another study involving political science measured public opinion before and after a Supreme Court case debating whether or not states can legalize physician assisted suicide. They found that participants' tendency to obey authorities was not as important to public opinion polling numbers as religious and moral beliefs. Although prior research has demonstrated that the tendency to obey persists across settings, this finding suggests that at personal factors like religion and morality can limit how much people obey authority.

Both the Milgram and Stanford experiments were conducted in research settings. In 1966, psychiatrist Charles K. Hofling published the results of a field experiment on obedience in the nurse–physician relationship in its natural hospital setting. Nurses, unaware they were taking part in an experiment, were ordered by unknown doctors to administer dangerous doses of a (fictional) drug to their patients. Although several hospital rules disallowed administering the drug under the circumstances, 21 out of the 22 nurses would have given the patient an overdose.

Many traditional cultures regard obedience as a virtue; historically, societies have expected children to obey their elders (compare patriarchy or matriarchy), slaves their owners, serfs their lords in feudal society, lords their king, and everyone God. Even long after slavery ended in the United States, the Black codes required black people to obey and submit to whites, on pain of lynching. Compare the religious ideal of surrender and its importance in Islam (the word "Islam" can literally mean "surrender").

In some Christian weddings, obedience was formally included along with honor and love as part of a conventional bride's (but not the bridegroom's) wedding vow. This came under attack with women's suffrage and the feminist movement. the inclusion of a promise to obey in marriage vows has become optional in some denominations.

Some animals can easily be trained to be obedient by employing operant conditioning, for example obedience schools exist to condition dogs to obey the orders of human owners.

Learning to obey adult rules is a major part of the socialization process in childhood, and many techniques are used by adults to modify the behavior of children. Additionally, extensive training is given in armies to make soldiers capable of obeying orders in situations where an untrained person would not be willing to follow orders. Soldiers are initially ordered to do seemingly trivial things, such as picking up the sergeant's hat off the floor, marching in just the right position, or marching and standing in formation. The orders gradually become more demanding, until an order to the soldiers to place themselves into the midst of gunfire gets an instinctively obedient response.

When the Milgram experimenters were interviewing potential volunteers, the participant selection process itself revealed several factors that affected obedience, outside of the actual experiment.

Interviews for eligibility were conducted in an abandoned complex in Bridgeport, Connecticut. Despite the dilapidated state of the building, the researchers found that the presence of a Yale professor as stipulated in the advertisement affected the number of people who obeyed. This was not further researched to test obedience without a Yale professor because Milgram had not intentionally staged the interviews to discover factors that affected obedience. A similar conclusion was reached in the Stanford prison experiment.

In the actual experiment, prestige or the appearance of power was a direct factor in obedience—particularly the presence of men dressed in gray laboratory coats, which gave the impression of scholarship and achievement and was thought to be the main reason why people complied with administering what they thought was a painful or dangerous shock. A similar conclusion was reached in the Stanford prison experiment.

Raj Persaud, in an article in the BMJ, comments on Milgram's attention to detail in his experiment:
Despite the fact that prestige is often thought of as a separate factor, it is, in fact, merely a subset of power as a factor. Thus, the prestige conveyed by a Yale professor in a laboratory coat is only a manifestation of the experience and status associated with it and/or the social status afforded by such an image.

According to Milgram, "the essence of obedience consists in the fact that a person comes to view himself as the instrument for carrying out another person's wishes, and he therefore no longer sees himself as responsible for his actions. Once this critical shift of viewpoint has occurred in the person, all of the essential features of obedience follow." Thus, "the major problem for the subject is to recapture control of his own regnant processes once he has committed them to the purposes of the experimenter." Besides this hypothetical agentic state, Milgram proposed the existence of other factors accounting for the subject's obedience: politeness, awkwardness of withdrawal, absorption in the technical aspects of the task, the tendency to attribute impersonal quality to forces that are essentially human, a belief that the experiment served a desirable end, the sequential nature of the action, and anxiety.

Another explanation of Milgram's results invokes belief perseverance as the underlying cause. What "people cannot be counted on is to realize that a seemingly benevolent authority is in fact malevolent, even when they are faced with overwhelming evidence which suggests that this authority is indeed malevolent. Hence, the underlying cause for the subjects' striking conduct could well be conceptual, and not the alleged 'capacity of man to abandon his humanity ... as he merges his unique personality into larger institutional structures."'

In humans:
In animals:



</doc>
<doc id="474880" url="https://en.wikipedia.org/wiki?curid=474880" title="Hawthorne effect">
Hawthorne effect

The Hawthorne effect (also referred to as the observer effect) is a type of reactivity in which individuals modify an aspect of their behavior in response to their awareness of being observed. This can undermine the integrity of a research, particularly the relationships between variables. The original research at the Hawthorne Works in Cicero, Illinois, on lighting changes and work structure changes such as working hours and break times was originally interpreted by Elton Mayo and others to mean that paying attention to overall worker needs would improve productivity. Later interpretations such as that done by Landsberger suggested that the novelty of being research subjects and the increased attention from such could lead to temporary increases in workers' productivity. This interpretation was dubbed "the Hawthorne effect". It is also similar to a phenomenon that is referred to as novelty/disruption effect.

The term was coined in 1958 by Henry A. Landsberger when he was analyzing earlier experiments from 1924–32 at the Hawthorne Works (a Western Electric factory outside Chicago). The Hawthorne Works had commissioned a study to see if its workers would become more productive in higher or lower levels of light. The workers' productivity seemed to improve when changes were made, and slumped when the study ended. It was suggested that the productivity gain occurred as a result of the motivational effect on the workers of the interest being shown in them.

This effect was observed for minute increases in illumination. In these lighting studies, light intensity was altered to examine its effect on worker productivity. Most industrial/occupational psychology and organizational behavior textbooks refer to the illumination studies. Only occasionally are the rest of the studies mentioned.

Although illumination research of workplace lighting formed the basis of the Hawthorne effect, other changes such as maintaining clean work stations, clearing floors of obstacles, and even relocating workstations resulted in increased productivity for short periods. Thus the term is used to identify any type of short-lived increase in productivity.
In one of the studies, researchers chose two women as test subjects and asked them to choose four other workers to join the test group. Together the women worked in a separate room over the course of five years (1927–1932) assembling telephone relays.

Output was measured mechanically by counting how many finished relays each worker dropped down a chute. This measuring began in secret two weeks before moving the women to an experiment room and continued throughout the study. In the experiment room they had a supervisor who discussed changes with their productivity. Some of the variables were:


Changing a variable usually increased productivity, even if the variable was just a change back to the original condition. However it is said that this is the natural process of the human being adapting to the environment, without knowing the objective of the experiment occurring. Researchers concluded that the workers worked harder because they thought that they were being monitored individually.

Researchers hypothesized that choosing one's own coworkers, working as a group, being treated as special (as evidenced by working in a separate room), and having a sympathetic supervisor were the real reasons for the productivity increase. One interpretation, mainly due to Elton Mayo, was that "the six individuals became a team and the team gave itself wholeheartedly and spontaneously to cooperation in the experiment." (There was a second relay assembly test room study whose results were not as significant as the first experiment.)

The purpose of the next study was to find out how payment incentives would affect productivity. The surprising result was that productivity actually decreased. Workers apparently had become suspicious that their productivity may have been boosted to justify firing some of the workers later on. The study was conducted by Elton Mayo and W. Lloyd Warner between 1931 and 1932 on a group of fourteen men who put together telephone switching equipment. The researchers found that although the workers were paid according to individual productivity, productivity decreased because the men were afraid that the company would lower the base rate. Detailed observation of the men revealed the existence of informal groups or "cliques" within the formal groups. These cliques developed informal rules of behavior as well as mechanisms to enforce them. The cliques served to control group members and to manage bosses; when bosses asked questions, clique members gave the same responses, even if they were untrue. These results show that workers were more responsive to the social force of their peer groups than to the control and incentives of management.

Richard Nisbett has described the Hawthorne effect as "a glorified anecdote", saying that "once you have got the anecdote, you can throw away the data." Other researchers have attempted to explain the effects with various interpretations.

Adair warns of gross factual inaccuracy in most secondary publications on Hawthorne effect and that many studies failed to find it. He argues that it should be viewed as a variant of Orne's (1973) experimental demand effect. So for Adair, the issue is that an experimental effect depends on the participants' interpretation of the situation; this is why manipulation checks are important in social sciences experiments. So he thinks it is not awareness "per se", nor special attention "per se", but participants' interpretation that must be investigated in order to discover if/how the experimental conditions interact with the participants' goals. This can affect whether participants believe something, if they act on it or do not see it as in their interest, etc.

Possible explanations for the Hawthorne effect include the impact of feedback and motivation towards the experimenter. Receiving feedback on their performance may improve their skills when an experiment provides this feedback for the first time. Research on the demand effect also suggests that people may be motivated to please the experimenter, at least if it does not conflict with any other motive. They may also be suspicious of the purpose of the experimenter. Therefore, Hawthorne effect may only occur when there is usable feedback or a change in motivation.

Parsons defines the Hawthorne effect as "the confounding that occurs if experimenters fail to realize how the consequences of subjects' performance affect what subjects do" [i.e. learning effects, both permanent skill improvement and feedback-enabled adjustments to suit current goals]. His key argument is that in the studies where workers dropped their finished goods down chutes, the participants had access to the counters of their work rate.

Mayo contended that the effect was due to the workers reacting to the sympathy and interest of the observers. He does say that this experiment is about testing overall effect, not testing factors separately. He also discusses it not really as an experimenter effect but as a management effect: how management can make workers perform differently because they feel differently. A lot to do with feeling free, not feeling supervised but more in control as a group. The experimental manipulations were important in convincing the workers to feel this way: that conditions were really different. The experiment was repeated with similar effects on mica-splitting workers.

Clark and Sugrue in a review of educational research say that uncontrolled novelty effects cause on average 30% of a standard deviation (SD) rise (i.e. 50%–63% score rise), which decays to small level after 8 weeks. In more detail: 50% of a SD for up to 4 weeks; 30% of SD for 5–8 weeks; and 20% of SD for > 8 weeks, (which is < 1% of the variance).

Harry Braverman points out that the Hawthorne tests were based on industrial psychology and were investigating whether workers' performance could be predicted by pre-hire testing. The Hawthorne study showed "that the performance of workers had little relation to ability and in fact often bore an inverse relation to test scores...". Braverman argues that the studies really showed that the workplace was not "a system of bureaucratic formal organisation on the Weberian model, nor a system of informal group relations, as in the interpretation of Mayo and his followers but rather a system of power, of class antagonisms". This discovery was a blow to those hoping to apply the behavioral sciences to manipulate workers in the interest of management.

The economists Steven Levitt and John A. List long pursued without success a search for the base data of the original illumination experiments, before finding it in a microfilm at the University of Wisconsin in Milwaukee in 2011. Re-analysing it, they found slight evidence for the Hawthorn effect over the long-run, but in no way as drastic as suggested initially This finding supported the analysis of an article by S R G Jones in 1992 examining the relay experiments. Despite the absence of evidence for the Hawthorne Effect in the original study, List has said that he remains confident that the effect is genuine.

It is also possible that the illumination experiments can be explained by a longitudinal learning effect. Parsons has declined to analyse the illumination experiments, on the grounds that they have not been properly published and so he cannot get at details, whereas he had extensive personal communication with Roethlisberger and Dickson.

Evaluation of the Hawthorne effect continues in the present day. Despite the criticisms, however, the phenomenon is often taken into account when designing studies and their conclusions. Some have also developed ways to avoid it. For instance, there is the case of holding the observation when conducting a field study from a distance, from behind a barrier such as a two-way mirror or using an unobtrusive measure.

Various medical scientists have studied possible trial effect (clinical trial effect) in clinical trials. Some postulate that, beyond just attention and observation, there may be other factors involved, such as slightly better care; slightly better compliance/adherence; and selection bias. The latter may have several mechanisms: (1) Physicians may tend to recruit patients who seem to have better adherence potential and lesser likelihood of future loss to follow-up. (2) The inclusion/exclusion criteria of trials often exclude at least some comorbidities; although this is often necessary to prevent confounding, it also means that trials may tend to work with healthier patient subpopulations.

Despite the observer effect as popularized in the Hawthorne experiments being perhaps falsely identified (see above discussion), the popularity and plausibility of the observer effect in theory has led researchers to postulate that this effect could take place at a second level. Thus it has been proposed that there is a secondary observer effect when researchers working with secondary data such as survey data or various indicators may impact the results of their scientific research. Rather than having an effect on the subjects (as with the primary observer effect), the researchers likely have their own idiosyncrasies that influence how they handle the data and even what data they obtain from secondary sources. For one, the researchers may choose seemingly innocuous steps in their statistical analyses that end up causing significantly different results using the same data; e.g., weighting strategies, factor analytic techniques, or choice of estimation. In addition, researchers may use software packages that have different default settings that lead to small but significant fluctuations. Finally, the data that researchers use may not be identical, even though it seems so. For example, the OECD collects and distributes various socio-economic data; however, these data change over time such that a researcher who downloads the Australian GDP data for the year 2000 may have slightly different values than a researcher who downloads the same Australian GDP 2000 data a few years later. The idea of the secondary observer effect was floated by Nate Breznau in a thus far relatively obscure paper.

Although little attention has been paid to this phenomenon, the scientific implications are very large. Evidence of this effect may be seen in recent studies that assign a particular problem to a number of researchers or research teams who then work independently using the same data to try and find a solution. This is a process called crowdsourcing data analysis and was used in a groundbreaking study by Silberzahn, Rafael, Eric Uhlmann, Dan Martin and Brian Nosek et al. (2015) about red cards and player race in football (i.e., soccer).




</doc>
<doc id="1962537" url="https://en.wikipedia.org/wiki?curid=1962537" title="Attitude change">
Attitude change

Attitudes are associated beliefs and behaviors towards some object. They are not stable, and because of the communication and behavior of other people, are subject to change by social influences, as well as by the individual's motivation to maintain cognitive consistency when cognitive dissonance occurs—when two attitudes or attitude and behavior conflict. Attitudes and attitude objects are functions of affective and cognitive components. It has been suggested that the inter-structural composition of an associative network can be altered by the activation of a single node. Thus, by activating an affective or emotional node, attitude change may be possible, though affective and cognitive components tend to be intertwined.

There are three bases for attitude change: compliance, identification, and internalization. These three processes represent the different levels of attitude change.

Compliance refers to a change in behavior based on consequences, such as an individual's hopes to gain rewards or avoid punishment from another group or person. The individual does not necessarily experience changes in beliefs or evaluations of an attitude object, but rather is influenced by the social outcomes of adopting a change in behavior. 
The individual is also often aware that he or she is being urged to respond in a certain way.

Compliance was demonstrated through a series of laboratory experiments known as the Asch experiments. Experiments led by Solomon Asch of Swarthmore College asked groups of students to participate in a "vision test". In reality, all but one of the participants were confederates of the experimenter, and the study was really about how the remaining student would react to the confederates' behavior. Participants were asked to pick, out of three line options, the line that is the same length as a sample and were asked to give the answer out loud. Unbeknown to the participants, Asch had placed a number of confederates to deliberately give the wrong answer before the participant. The results showed that 75% of responses were in line with majority influence and were the same answers the confederates picked. Variations in the experiments showed that compliance rates increased as the number of confederates increased, and the plateau was reached with around 15 confederates. The likelihood of compliance dropped with minority opposition, even if only one confederate gave the correct answer. The basis for compliance is founded on the fundamental idea that people want to be accurate and right.

Identification explains one's change of beliefs and affect in order to be similar to someone one admires or likes. In this case, the individual adopts the new attitude, not due to the specific content of the attitude object, but because it is associated with the desired relationship. Often, children's attitudes on race, or their political party affiliations are adopted from their parents' attitudes and beliefs.

Internalization refers to the change in beliefs and affect when one finds the content of the attitude to be intrinsically rewarding, and thus leads to actual change in beliefs or evaluations of an attitude object. The new attitude or behavior is consistent with the individual's value system, and tends to be merged with the individual's existing values and beliefs. Therefore, behaviors adopted through internalization are due to the content of the attitude object.

The expectancy-value theory is based on internalization of attitude change. This model states that the behavior towards some object is a function of an individual's intent, which is a function of one's overall attitude towards the action.

Emotion plays a major role in persuasion, social influence, and attitude change. Much of attitude research has emphasised the importance of affective or emotion components. Emotion works hand-in-hand with the cognitive process, or the way we think, about an issue or situation. Emotional appeals are commonly found in advertising, health campaigns and political messages. Recent examples include no-smoking health campaigns (see tobacco advertising) and political campaigns emphasizing the fear of terrorism. Attitude change based on emotions can be seen vividly in serial killers who are faced with major stress. There is considerable empirical support for the idea that emotions in the form of fear arousal, empathy, or a positive mood can enhance attitude change under certain conditions.

Important factors that influence the impact of emotional appeals include self-efficacy, attitude accessibility, issue involvement, and message/source features. Attitudes that are central to one's being are highly resistant to change while others that are less fixed may change with new experiences or information. A new attitude (e.g. to time-keeping or absenteeism or quality) may challenge existing beliefs or norms so creating a feeling of psychological discomfort known as cognitive dissonance. It is difficult to measure attitude change since attitudes may only be inferred and there might be significant divergence between those publicly declared and privately held. Self-efficacy is a perception of one's own human agency; in other words, it is the perception of our own ability to deal with a situation. It is an important variable in emotional appeal messages because it dictates a person's ability to deal with both the emotion and the situation. For example, if a person is not self-efficacious about their ability to impact the global environment, they are not likely to change their attitude or behaviour about global warming.

Affective forecasting, otherwise known as intuition or the prediction of emotion, also impacts attitude change. Research suggests that predicting emotions is an important component of decision making, in addition to the cognitive processes. How we feel about an outcome may override purely cognitive rationales.
In terms of research methodology, the challenge for researchers is measuring emotion and subsequent impacts on attitude. Since we cannot see into the brain, various models and measurement tools have been constructed to obtain emotion and attitude information. Measures may include the use of physiological cues like facial expressions, vocal changes, and other body rate measures. For instance, fear is associated with raised eyebrows, increased heart rate and increased body tension. Other methods include concept or network mapping, and using primes or word cues.

Many dual process models are used to explain the affective (emotional) and cognitive processing and interpretations of messages, as well as the different depths of attitude change. These include the heuristic-systematic model of information processing and the elaboration likelihood model.

The heuristic-systematic model of information processing describes two depths in the processing of attitude change, systematic processing and heuristic processing. In this model information is either processed in a high-involvement and high-effort systematic way, or information is processed through shortcuts known as "heuristics". For example, emotions are affect-based heuristics, in which feelings and gut-feeling reactions are often used as shortcuts.

Systematic processing occurs when individuals are motivated and have high cognition to process a message. Individuals using systematic processing are motivated to pay attention and have the cognitive ability to think deeply about a message; they are persuaded by the content of the message, such as the strength or logic of the argument. Motivation can be determined by many factors, such as how personally relevant the topic is, and cognitive ability can be determined by how knowledgeable an individual is on the message topic, or whether or not there is a distraction in the room. Individuals who receive a message through systematic processing usually internalize the message, resulting in a longer and more stable attitude change.

According to the heuristic-systematic model of information processing, people are motivated to use systematic processing when they want to achieve a "desired level of confidence" in their judgments. There are factors that have been found to increase the use of systematic processing; these factors are associated with either decreasing an individual's actual confidence or increasing an individual's perceived confidence. These factors may include framing persuasive messages in an unexpected manner; self-relevancy of the message.

Systematic processing has been shown to be beneficial in social influence settings. Systematic reasoning has been shown to be successful in producing more valid solutions during group discussions and greater solution accuracy. Shestowsky's (1998) research in dyad discussions revealed that the individual in the dyad who had high motivation and high need in cognition had the greater impact on group decisions.

Heuristic processing occurs when individuals have low motivation and/or low cognitive ability to process a message. Instead of focusing on the argument of the message, recipients using heuristic processing focus on more readily accessible information and other unrelated cues, such as the authority or attractiveness of the speaker. Individuals who process a message through heuristic processing do not internalize the message, and thus any attitude change resulting from the persuasive message is temporary and unstable.

For example, people are more likely to grant favors if reasons are provided. A study shows that when people said, "Excuse me, I have five pages to xerox. May I use the copier?" they received a positive response of 60%. The statement, "Excuse me, I have five pages to xerox. I am in a rush. May I use the copier?" produced a 95% success rate.

Heuristic processing examples include social proof, reciprocity, authority, and liking.


The elaboration likelihood model is similar in concept to and shares many ideas with other dual processing models, such as the heuristic-systematic model of information processing. In the elaboration likelihood model, cognitive processing is the central route and affective/emotion processing is often associated with the peripheral route. The central route pertains to an elaborate cognitive processing of information while the peripheral route relies on cues or feelings. The ELM suggests that true attitude change only happens through the central processing route that incorporates both cognitive and affective components as opposed to the more heuristics-based peripheral route. This suggests that motivation through emotion alone will not result in an attitude change.

Cognitive dissonance, a theory originally developed by Festinger (1957), is the idea that people experience a sense of guilt or uneasiness when two linked cognitions are inconsistent, such as when there are two conflicting attitudes about a topic, or inconsistencies between one's attitude and behavior on a certain topic. The basic idea of the Cognitive Dissonance Theory relating to attitude change, is that people are motivated to reduce dissonance which can be achieved through changing their attitudes and beliefs. Cooper & Fazio's (1984) have also added that cognitive dissonance does not arise from any simple cognitive inconsistency, but rather results from freely chosen behavior that may bring about negative consequences. These negative consequences may be threats to the consistency, stability, predictability, competence, moral goodness of the self-concept, or violation of general self-integrity.

Research has suggested multiple routes that cognitive dissonance can be reduced. Self-affirmation has been shown to reduce dissonance, however it is not always the mode of choice when trying to reduce dissonance. When multiple routes are available, it has been found that people prefer to reduce dissonance by directly altering their attitudes and behaviors rather than through self-affirmation. People who have high levels of self-esteem, who are postulated to possess abilities to reduce dissonance by focusing on positive aspects of the self, have also been found to prefer modifying cognitions, such as attitudes and beliefs, over self-affirmation. A simple example of cognitive dissonance resulting in attitude change would be when a heavy smoker learns that his sister died young from lung cancer due to heavy smoking as well, this individual experiences conflicting cognitions: the desire to smoke, and the knowledge that smoking could lead to death and a desire not to die. In order to reduce dissonance, this smoker could change his behavior (i.e. stop smoking), change his attitude about smoking (i.e. smoking is harmful), or retain his original attitude about smoking and modify his new cognition to be consistent with the first one--"I also work out so smoking won't be harmful to me". Thus, attitude change is achieved when individuals experience feelings of uneasiness or guilt due to cognitive dissonance, and actively reduce the dissonance through changing their attitude, beliefs, or behavior relating in order to achieve consistency with the inconsistent cognitions.

Carl Hovland and his band of persuasion researchers learned a great deal during World War 2 and later at Yale about the process of attitude change.

The process of how people change their own attitudes has been studied for years. Belief rationalization has been recognized as an important aspect to understand this process. The stability of people's past attitudes can be influenced if they hold beliefs that are inconsistent with their own behaviors. The influence of past behavior on current attitudes is stable when little information conflicts with the behavior. Alternatively, people's attitudes may lean more radically toward the prior behavior if the conflict makes it difficult to ignore, and forces them to rationalize their past behavior. For example, if you believe that Greece is a beautiful country and that the people there are very hardworking, then your belief will be stable or even become reinforced when you visit the country and are amazed by its beauty. However, when you interact with local Greeks, you realize they are in actuality not as hard-working as you had imagined. This inconsistency becomes difficult to ignore once it stands out.

Attitudes are often restructured at the time people are asked to report them. As a result, inconsistencies between the information that enters into the reconstruction and the original attitudes can produce changes in prior attitudes, whereas consistency between these elements often elicits stability in prior attitudes. Individuals need to resolve the conflict between their own behaviors and the subsequent beliefs. However, people usually align themselves with their attitudes and beliefs instead of their behaviors. More importantly, this process of resolving people's cognitive conflicts that emerges cuts across both self-perception and dissonance even when the associated effect may only be strong in changing prior attitudes

Human judgment is comparative in nature. Departing from identifying people's need to justify their own beliefs in the context of their own behaviors, psychologists also believe that people have the need to carefully evaluate new messages on the basis of whether these messages support or contradict with prior messages, regardless of whether they can recall the prior messages after they reach a conclusion. This comparative processing mechanism is built on "information-integration theory" and "social judgement theory". Both of these theories have served to model people's attitude change in judging the new information while they haven't adequately explained the influential factors that motivate people to integrate the information.

More recent work in the area of persuasion has further explored this "comparative processing" from the perspective of focusing on comparing between different sets of information on one single issue or object instead of simply making comparisons among different issues or objects. As previous research demonstrated, analyzing information on one target product may trigger less impact of comparative information than comparing this product with the same product under competing brands.

When people compare different sets of information on one single issue or object, the effect of people's effort to compare new information with prior information seemed to correlate with the perceived strength of the new, strong information when considered jointly with the initial information. Comparison processes can be enhanced when prior evaluations, associated information, or both are accessible. People will simply construct a current judgment based on the new information or adjust the prior judgment when they are not able to retrieve the information from prior messages. The impact this comparative process can have on people's attitude change is mediated by changes in the strength of new information perceived by receivers. The effects of comparison on judgment change were mediated by changes in the perceived strength of the information. These findings above have wide range of applications in social marketing, political communication, and health promotion. For example, designing an advertisement that is counteractive against an existing attitude towards a behavior or policy is perhaps most effective if the advertisement uses the same format, characters, or music of ads associated with the initial attitudes.



</doc>
<doc id="674639" url="https://en.wikipedia.org/wiki?curid=674639" title="Neophobia">
Neophobia

Neophobia is the fear of anything new, especially a persistent and abnormal fear. In its milder form, it can manifest as the unwillingness to try new things or break from routine. In the context of children the term is generally used to indicate a tendency to reject unknown or novel foods. Food neophobia, as it may be referred to, is an important concern in pediatric psychology.

In biomedical research, neophobia is often associated with the study of taste.

The word "neophobia" comes from the Greek νέος, "neos", meaning "new, young", and φόβος, "phobos", for "fear". "Cainophobia" comes from the Greek καινός, "kainos", meaning "new, fresh". Alternative terms for neophobia include metathesiophobia, prosophobia, cainotophobia (or cainophobia), and kainophobia (or kainolophobia).

Norway rats and house mice are thought to have evolved increased levels of neophobia as they became commensal with humans because humans were routinely devising new methods (e.g., mousetraps) to eradicate them.

Neophobia is also a common finding in aging animals, although apathy could also explain, or contribute to explain, the lack of exploratory drive systematically observed in aging. Researchers argued that the lack of exploratory drive was likely due neurophysiologically to the dysfunction of neural pathways connected to the prefrontal cortex observed during aging.

Robert Anton Wilson theorized in his book "Prometheus Rising" that neophobia is instinctual in people after they begin to raise children. Wilson's views on neophobia are mostly negative, believing that it is the reason human culture and ideas do not advance as quickly as our technology. His model includes an idea from Thomas Kuhn's "The Structure of Scientific Revolutions", which is that new ideas, however well proven and evident, are implemented only when the generations who consider them "new" die and are replaced by generations who consider the ideas accepted and old.

Food neophobia in humans has been described as the fear of eating new or unfamiliar foods. It differs from selective eating disorder. Food neophobia is particularly common in toddlers and young children. It is often related to an individual’s level of sensation-seeking, meaning a person's willingness to try new things and take risks. Not only do people with high food neophobia resist trying new food, they also rate new foods that they do try as lower than neophilics.

It is very typical for people to generally have a fear of new things and to prefer things that are familiar and common. Most people experience food neophobia to a certain extent, though some people are more neophobic than others. A measure of individual differences in food neophobia is the Food Neophobia Scale (FNS), which consists of a 10-item survey that requires self-reported responses on a seven-point Likert scale. There is also a separate scale geared towards children called the Food Neophobia Scale for Children (FNSC), in which the parents actually do the reporting for the survey.

In animals it has been shown that food neophobia is a fear of novelty lasting only a short duration (minutes at most), which is distinct from dietary conservatism, the prolonged refusal to add a novel food to the diet, which can last many days or even years. Dietary conservatism has never yet been demonstrated in humans, although the genetically influenced behaviour of "fussy eating" in children resembles the behaviour seen in animals.

Food neophobia relates to the omnivore's dilemma, a phenomenon that explains the choice that omnivores, and humans in particular, have between eating a new food and risking danger or avoiding it and potentially missing out on a valuable food source. Having at least some degree of food neophobia has been noted to be evolutionarily advantageous as it can help people to avoid eating potentially poisonous foods.

Genetics seem to play a role in both food neophobia and general neophobia. Research shows that about two-thirds of the variation in food neophobia is due to genetics. A study done on twin pairs showed an even higher correlation, indicating that genetics do play a factor in food neophobia.

Psychosocial factors can also increase a child's chances of developing food neophobia. Young children carefully watch parental food preferences, and this may produce neophobic tendencies with regard to eating if parents tend to avoid some foods.

Another cause includes being more sensitive than average to bitter tastes, which may be associated with a significant history of middle ear infection or an increased perception of bitter foods, known as a supertaster.

Sometimes food neophobia is more directly caused by an environmental occurrence. For example, with poison-induced neophobia, a food-poisoning experience can lead to people not only avoiding the flavor(s) they associate with creating their illness but also avoiding all novel flavors during the period directly following the poisoning experience. This can be seen as the body’s attempt to prevent any new and risky food items from entering the body.

Besides food poisoning, food neophobia also arises from the person associating a negative experience with new foods, for example suffering from gastroenteritis or other gastrointestinal illnesses after eating undercooked food.

Another environmental factor influencing levels of food neophobia is the current arousal level of the individual. Trying a new food is an arousing experience, and if the person prefers to maintain a lower arousal level in general, then he or she might avoid new foods as a method of managing his or her current arousal level. Also, if people are currently experiencing situations with a lot of novelty and are therefore more aroused, they might be reluctant to try new foods as doing so would increase their arousal level to an uncomfortable level. This example can help explain why Americans visiting a foreign country might be less likely to try a new food item and instead gravitate towards the familiar McDonald’s food.

Some efforts to address this situation, such as pressuring the child to eat a disliked food or threatening punishment for not eating it, tend to exacerbate the problem.

Effective solutions include offering non-food rewards, such as a small sticker, for tasting a new or disliked food, and for parents to model the behavior they want to see by cheerfully eating the new or disliked foods in front of the children.

Exposing someone to a new food increases the chances of liking that food item. However, it is not enough to merely look at a new food. Novel food must be repeatedly tasted in order to increase preference for eating it. It can take as many as 15 tries of a novel food item before a child accepts it. There also appears to be a critical period for lowering later food neophobia in children during the weaning process. The variety of solid foods first exposed to children can lower later food refusal. Some researchers believe that even the food variety of a nursing mother and the consequent variety of flavors in her breastmilk can lead to greater acceptance of novel food items later on in life. Food neophobia does tend to naturally decrease as people age.



</doc>
<doc id="236895" url="https://en.wikipedia.org/wiki?curid=236895" title="Negative capability">
Negative capability

Negative capability was a phrase first used by Romantic poet John Keats in 1817 to characterise the capacity of the greatest writers (particularly Shakespeare) to pursue a vision of artistic beauty even when it leads them into intellectual confusion and uncertainty, as opposed to a preference for philosophical certainty over artistic beauty. The term has been used by poets and philosophers to describe the ability of the individual to perceive, think, and operate beyond any presupposition of a predetermined capacity of the human being.

Keats used the phrase only briefly in a private letter, and it became known only after his correspondence was collected and published. In a letter to his brothers, George and Thomas, on 22 December 1817, Keats described a conversation he had been engaged in a few days previously: 
I had not a dispute but a disquisition with Dilke, upon various subjects; several things dove-tailed in my mind, and at once it struck me what quality went to form a Man of Achievement, especially in Literature, and which Shakespeare possessed so enormously—I mean Negative Capability, that is, when a man is capable of being in uncertainties, mysteries, doubts, without any irritable reaching after fact and reason—Coleridge, for instance, would let go by a fine isolated verisimilitude caught from the Penetralium of mystery, from being incapable of remaining content with half-knowledge. This pursued through volumes would perhaps take us no further than this, that with a great poet the sense of Beauty overcomes every other consideration, or rather obliterates all consideration.
Samuel Taylor Coleridge was, by 1817, a frequent target of criticism by the younger poets of Keats's generation, often ridiculed for his infatuation with German idealistic philosophy. Against Coleridge's obsession with philosophical truth, Keats sets up the model of Shakespeare, whose poetry articulated various points of view and never advocated a particular vision of truth.

Keats's ideas here, as was usually the case in his letters, were expressed tersely with no effort to fully expound what he meant, but passages from other letters enlarge on the same theme. In a letter to J.H. Reynolds in February, 1818, he wrote: 
We hate poetry that has a palpable design upon us—and if we do not agree, seems to put its hand in its breeches pocket. Poetry should be great & unobtrusive, a thing which enters into one's soul, and does not startle it or amaze it with itself but with its subject.
In another letter to Reynolds the following May, he contrived the metaphor of 'the chamber of maiden thought' and the notion of the 'burden of mystery', which together express much the same idea as that of negative capability:
I compare human life to a large Mansion of Many Apartments, two of which I can only describe, the doors of the rest being as yet shut upon me—The first we step into we call the infant or thoughtless Chamber, in which we remain as long as we do not think—We remain there a long while, and notwithstanding the doors of the second Chamber remain wide open, showing a bright appearance, we care not to hasten to it; but are at length imperceptibly impelled by the awakening of the thinking principle—within us—we no sooner get into the second Chamber, which I shall call the Chamber of Maiden-Thought, than we become intoxicated with the light and the atmosphere, we see nothing but pleasant wonders, and think of delaying there for ever in delight: However among the effects this breathing is father of is that tremendous one of sharpening one's vision into the heart and nature of Man—of convincing ones nerves that the World is full of Misery and Heartbreak, Pain, Sickness, and oppression—whereby This Chamber of Maiden Thought becomes gradually darken'd and at the same time on all sides of it many doors are set open—but all dark—all leading to dark passages—We see not the balance of good and evil. We are in a Mist—We are now in that state—We feel the 'burden of the Mystery,' To this point was Wordsworth come, as far as I can conceive when he wrote 'Tintern Abbey' and it seems to me that his Genius is explorative of those dark Passages. Now if we live, and go on thinking, we too shall explore them. he is a Genius and superior to us, in so far as he can, more than we, make discoveries, and shed a light in them—Here I must think Wordsworth is deeper than Milton[.]
Keats understood Coleridge as searching for a single, higher-order truth or solution to the mysteries of the natural world. He went on to find the same fault in Dilke and Wordsworth. All these poets, he claimed, lacked objectivity and universality in their view of the human condition and the natural world. In each case, Keats found a mind which was a narrow private path, not a "thoroughfare for all thoughts". Lacking for Keats were the central and indispensable qualities requisite for flexibility and openness to the world, or what he referred to as negative capability.

This concept of Negative Capability is precisely a rejection of set philosophies and preconceived systems of nature. He demanded that the poet be receptive rather than searching for fact or reason, and to not seek absolute knowledge of every truth, mystery, or doubt.

It is not known why Keats settled on the phrase 'negative capability', but some scholars have hypothesized that Keats was influenced in his studies of medicine and chemistry, and that it refers to the negative pole of an electric current which is passive and receptive. In the same way that the negative pole receives the current from the positive pole, the poet receives impulses from a world that is full of mystery and doubt, which cannot be explained but which the poet can translate into art.

Roberto Unger appropriated Keats' term in order to explain resistance to rigid social divisions and hierarchies. For Unger, "negative capability" is the "denial of whatever in our contexts delivers us over to a fixed scheme of division and hierarchy and to an enforced choice between routine and rebellion." It is thus through "negative capability" that we can further empower ourselves against social and institutional constraints, and loosen the bonds that entrap us in a certain social station.

An example of negative capability can be seen at work in industrial innovation. In order to create an innovator's advantage and develop new forms of economic enterprise, the modern industrialist could not just become more efficient with surplus extraction based on pre-existing work roles, but rather needed to invent new styles of flexible labor, expertise, and capital management. The industrialist needed to bring people together in new and innovative ways and redefine work roles and workplace organization. The modern factory had to at once stabilize its productive environment by inventing new restraints upon labor, such as length of the work day and division of tasks, but at the same time could not be too severe or risk being at a disadvantage to competitors, e.g. not being able to shift production tasks or capacity. Those industrialists and managers who were able to break old forms of organizational arrangements exercised negative capability.

This thesis of "negative capability" is a key component in Unger's theory of false necessity and formative context. The theory of false necessity claims that our social worlds are the artifact of our own human endeavors. There is no pre-set institutional arrangement that our societies adhere to, and there is no necessary historical mold of development that they will follow. Rather we are free to choose and develop the forms and the paths that our societies will take through a process of conflicts and resolutions. However, there are groups of institutional arrangements that work together to bring out certain institutional forms, liberal democracy, for example. These forms are the basis of a social structure, and which Unger calls formative contexts. In order to explain how we move from one formative context to another without the conventional social theory constraints of historical necessity (e.g. feudalism to capitalism), and to do so while remaining true to the key insight of individual human empowerment and anti-necessitarian social thought, Unger recognized that there are an infinite number of ways of resisting social and institutional constraints, which can lead to an infinite number of outcomes. This variety of forms of resistance and empowerment (i.e. negative capability) make change possible.

This thesis of "negative capability" addresses the problem of agency in relation to structure. It recognizes the constraints of structure and its molding influence upon the individual, but at the same time finds the individual able to resist, deny, and transcend their context. Unlike other theories of structure and agency, "negative capability" does not reduce the individual to a simple actor possessing only the dual capacity of compliance or rebellion, but rather sees him as able to partake in a variety of activities of self empowerment.

The twentieth-century British psychoanalyst Wilfred Bion elaborated on Keats's term to illustrate an attitude of openness of mind which he considered of central importance, not only in the psychoanalytic session, but in life itself. For Bion, negative capability was the ability to tolerate the pain and confusion of not knowing, rather than imposing ready-made or omnipotent certainties upon an ambiguous situation or emotional challenge. His idea has been taken up more widely in the British Independent School, as well as elsewhere in psychoanalysis and psychotherapy.

The notion of negative capability has been associated with the Zen philosophy. Keats' man of negative capability had qualities that enabled him to "lose his self-identity, his 'imaginative identification' with and submission to things, and his power to achieve a unity with life". The Zen concept of satori is the outcome of passivity and receptivity, culminating in "sudden insight into the character of the real". Satori is reached without deliberate striving. The antecedent stages to satori: quest, search, ripening and explosion. The "quest" stage is accompanied by a strong feeling of uneasiness, resembling the capacity to practice negative capability while the mind is in a state of "uncertainties, mysteries and doubts". In the explosive stage (akin to Keats' 'chief intensity'), a man of negative capability effects a "fellowship with essence".

Stanley Fish has expressed strong reservations about the attempt to apply the concept of negative capability to social contexts. He has written in critique of Unger's early work as being unable to chart a route for the idea to pass into reality, which leaves history closed and the individual holding onto the concept while kicking against air. Fish finds the capability Unger invokes in his early works unimaginable and unmanufacturable that can only be expressed outright in blatant speech, or obliquely in concept. More generally, Fish finds the idea of radical culture as an oppositional ideal in which context is continuously refined or rejected impracticable at best, and impossible at worst. Unger has addressed these criticisms by developing a full theory of historical process in which negative capability is employed.



</doc>
<doc id="4296386" url="https://en.wikipedia.org/wiki?curid=4296386" title="Cultural divide">
Cultural divide

A cultural divide is "a boundary in society that separates communities whose social economic structures, opportunities for success, conventions, styles, are so different that they have substantially different psychologies". A cultural divide is the virtual barrier caused by cultural differences, that hinder interactions, and harmonious exchange between people of different cultures. For example, avoiding eye contact with a superior shows deference and respect in East Asian cultures, but can be interpreted as suspicious behavior in Western cultures. Studies on cultural divide usually focus on identifying and bridging the cultural divide at different levels of society.

A cultural divide can have significant impact on international operations on global organizations that require communication between people from different cultures. Commonly, ignorance of the cultural differences such as social norms and taboos may lead to communication failure within the organization.

Sufficiently large cultural divides may also discourage groups from seeking to understand the other party's point of view, as differences between the groups are seen as immutable. Such gaps may in turn inhibit efforts made to reach a consensus between these groups.

Internal causes of Cultural Divide refer to causes based on innate or personal characteristics of an individual, such as a personal way of thinking, an internal mental structure or habit that influences how a person acts.

Rules, norms and way of thinking are often inculcated since young and these help to shape a person’s mindset and their thinking style, which will explain how two different cultural groups can view the same thing very differently. For example, Western cultures with their history of Judeo-Christian belief in the individual soul and focus on the pursuit of individual rights tend to adopt an individualistic mindset whereas East Asian cultures with a history of teachings based on Confucianism tend to view the individual as a relation to the larger community and hence develop a more collectivist mindset. Hence, it is more common for people in collectivist cultures to make an external attribution while people in individualistic cultures making an internal attribution. Thus, these differences can cause how people, situations or objects are perceived differently.

Perceptions about an out-group or of a different culture may tend to be perpetuated and reinforced by the media or long-standing notions of stereotypes. As a result of using schemas to simplify the world as we look at it, we rely on a set of well-established stereotypes available in our own culture to define and view the out-group. As such, the risk of stereotypes is if it is inaccurate and blinds us to certain key understanding of a certain class of people, and as stereotypes tend to persist even with new information, the problem of cultural divide can be perpetuated.

The social identity theory implies an inherent and inclined favoritism towards people of the same social group as you or people who share similar characteristics, also known as the in-group favoritism. This desire to achieve and maintain a positive self-image motivates people to place their own group in a superior position as compared to the out-group.

Cultural divide can also be caused by external influences that shape the way an individual thinks about people from other cultures. For example, the cultural disconnect and misunderstandings between USA and the Arab countries has been attributed to the spread of superficial information that "serve to promote self-interests and perpetuate reckless acts of individuals, misguided official policies and irresponsible public narratives, all colored by self-righteousness and hypocrisy". An individual’s experience of foreign cultures can be largely shaped by the information available to the individual and the cultural divide arises due to the difference between a culture and how it is perceived by people foreign to the culture.

Some examples of external sources that influence views on other cultures include:

This also includes any official source of information by the government such as speeches by government officials. Government attitudes to foreign governments often lead to information released to citizens that influence the way they think about foreign governments and foreign peoples. One extreme example of this propaganda.

Media bias can cause misunderstandings and cultural divide by controlling the information and perceptions of other cultures. For example, media bias in the United States can exacerbate the political divide between the liberals and the conservatives.

Due to a fundamental need for social companionship and a desire to be accepted and liked by others, people often conform to social norms and adopt the group’s beliefs and values. Hence, groups that are already culturally divided will tend to remain that way as the effect of normative social influence is self-perpetuating.

When a cultural divide can be bridged, it can be beneficial for all parties. However, when cultures are vastly different, or if people are opposed to such exchange, the cultural divide may prove difficult to cross.

Being aware of cultural boundaries when dealing with others is important to avoid accidentally offending the other party and turning the difference into a divide. Educating both parties in the reasons behind these boundaries would also help foster trust and cooperation between them. This also has a side effect of creating a virtuous cycle, where the improved understanding between both parties grants them an advantage when dealing with members of the opposite culture, encouraging future communication and reducing the impact of a cultural divide.

Developing high cultural intelligence increases one’s openness and hardiness when dealing with major differences in culture. Improving one’s openness requires both humility when learning from others and inquisitiveness in actively pursuing opportunities to develop one’s cultural awareness. Strong hardiness allows one to deal better with stress, cultural shocks and tension when interacting with others in a foreign context.

Increasing interaction between two groups of people will help increase mutual understanding and fill in any gaps in knowledge of another group's culture. However, the quality of the interaction and not just mere contact is key to bridging cultural divide too, as supported by the contact hypothesis whereby certain key components (such as a common goal and equal status) are required before such stereotypes and preconceived mindsets which might have already been deeply entrenched can be changed.

An example of culture divide is that of western Europe and eastern Europe. While western Europe appears to be mostly Catholic
eastern Europe is orthodox. Eastern Europe is poorer than western Europe and while the west is part of the EU the east seem to
avoid the union.



</doc>
<doc id="14946407" url="https://en.wikipedia.org/wiki?curid=14946407" title="The Lucifer Principle">
The Lucifer Principle

The Lucifer Principle is a 1995 book by Howard Bloom, in which the author argues that social groups, not individuals, are the primary "unit of selection" on genes and human psychological development. He states that both competition between groups and competition between individuals shape the evolution of the genome. Bloom "explores the intricate relationships among genetics, human behavior, and culture" and argues that "evil is a by-product of nature's strategies for creation and that it is woven into our most basic biological fabric". It sees selection (i.e. through violent competition) as central to the creation of the 'superorganism' of society. It also focuses on competition between individuals for position in the 'pecking order' and competition between groups for standing in pecking orders of groups. The Lucifer Principle shows how ideas are vital in creating cohesion and cooperation in these pecking order battles. Says "The Lucifer Principle": "Superorganism, ideas and the pecking order...these are the primary forces behind much of human creativity and earthly good."

Reviews of the book saw it as 'ambitious' and 'disturbing' in its conclusions that societies based on individual freedom might succumb to systems such as bureaucratic Communism or Islamic fundamentalism. "The Washington Post" said that "Readers will be mesmerized by the mirror Bloom holds to the human condition... He draws on a dozen years of research into a jungle of scholarly fields...and meticulously supports every bit of information..." while Chet Raymo in the "Boston Globe" termed it "a string of rhetorical firecrackers that challenge our many forms of self-righteousness".

Bloom later wrote that he and his publisher had been threatened by Islamic groups who objected to aspects of the book. He claimed that "Arab pressure groups asked ever so politely that "The Lucifer Principle" be withdrawn from print and that nothing that I write be published again. They offered to boycott my publisher's products—all of them—worldwide. And they backed their warning with a call for my punishment in seventeen Islamic countries." Bloom states that the attorney for the Authors Guild wrote to his publishers, warning of an author boycott if the book was pulled from the shelves. The publishers asked Bloom to rewrite a chapter on Islamic violence, which led to the creation of 358 lines of footnotes attesting to the facts he presented within it, that what Bloom wrote about Islam in "The Lucifer Principle" is based on expertise. 

Bloom is a frequent guest on Iran's Press TV, Iran's Al-Alam TV, Saudi Arabia's KSA-2 TV, and on Syria's Alikhbariya TV.



</doc>
<doc id="54863717" url="https://en.wikipedia.org/wiki?curid=54863717" title="Brodie's Law (act)">
Brodie's Law (act)


</doc>
<doc id="1791712" url="https://en.wikipedia.org/wiki?curid=1791712" title="Normality (behavior)">
Normality (behavior)

Normality is a behavior that can be normal for an individual (intrapersonal normality) when it is consistent with the most common behaviour for that person. Normal is also used to describe individual behaviour that conforms to the most common behaviour in society (known as conformity). Definitions of normality vary by person, time, place, and situation – it changes along with changing societal standards and norms. Normal behavior is often only recognized in contrast to abnormality. In its simplest form, normality is seen as good while abnormality is seen as bad. Someone being seen as normal or not normal can have social ramifications, such as being included, excluded or stigmatized by larger society.

Normality has been functionally and differentially defined by a vast number of disciplines, so there is not one single definition.

In general, 'normal' refers to a lack of significant deviation from the average. The word normal is used in a more narrow sense in mathematics, where a normal distribution describes a population whose characteristics centers around the average or the norm. When looking at a specific behaviour, such as the frequency of lying, a researcher may use a Gaussian bell curve to plot all reactions, and a normal reaction would be within one standard deviation, or the most average 68.3%. However, this mathematical model only holds for one particular trait at a time, since, for example, the probability of a single individual being within one standard deviation for 36 independent variables would be one in a million. In statistics, normal is often arbitrarily considered anything that falls within about 1.96 standard deviations of the mean, or the most average 95% (see 1.96). The probability of an individual being within 1.96 standard deviations for 269 independent variables is approximately one in a million. For only 59 independent variables, the probability is just under 5%. Under this definition of normal, it is abnormal to be normal for 59 independent variables.

The French sociologist Émile Durkheim indicated in his "Rules of the Sociological Method" that it was necessary for the sociological method to offer parameters to distinguish normality from pathology or abnormality. He suggested that behaviors or "social facts" which are present in the majority of cases are normal, and exceptions to that behavior indicate pathology. Durkheim's model of normality further explained that the most frequent or general behaviors, and thus the most normal behaviors, will persist through transition periods in society. Crime, for instance, exists under every society through every time period, and so should be considered normal. There is a two-fold version of normality; behaviors considered normal on a societal level may still be considered pathological on an individual level. On the individual level, people who violate social norms, such as criminals, will invite a punishment from others in the society.

Individuals' behaviours are guided by what they perceive to be society's expectations and their peers' norms. People measure the appropriateness of their actions by how far away they are from those social norms. However, what is perceived as the norm may or may not actually be the most common behaviour. In some cases of pluralistic ignorance, most people wrongly believe the social norm is one thing, but in fact very few people hold that view.

When people are made more aware of a social norm, particularly a descriptive norm (a norm describing what is done), their behaviour changes to become closer to that norm. The power of these norms can be harnessed by social norms marketing, where the social norm is advertised to people in an attempt to stop extreme behaviour, such as binge drinking. However, people at the other extreme (very little alcohol consumption) are equally likely to change their behaviour to become closer to the norm, in this case by increasing alcohol consumption. Instead of using descriptive norms, more effective social norms marketing may use injunctive norms. Instead of describing what behaviour is most commonly done, an injunctive norm is what is approved or disapproved of by society. When individuals become aware of the injunctive norm, only the extremes will change their behaviour (by decreasing alcohol consumption) without the boomerang effect of under-indulgers increasing their drinking.

The social norms that guide people are not always normal for everyone. Behaviours that are abnormal for most people may be considered normal for a subgroup or subculture. For example, normal college student behaviour may be to party and drink alcohol, but for a subculture of religious students, normal behaviour may be to go to church and pursue religion related activities. Subcultures may actively reject "normal" behaviour, instead replacing society norms with their own.

A disharmony exists between a virtual identity of the self and a real social identity, whether it be in the form of a trait or attribute. If a person does not have this disharmony, then he or she is described as normal. A virtual identity can take many definitions, but in this case a virtual identity is the identity that persons mentally create that conforms to societal standards and norms, it may not represent how they actually are, but it represents what they believe is the typical "normal" person. A real social identity is the identity that persons actually have in their society or is perceived, by themselves or others, to have. If these two identities have differences between each other, there is said to be disharmony. Individuals may monitor and adapt their behaviour in terms of others' expected perceptions of the individual, which is described by the social psychology theory of self-presentation. In this sense, normality exists based on societal norms, and whether someone is normal is entirely up to how he or she views him- or herself in contrast to how society views him or her. While trying to define and quantify normality is a good start, all definitions confront the problem of whether we are even describing an idea that even exists since there are so many different ways of viewing the concept.

Many difficulties arise in measuring normal behaviors – biologists come across parallel issues when defining normality. One complication which arises regards whether 'normality' is used correctly in everyday language. People say "This heart is abnormal" if only a portion of it is not working correctly, yet it may be inaccurate to include the entirety of the heart under the abnormal description. There can be a difference between the normality of the structure and function of a body part. Similarly, a behavioural pattern may not conform to social norms, but still be effective and non-problematic for that individual. Where there is a dichotomy between appearance and function of a behaviour, it may be difficult to measure its normality. This is applicable when trying to diagnose a pathology and is addressed in the DSM.

What is viewed as normal can change dependent on both timeframe and environment. Normality can be viewed as "an endless process of man's self-creation and his reshaping of the world". Within this idea, it is possible to surmise that normality is not an all-encompassing term, but simply a relative term based around a current trend in time. With statistics, this is likened to the thought that if the data gathered provides a mean and standard deviation, over time these data that predict "normalness" start to predict or dictate it less and less since the social idea of normality is dynamic. This is shown in studies done on behavior in psychology and sociology where behavior in mating rituals or religious rituals can change within a century in humans, showing that the "normal" way that these rituals are performed shift and a new procedure becomes the normal one.

As another example, understandings of what is normal sexual behaviour varies greatly across time and place. In many countries, perceptions on sexuality are largely becoming more liberal, especially views on the normality of masturbation and homosexuality. Social understanding on normal sexual behaviour also varies greatly country by country – countries can be divided into categories of how they approach sexual normality, as conservative, homosexual-permissive, or liberal. The United States, Ireland, and Poland have more conservative social understanding of sexuality among university students, while Scandinavian students consider a wider variety of sexual acts as normal. Although some attempts have been made to define sexual acts as normal, abnormal, or indeterminate, these definitions are time-sensitive. Gayle Rubin's 1980s model of sexual 'normality' was comprehensive at the time but has since become outdated as society has liberalized.

Since normality shifts in time and environment, the mean and standard deviation are only useful for describing normality from the environment from which they are collected.

Most definitions of normality consider interpersonal normality, the comparison between many different individual's behaviours to distinguish normality from abnormality. Intrapersonal normality looks at what is normal behaviour for one particular person (consistency within a person) and would be expected to vary person-to-person. A mathematical model of normality could still be used for intrapersonal normality, by taking a sample of many different occurrences of behaviour from one person over time. Also like interpersonal normality, intrapersonal normality may change over time, due to changes in the individual as they age and due to changes in society (since society's view of normality influences individual peoples' behaviour).

It is most comfortable for people to engage in behaviour which conforms to their own personal habitual norms. When things go wrong, people are more likely to attribute the negative outcome on any abnormal behaviour leading up to the mishap. After a car crash, people may say "if only I didn't leave work early", blaming the crash on their actions which were not normal. This counterfactual thinking particularly associates abnormal behaviour with negative outcomes.

In medicine, behavioral normality pertains to a patient's mental condition aligning with that of a model, healthy patient. A person without any mental illness is considered a normal patient, whereas a person with a mental disability or illness is viewed as abnormal. These normals and abnormals in the context of mental health subsequently create negative stigmatic perceptions towards individuals with mental illness. The Brain & Behavior Research Foundation stated that "an estimated 26.2 percent of Americans ages 18 and older – about 1 in 4 adults – suffer from one or more of (several) disorders in a given year". Though the population of American individuals living with mental illness is not as small of a minority as commonly perceived, it is considered abnormal nonetheless, therefore the subject of discrimination and abuse such as violent therapies, punishments, or labeling for life by the normal, healthy majority. The CDC reported that "cluster[s] of negative attitudes and beliefs motivate the general public to fear, reject, avoid, and discriminate against people with mental illnesses". In continuum, the resources available to those who suffer from such illness are limited, and government support is constantly being cut from programs that help individuals living with mental illness live more comfortable, accommodative, happier lives.

Hebbian associative learning and memory maintenance depends on synaptic normalization mechanisms to prevent synaptic runaway. Where synaptic runaway describes overcrowding of dendritic associations, which reduce sensory or behavioural acuteness proportional to the level of synaptic runaway. Synaptic/neuronal normalization refers to synaptic competition, where the prosper of one synapse may weakening the efficacy of other nearby surrounding synapses with redundant neurotransmission.

Animal dendritic density greatly increases throughout waking hours despite intrinsic normalization mechanisms as described as above. The growth rate of synaptic density is not sustained in a cumulative fashion. Without a pruning state, the signal to noise ratio of CNS mechanism would not be able to operate with maximum effectiveness, and learning would be detrimental to animal survival. Neuronal and synaptic normalization mechanisms must operate so positive association feedback loops to not become rampant while constantly processing new environmental information.

Some researchers speculate that the slow oscillation (nREM) cycles of animal sleep constitute an essential 're-normalization' phase. The re-normalization occurs from cortical large amplitude brain rhythm, in the low delta range (0.5–2 Hz), synaptically downscaling the associations from the wakeful learning state. Only the strongest associations survive the pruning from this phase. This allows retention of salient information coding from the previous day, but also allows more cortical space and energy distribution to continue effective learning subsequently after a slow-wave oscillation episode of sleep.

Also, organisms tend to have a normal biological developmental pathway as a central nervous system ages and/or learns. Deviations for a species' normal development frequently will result in behaviour dysfunction, or death, of that organism.

When people do not conform to the normal standard, they are often labelled as sick, disabled, abnormal, or unusual, which can lead to marginalization or stigmatization. Most people want to be normal and strive to be perceived as such, so that they can relate to society at large. Without having things in common with the general population, people may feel isolated among society. The abnormal person feels like they have less in common with the normal population, and others have difficulty relating to things that they have not experienced themselves. Additionally, abnormality may make others uncomfortable, further separating the abnormally labelled individual.

Since being normal is generally considered an ideal, there is often pressure from external sources to conform to normality, as well as pressure from people's intrinsic desire to feel included. For example, families and the medical community will try to help disabled people live a normal life. However, the pressure to appear normal, while actually having some deviation, creates a conflict – sometimes someone will appear normal, while actually experiencing the world differently or struggling. When abnormality makes society feel uncomfortable, it is the exceptional person themselves who will laugh it off to relieve social tension. A disabled person is given normal freedoms, but may not be able to show negative emotions. Lastly, society's rejection of deviance and the pressure to normalize may cause shame in some individuals. Abnormalities may not be included in an individual's sense of identity, especially if they are unwelcome abnormalities.

When an individual's abnormality is labelled as a pathology, it is possible for that person to take on both elements of the sick role or the stigmatization that follows some illnesses. Mental illness, in particular, is largely misunderstood by the population and often overwhelms others' impression of the patient.

Applying normality clinically depends on the field and situation a practitioner is in. In the broadest sense, clinical normality is the idea of uniformity of physical and psychological functioning across individuals. Normality, and abnormality, can be characterized statistically.
Related to the previous definition, statistically normality is usually defined it in terms of a normal distribution curve, with the so-called 'normal zone' commonly accounting for 95.45% percent of all the data. The remaining 4.55% will lie split outside of two standard deviations from the mean. Thus any variable case that lies outside of two deviations from the mean would be considered abnormal. However, the critical value of such statistical judgments may be subjectively altered to a less conservative estimate. It is in fact normal for a population to have a proportion of abnormals. The presence of abnormals is important because it is necessary to define what 'normal' is, as normality is a relative concept. So at a group, or macro level, of analysis; abnormalities are normal given a demographic survey, but at an individual level abnormal individuals are seen as being deviant in someway that needs to be corrected.
Statistical normality is important in determining demographic pathologies. When a variable rate, such as virus spread within a human population, exceeds its normal infection rate then preventative or emergency measures can be introduced. 
It is often impractical to apply statistical normality to diagnose individuals. Symptom normality is the current, and assumed most effective, way to assess patient pathology. Psychiatric normality, in a broad sense, states that psychopathology are disorders that are deviations from normality.

Normality, as a relative concept, is intrinsically involved with contextual elements. As a result, clinical disorder classification has particular challenges in discretely diagnosing 'normal' constitutions from true disorders. The "Diagnostic and Statistical Manual of Mental Disorders" (DSM) is the psychiatric profession's official classification manual of mental disorders since its first published version DSM-I in by the APA, 1952. As the DSM evolved into its current version, DSM-5 in late 2013, there have been numerous conflicts in proposed classification between mental illness and normal mentality. Dr. Allen Frances, who chaired the task force for content in the DSM-IV and DSM-IV-TR even wrote a scathing indictment of the pressures incumbent on the definition of "normal" relative to psychological constructs and mental illness in his book, "Saving Normal".

Most of this difficulty stems from the DSM's ambiguity of natural contextual stressor reactions versus individual dysfunction. There are some key progressions along the DSM history that have attempted to integrate some aspects of normality into proper diagnosis classification. As a diagnostic manual for classification of abnormalities, all DSMs have been biased towards classifying symptoms as disorders by emphasizing symptomatic singularity. The result is an encompassing misdiagnosis of possible normal symptoms, appropriate as contextually derived.

The second edition of the DSM, DSM-II, could not be effectively applied because of its vague descriptive nature. Psychodynamic etiology was a strong theme in classifying mental illnesses. The applied definitions became idiosyncratic, stressing individual unconscious roots. This made applying the DSM unreliable across psychiatrists. No distinction between abnormal to normal was established.

Evidence of the classification ambiguity were punctated by the Rosenhan experiment of 1972. This experiment demonstrated that the methodology of psychiatric diagnosis could not effectively distinguish normal from disordered mentalities. DSM-II labelled 'excessive' behavioral and emotional response as an index of abnormal mental wellness to diagnose some particular disorders. 'Excessiveness' of a reaction implied alternative normal behaviour which would have to include a situational factor in evaluation. As an example; a year of intense grief from the death of a spouse may be a normal appropriate response. To have intense grief for twenty years would be indicative of a mental disorder. As well, to grieve intensely over the loss of a sock would also not be considered normal responsiveness and indicate a mental disorder. The consideration of proportionality to stimuli was a perceived strength in psychiatric diagnosis for the DSM-II.

Another characteristic of the DSM-II systemization was that it classified homosexuality as a mental disorder. Thus, homosexuality was psychiatrically defined a pathological deviation from 'normal' sexual development. Homosexuality was later replaced in the 7th printing of DSM-II, instead categorized as a 'Sexual orientation disturbance'. The intent was to have a label that applied only to those homosexual individuals who were bothered by their sexual orientation. In this manner homosexuality would not be viewed as an atypical illness. Only if it was distressing would homosexuality be classified as a mental illness. However, the DMS-II did not explicitly state that any homosexuality was normal either. This stigma lasted into DSM-III until it was reformed entirely from DSM classifications in 1987.

DSM-III was a best attempt to credit psychiatry as a scientific discipline, from the opprobrium resulting from DSM-II. A reduction in the psychodynamic etiologies of DSM-II spilled over into a reduction symptom etiology altogether. Thus, DSM-III was a specific set of definitions for mental illnesses, and entities more suited to diagnostic psychiatry, but which annexed response proportionality as a classification factor. The product was that all symptoms, whether normal proportional response or inappropriate pathological tendencies, could both be treated as potential signs of mental illness.

DSM-IV explicitly distinguishes mental disorders and non-disordered conditions. A non-disordered condition results from, and is perpetuated by, social stressors. Included in DSM-IV's classification is that a mental disorder "must not be merely an expectable and culturally sanctioned response to a particular event, for example, the death of a loved one. Whatever its original cause, it must currently be considered a manifestation of a behavioral, psychological, or biological dysfunction in the individual" (American Psychiatric Association 2000:xxxi)
This had supposedly injected normality consideration back into the DSM, from its removal from DSM-II. However, it has been speculated that DSM-IV still does not escape the problems DSM-III faced, where psychiatric diagnoses still include symptoms of expectable responses to stressful circumstances to be signs of disorders, along with symptoms that are individual dysfunctions. The example set by DSM-III, for principally symptom-based disorder classification, has been integrated as the norm of mental diagnostic practice.

The DSM-5 was released in the second half of 2013. It has significant differences from DSM IV-TR, including the removal of the multi-axial classifications and reconfiguring the Asperger's/autistic spectrum classifications.

Since the advent of DSM-III, the subsequent editions of the DSM have all included a heavy symptom based pathology diagnosis system. Although there have been some attempts to incorporate environmental factors into mental and behavioural diagnostics, many practitioners and scientists believe that the most recent DSM's are misused. The symptom bias makes diagnosing quick and easier allowing for practitioners to increase their clientele because symptoms can be easier to classify and deal with than dealing with life or event histories which have evoked what may be a temporary and normal mental state in reaction to a patients environmental circumstances. 
The easy-to-use manual not only has increased the perceived need for more mental health care, stimulating funding for mental health care facilities, but also has had a global impact on marketing strategies. Many pharmaceutical commercial ads list symptoms such as fatigue, depression, or anxiety. However, such symptoms are not necessarily abnormal, and are appropriate responses to such occurrences as the loss of a loved one. The targets of such ads in such cases do not need medication, and can naturally overcome their grief, but with such an advertising strategy pharmaceutical companies can greatly expand their marketing.




</doc>
<doc id="7745490" url="https://en.wikipedia.org/wiki?curid=7745490" title="Cultural universal">
Cultural universal

A cultural universal (also called an anthropological universal or human universal), as discussed by Emile Durkheim, George Murdock, Claude Lévi-Strauss, Donald Brown and others, is an element, pattern, trait, or institution that is common to all human cultures worldwide. Taken together, the whole body of cultural universals is known as the human condition. Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Some anthropological and sociological theorists that take a cultural relativist perspective may deny the existence of cultural universals: the extent to which these universals are "cultural" in the narrow sense, or in fact biologically inherited behavior is an issue of "nature versus nurture".

The emergence of these universals dates to the Upper Paleolithic, with the first evidence of full behavioral modernity. 

In his book "Human Universals" (1991), Donald Brown defines human universals as comprising "those features of culture, society, language, behavior, and psyche for which there are no known exception", providing a list of hundreds of items he suggests as universal. Among the cultural universals listed by Donald Brown are:

The observation of the same or similar behavior in different cultures does not prove that they are the results of a common underlying psychological mechanism. One possibility is that they may have been invented independently due to a common practical problem.

Since any cultures that have been studied by anthropologists have had contact with at least the anthropologists that studied it, and anthropological research ethics slows the studies down so that other groups unbound by such ethics, often at least locally represented by people of the same skin color as the supposedly isolated tribe but significantly culturally globalized, reach the tribe before the anthropologists do, no truly uncontacted culture has ever been scientifically studied. This allows outside influence to be an explanation for cultural universals as well. This does not preclude multiple independent inventions of civilization and is therefore not the same thing as hyperdiffusionism, it merely means that cultural universals are not proof of innateness.

Anthropology's application of one ordinary evidence standard for data that agrees with its theories of cultural universals and one extraordinary standard of evidence for data that disagree with it raises issues with population level publication bias. While it is possible with strong enough evidence for the validity of one case study that disagree to pass peer review, those that agree will statistically do so more often than those that disagree even if the level of evidence is the same. Since multiple field studies regarding the same culture are combined when statistical prevalences of behaviors are assessed, the result may cause appearances of behaviors having a supposedly universal statistical demographic even if it does not. Some anthropologists suggest that this may give an appearance of statistics such as more men than women hunting large prey being universal without actually being universal, for example that field studies in which more men than women hunted passed peer review while field studies for the same culture in which the genders were equal or inverted for hunting were not published.




</doc>
<doc id="5605670" url="https://en.wikipedia.org/wiki?curid=5605670" title="Narcissism">
Narcissism

Narcissism is the pursuit of gratification from vanity or egotistic admiration of one's idealised self image and attributes. The term originated from Greek mythology, where the young Narcissus fell in love with his own image reflected in a pool of water. Narcissism is a concept in psychoanalytic theory, which was popularly introduced in Sigmund Freud's essay "On Narcissism" (1914). The American Psychiatric Association has listed the classification narcissistic personality disorder in its "Diagnostic and Statistical Manual of Mental Disorders" (DSM) since 1968, drawing on the historical concept of megalomania.

Narcissism is also considered a social or cultural problem.
It is a factor in trait theory used in various self-report inventories of personality such as the Millon Clinical Multiaxial Inventory. It is one of the three dark triadic personality traits (the others being psychopathy and Machiavellianism). Except in the sense of primary narcissism or healthy self-love, narcissism is usually considered a problem in a person's or group's relationships with self and others. Narcissism is not the same as egocentrism.

The term "narcissism" comes from the Greek myth about Narcissus (, ""), a handsome Greek youth who, according to Ovid, rejected the desperate advances of the nymph Echo. This caused Narcissus to fall in love with his own reflection in a pool of water. Unable to consummate his love, Narcissus "lay gazing enraptured into the pool, hour after hour," and finally changed into a flower that bears his name, the narcissus. The concept of excessive selfishness has been recognized throughout history. In ancient Greece the concept was understood as hubris. It is only more recently that narcissism has been defined in psychological terms.

Four dimensions of narcissism as a personality variable have been delineated: leadership/authority, superiority/arrogance, self-absorption/self-admiration, and exploitativeness/entitlement.

These criteria have been criticized because they presume a knowledge of intention (for example, the phrase "pretending to be"). Behavior is observable, but intention is not. Thus classification requires assumptions which need to be tested before they can be asserted as fact, especially considering multiple explanations could be made as to why a person exhibits these behaviors.

Psychiatrists Hotchkiss and James F. Masterson identified what they called the seven deadly sins of narcissism:

Narcissistic personality disorder affects an estimated 1% of the general population. Although most individuals have some narcissistic traits, high levels of narcissism can manifest themselves in a pathological form as narcissistic personality disorder (NPD), whereby the individual overestimates his or her abilities and has an excessive need for admiration and affirmation. NPD was revised in the DSM-5. The general move towards a dimensional (personality trait-based) view of the Personality Disorders has been maintained. Some narcissists may have a limited or minimal capability to experience emotions.

The Cochrane Collaboration has commissioned two reviews of the evidence for psychological and medical treatments for Narcissistic Personality Disorder (NPD). In both cases, they suspended their initiatives after the authors had made no progress in over a year. There are no clear treatment strategies for NPD, neither medication, nor Psychotherapy. There is evidence that therapies effective in the treatment of other personality disorders do not generalise to NPD. Psychiatric diagnoses are not formulated for stability over time. Spontaneous recovery from mental ill-health does sometime occur and many comorbid conditions (e.g. anxiety) can be treated.

Karen Horney saw the narcissistic personality as a temperament trait molded by a certain kind of early environment. She did not see narcissistic needs and tendencies as inherent in human nature.

Craig Malkin called a lack of healthy narcissism "echoism" after the nymph Echo in the mythology of Narcissus. Healthy narcissism might exist in all individuals.

Freud said that narcissism was an original state from which the individual develops the love object. He argued that healthy narcissism is an essential part of normal development. According to Freud, the love of the parents for their child and their attitude toward their child could be seen as a revival and reproduction of their own narcissism. The child has a megalomaniac omnipotence of thought; the parents stimulate that feeling because in their child they see the things that they have never reached themselves. Compared to neutral observers, parents tend to overvalue the qualities of their child. When parents act in an extreme opposite style and the child is rejected or inconsistently reinforced depending on the mood of the parent, the self-needs of the child are not met.

Freud contrasted the natural development of active-egoistic and passive-altruistic tendencies in the individual with narcissism, in the former, and what Trevor Pederson referred to as echoism, in the latter.

Where the egoist can give up love in narcissism, the altruist can give up on competition, or "the will," in echoism. The individual first has a non-ambivalent relations of "fusion" with authority or love figures, which are characterized by the egoistic or altruistic drives. Second, the individual can move to "defusion" from authority or love figures which leads to repetitions of ambivalent, narcissistic or echoistic relations. In the third movement the individual becomes the dead or absent parental figure that never returned love to the echoist, or the perfect, grandiose parental figure in narcissism. While egoism and narcissism concern dynamics of power and inferiority/superiority, Pederson argues that altruism and echoism concern dynamics of belonging and inclusion/exclusion. Pederson has two types of echoists: the "subject altruist" and the "object altruist", with the former being concerned with the belonging of others and loving them, and the latter being concerned with their own belonging and being loved. The subject altruist is self-effacing, a people pleaser, and sacrifices her desire to help others who are outsiders become insiders, or to be the submissive helper of an insider. The object altruist is gregarious, a people person, and wants to be interesting which is based on wanting to fit in and not be an outsider or wanting to be unique as an insider. Both types of echoists show issues with being submissive, having problems saying no, and avoiding conflict.

Freud's idea of narcissism described a pathology which manifests itself in the inability to love others, a lack of empathy, emptiness, boredom, and an unremitting need to search for power, while making the person unavailable to others.

Healthy narcissism has to do with a strong feeling of "own love" protecting the human being against illness. Eventually, however, the individual must love the other, "the object love to not become ill". The individual becomes ill as a result of the frustration created when he is unable to love the object. In pathological narcissism such as the narcissistic personality disorder, the person's libido has been withdrawn from objects in the world and produces megalomania. The clinical theorists Kernberg, Kohut and Theodore Millon all saw pathological narcissism as a possible outcome in response to unempathic and inconsistent early childhood interactions. They suggested that narcissists try to compensate in adult relationships. The pathological condition of narcissism is, as Freud suggested, a magnified, extreme manifestation of healthy narcissism.

Healthy narcissism has been suggested to be correlated with good psychological health. Self-esteem works as a mediator between narcissism and psychological health. Therefore, because of their elevated self-esteem, deriving from self-perceptions of competence and likability, high narcissists are relatively free of worry and gloom.

Other researchers have suggested that healthy narcissism cannot be seen as 'good' or 'bad', but that it depends on the contexts and outcomes being measured. In certain social contexts such as initiating social relationships, and with certain outcome variables, such as feeling good about oneself, healthy narcissism can be helpful. In other contexts, such as maintaining long-term relationships and with outcome variables, such as accurate self-knowledge, healthy narcissism can be unhelpful.

The Narcissistic Personality Inventory (NPI) is the most widely used measure of narcissism in social psychological research. Although several versions of the NPI have been proposed in the literature, a forty-item forced-choice version (Raskin & Terry, 1988) is the one most commonly employed in current research. The NPI is based on the DSM-III clinical criteria for narcissistic personality disorder (NPD), although it was designed to measure these features in the general population. Thus, the NPI is often said to measure "normal" or "subclinical" (borderline) narcissism (i.e., in people who score very high on the NPI do not necessarily meet criteria for diagnosis with NPD).

The Millon Clinical Multiaxial Inventory (MCMI) is a widely used diagnostic test developed by Theodore Millon. The MCMI includes a scale for Narcissism. The NPI and MCMI have been found to be well correlated, "r"(146) = 0.55, "p" < 0.001. Whereas the MCMI measures narcissistic personality disorder (NPD), the NPI measures narcissism as it occurs in the general population. In other words, the NPI measures "normal" narcissism; i.e., most people who score very high on the NPI do not have NPD. Indeed, the NPI does not capture any sort of narcissism taxon as would be expected if it measured NPD.

Within the field of psychology, there are two main branches of research into narcissism: (1) clinical and (2) social psychology.

These two approaches differ in their view of narcissism, with the former treating it as a disorder, thus as discrete, and the latter treating it as a personality trait, thus as a "continuum". These two strands of research tend loosely to stand in a divergent relation to one another, although they converge in places.

Campbell and Foster (2007) review the literature on narcissism. They argue that narcissists possess the following "basic ingredients":

Narcissists tend to demonstrate a lack of interest in warm and caring interpersonal relationships. There are several ongoing controversies within narcissism literature, namely: whether narcissism is healthy or unhealthy; a personality disorder; a discrete or continuous variable; defensive or offensive; the same across genders; the same across cultures; and changeable or unchangeable.

Campbell and Foster (2007) argue that self-regulatory strategies are of paramount importance to understanding narcissism. Self-regulation in narcissists involves such things as striving to make one's self look and feel positive, special, successful and important. It comes in both intra-psychic, such as blaming a situation rather than self for failure, and interpersonal forms, such as using a relationship to serve one's own self. Some differences in self-regulation between narcissists and non-narcissists can be seen with Campbell, Reeder, Sedikides & Elliot (2000) who conducted a study with two experiments. In each experiment, participants took part in an achievement task, following which they were provided with false feedback; it was either bogus success or failure. The study found that both narcissists and non-narcissists self-enhanced, but non-narcissists showed more flexibility in doing so. Participants were measured on both a comparative and a non-comparative self-enhancement strategy. Both narcissists and non-narcissists employed the non-comparative strategy similarly; however, narcissists were found to be more self-serving with the comparative strategy, employing it far more than non-narcissists, suggesting a greater rigidity in their self-enhancement. When narcissists receive negative feedback that threatens the self, they self-enhance at all costs, but non-narcissists tend to have limits.

Sorokowski et al. (2015) showed that narcissism is related to the frequency of posting selfie-type pictures on social media. Sorokowski's study showed that this relationship was stronger among men than women.

Research indicates that being in devalued social group can encourage narcissism in some members of that group, as said individuals attempt to compensate for their low social status (due to being a member of a stigmatised group) by exaggerating their own self-worth by engaging in narcissism, which may also help them psychologically cope with negative treatment at the hands of others, though it may also cause them to engage in behaviour detrimental to themselves.

Livesley et al. concluded, in agreement with other studies, that narcissism as measured by a standardized test was a common inherited trait. Additionally, in similar agreement with those other studies, it was found that there exists a continuum between normal and disordered personality. The study subjects were 175 volunteer twin pairs (ninety identical, eighty-five fraternal) drawn from the general population. Each twin completed a questionnaire that assessed eighteen dimensions of personality disorder. The authors estimated the heritability of each dimension of personality by standard methods, thus providing estimates of the relative contributions of genetic and environmental causation. Of the eighteen personality dimensions, narcissism was found to have the highest heritability (0.64), indicating that the concordance of this trait in the identical twins was significantly influenced by genetics. Of the other dimensions of personality, only four were found to have heritability coefficients of greater than 0.5: callousness, identity problems, oppositionality and social avoidance.

Arikan found that a stigmatising attitude to psychiatric patients is associated with narcissistic personality traits.

The concept of narcissism is used in evolutionary psychology in relation to the mechanisms of assortative mating, or the non-random choice of a partner for purposes of procreation. Evidence for assortative mating among humans is well established; humans mate assortatively regarding age, IQ, height, weight, nationality, educational and occupational level, physical and personality characteristics, and family relatedness. In the "self seeking like" hypothesis, individuals unconsciously look for a "mirror image" of themselves in others, seeking criteria of beauty or reproductive fitness in the context of self-reference. Alvarez et al. found that facial resemblance between couples was a strong driving force among the mechanisms of assortative mating: human couples resemble each other significantly more than would be expected from random pair formation. Since facial characteristics are known to be inherited, the "self seeking like" mechanism may enhance reproduction between genetically similar mates, favoring the stabilization of genes supporting social behavior, with no kin relationship among them.

Narcissistic supply is a concept introduced into psychoanalytic theory by Otto Fenichel in 1938, to describe a type of admiration, interpersonal support or sustenance drawn by an individual from his or her environment and essential to their self-esteem. The term is typically used in a negative sense, describing a pathological or excessive need for attention or admiration in codependents and the orally fixated, that does not take into account the feelings, opinions or preferences of other people.

Narcissistic rage is a reaction to narcissistic injury, which is a perceived threat to a narcissist's self-esteem or self-worth. "Narcissistic injury" and "narcissistic scar" are terms used by Sigmund Freud in the 1920s. "Narcissistic wound" and "narcissistic blow" are other, almost interchangeable, terms.

The term "narcissistic rage" was coined by Heinz Kohut in 1972. Narcissistic rage occurs on a continuum from aloofness, to expressions of mild irritation or annoyance, to serious outbursts, including violent attacks.

Narcissistic rage reactions are not limited to personality disorders. They may also be seen in catatonic, paranoid delusion, and depressive episodes. It has been suggested that narcissists have two layers of rage. The first layer of rage can be thought of as a constant anger towards someone else, with the second layer being a self-aimed anger.

Narcissistic defences are those processes whereby the idealized aspects of the self are preserved, and its limitations denied. They tend to be rigid and totalistic. They are often driven by feelings of shame and guilt, conscious or unconscious.

Narcissistic abuse was originally just defined as a specific form of emotional abuse of children by narcissistic parents – parents who require the child to give up their own wants and feelings in order to serve the parent's needs for esteem.
The term emerged in the late twentieth century due to the works of Alice Miller and other Neo-Freudians, rejecting psychoanalysis as being similar to the poisonous pedagogies.

Self-help culture assumes that someone abused by narcissistic parenting as a child likely struggles with codependency issues in adulthood. An adult who is or has been in a relationship with a narcissist likely struggles with not knowing what constitutes a "normal" relationship.

In recent years the term has been applied more broadly to refer to any abuse by a narcissist including in adult to adult relationships.

In 1993, James F. Masterson proposed two categories for pathological narcissism, "exhibitionist" and "closet". Both fail to adequately develop an age- and phase- appropriate self because of defects in the quality of psychological nurturing provided, usually by the mother. The exhibitionist narcissist is the one described in DSM-IV and differs from the closet narcissist in several important ways. The closet narcissist is more likely to be described as having a deflated, inadequate self-perception and greater awareness of emptiness within. The exhibitionist narcissist would be described as having an inflated, grandiose self-perception with little or no conscious awareness of the emptiness within. Such a person would assume that this condition was normal and that others were just like him. The closet narcissist seeks constant approval from others and appears similar to the borderline in the need to please others. The exhibitionist narcissist seeks perfect admiration all the time from others.

In 1996 Theodore Millon identified five variations of narcissist. Any individual narcissist may exhibit none or one of the following:

Acquired situational narcissism (ASN) is a form of narcissism that develops in late adolescence or adulthood, brought on by wealth, fame and the other trappings of celebrity. It was coined by Robert B. Millman, professor of psychiatry at the Weill Cornell Medical College of Cornell University. ASN differs from conventional narcissism in that it develops after childhood and is triggered and supported by the celebrity-obsessed society. Fans, assistants and tabloid media all play into the idea that the person really is vastly more important than other people, triggering a narcissistic problem that might have been only a tendency, or latent, and helping it to become a full-blown personality disorder. "Millman says that what happens to celebrities is that they get so used to people looking at them that they stop looking back at other people." In its presentation and symptoms, it is indistinguishable from narcissistic personality disorder, differing only in its late onset and its support by large numbers of others. "The lack of social norms, controls, and of people telling them how life really is, also makes these people believe they're invulnerable," so that the person with ASN may suffer from unstable relationships, substance abuse and erratic behaviour. A famous fictional character with ASN is Norma Desmond, the main character of "Sunset Boulevard".

Codependency is a tendency to behave in overly passive or excessively caretaking ways that negatively impact one's relationships and quality of life. Narcissists are considered to be natural magnets for the codependent. Rappoport identifies codependents of narcissists as "co-narcissists".

Collective narcissism (or group narcissism) is a type of narcissism where an individual has an inflated self-love of his or her own ingroup, where an "ingroup" is a group in which an individual is personally involved. While the classic definition of narcissism focuses on the individual, collective narcissism asserts that one can have a similar excessively high opinion of a group, and that a group can function as a narcissistic entity. Collective narcissism is related to ethnocentrism; however, ethnocentrism primarily focuses on self-centeredness at an ethnic or cultural level, while collective narcissism is extended to any type of ingroup beyond just cultures and ethnicities.

Conversational narcissism is a term used by sociologist Charles Derber in his book, "The Pursuit of Attention: Power and Ego in Everyday Life".
Derber observed that the social support system in America is relatively weak, and this leads people to compete mightily for attention. In social situations, they tend to steer the conversation away from others and toward themselves. "Conversational narcissism is the key manifestation of the dominant attention-getting psychology in America," he wrote. "It occurs in informal conversations among friends, family and coworkers. The profusion of popular literature about listening and the etiquette of managing those who talk constantly about themselves suggests its pervasiveness in everyday life." What Derber describes as "conversational narcissism" often occurs subtly rather than overtly because it is prudent to avoid being judged an egotist.
Derber distinguishes the "shift-response" from the "support-response," as in the following two hypothetical conversation fragments:

In "The Culture of Narcissism", Christopher Lasch defines a narcissistic culture as one where every activity and relationship is defined by the hedonistic need to acquire the symbols of wealth, this becoming the only expression of rigid, yet covert, social hierarchies. It is a culture where liberalism only exists insofar as it serves a consumer society, and even art, sex and religion lose their liberating power. In such a society of constant competition, there can be no allies, and little transparency. The threats to acquisitions of social symbols are so numerous, varied and frequently incomprehensible, that defensiveness, as well as competitiveness, becomes a way of life. Any real sense of community is undermined—or even destroyed—to be replaced by virtual equivalents that strive, unsuccessfully, to synthesize a sense of community.

Destructive narcissism is the constant exhibition of numerous and intense characteristics usually associated with the pathological narcissist but having fewer characteristics than pathological narcissism.

Malignant narcissism, a term first coined in a book by Erich Fromm in 1964, is a syndrome consisting of a cross breed of the narcissistic personality disorder, the antisocial personality disorder, as well as paranoid traits. The malignant narcissist differs from one suffering from narcissistic personality disorder in that the malignant narcissist derives higher levels of psychological gratification from accomplishments over time (thus worsening the disorder). Because the malignant narcissist becomes more involved in this psychological gratification, in the context of the right conditions, the narcissist is apt to develop the antisocial, the paranoid, and the schizoid personality disorders. The term malignant is added to the term "narcissist" to indicate that individuals with this disorder have a severe form of narcissistic disorder that is characterized also by features of paranoia, psychopathy (anti-social behaviors), aggression, and sadism according to Kernberg and colleagues.

Medical narcissism is a term coined by John Banja in his book, "Medical Errors and Medical Narcissism". Banja defines "medical narcissism" as the need of health professionals to preserve their self-esteem leading to the compromise of error disclosure to patients. In the book he explores the psychological, ethical and legal effects of medical errors and the extent to which a need to constantly assert their competence can cause otherwise capable, and even exceptional, professionals to fall into narcissistic traps. He claims that:

Narcissism as a personality trait, generally assessed with the Narcissistic Personality Inventory, is related to some types of behavior in the workplace. For example, individuals high in narcissism inventories are more likely to engage in counterproductive work behavior (CWB, behavior that harms organizations or other people in the workplace). Although individuals high in narcissism inventories might engage in more aggressive (and counterproductive) behaviors, they mainly do so when their self-esteem is threatened. Thus narcissistic employees are more likely to engage in CWB when they feel threatened. Individuals high in narcissism have fragile self-esteem and are easily threatened. One study found that employees who are high on narcissism are more likely to perceive the behaviors of others in the workplace as abusive and threatening than individuals who are low on narcissism.

The narcissistic manager will have two main sources of narcissistic supply: inanimate – status symbols like company cars, company-issued smartphone or prestigious offices with window views; and animate – flattery and attention from colleagues and subordinates. Teammates may find everyday offers of support swiftly turn them into enabling sources of permanent supply, unless they are very careful to maintain proper boundaries. The need to protect such supply networks will prevent the narcissistic managers from taking objective decisions; while long-term strategies will be evaluated according to their potential for attention-gaining for the manager themself. Organizational psychologist Alan Downs wrote a book in 1997 describing corporate narcissism. He explores high-profile corporate leaders (such as Al Dunlap and Robert Allen) who, he suggests, literally have only one thing on their minds: profits. According to Downs, such narrow focus actually may yield positive short-term benefits, but ultimately it drags down individual employees as well as entire companies. Alternative thinking is proposed, and some firms now utilizing these options are examined. Downs' theories are relevant to those suggested by Victor Hill in his book, "Corporate Narcissism in Accounting Firms Australia".

Psychiatrist Ernst Simmel first defined primordial narcissism in 1944. Simmel's fundamental thesis is that the most primitive stage of libidinal development is not the oral, but the gastrointestinal one. Mouth and anus are merely to be considered as the terminal parts of this organic zone. Simmel terms the psychological condition of prenatal existence "primordial narcissism." It is the vegetative stage of the pre-ego, identical with the id. At this stage there is complete instinctual repose, manifested in unconsciousness. Satiation of the gastrointestinal zone, the representative of the instinct of self-preservation, can bring back this complete instinctual repose, which, under pathological conditions, can become the aim of the instinct. Contrary to Lasch, Bernard Stiegler argues in his book, "Acting Out", that consumer capitalism is in fact destructive of what he calls primordial narcissism, without which it is not possible to extend love to others. In other words, he is referring to the natural state of an infant as a fetus and in the first few days of its life, before it has learned that other people exist besides itself, and therefore cannot possibly be aware that they are human beings with feelings, rather than having anything to do with actual narcissism.

Sexual narcissism has been described as an egocentric pattern of sexual behavior that involves an inflated sense of sexual ability and sexual entitlement. In addition, sexual narcissism is the erotic preoccupation with oneself as a superb lover through a desire to merge sexually with a mirror image of oneself. Sexual narcissism is an intimacy dysfunction in which sexual exploits are pursued, generally in the form of extramarital affairs, to overcompensate for low self-esteem and an inability to experience true intimacy. This behavioral pattern is believed to be more common in men than in women and has been tied to domestic violence in men and sexual coercion in couples. Hurlbert argues that sex is a natural biological given and therefore cannot be deemed as an addiction. He and his colleagues assert that any sexual addiction is nothing more than a misnomer for what is actually sexual narcissism or sexual compulsivity. While Hurlbert writes mainly of sexual narcissism in men, Schoenewolf (2013) describes what he calls "gender narcissism" which occurs in both males and females who compensate for feelings of sexual inadequacy by becoming overly proud and obsessed with their masculinity or femininity.

Narcissistic parents demand certain behavior from their children because they see the children as extensions of themselves, and need the children to represent them in the world in ways that meet the parents' emotional needs. This parenting 'style' most often results in estranged relationships with the children, coupled with feelings of resentment and self-destructive tendencies.

Narcissistic leadership is a common form of leadership. The narcissism may be healthy or destructive although there is a continuum between the two. A study published in the journal "Personality and Social Psychology Bulletin" suggests that when a group is without a leader, you can often count on a narcissist to take charge. Researchers found that people who score high in narcissism tend to emerge as group leader.

Some critics contend that pop culture has become more narcissistic in recent decades. This claim is supported by scholarship indicating some celebrities hire "fake paparazzi", the frequency with which "reality TV" programs populate the television schedules, and the growth of an online culture in which digital media, social media and the "will-to-fame" are generating a "new era of public narcissism [that] is mutating with new media forms." In this analysis, narcissism, rather than being the pathologized property of a discrete personality type, has been asserted as a constituent cultural feature of an entire generation since the end of World War II.

Supporting the contention that American culture has become more narcissistic and that this is increasingly reflected in its cultural products is an analysis of US popular song lyrics between 1987 and 2007. This found a growth in the use of first-person singular pronouns, reflecting a greater focus on the self, and also of references to antisocial behavior; during the same period, there was a diminution of words reflecting a focus on others, positive emotions, and social interactions. Similar patterns of change in cultural production are observable in other Western states. A linguistic analysis of the largest circulation Norwegian newspaper found that the use of self-focused and individualistic terms increased in frequency by 69 per cent between 1984 and 2005 while collectivist terms declined by 32 per cent. References to narcissism and self-esteem in American popular print media have experienced vast inflation since the late 1980s. Between 1987 and 2007 direct mentions of self-esteem in leading US newspapers and magazines increased by 4,540 per cent while narcissism, which had been almost non-existent in the press during the 1970s, was referred to over 5,000 times between 2002 and 2007.

Cross-cultural studies of differences in narcissism are rare. Instead, as there is a positive association between narcissism and individualism and a negative one between it and collectivism, these traits have been used as proxies for narcissism in some studies. This approach, however, risks the misapplication of the concepts of individualism and collectivism to create overly-fixed, "caricature-like", oppositional categories. Nonetheless, one study looked at differences in advertising products between an individualistic culture, America, and a collectivist one, South Korea. In American magazine advertisements, it found, there was a greater tendency to stress the distinctiveness and uniqueness of the person; conversely the South Korean ones stressed the importance of social conformity and harmony. This observation holds true for a cross-cultural analysis across a wide range of cultural outputs where individualistic national cultures produce more individualistic cultural products and collectivist national cultures produce more collectivist national products; these cultural effects were greater than the effects of individual differences within national cultures.



</doc>
<doc id="2925542" url="https://en.wikipedia.org/wiki?curid=2925542" title="Dropping out">
Dropping out

Dropping out means leaving high school, college, university or another group for practical reasons, necessities, or disillusionment with the system from which the individual in question leaves.

In Canada, most individuals graduate grade 12 by the age of 18, according to Jason Gilmore who collects data of employment and education using the Labour Force Survey. The LFS is the official survey used to collect unemployment data in Canada (2010). Using this tool, assessing educational attainment and school attendance can calculate a dropout rate (Gilmore, 2010). It was found by the LFS that by 2009, 1 in 12 20-24-year-old adults did not have a high school diploma (Gilmore, 2010). It was also found by the study that men still have higher drop out rates than women, and that students outside of major cities and in the northern territories also have a higher risk of dropping out. Although since 1990, dropout rates have gone down from 20% to a low of 9% in 2010, it does not seem to be dropping since this time (2010).

The average Canadian dropout earns $70 less per week than their peers with a high school diploma. Graduates (without post-secondary) earned an average of $621 per week, whereas dropout students earned an average of $551 (Gilmore, 2010).

Even though dropout rates have gone down in the last 20–25 years, the concerns of the impact dropping out has on the labor market is very real (Gilmore, 2010). One in four students without a high school diploma who were in the labor market in 2009-2010 had less likelihood of finding a job due to economic, downturn (Gilmore, 2010).

In the United Kingdom, a dropout is anyone who leaves school, college or university without either completing their course of study or transferring to another educational institution. Dropping out of school is not allowed. Attendance at a school is mandatory until age 16 (GCSE exams) and students must be in some form of education or training (either full-time or part-time) until age 18.

Dropout rate benchmarks are set for each higher education institution and monitored by the Higher Education Funding Council for England (HEFCE), the Higher Education Funding Council for Wales (HEFCW) and the Scottish Funding Council (SFC). Dropout rates are often one of the factors assessed when ranking UK universities in league tables.

In November 2014, a report from the Institute for Fiscal Studies found that students from poorer home backgrounds were 8.4 percentage points more likely to drop out of university in the first two years of an undergraduate course than those from the richest homes; they were also 22.9 percentage points less likely to obtain a or first degree. For students studying on the same course and who arrived at university with similar grades, the differences fell but remained significant. The report concluded that more should be done both to raise the attainment levels of poorer students prior to their arrival at university and to provide additional support to them at university.

In the United States, dropping out most commonly refers to a student quitting school before he or she graduates or avoiding entering a university or college. It cannot always be ascertained that a student has dropped out, as he or she may stop attending without terminating enrollment. It is estimated 1.2 million students annually drop out of high school in the United States, where high school graduation rates rank 19th in the world. Reasons are varied and may include: to find employment, avoid bullying, family emergency, poor grades, depression and other mental illnesses, unexpected pregnancy, bad environment, lack of freedom, and boredom. "The Silent Epidemic: Perspectives of High School Dropouts" by Civic Enterprises explores reasons students leave school without 
graduating. The consequences of dropping out of school can have long-term economic and social repercussions. Students who drop out of school in the United States are more likely to be 
unemployed, homeless, receiving welfare and incarcerated. A four-year study in San Francisco found that 94 percent of young murder victims were high school dropouts.

The United States Department of Education's measurement of the "status" dropout rate is the percentage of 16-24-year-olds who are not enrolled in school and have not earned a high school credential. This rate is different from the event dropout rate and related measures of the status completion and average freshman completion rates. The status high school dropout rate in 2009 was 8.1%. There are many risk factors for high school dropout. These can be categorized into social and academic risk factors. Members of racial and ethnic minority groups drop out at higher rates than white students, as do those from low-income families, from single-parent households, and from families in which one or both parents also did not complete high school. Students at risk for dropout based on academic risk factors are those who often have a history of absenteeism and grade retention, academic trouble, and more general disengagement from school life.

In Australia, dropping out most commonly refers to a student quitting school before he or she graduates. Reasons for students dropping out vary but usually include: Avoiding bullies, finding employment, family problems, depression and other mental illnesses, teenage pregnancy, substance abuse and in some cases even boredom. Researchers at Melbourne's Mitchell Institute has found that a quarter of Australian high school students are not graduating year 12, and that completion rates are much worse in remote or economically disadvantaged communities. Professor Teese believes the segregation of students in schools through geography as well as in the private and public systems means student disadvantage is stronger in Australia than other western nations such as New Zealand and Canada. Drop out rates vary throughout different locations in Australia. Students that attend school in remote communities have a higher chance of not completing year 12 (56.6%), whereas students that come from a wealthy background share an average completion rate of 90%. These remote schooling programs serve primarily indigenous students. Geography and lack of resources cause indigenous students to have lower rates of completion: the gap between indigenous and non-indigenous year 12 graduates is over 40 percentage points. As a result of this substantial difference, lower socioeconomic students who drop out are considered at-risk-students and are ultimately prone to unemployment, incarceration, low-paying employment and having children at early ages.

When analysing the household surveys of some countries in the Latin American region – notably, those of Bolivia, Chile, Panama, Costa Rica, Nicaragua and Paraguay – researching the opinions of boys, girls, adolescents, young people as well as their families on the reasons they drop out of school, some recurring features surface that enable us to group the analyses into two main categories. The first is directly related to 'the material dimension' of education. In this case, financial difficulties are the main reason why families do not manage to keep their children and adolescents in school. The other major group of factors for school drop out falls into the 'subjective dimension' of the educational experience. Surveys revealed that 22% of out-of-school boys and girls aged 10 or 11 years state that they are in this situation because they have no interest in studying. This percentage jumps to 38% in adolescents aged 15 to 17 years who also provided this reason for their disengagement with the education system.

A "dropout recovery" initiative is any community, government, non-profit or business program in which students who have previously left school are sought out for the purpose of re-enrollment. In the U.S., such initiatives are often focused on former high school students who are still young enough to have their educations publicly subsidized, generally those 22 years of age and younger. In Rwanda, dropout recovery is often focused on primary and ordinary level students who are still young to have their educations.
Dropout recovery programs can be initiated in traditional "brick-and-mortar" institutions of learning, in community centers or online.

Dropping out of high school can have drastic long-term economic and social repercussions, especially in Australia which has a less equitable education system than many other western countries. Therefore, different pathways and courses of study are being implemented by the government, non-profit organizations, and private companies to offer a selection of education recovery plans for young adults around the age of 22 and below.

Students that drop out of high-school are generally those that struggle to engage behaviorally and/or academically. Although, it is not entirely clear whether different types of contextual or self-system variables affect students' engagement or contribute to their decision to drop out. According to data collected by the national education longitudinal study of 1988, Rumberger found that students with moderate to high absenteeism, behavior problems and having no school or outside activities were highly predictive of dropping out.

Under the pact of educational inclusion at the secondary level, how families organize themselves internally to produce well-being is an unavoidable topic for countries to address when seeking to broaden the effective opportunities of access to, retention in and graduation from the secondary education. Therefore, the construction of a new policy, the adolescent and the young person at school, is an acknowledgement of what is happening in reality and shapes a mutually beneficial alliance between the state and families to generate dynamics where young people can become exclusive recipients of care – at least until completion of their secondary schooling.



</doc>
<doc id="728513" url="https://en.wikipedia.org/wiki?curid=728513" title="Laziness">
Laziness

Laziness (also known as indolence) is disinclination to activity or exertion despite having the ability to act or to
exert oneself. It is often used as a pejorative; terms for a person seen to be lazy
include "couch potato", "slacker", and "bludger".

Despite Sigmund Freud's discussion of the pleasure principle, Leonard Carmichael notes that "laziness is not a word that appears in the table of contents of most technical books on psychology... It is a guilty secret of modern psychology that more is understood about the motivation of thirsty rats and hungry pecking pigeons as they press levers than about the way in which poets make themselves write poems or scientists force themselves into the laboratory when the good golfing days of spring arrive." A 1931 survey found high-school students more likely to attribute their failing performance to laziness, while teachers ranked "lack of ability" as the major cause, with laziness coming in second. Laziness is not to be confused with avolition, a negative symptom of certain mental-health issues such as depression, ADHD, sleep disorders, and schizophrenia.

Laziness is a habit rather than a mental health issue. It may reflect a lack of self-esteem, a lack of positive recognition by others, a lack of discipline stemming from low self-confidence, or a lack of interest in the activity or belief in its efficacy. Laziness may manifest as procrastination or vacillation. Studies of motivation suggest that laziness may be caused by a decreased level of motivation, which in turn can be caused by over-stimulation or excessive impulses or distractions. These increase the release of dopamine, a neurotransmitter responsible for reward and pleasure. The more dopamine that is released, the greater intolerance one has for valuing and accepting productive and rewarding action. This desensitization leads to dulling of the neural patterns and affects negatively the anterior insula of the brain responsible for risk perception.

ADHD specialists say engaging in multiple activities can cause behavioral problems such as attention/focus failure or perfectionism and subsequently pessimism. In these circumstances laziness can manifest as a negative coping mechanism (aversion), the desire to avoid certain situations in the hopes of countering certain experiences and preconceived ill results. Lacanian thought says laziness is the "acting out" of archetypes from societal programming and negative child rearing practices. Boredom is sometimes conflated with laziness; one study shows that the average Briton is bored 6 hours a week. Thomas Goetz, University of Konstanz, Germany, and John Eastwood, York University, Canada, concur that aversive states such as laziness can be equally adaptive for making change and toxic if allowed to fester. An outlook found to be helpful in their studies is "being mindful and not looking for ways out of it, simultaneously to be also open to creative and active options if they should arise." They point out that a relentless engaging in activities without breaks can cause oscillations of failure, which may result in mental health issues. 

It has also been shown that laziness can render one apathetic to reactant mental health issues such as anger, anxiety, indifference, substance abuse, and depression.

Economists have differing views of laziness. Frédéric Bastiat argues that idleness is the result of people focusing on the pleasant immediate effects of their actions rather than potentially negative long-term consequences. Others note that humans seem to have a tendency to seek after leisure. Hal Cranmer writes, "For all these arguments against laziness, it is amazing we work so hard to achieve it. Even those hard-working Puritans were willing to break their backs every day in exchange for an eternity of lying around on a cloud and playing the harp. Every industry is trying to do its part to give its customers more leisure time." Ludwig von Mises writes, "The expenditure of labor is deemed painful. Not to work is considered a state of affairs more satisfactory than working. Leisure is, other things being equal, preferred to travail (work). People work only when they value the return of labor higher than the decrease in satisfaction brought about by the curtailment of leisure. To work involves disutility."

Laziness in American literature is figured as a fundamental problem with social and spiritual consequences. In 1612 John Smith in his "Map to Virginia" is seen using a jeremiad to address idleness. In the 1750s this sort of advocating reached at its apex in literature's. David Bertelson in "The Lazy South" (1767) expressed this as a substitution of "spiritual industry" over "patriotic industry". Writers like William Byrd went to a great extent and censured North Carolina as land of lubbers. Thomas Jefferson in his "Notes on the State of Virginia" (1785) acknowledges a small portion of the people have only seen labor and identifies the cause of this indolence to the rise of "slave-holding" society. Jefferson raised his concerns what this deleterious system will bring to the economic system. Later by the 1800s the rise of Romanticism changed attitudes of the society, values of work were re-written; stigmatization of idleness was overthrown with glamorous notions. John Pendleton Kennedy was a prominent writer in romanticizing sloth and slavery, In Swallow Barn (1832) he equated idleness and its flow as living in oneness with nature. Mark Twain in "Adventures of Huckleberry Finn" (1885) contrasts realist and romantic perspective of "laziness" and calls attention to the essential convention of aimlessness and transcendence that connects the character. In 20th century the poor whites were portrayed in the grotesque caricatures of early southern laziness. In Flannery O'Connor's Wise Blood (1952) and Good Country People (1955) depicts spiritual backwardness as the cause for disinclination to work. Lacking in any social function which was termed equally with luxurious lifestyle was closely portrayed through lives of displaced aristocrats and their indolence. Jason Compson, Robert Penn Warren, William Styron were some of the writers who explored this perspective. The lack of meaningful work was defined as a void which aristocrats needed to fill with pompous culture, Walker Percy is a writer who have thoroughly mined on the subject. Percy's characters often exposes to the emptiness (spiritual sloth) of contemporary life and come to rectify it with renewed resources of spiritual resources.

One of the Catholic seven deadly sins is sloth, which is often defined as spiritual and/or physical apathy or laziness. Sloth is discouraged in (), 2 Thessalonians, and associated with wickedness in one of the parables of Jesus in the "Gospel of Matthew" (). In the Wisdom books of "Proverbs" and "Ecclesiastes", it is stated that laziness can lead to poverty (, ). According to Peter Binsfeld's "Binsfeld's Classification of Demons", Belphegor is thought to be its chief demon.

The Arabic term used in the Quran for laziness, inactivity and sluggishness is كَسَل ("kasal"). The opposite of laziness is Jihad al-Nafs, i.e. the struggle against the self, against one’s own ego. Among the five pillars of Islam, praying five times a day and fasting during Ramaḍān are part of actions against laziness.

In Buddhism, the term "kausīdya" is commonly translated as "laziness" or "spiritual sloth". "Kausīdya" is defined as clinging to unwholesome activities such as lying down and stretching out, procrastinating, and not being enthusiastic about or engaging in virtuous activity.

From 1909 to 1915, the Rockefeller Sanitary Commission for the Eradication of Hookworm Disease sought to eradicate hookworm infestation from 11 southern U.S. states. Hookworms were popularly known as "the germ of laziness" because they produced listlessness and weakness in the people they infested. Hookworms infested 40 percent of southerners and were identified in the North as the cause of the South's alleged backwardness.

It was alleged that indolence was the reason for backward conditions in Indonesia, such as the failure to implement Green Revolution agricultural methods. But a counter-argument is that the Indonesians, living very precariously, sought to play it safe by not risking a failed crop, given that not all experiments introduced by outsiders had been successful.

It is common for animals (even those like hummingbirds that have high energy needs) to forage for food until satiated, and then spend most of their time doing nothing, or at least nothing in particular. They seek to "satisfice" their needs rather than obtaining an optimal diet or habitat. Even diurnal animals, which have a limited amount of daylight in which to accomplish their tasks, follow this pattern. Social activity comes in a distant third to eating and resting for foraging animals. When more time must be spent foraging, animals are more likely to sacrifice time spent on aggressive behavior than time spent resting. Extremely efficient predators have more free time and thus often appear more lazy than relatively inept predators that have little free time. Beetles likewise seem to forage lazily due to a lack of foraging competitors. On the other hand, some animals, such as pigeons and rats, seem to prefer to respond for food rather than eat equally available "free food" in some conditions.



</doc>
<doc id="1127548" url="https://en.wikipedia.org/wiki?curid=1127548" title="Passive-aggressive behavior">
Passive-aggressive behavior

Passive-aggressive behavior is characterized by a pattern of indirect resistance to the demands or requests of others and an avoidance of direct confrontation. Pretending not to understand is a typical passive-aggressive strategy. Such behavior is often protested by associates, evoking frustration or anger, and labelled "catty", "manipulative", or "acting/going dumb". Passive-aggressive behavior may be subconsciously or consciously used to evoke these emotions and reactions in others. It may also be used as an alternative to verbalizing or acting out their own anger.

It is an "act" if it is occasional and does not substantially interfere with social or occupational function, or relationships; it is a "behavior" if it used more persistently; it is a "personality disorder" if there is a pervasive pattern of such behavior which does interfere in these areas. 

The Diagnostic and Statistical Manual of Mental Disorders revision IV (DSM-IV) describes passive-aggressive personality disorder as a "pervasive pattern of negativistic attitudes and passive resistance to demands for adequate performance in social and occupational situations".

Passive-aggressive behavior is not necessarily a personality disorder. A personality disorder includes deviation in affectivity, cognition, control over impulses, and need for gratification, ways of perceiving and thinking, and inflexible, maladaptive, or otherwise dysfunctional behaviour. There must be personal distress attributable to such behaviour.

In psychology, passive-aggressive behavior is characterized by a habitual pattern of non-active resistance to expected work requirements, opposition, sullenness, stubbornness, and negative attitudes in response to requirements for normal performance levels expected by others. Most frequently it occurs in the workplace, where resistance is exhibited by indirect behaviors as procrastination, forgetfulness, and purposeful inefficiency, especially in reaction to demands by authority figures, but it can also occur in interpersonal contexts.

Another source characterizes passive-aggressive behavior as: "a personality trait marked by a pervasive pattern of negative attitudes and characterized by passive, sometimes obstructionist resistance to complying with expectations in interpersonal or occupational situations. Behaviors: learned helplessness, procrastination, stubbornness, resentment, sullenness, or deliberate/repeated failure to accomplish requested tasks for which one is (often explicitly) responsible". Other examples of passive-aggressive behavior might include avoiding direct or clear communication, evading problems, fear of intimacy or competition, making excuses, blaming others, obstructionism, playing the victim, feigning compliance with requests, sarcasm, backhanded compliments, and hiding anger.

In conflict theory, passive-aggressive behavior can resemble a behavior better described as catty, as it consists of deliberate, active, but carefully veiled hostile acts which are distinctively different in character from the non-assertive style of passive resistance.

Passive-aggressive behavior from workers and managers is damaging to team unity and productivity. In the ad for Warner's online ebook, it says: "The worst case of passive-aggressive behavior involves destructive attitudes such as negativity, sullenness, resentment, procrastination, 'forgetting' to do something, chronic lateness, and intentional inefficiency." If this behavior is ignored it could result in decreased office efficiency and frustration among workers. If managers are passive-aggressive in their behavior, it can end up stifling team creativity. De Angelis says, "It would actually make perfect sense that those promoted to leadership positions might often be those who on the surface appear to be agreeable, diplomatic and supportive, yet who are actually dishonest, backstabbing saboteurs behind the scenes."

Passive-aggressive behavior was first defined clinically by Colonel William Menninger during World War II in the context of men's reaction to military compliance. Menninger described soldiers who were not openly defiant but expressed their civil disobedience (what he called "aggressiveness") "by passive measures, such as pouting, stubbornness, procrastination, inefficiency, and passive obstructionism" due to what Menninger saw as an "immaturity" and a reaction to "routine military stress".

According to some psychoanalytic views, noncompliance is not indicative of true passive-aggressive behavior, which may instead be defined as the manifestation of emotions that have been repressed based on a self-imposed need for acceptance.




</doc>
<doc id="55789686" url="https://en.wikipedia.org/wiki?curid=55789686" title="Human information interaction">
Human information interaction

Human-information interaction or HII is the formal term for information behavior research in archival science; the term was invented by Nahum Gershon in 1995. HII is not transferrable from analog to digital research because nonprofessional researchers greatly emphasize the need for further elaboration of context and scope finding aid elements. Researchers in HII take on many tasks, including helping to design information systems from a biological perspective that conform to the requirements of different segments of society, along with other behaviour intended to improve interaction between humans and information systems. HII is generally considered to be multi-disciplinary as different disciplines have different viewpoints on these interactions and their consequences. HII is considered especially important due to humanity's dependence on information and the technology needed to access it.


</doc>
<doc id="563299" url="https://en.wikipedia.org/wiki?curid=563299" title="Human behavior">
Human behavior

Human behavior is the response of individuals or groups of humans to internal and external stimuli. It refers to the array of every physical action and observable emotion associated with individuals, as well as the human race. While specific traits of one's personality and temperament may be more consistent, other behaviors will change as one moves from birth through adulthood. In addition to being dictated by age and genetics, behavior, driven in part by thoughts and feelings, is an insight into individual psyche, revealing among other things attitudes and values. Social behavior, a subset of human behavior, study the considerable influence of social interaction and culture. Additional influences include ethics, social environment, authority, persuasion and coercion.

The behavior of humans (and other organisms or even mechanisms) falls within a range with some behavior being common, some unusual, some acceptable, and some beyond acceptable limits. In sociology, behavior in general includes actions having no meaning, being not directed at other people, and thus all basic human actions. Behavior in this general sense should not be mistaken with social behavior, which is a more advanced social action, specifically directed at other people. The acceptability of behavior depends heavily upon social norms and is regulated by various means of social control. Human behavior is studied by the social sciences, which include psychology, sociology, economics, and anthropology.

Behavior changes throughout an individual’s life, as they move through different stages of life. For example, adolescence, parenthood and retirement. Human behavior is shaped by psychological traits. For example, extraverted people are more likely to introverted people to participate in social activities like parties. Personality traits vary from person to person and can produce different actions or behavior from each person. Social norms also impact behavior. Due to the inherently conformist nature of human society in general, humans are pressured into following certain rules and displaying certain behaviors in society, which conditions the way people behave. Different behaviors are deemed to be either acceptable or unacceptable in different societies and cultures.

Long before Charles Darwin published his book "On the Origin of Species" in 1858, animal breeders knew that patterns of behavior are somehow influenced by inheritance from parents. Studies of identical twins as compared to less closely related human beings, and of children brought up in adoptive homes, have helped scientists understand the influence of genetics on human behavior. The study of human behavioral genetics is still developing steadily with new methods such as genome-wide association studies. Evolutionary psychology studies human behavior as the product of natural selection. Human psychology and behavior is shaped by our evolutionary past. According to evolutionary psychology, humans try to increase their social status as much as possible. This increases their chances of reproductive success. They may do this by fighting, amassing wealth or helping others with their problems.

Social norms, the often-unspoken rules of a group, shape not just our behaviors but also our attitudes.
An individual’s behavior varies depending on the group(s) they are a part of, a characteristic of society that allows their norms to heavily impact society. Without social norms, human society would not function as it currently does; humans would have to be more abstract in their behavior, as there would not be a pre-tested 'normal' standardized lifestyle, and individuals would have to make many more choices for themselves. The institutionalization of norms is, however, inherent in human society perhaps as a direct result of the desire to be accepted by others, which leads humans to manipulate their own behavior in order to 'fit in' with others. Depending on their nature and upon one's perspective, norms can impact different sections of society both positively (e.g. attending birthday celebrations, dressing warm in the winter) and negatively (e.g. racism, drug use).

Creativity is a fundamental human trait. It can be seen in tribes' adaptation of natural objects to make tools, and in the uniquely human pursuits of art and music.
The creative impulse explains the constant change in fashion, technology and food in modern society. People use creative endeavors like art and literature to distinguish themselves within their social group. 
They also use their creativity to make money and persuade others of the value of their ideas.

Another important aspect of human behavior is religion and spirituality. According to a Pew Research Center report, 54% of adults around the world state that religion is very important in their lives. Religion plays a large role in the lives of many people around the world, and it affects their behavior towards others.. For example, one of the five pillars of Islam is Zakat. This is the practice whereby Muslims who can afford to are required to donate 2.5% of their wealth to those in need. Many religious people regularly attend services with other members of their religion. They may take part in religious rituals, and festivals like Diwali and Easter.

An attitude is an expression of favor or disfavor toward a person, place, thing, or event; it alters between each individual. Everyone has a different attitude towards different things. A main factor that determines attitude is likes and dislikes. The more one likes something or someone the more one is willing to open up and accept what they have to offer. When one doesn’t like something, one is more likely to get defensive and shut down. An example of how one's attitude affects one's human behavior could be as simple as taking a child to the park or to the doctor. Children know they have fun at the park so their attitude becomes willing and positive, but when a doctor is mentioned, they shut down and become upset with the thought of pain. Attitudes can sculpt personalities and the way people view who we are. People with similar attitudes tend to stick together as interests and hobbies are common. This does not mean that people with different attitudes do not interact, the fact is they do. What it means is that specific attitudes can bring people together (e.g., religious groups). Attitudes have a lot to do with the mind which highly relates to human behavior. The way a human behaves depends a lot on how they look at the situation and what they expect to gain from it.

The weather and the climate have a significant influence on human behavior. The average temperature of a country affects its traditions and people's everyday routines. For example, Spain used to be a primarily agrarian country, with much of its labour force working in the fields. Spaniards developed the tradition of the siesta, an after lunch nap, to cope with the intense midday heat. The siesta persists despite the increased use of air conditioning, and the move from farming to office jobs. However, it is less common today than in the past. Norway is a northern country with cold average temperatures and short hours of daylight in winter. This has shaped its lunchtime habits. Norwegians have a fixed half an hour lunch break. This enables them to go home earlier, with many leaving work at three o'clock in the afternoon. This allows them to make the most of the remaining daylight. There is a correlation between higher temperatures and increased levels of violent crime. There are number of theories for why this is. One theory is that people are more inclined to go outside during warmer weather, and this increases the number of opportunities for criminals. Another is that high temperatures cause a physiological response that increases people's irritability, and therefore their likeliness to escalate perceived slights into violence. There is some research detailing that changes in the weather can affect the behavior of children.




</doc>
<doc id="2081132" url="https://en.wikipedia.org/wiki?curid=2081132" title="Antilocution">
Antilocution

Antilocution is a form of prejudice in which negative verbal remarks against a person, group, or community, are made in a public or private setting and not addressed directly to the target. American psychologist Gordon Allport first used this term in his 1954 book, "The Nature of Prejudice", to label the first of the five degrees of antipathy that measure manifestation of prejudice in a society as antilocution (see also: "Allport's Scale"). Antilocution is similar to the rather common form of betrayal in which a person "talks behind someone's back.", but antilocution involves an in-group ostracizing an out-group on a biased basis.

The use of the term antilocution is overshadowed by the term hate speech, which holds a similar meaning but places no regard on the fact that the out-group is unaware of the discrimination.

The term antilocution first appeared in 1954 in "The Nature of Prejudice", a book written by an American psychologist "Gordon Allport". Antilocution is the first stage of Allport's scale, a scale to measure the degree of bias or prejudice in one's society. Following antilocution, the greater stages of prejudice are avoidance, discrimination, physical attack, and extermination respectively. Antilocution is a compound noun which consists of the word locution and the word anti, to express a counterproductive way to employ locution.

Antilocution is considered the least aggressive form of prejudice and represented by conversations of individuals in a society sharing similar opinions, which often are biased and result in negative reputation of the target of antilocution. Although this form of prejudice maybe the least aggressive, it nevertheless can be very destructive and life-changing for the subject. Those who employ antilocution may not be aware or consider themselves as involved in any discriminatory act. Antilocution is perhaps the most common form of discriminative act, and is more commonly referred as minor gossips or scandals. The subject may feel the need to join in with the flow or comedize it if the antilocution is employed by the majority. This can either create a place for the subject in the pack or lead to a spread of biased information through the society and can lead to negative modification in behaviors toward the subject, thus will significantly put the subject in a disadvantaged way both socially and mentally.

Individuals often engage in prejudicial conversation when they feel threatened and frequently is based on misperceptions and stereotyping of the subject. Antilocution often related to the fact that the subject is viewed as an anomaly to the group, for example the subject maybe a newcoming member thus no one actually know the subject so they judge the subject based on their perceptions and stereotyping, often involve racism, ethnic, or gender. Most individuals engaged in this type of behavior will most likely deny that their behavior is prejudicial in any way, and often indicate this as a matter of expressing their own opinions. This kind of treatment to antilocution is very dangerous in its own as it leads to a widespread in discriminative acts toward the subject as individuals feel little to no guilt in doing so. Facts are often needed to contradict misinformation and create a positive influence in the society toward the subject, or else opinions will continue to multiply and lead to further misconceptions, which can eventually lead to more serious forms of prejudice and discrimination and will harm the subject socially, mentally, and even physically if the problem worsen to a certain degree.




</doc>
<doc id="56733622" url="https://en.wikipedia.org/wiki?curid=56733622" title="Coping planning">
Coping planning

Coping planning is an approach to supporting people who are distressed. It is part of a biopsychosocial approach to mental health and wellbeing that comprises healthy environments, responsive parenting, belonging, healthy activities, coping, psychological resilience and treatment of illness. Coping planning normalises distress as a universal human experience. It draws on a health-focused approach to coping, to improve emotion regulation and decrease the memory of unpleasant emotions. Coping planning interventions are effective when people are supported in the process of forming coping plans.

Coping planning aims to meet the needs of people who ask for help with distress, including suicidal ideation. By addressing "why" someone asks for help, the focus stays on what the person needs rather than on what the helper wants to do. It provides an alternative to the widely used, but non-evidence-based risk-assessment approach to suicide prevention. Needs assessment and support focuses on the individual needs of each person. They are rated as low (coping independently), moderate (may need additional low-intensity professional support), or high (needs immediate high-intensity professional support).

In addition to suicide prevention training for health professionals, coping planning has been used to train journalists, and to help a range of people cope better, including carers, university students, and with children to improve emotional regulation.

Coping planning is designed to contribute to suicide prevention in a number of ways. Firstly, it provides a framework to support people whenever they seek help, rather than waiting until they are considered high-risk for death by suicide. Secondly, it aims to focus on helping people to cope, rather than to stay safe from suicide, which, according to ironic process theory, makes it more likely that people will think about suicide. Healthy coping strategies improve overall wellbeing and reduce distress. The approach is designed for use in both low (e.g., psychological first aiders or telephone helplines) and high intensity services (e.g., emergency departments or inpatient care units).



</doc>
<doc id="56784437" url="https://en.wikipedia.org/wiki?curid=56784437" title="Sexual harassment in the military">
Sexual harassment in the military

Sexual harassment in the military is unwanted sexual behaviour, which is defined as threatening, offensive, or otherwise upsetting to others in a military setting. The behaviour is more common in the military than in civilian life. Women are substantially more likely than men to experience sexual harassment in the armed forces. Other groups at higher risk include child cadets/recruits and military detainees.

Sexual harassment is unwanted sexual behaviour which is threatening or otherwise upsetting to others. Some example definitions in use by state armed forces are:

Sexual harassment in the military includes a broad spectrum of behaviour. Undirected behaviours that affect the working environment, such as sexist jokes and the prominent display of pornographic material, may constitute sexual harassment, as do directed behaviours targeted at one or more individuals, such as unwanted sexual advances and sexual assault. Research in Canada has found that a culture of undirected sexual harassment increases the risk of directed sexual harassment and assault.

In the British army, a woman told the following story to researchers in 2006:‘A friend was out on an exercise when a group of men ducked her head in a bucket of water and each time she came up for breath she had to repeat “I am useless and I am a female”. She told the story and said it was a joke but I could see she was upset.’In the Canadian armed forces, demeaning attitudes to women are pervasive, according to the "Deschamps Review" of 2015:'Interviewees reported regularly being told of orders to “stop being pussies” and to “leave your purses at home” [...] The use of the word “cunt”, for example, is commonplace, and rape jokes are tolerated. [...] A commonly held attitude is that, rather than be a soldier, a sailor or an aviator, a woman will be labeled an “ice princess”, a “bitch”, or a “slut”. Another saying is that women enter the CAF “to find a man, to leave a man, or to become a man”.'A woman in the French army, pseudonymised by the "Independent" newspaper as Captain Carole, was raped by her commanding officer:'It was months before I could pronounce the word "rape"... I blamed myself. I said: "We are trained in hand to hand combat. Why didn’t I stop him?" But when that happens you are terrorised.'In the US Army, a woman recalls attending a class for sexual abuse and sexual harassment training, titled 'personal development training'. The senior officer teaching the class asked participants whether they would hit on 'a naked, drunk girl on the bench outside your barracks', adding, 'you're not supposed to but I probably would.'

US Senator Martha McSally, an Arizona Republican, said during a Senate meeting on sexual assault in the military that she was raped by a superior officer in the US Air Force. McSally was the first female pilot in the US Air Force to fly in combat operations. She said that she never reported the rape because she did not trust the military justice system; she also said that she blamed herself for the incident, and that she had thought she was strong but felt powerless.

Studies of sexual harassment have found that it is markedly more common in military than civilian settings. Several reasons for this have been suggested. A Canadian study found that key risk factors associated with military settings are the typically young age of personnel, the isolated locations of bases, the minority status of women, and the disproportionate number of men in senior positions. Other Canadian research has found that an emphasis in military organisations on conformity, obedience, and hierarchical power relations, combine to increase the risk, particularly to personnel of low rank, who are less able than others to resist inappropriate expectations made of them. The traditionally masculine values and behaviours that are rewarded and reinforced in military settings are also thought to play a role.

Canadian research has also found that the risk of sexual misconduct increases during deployment on military operations.

While some male personnel are sexually harassed, women are substantially more likely to be affected. Women who are younger and joined the military at a younger age face a greater risk, according to American, British, Canadian, and French research.

Cadet forces are military youth organisations for younger teenagers based in communities and schools, and are common around the world. There is some evidence from the UK, where hundreds of complaints of the sexual abuse of cadets have been recorded since 2012, and from Canada, where one in ten complaints of sexual assault in the military are from cadets, that these institutions are susceptible to a culture of sexual harassment.

Individuals detained by the military are particularly vulnerable to sexual harassment. During the Iraq War, for example, personnel of the US army and US Central Intelligence Agency committed a number of human rights violations against detainees in the Abu Ghraib prison, including rape, sodomy, and other forms of sexual abuse. Another example is the detention of two Iraqi men on a British warship at the start of the Iraq War, when they were made to strip naked and were then sexually humiliated.

Although the risk of sexual misconduct in the armed forces is widely acknowledged, personnel are frequently reluctant to report incidents. For example:

Since the number of official complaints represents only a fraction of sexual harassment in the military, armed forces that have committed to reduce prevalence produce periodic estimates of its extent using anonymised surveys.

Women affected by sexual harassment are more likely than other women to suffer stress-related mental illness afterwards. Research in the US found that when sexual abuse of female military personnel is psychiatrically traumatic, the odds of suffering from post-traumatic stress disorder (PTSD) after deployment on operations increase by a factor of nine.

Research in the US has found that personnel affected by sexual harassment are somewhat less likely to develop depression or PTSD if a formal report leads to effective action to address the issue.

The military leadership in some countries has begun to acknowledge a culture of sexual misconduct in the armed forces. For example:

Widespread reports of sexual harassment in the Australian armed forces led to the establishment of the Defence Abuse Response Taskforce to investigate complaints from women between 1991 and 2011. It received 2,439 complaints, of which it deemed 1,751 to be plausible.

A Royal Commission into institutional child sexual abuse was established in 2012, which investigated widespread allegations of historical abuse in the navy. The Commission took evidence from 8,000 individuals and reported in 2017 that many recruits of both sexes and from the age of 15 had been repeatedly sexually abused by older recruits between 1967 and 1971, including by anal gang rape, and in some cases young recruits had been forced to rape each other. The practice was ‘tolerated’ by senior staff, according to the Commission.

According to official statistics gathered by "Maclean's" magazine, the Canadian military police received an average of 178 complaints of sexual assault between 2000 and 2014, representing an estimated 10% of all cases.

In 2014, the ombudsman of the Canadian armed forces described sexual harassment in the institution as 'a huge problem'.

In 2015, after widespread allegations of sexual misconduct in the military, a major official report, the "External Review into Sexual Misconduct and Sexual Harassment in the Canadian Armed Forces" (the "Deschamps Review"), was published. It found that sexual harassment was commonplace and embedded in military culture, and that pervasive degrading attitudes to women and LGBTQ personnel were jeopardising their safety. The "Deschamps Review" also criticised the armed forces for a culture of dismissiveness. One male interview told the Review, for example: "Girls that come to the Army know what to expect." It stated that senior NCOs are frequently seen as tolerating sexual harassment and discouraging the individuals affected from making a complaint.

The following year, in 2016, a major study of 43,000 Canadian armed forces personnel reported that 27% of female personnel reported at least one incident of sexual assault since they joined the military, and 5% of female regular armed forces personnel reported the same in the previous 12 months (equivalent to approximately 960 women per year).

The extent of sexual harassment in the French armed forces first came to light in 2014 when 35 cases of sexual harassment and assault were detailed in "La Guerre Invisible", a book by Leila Minano and Julia Pascual. According to the "Independent" newspaper, the armed forces had not been required to report incidents or to keep statistics, and an official report acknowledged that awareness of the problem had been institutionally suppressed.

Following concerns expressed in 2004 by the UK Equal Opportunities Commission (now the Equality and Human Rights Commission) about persistent sexual harassment in the British armed forces, a number of anonymised, official surveys have been undertaken. The first, in 2006, found that a male-dominated culture sexualised women and diminished their military competence. Among the comments made to researchers by male personnel about their female counterparts were: ‘Ok there are a few exceptions but on the whole they [women] shouldn’t be here.’; 'They're all lesbians or sluts.'; and 'They are emotionally unstable.' The report found that 15% of women had had a 'particularly upsetting' experience of sexual harassment in the previous 12 months; the proportion rose to 20% in the youngest age group. A similar survey in 2009 found the 8% of women in the army had had a ‘particularly upsetting’ experience in the previous 12 months, and the most recent survey, in 2015, found that 13% of women reported the same.

In 2017, a BBC "Panorama" documentary found multiple cases of the sexual abuse of cadets from age 11 during the 1980s, and reported that the victims and their parents were discouraged from making a formal complaint or contacting the police. The Ministry of Defence paid £2 million in 2012 and 2013 to settle allegations of the child sexual abuse of military cadets, and between 2012 and 2017 recorded a further 363 allegations, of which 282 were referred to the police.

The Navy's 1991 Tailhook scandal exposed the egregious acts that occurred during the 35th annual convention in Las Vegas, NV, where 100 Navy and Marine Corp. aviation officers sexually assaulted 83 women and 7 men. Active Navy Lieutenant, aviator, victim, and whistleblower, Paula Coughlin, appeared before congress in 1991 to testify about the sexual abuse and sexual harassment she endured at the hands of her comrades at Tailhook. Her testimony led to improved regulations intended to combat sexual harassment and assault in the military.

Accurate statistics of sexual harassment in the armed forces are elusive as the Pentagon tracks only reports of sexual assault, not sexual harassment. A 2017 Department of Defense report for the 2016 fiscal year indicates that reported incidents of sexual assault increased 10 percent over the previous year, and most often, active duty members were victimized by higher ranking service members. It further states that the Department of Defense does not specifically confirm that sexual harassment causes sexual assault in the armed forces, but does confirm that the two are inexplicably intertwined. The report found that an active duty military woman who reported sexual harassment to a superior was 16 percent more likely to be sexually assaulted than one who did not report, while a man who reported increased his chance of sexual assault after reporting by 50 percent.

In 2017, the Department of Defense reported that an estimated 14,900 military personnel were sexually assaulted in 2016, of whom 6,172 made an official complaint. According to an article published in "Medscape", up to 80% of women in the armed forces have been sexually harassed, and 25% have experienced a sexual assault.





</doc>
<doc id="14337" url="https://en.wikipedia.org/wiki?curid=14337" title="Human sexual activity">
Human sexual activity

Human sexual activity, human sexual practice or human sexual behaviour is the manner in which humans experience and express their sexuality. People engage in a variety of sexual acts, ranging from activities done alone (e.g., masturbation) to acts with another person (e.g., sexual intercourse, non-penetrative sex, oral sex, etc.) in varying patterns of frequency, for a wide variety of reasons. Sexual activity usually results in sexual arousal and physiological changes in the aroused person, some of which are pronounced while others are more subtle. Sexual activity may also include conduct and activities which are intended to arouse the sexual interest of another or enhance the sex life of another, such as strategies to find or attract partners (courtship and display behaviour), or personal interactions between individuals (for instance, foreplay or BDSM). Sexual activity may follow sexual arousal.

Human sexual activity has sociological, cognitive, emotional, behavioural and biological aspects; these include personal bonding, sharing emotions and the physiology of the reproductive system, sex drive, sexual intercourse and sexual behaviour in all its forms.

In some cultures, sexual activity is considered acceptable only within marriage, while premarital and extramarital sex are taboo. Some sexual activities are illegal either universally or in some countries or subnational jurisdictions, while some are considered contrary to the norms of certain societies or cultures. Two examples that are criminal offences in most jurisdictions are sexual assault and sexual activity with a person below the local age of consent.

Sexual activity can be classified in a number of ways: acts which involve one person (also called autoeroticism) such as masturbation, or two or more people such as vaginal sex, anal sex, oral sex or mutual masturbation. Penetrative sex between two people may be described as sexual intercourse, but definitions vary. If there are more than two participants in a sex act, it may be referred to as group sex. Autoerotic sexual activity can involve use of dildos, vibrators, butt plugs, and other sex toys, though these devices can also be used with a partner.

Sexual activity can be classified into the gender and sexual orientation of the participants, as well as by the relationship of the participants. For example, the relationships can be ones of marriage, intimate partners, casual sex partners or anonymous. Sexual activity can be regarded as conventional or as alternative, involving, for example, fetishism, paraphilia, or BDSM activities. Fetishism can take many forms ranging from the desire for certain body parts, for example large breasts, navels or foot worship. The object of desire can often be shoes, boots, lingerie, clothing, leather or rubber items. Some non-conventional autoerotic practices can be dangerous. These include erotic asphyxiation and self-bondage. The potential for injury or even death that exists while engaging in the partnered versions of these fetishes (choking and bondage, respectively) becomes drastically increased in the autoerotic case due to the isolation and lack of assistance in the event of a problem.

Sexual activity can be consensual, which means that both or all participants agree to take part and are of the age that they can consent, or it may take place under force or duress, which is often called sexual assault or rape. In different cultures and countries, various sexual activities may be lawful or illegal in regards to the age, gender, marital status or other factors of the participants, or otherwise contrary to social norms or generally accepted sexual morals.

In evolutionary psychology and behavioral ecology, human mating strategies are a set of behaviors used by individuals to attract, select, and retain mates. Mating strategies overlap with reproductive strategies, which encompass a broader set of behaviors involving the timing of reproduction and the trade-off between quantity and quality of offspring (see life history theory).

Relative to other animals, human mating strategies are unique in their relationship with cultural variables such as the institution of marriage. Humans may seek out individuals with the intention of forming a long-term intimate relationship, marriage, casual relationship, or friendship. The human desire for companionship is one of the strongest human drives. It is an innate feature of human nature, and may be related to the sex drive. The human mating process encompasses the social and cultural processes whereby one person may meet another to assess suitability, the courtship process and the process of forming an interpersonal relationship. Commonalities, however, can be found between humans and nonhuman animals in mating behavior (see animal sexual behavior).

The physiological responses during sexual stimulation are fairly similar for both men and women and there are four phases.

Sexual dysfunction is the inability to react emotionally or physically to sexual stimulation in a way projected of the average healthy person; it can affect different stages in the sexual response cycles, which are desire, excitement and orgasm. In the media, sexual dysfunction is often associated with men, but in actuality, it is more commonly observed in females (43 percent) than males (31 percent).

Sexual activity can lower blood pressure and overall stress levels, regardless of age. It serves to release tension, elevate mood, and possibly create a profound sense of relaxation, especially in the postcoital period. From a biochemical perspective, sex causes the release of endorphins and increases levels of white blood cells that actually boost the immune system. A study published in the journal Biological Psychology described how men who had had sex the previous night responded better to stressful situations, it suggested that if a person is regularly sexual, they're regularly relaxed, and when the person is relaxed, they cope better with stressful situations. A 2007 study published in the Archives of Sexual Behavior 36, (no. 3 (June 2007): 357–68) reported that sexual behavior with a partner on one day significantly predicted lower negative mood and stress, and higher positive mood, on the following day.

People engage in sexual activity for any of a multitude of possible reasons. Although the primary evolutionary purpose of sexual activity is reproduction, research on college students suggested that people have sex for four general reasons: "physical attraction", as a "means to an end", to increase "emotional connection", and to "alleviate insecurity".

Most people engage in sexual activity because of pleasure they derive from the arousal of their sexuality, especially if they can achieve orgasm. Sexual arousal can also be experienced from foreplay and flirting, and from fetish or BDSM activities, or other erotic activities. Most commonly, people engage in sexual activity because of the sexual desire generated by a person to whom they feel sexual attraction; but they may engage in sexual activity for the physical satisfaction they achieve in the absence of attraction for another, as in the case of casual or social sex. At times, a person may engage in a sexual activity solely for the sexual pleasure of their partner, such as because of an obligation they may have to the partner or because of love, sympathy or pity they may feel for the partner.

A person may engage in sexual activity for purely monetary considerations, or to obtain some advantage from either the partner or the activity. A man and woman may engage in sexual intercourse with the objective of conception. Some people engage in hate sex, which occurs between two people who strongly dislike or annoy each other. It is related to the idea that opposition between two people can heighten sexual tension, attraction and interest.

It has been shown that sexual activity plays a large part in the interaction of social species. Joan Roughgarden, in her book "Diversity, Gender, and Sexuality in Nature and People," postulates that this applies equally to humans as it does to other social species. She explores the purpose of sexual activity and demonstrates that there are many functions facilitated by such activity including pair bonding, group bonding, dispute resolution and reproduction.

Research has found that people also engage in sexual activity for reasons associated with self-determination theory. The self-determination theory can be applied to a sexual relationship when the participants have positive feelings associated with the relationship. These participants do not feel guilty or coerced into the partnership. Researchers have proposed the model of self-determined sexual motivation. The purpose of this model is to connect self-determination and sexual motivation. This model has helped to explain how people are sexually motivated when involved in self-determined dating relationships. This model also links the positive outcomes, (satisfying the need for autonomy, competence, and relatedness) gained from sexual motivations.

According to the completed research associated with this model, it was found that people of both sexes who engaged in sexual activity for self-determined motivation had more positive psychological well-being. While engaging in sexual activity for self-determined reasons, the participants also had a higher need for fulfillment. When this need was satisfied, they felt better about themselves. This was correlated with greater closeness to their partner and higher overall satisfaction in their relationship. Though both sexes engaged in sexual activity for self-determined reasons, there were some differences found between males and females. It was concluded that females had more motivation than males to engage in sexual activity for self-determined reasons. Females also had higher satisfaction and relationship quality than males did from the sexual activity. Overall, research concluded that psychological well-being, sexual motivation, and sexual satisfaction were all positively correlated when dating couples partook in sexual activity for self-determined reasons.

The frequency of sexual activity might range from zero (sexual abstinence) to 15 or 20 times a week. In the United States, the average frequency of sexual intercourse for married couples is 2 to 3 times a week. It is generally recognized that postmenopausal women experience declines in frequency of sexual intercourse and that average frequency of intercourse declines with age. According to the Kinsey Institute, the average frequency of sexual intercourse in the US for individuals who have partners is 112 times per year (age 18–29), 86 times per year (age 30–39), and 69 times per year (age 40–49).

The age at which adolescents tend to become sexually active varies considerably between different cultures and from time to time. (See Prevalence of virginity.) The first sexual act of a child or adolescent is sometimes referred to as the sexualization of the child, and may be considered as a milestone or a change of status, as the loss of virginity or innocence. Youth are legally free to have intercourse after they reach the age of consent.

A 1999 survey of students indicated that approximately 40% of ninth graders across the United States report having had sexual intercourse. This figure rises with each grade. Males are more sexually active than females at each of the grade levels surveyed. Sexual activity of young adolescents differs in ethnicity as well. A higher percent of African American and Hispanic adolescents are shown to be more sexually active than White adolescents.

Research on sexual frequency has also been conducted solely on female adolescents who engage in sexual activity. Female adolescents tended to engage in more sexual activity due to positive mood. In female teenagers, engaging in sexual activity was directly positively correlated with being older, greater sexual activity in the previous week or prior day, and more positive mood the previous day or the same day as the sexual activity occurred. Decreased sexual activity was associated with prior or current day negative mood or menstruating.

Although opinions differ, others suggest that sexual activity is an essential part of humans, and that teenagers need to experience sex. According to a research study, sexual experiences help teenagers understand pleasure and satisfaction. In relation to hedonic and eudaimonic well-being, it stated that teenagers can positively benefit from sexual activity. The cross-sectional study was conducted in 2008 and 2009 at a rural upstate New York community. Teenagers who had their first sexual experience at age 16 revealed a higher well-being than those who were sexually inexperienced or who were first sexually active at a later age of 17. Furthermore, teenagers who had their first sexual experience at age 15 or younger, or who had many sexual partners were not negatively affected and did not have associated lower well-being.

Sexual activity is an innately physiological function, but like other physical activity, it comes with risks. There are four main types of risks that may arise from sexual activity: unwanted pregnancy, contracting a sexually transmitted infection (STI/STD), physical injury, and psychological injury.

Any sexual activity that involves the introduction of semen into a woman's vagina, such as during sexual intercourse, or even contact of semen with her vulva, may result in a pregnancy. To reduce the risk of unintended pregnancies, some people who engage in penile-vaginal sex may use contraception, such as birth control pills, a condom, diaphragms, spermicides, hormonal contraception or sterilization. The effectiveness of the various contraceptive methods in avoiding pregnancy varies considerably.

Sexual activity that involves skin-to-skin contact, exposure to an infected person's bodily fluids or mucosal membranes carries the risk of contracting a sexually transmitted infection. People may not be able to detect that their sexual partner has one or more STIs, for example if they are asymptomatic (show no symptoms). The risk of STIs can be reduced by safe sex practices, such as using condoms. Both partners may opt be tested for STIs before engaging in sex. The exchange of body fluids is not necessary to contract an infestation of crab lice. Crab lice typically are found attached to hair in the pubic area but sometimes are found on coarse hair elsewhere on the body (for example, eyebrows, eyelashes, beard, mustache, chest, armpits, etc.). Pubic lice infestations (pthiriasis) are spread through direct contact with someone who is infested with the louse.

Some STIs like HIV/AIDS can also be contracted by using IV drug needles after their use by an infected person, as well as through childbirth or breastfeeding.

Typically, older men and women maintaining interest in sexual interest and activity could be therapeutic; it is a way of expressing their love and care for one another. Factors such as biological and psychological factors, diseases, mental conditions, boredom with the relationship, and widowhood have been found to contribute with the common decrease in sexual interest and activity in old age. National sex surveys given in Finland in the 1990s revealed aging men had a higher incidence of sexual intercourse compared to aging women and that women were more likely to report a lack of sexual desire compared to men. Regression analysis, factors considered important to female sexual activity included: sexual desire, valuing sexuality, and a healthy partner, while high sexual self-esteem, good health, and active sexual history were important to male sexual activity. Both genders in the study agreed they needed good health, good sexual functioning, positive sexual self-esteem, and a sexually skilful partner to maintain sexual desire.

Heterosexuality is the romantic or sexual attraction to the opposite sex. Heterosexual sexual practices are subject to laws in many places. In some countries, mostly those where religion has a strong influence on social policy, marriage laws serve the purpose of encouraging people to have sex only within marriage. Sodomy laws were seen as discouraging same-sex sexual practices, but may affect opposite-sex sexual practices. Laws also ban adults from committing sexual abuse, committing sexual acts with anyone under an age of consent, performing sexual activities in public, and engaging in sexual activities for money (prostitution). Though these laws cover both same-sex and opposite-sex sexual activities, they may differ in regard to punishment, and may be more frequently (or exclusively) enforced on those who engage in same-sex sexual activities.

Different-sex sexual practices may be monogamous, serially monogamous, or polyamorous, and, depending on the definition of sexual practice, abstinent or autoerotic (including masturbation). Additionally, different religious and political movements have tried to influence or control changes in sexual practices including courting and marriage, though in most countries changes occur at a slow rate.

Homosexuality is the romantic or sexual attraction to the same sex. People with a homosexual orientation can express their sexuality in a variety of ways, and may or may not express it in their behaviors. Research indicates that many gay men and lesbians want, and succeed in having, committed and durable relationships. For example, survey data indicate that between 40% and 60% of gay men and between 45% and 80% of lesbians are currently involved in a romantic relationship.

It is possible for a person whose sexual identity is mainly heterosexual to engage in sexual acts with people of the same sex. For example, mutual masturbation in the context of what may be considered normal heterosexual teen development. Gay and lesbian people who pretend to be heterosexual are often referred to as being closeted (hiding their sexuality in "the closet"). "Closet case" is a derogatory term used to refer to people who hide their sexuality. Making that orientation public can be called "coming out of the closet" in the case of voluntary disclosure or "outing" in the case of disclosure by others against the subject's wishes (or without their knowledge). Among some communities (called "men on the DL" or "down-low"), same-sex sexual behavior is sometimes viewed as solely for physical pleasure. Men who have sex with men, as well as women who have sex with women, or men on the "down-low" may engage in sex acts with members of the same sex while continuing sexual and romantic relationships with the opposite sex.

People who engage exclusively in same-sex sexual practices may not identify themselves as gay or lesbian. In sex-segregated environments, individuals may seek relationships with others of their own gender (known as situational homosexuality). In other cases, some people may experiment or explore their sexuality with same (or different) sex sexual activity before defining their sexual identity. Despite stereotypes and common misconceptions, there are no forms of sexual acts exclusive to same-sex sexual behavior that cannot also be found in opposite-sex sexual behavior, except those involving the meeting of the genitalia between same-sex partners – tribadism (generally vulva-to-vulva rubbing, commonly known by its "scissoring" position) and frot (generally penis-to-penis rubbing).

People who have a romantic or sexual attraction to both sexes are referred to as bisexual. People who have a distinct but not exclusive preference for one sex/gender over the other may also identify themselves as bisexual. Like gay and lesbian individuals, bisexual people who pretend to be heterosexual are often referred to as being closeted.

Pansexuality (also referred to as omnisexuality) may or may not be subsumed under bisexuality, with some sources stating that bisexuality encompasses sexual or romantic attraction to all gender identities. Pansexuality is characterized by the potential for aesthetic attraction, romantic love, or sexual desire towards people without regard for their gender identity or biological sex. Some pansexuals suggest that they are gender-blind; that gender and sex are insignificant or irrelevant in determining whether they will be sexually attracted to others. As defined in the "Oxford English Dictionary," pansexuality "encompasses all kinds of sexuality; not limited or inhibited in sexual choice with regards to gender or practice".

Alex Comfort and others propose three potential social aspects of sexual intercourse in humans, which are not mutually exclusive: reproductive, relational, and recreational. The development of the contraceptive pill and other highly effective forms of contraception in the mid- and late 20th century has increased people's ability to segregate these three functions, which still overlap a great deal and in complex patterns. For example: A fertile couple may have intercourse while using contraception to experience sexual pleasure (recreational) and also as a means of emotional intimacy (relational), thus deepening their bonding, making their relationship more stable and more capable of sustaining children in the future (deferred reproductive). This same couple may emphasize different aspects of intercourse on different occasions, being playful during one episode of intercourse (recreational), experiencing deep emotional connection on another occasion (relational), and later, after discontinuing contraception, seeking to achieve pregnancy (reproductive, or more likely reproductive and relational).

Most world religions have sought to address the moral issues that arise from people's sexuality in society and in human interactions. Each major religion has developed moral codes covering issues of sexuality, morality, ethics etc. Though these moral codes do not address issues of sexuality directly, they seek to regulate the situations which can give rise to sexual interest and to influence people's sexual activities and practices. However, the effect of religious teaching has at times been limited. For example, though most religions disapprove of extramarital sexual relations, it has always been widely practiced. Nevertheless, these religious codes have always had a strong influence on peoples' attitudes to issues of modesty in dress, behavior, speech etc.

On the other hand, some people adopt the view that pleasure is its own justification for sexual activity. Hedonism is a school of thought which argues that pleasure is the only intrinsic good.

Human sexual activity, like many other kinds of activity engaged in by humans, is generally influenced by social rules that are culturally specific and vary widely. These social rules are referred to as sexual morality (what can and can not be done by society's rules) and sexual norms (what is and is not expected).

Sexual ethics, morals, and norms relate to issues including deception/honesty, legality, fidelity and consent. Some activities, known as sex crimes in some locations, are illegal in some jurisdictions, including those conducted between (or among) consenting and competent adults (examples include sodomy law and adult-adult incest).

Some people who are in a relationship but want to hide polygamous activity (possibly of opposite sexual orientation) from their partner, may solicit consensual sexual activity with others through personal contacts, online chat rooms, or, advertising in select media.

Swinging, on the other hand, involves singles or partners in a committed relationship engaging in sexual activities with others as a recreational or social activity. The increasing popularity of swinging is regarded by some as arising from the upsurge in sexual activity during the sexual revolution of the 1960s.

Some people engage in various sexual activities as a business transaction. When this involves having sex with, or performing certain actual sexual acts for another person in exchange for money or something of value, it is called prostitution. Other aspects of the adult industry include phone sex operators, strip clubs, and pornography.

Social gender roles can influence sexual behavior as well as the reaction of individuals and communities to certain incidents; the World Health Organization states that, "Sexual violence is also more likely to occur where beliefs in male sexual entitlement are strong, where gender roles are more rigid, and in countries experiencing high rates of other types of violence." Some societies, such as those where the concepts of family honor and female chastity are very strong, may practice violent control of female sexuality, through practices such as honor killings and female genital mutilation.

The relation between gender equality and sexual expression is recognized, and promotion of equity between men and women is crucial for attaining sexual and reproductive health, as stated by the UN International Conference on Population and Development Program of Action:

BDSM is a variety of erotic practices or roleplaying involving bondage, dominance and submission, sadomasochism, and other interpersonal dynamics. Given the wide range of practices, some of which may be engaged in by people who do not consider themselves as practicing BDSM, inclusion in the BDSM community or subculture is usually dependent on self-identification and shared experience. BDSM communities generally welcome anyone with a non-normative streak who identifies with the community; this may include cross-dressers, extreme body modification enthusiasts, animal players, latex or rubber aficionados, and others.

B/D, a form of BDSM, is bondage and discipline. Bondage includes the restraint of the body or mind. D/S means "dominant and submissive." A dominant is someone who takes control of someone who wishes to give up control. A submissive is someone who gives up the control to a person who wishes to take control. S/M (sadism and masochism) means an individual who takes pleasure in the humiliation or pain of others. Masochism means an individual who takes pleasure from their own pain or humiliation.

Unlike the usual "power neutral" relationships and play styles commonly followed by couples, activities and relationships within a BDSM context are often characterized by the participants' taking on complementary, but unequal roles; thus, the idea of informed consent of both the partners becomes essential. Participants who exert sexual dominance over their partners are known as dominants or tops, while participants who take the passive, receiving, or obedient role are known as submissives or bottoms.

Individuals are also sometimes abbreviated when referred to in writing, so a dominant person may be referred to as a "dom" for a man or a woman. Sometimes a woman may choose to use the female specific term "Domme". Both terms are pronounced the same when spoken. Individuals who can change between top/dominant and bottom/submissive roles—whether from relationship to relationship or within a given relationship—are known as "switches". The precise definition of roles and self-identification is a common subject of debate within the community.

In a 2013 study, the researchers state that BDSM is a sexual act where they play role games, use restraint, use power exchange,
use suppression and pain is sometimes involved depending on individual(s). The study serves to challenge the widespread notion that BDSM could be in some way linked to psychopathology. According to the findings, one who participates in BDSM may have greater strength socially and mentally as well as greater independence than those who do not practice BDSM. It suggests that people who participate in BDSM play have higher subjective well-being, and that this might be due to the fact that BDSM play requires extensive communication. Before any sexual act occurs, the partners must discuss their agreement of their relationship. They discuss how long the play will last, the intensity, their actions, what each participant needs or desires. The sexual acts are all recorded as consensual and pleasurable to both parties.

In a 2015 study, interviewed BDSM participants have mentioned that the activities have helped to create higher levels of connection, intimacy, trust and communication between partners. The study suggests that dominants and submissives exchange control for each other's pleasure and to satisfy a need. The participants have remarked that they enjoy pleasing their partner in any way they can and many surveyed have felt that this is one of the best things about BDSM. It gives a submissive pleasure to do things in general for their dominant. Where a Dominant enjoys making their encounters all about the submissive. They enjoy doing things that makes their submissive happy. The findings indicate that the surveyed submissives and dominants found BDSM play more pleasurable and fun. The participants have also mentioned improvements in their personal growth, romantic relationships, sense of community and self, the dominant's confidence, and their coping with everyday things by giving them a psychological release.

There are many laws and social customs which prohibit, or in some way affect sexual activities. These laws and customs vary from country to country, and have varied over time. They cover, for example, a prohibition to non-consensual sex, to sex outside marriage, to sexual activity in public, besides many others. Many of these restrictions are non-controversial, but some have been the subject of public debate.

Most societies consider it a serious crime to force someone to engage in sexual acts or to engage in sexual activity with someone who does not consent. This is called sexual assault, and if sexual penetration occurs it is called rape, the most serious kind of sexual assault. The details of this distinction may vary among different legal jurisdictions. Also, what constitutes effective consent in sexual matters varies from culture to culture and is frequently debated. Laws regulating the minimum age at which a person can consent to have sex (age of consent) are frequently the subject of debate, as is adolescent sexual behavior in general. Some societies have forced marriage, where consent may not be required.

Many locales have laws that limit or prohibit same-sex sexual activity.

In the West, sex before marriage is not illegal. There are social taboos and many religions condemn pre-marital sex. In many Muslim countries, such as Saudi Arabia, Pakistan, Afghanistan, Iran, Kuwait, Maldives, Morocco, Oman, Mauritania, United Arab Emirates, Sudan, Yemen, any form of sexual activity outside marriage is illegal. Those found guilty, especially women, may be forced to wed the sexual partner, publicly beaten, or stoned to death. In many African and native tribes, sexual activity is not viewed as a privilege or right of a married couple, but rather as the unification of bodies and is thus not frowned upon.

Other studies have analyzed the changing attitudes about sex that American adolescents have outside marriage. Adolescents were asked how they felt about oral and vaginal sex in relation to their health, social, and emotional well-being. Overall, teenagers felt that oral sex was viewed as more socially positive amongst their demographic. Results stated that teenagers believed that oral sex for dating and non-dating adolescents was less threatening to their overall values and beliefs than vaginal sex was. When asked, teenagers who participated in the research viewed oral sex as more acceptable to their peers, and their personal values than vaginal sex.

The laws of each jurisdiction set the minimum age at which a young person is allowed to engage in sexual activity. This age of consent is typically between 14 and 18 years, but laws vary. In many jurisdictions, age of consent is a person's mental or functional age. As a result, those above the set age of consent may still be considered unable to legally consent due to mental immaturity. Many jurisdictions regard any sexual activity by an adult involving a child as child sexual abuse.

Age of consent may vary by the type of sexual act, the sex of the actors, or other restrictions such as abuse of a position of trust. Some jurisdictions also make allowances for young people engaged in sexual acts with each other.

Most jurisdictions prohibit sexual activity between certain close relatives. These laws vary to some extent; such acts are called incestuous.

Non-consensual sexual activity or subjecting an unwilling person to witnessing a sexual activity are forms of sexual abuse, as well as (in many countries) certain non-consensual paraphilias such as frotteurism, telephone scatophilia (indecent phonecalls), and non-consensual exhibitionism and voyeurism (known as "indecent exposure" and "peeping tom" respectively).

People sometimes exchange sex for money or access to other resources. Work takes place under many varied circumstances. The person who receives payment for sexual services is called a prostitute and the person who receives such services is known by a multitude of terms, including (and most commonly) "john." Prostitution is one of the branches of the sex industry. The legal status of prostitution varies from country to country, from being a punishable crime to a regulated profession. Estimates place the annual revenue generated from the global prostitution industry to be over $100 billion. Prostitution is sometimes referred to as "the world's oldest profession". Prostitution may be a voluntary individual activity or facilitated or forced by pimps.

Survival sex is a form of prostitution engaged in by people in need, usually when homeless or otherwise disadvantaged people trade sex for food, a place to sleep, or other basic needs, or for drugs. The term is used by sex trade and poverty researchers and aid workers.



</doc>
<doc id="1060279" url="https://en.wikipedia.org/wiki?curid=1060279" title="Coping (psychology)">
Coping (psychology)

In psychology, coping means to invest own conscious effort, to solve personal and interpersonal problems, in order to try to master, minimize or tolerate stress and conflict.

The psychological coping mechanisms are commonly termed "coping strategies" or "coping skills". The term coping generally refers to adaptive (constructive) coping strategies. That is strategies which reduce stress. In contrast, other coping strategies may be coined as maladaptive, if they increase stress. Maladaptive coping is therefore also described, when looking at the outcome, as non-coping. Furthermore, the term coping generally refers to reactive coping, i.e. the coping response which follows the stressor. This differs from proactive coping, in which a coping response aims to neutralize a future stressor. Subconscious or non-conscious strategies (e.g. defense mechanisms) are generally excluded from the area of coping.

The effectiveness of the coping effort depends on the type of stress, the individual, and the circumstances. Coping responses are partly controlled by personality (habitual traits), but also partly by the social environment, particularly the nature of the stressful environment.
Hundreds of coping strategies have been identified. Classification of these strategies into a broader architecture has not been agreed upon. Researchers try to group coping responses rationally, empirically by factor analysis, or through a blend of both techniques. In the early days, Folkman and Lazarus spilt the coping strategies into four groups, namely problem-focused, emotion-focused, support-seeking, and meaning-making coping. Other research, like Weiten, for instance, identifies four types of coping strategies:, like Appraisal-focused (adaptive cognitive), Problem-focused (adaptive behavioral), Emotion-focused, Occupation-focused coping. Billings and Moos added avoidance coping as one of the emotion-focused coping. Some scholar questioned the psychometric validity of forced categorisation as those strategies are not independent to each other. Besides, in reality, people can adopt multiple coping strategies simultaneously.

Appraisal-focused strategies occur when the person modifies the way they think, for example: employing denial, or distancing oneself from the problem. People may alter the way they think about a problem by altering their goals and values, such as by seeing the humor in a situation: "some have suggested that humor may play a greater role as a stress moderator among women than men".

People using problem-focused strategies try to deal with the cause of their problem. They do this by finding out information on the problem and learning new skills to manage the problem. Problem-focused coping is aimed at changing or eliminating the source of the stress. The three problem-focused coping strategies identified by Folkman and Lazarus are: taking control, information seeking, and evaluating the pros and cons. However, problem-focused coping may not necessarily adaptive, but backfire, especially in the uncontrollable case that one cannot make the problem away.

Emotion-focused strategies involve:
Emotion-focused coping "is oriented toward managing the emotions that accompany the perception of stress". The five emotion-focused coping strategies identified by Folkman and Lazarus are:
Emotion-focused coping is a mechanism to alleviate distress by minimizing, reducing, or preventing, the emotional components of a stressor. This mechanism can be applied through a variety of ways, such as:
The focus of this coping mechanism is to change the meaning of the stressor or transfer attention away from it. For example, reappraising tries to find a more positive meaning of the cause of the stress in order to reduce the emotional component of the stressor. Avoidance of the emotional distress will distract from the negative feelings associated with the stressor. Emotion-focused coping is well suited for stressors that seem uncontrollable (ex. a terminal illness diagnosis, or the loss of a loved one). Some mechanisms of emotion focused coping, such as distancing or avoidance, can have alleviating outcomes for a short period of time, however they can be detrimental when used over an extended period. Positive emotion-focused mechanisms, such as seeking social support, and positive re-appraisal, are associated with beneficial outcomes. Emotional approach coping is one form of emotion-focused coping in which emotional expression and processing is used to adaptively manage a response to a stressor.

Typically, people use a mixture of several types of coping strategies, which may change over time. All these strategies can prove useful, but some claim that those using problem-focused coping strategies will adjust better to life. Problem-focused coping mechanisms may allow an individual greater perceived control over their problem, whereas emotion-focused coping may sometimes lead to a reduction in perceived control (maladaptive coping).

Lazarus "notes the connection between his idea of 'defensive reappraisals' or cognitive coping and Freud's concept of 'ego-defenses'", coping strategies thus overlapping with a person's defense mechanisms.

One coping method, anticipating a problem, is known as "proactive coping". Anticipation is when one reduces the stress of some difficult challenge by anticipating what it will be like and preparing for how one is going to cope with it.

Two others are "social coping", such as seeking social support from others, "meaning-focused coping", in which the person concentrates on deriving meaning from the stressful experience.

Adequate nutrition, exercise, sleep contribute to stress management, as do physical fitness and relaxation techniques such as progressive muscle relaxation.

Humor used as a positive coping method may have useful benefits to emotional and mental health well-being. By having a humorous outlook on life, stressful experiences can be and are often minimized. This coping method corresponds with positive emotional states and is known to be an indicator of mental health. Physiological processes are also influenced within the exercise of humor. For example, laughing may reduce muscle tension, increase the flow of oxygen to the blood, exercise the cardiovascular region, and produce endorphins in the body. Using humor in coping while processing through feelings can vary depending on life circumstance and individual humor styles. In regards to grief and loss in life occurrences, it has been found that genuine laughs/smiles when speaking about the loss predicted later adjustment and evoked more positive responses from other people. A person of the deceased family member may resort to making jokes of when the deceased person used to give unwanted “wet willies” (term used for when a person sticks their finger inside their mouth then inserts the finger into another person's ear) to any unwilling participant. A person might also find comedic relief with others around irrational possible outcomes for the deceased funeral service. It is also possible that humor would be used by people to feel a sense of control over a more powerless situation and used as way to temporarily escape a feeling of helplessness. Exercised humor can be a sign of positive adjustment as well as drawing support and interaction from others around the loss.

While adaptive coping strategies improve functioning, a maladaptive coping technique will just reduce symptoms while maintaining and strengthening the disorder. Maladaptive techniques are more effective in the short term rather than long term coping process.

Examples of maladaptive behavior strategies include dissociation, sensitization, safety behaviors, anxious avoidance, and escape (including self-medication).

These coping strategies interfere with the person's ability to unlearn, or break apart, the paired association between the situation and the associated anxiety symptoms. These are maladaptive strategies as they serve to maintain the disorder.

Dissociation is the ability of the mind to separate and compartmentalize thoughts, memories, and emotions. This is often associated with post traumatic stress syndrome.

Sensitization is when a person seeks to learn about, rehearse, and/or anticipate fearful events in a protective effort to prevent these events from occurring in the first place.

Safety behaviors are demonstrated when individuals with anxiety disorders come to rely on something, or someone, as a means of coping with their excessive anxiety.

Anxious avoidance is when a person avoids anxiety provoking situations by all means. This is the most common method.

Escape is closely related to avoidance. This technique is often demonstrated by people who experience panic attacks or have phobias. These people want to flee the situation at the first sign of anxiety.

Further examples of coping strategies include emotional or instrumental support, self-distraction, denial, substance use, self-blame, behavioral disengagement and the use of drugs or alcohol.

Many people think that meditation "not only calms our emotions, but...makes us feel more 'together'", as too can "the kind of prayer in which you're trying to achieve an inner quietness and peace".

Low-effort syndrome or low-effort coping refers to the coping responses of minority groups in an attempt to fit into the dominant culture. For example, minority students at school may learn to put in only minimal effort as they believe they are being discriminated against by the dominant culture.

Otto Fenichel summarized early psychoanalytic studies of coping mechanisms in children as "a gradual substitution of actions for mere discharge reactions...[&] the development of the function of judgement" - noting however that "behind all active types of mastery of external and internal tasks, a readiness remains to fall back on passive-receptive types of mastery."

In adult cases of "acute and more or less 'traumatic' upsetting events in the life of normal persons", Fenichel stressed that in coping, "in carrying out a 'work of learning' or 'work of adjustment', [s]he must acknowledge the new and less comfortable reality and fight tendencies towards regression, towards the misinterpretation of reality", though such rational strategies "may be mixed with relative allowances for rest and for small regressions and compensatory wish fulfillment, which are recuperative in effect".

In the 1940s, the German Freudian psychoanalyst Karen Horney "developed her mature theory in which individuals cope with the anxiety produced by feeling unsafe, unloved, and undervalued by disowning their spontaneous feelings and developing elaborate strategies of defence." She defined four so-called coping strategies to define interpersonal relations, one describing psychologically healthy individuals, the others describing neurotic states.

The healthy strategy she termed "Moving with" is that with which psychologically healthy people develop relationships. It involves compromise. In order to move with, there must be communication, agreement, disagreement, compromise, and decisions. The three other strategies she described – "Moving toward", "Moving against" and "Moving away" – represented neurotic, unhealthy strategies people utilize in order to protect themselves.

Horney investigated these patterns of neurotic needs (compulsive attachments). The neurotics might feel these attachments more strongly because of difficulties within their lives. If the neurotic does not experience these needs, he or she will experience anxiety. The ten needs are:


In Compliance, also known as "Moving toward" or the "Self-effacing solution", the individual moves towards those perceived as a threat to avoid retribution and getting hurt, "making any sacrifice, no matter how detrimental." The argument is, "If I give in, I won't get hurt." This means that: if I give everyone I see as a potential threat whatever they want, I won't be injured (physically or emotionally). This strategy includes neurotic needs one, two, and three.

In Withdrawal, also known as "Moving away" or the "Resigning solution", individuals distance themselves from anyone perceived as a threat to avoid getting hurt – "the 'mouse-hole' attitude ... the security of unobtrusiveness." The argument is, "If I do not let anyone close to me, I won't get hurt." A neurotic, according to Horney desires to be distant because of being abused. If they can be the extreme introvert, no one will ever develop a relationship with them. If there is no one around, nobody can hurt them. These "moving away" people fight personality, so they often come across as cold or shallow. This is their strategy. They emotionally remove themselves from society. Included in this strategy are neurotic needs three, nine, and ten.

In Aggression, also known as the "Moving against" or the "Expansive solution", the individual threatens those perceived as a threat to avoid getting hurt. Children might react to parental in-differences by displaying anger or hostility. This strategy includes neurotic needs four, five, six, seven, and eight.

Related to the work of Karen Horney, public administration scholars developed a classification of coping by frontline workers when working with clients (see also the work of Michael Lipsky on street-level bureaucracy). This coping classification is focused on the behavior workers can display towards clients when confronted with stress. They show that during public service delivery there are three main families of coping:

- Moving "towards" clients: Coping by helping clients in stressful situations. An example is a teacher working overtime to help students.
- Moving "away from" clients: Coping by avoiding meaningful interactions with clients in stressful situations. An example is a public servant stating "the office is very busy today, please return tomorrow." 
- Moving "against" clients: Coping by confronting clients. For instance, teachers can cope with stress when working with students by imposing very rigid rules, such as no cellphone use in class and sending everyone to the office when they use a cellphone. Furthermore, aggression towards clients is also included here.

In their systematic review of 35 years of the literature, the scholars found that the most often used family is moving "towards" clients (43% of all coping fragments). Moving "away from" clients was found in 38% of all coping fragments and Moving "against" clients in 19%.

In 1937, the psychoanalyst (as well as a physician, psychologist, and psychiatrist) Heinz Hartmann marked it as the evolution of ego psychology by publishing his paper, "Me" (which was later translated into English in 1958, titled, "The Ego and the Problem of Adaptation"). Hartmann focused on the adaptive progression of the ego "through the mastery of new demands and tasks". In fact, according to his "adaptive point of view", once infants were born they have the ability to be able to cope with the demands of their surroundings. In his wake, ego psychology further stressed "the development of the personality and of 'ego-strengths'...adaptation to social realities".

Emotional intelligence has stressed the importance of "the capacity to soothe oneself, to shake off rampant anxiety, gloom, or irritability...People who are poor in this ability are constantly battling feelings of distress, while those who excel in it can bounce back far more quickly from life's setbacks and upsets". From this perspective, "the art of soothing ourselves is a fundamental life skill; some psychoanalytic thinkers, such as John Bowlby and D. W. Winnicott see this as the most essential of all psychic tools."

Object relations theory has examined the childhood development both of "[i]ndependent coping...capacity for self-soothing", and of "[a]ided coping. Emotion-focused coping in infancy is often accomplished through the assistance of an adult."

Gender differences in coping strategies are the ways in which men and women differ in managing psychological stress. There is evidence that males often develop stress due to their careers, whereas females often encounter stress due to issues in interpersonal relationships. Early studies indicated that "there were gender differences in the sources of stressors, but gender differences in coping were relatively small after controlling for the source of stressors"; and more recent work has similarly revealed "small differences between women's and men's coping strategies when studying individuals in similar situations."

In general, such differences as exist indicate that women tend to employ emotion-focused coping and the "tend-and-befriend" response to stress, whereas men tend to use problem-focused coping and the "fight-or-flight" response, perhaps because societal standards encourage men to be more individualistic, while women are often expected to be interpersonal. An alternative explanation for the aforementioned differences involves genetic factors. The degree to which genetic factors and social conditioning influence behavior, is the subject of ongoing debate.

Hormones also play a part in stress management. Cortisol, a stress hormone, was found to be elevated in males during stressful situations. In females, however, cortisol levels were decreased in stressful situations, and instead, an increase in limbic activity was discovered. Many researchers believe that these results underlie the reasons why men administer a fight-or-flight reaction to stress; whereas, females have a tend-and-befriend reaction. The "fight-or-flight" response activates the sympathetic nervous system in the form of increased focus levels, adrenaline, and epinephrine. Conversely, the "tend-and-befriend" reaction refers to the tendency of women to protect their offspring and relatives. Although these two reactions support a genetic basis to differences in behavior, one should not assume that in general females cannot implement "fight-or-flight" behavior or that males cannot implement "tend-and-befriend" behavior.




</doc>
<doc id="1834924" url="https://en.wikipedia.org/wiki?curid=1834924" title="Spoiled child">
Spoiled child

A spoiled child or spoiled brat is a derogatory term aimed at children who exhibit behavioral problems from being overindulged by their parents. Children and teens who are perceived as spoiled may be described as "overindulged", "grandiose", "narcissistic" or "egocentric-regressed". Perception is important to take into account, because when the child has a neurological condition such as autism, ADHD or intellectual disability, observers may judge them as "spoiled" without understanding the whole picture. There is no accepted scientific definition of what "spoiled" means, and professionals are often unwilling to use the label because it is considered vague and derogatory. Being spoiled is not recognized as a mental disorder in any of the medical manuals, such as the ICD-10 or the DSM-IV, or its successor, the DSM-5.

Richard Weaver, in his work "Ideas Have Consequences", introduced the term “spoiled child psychology” in 1948. In 1989, Bruce McIntosh coined the term the "spoiled child syndrome". The syndrome is characterized by "excessive, self-centered, and immature behavior". It includes lack of consideration for other people, recurrent temper tantrums, an inability to handle the delay of gratification, demands for having one's own way, obstructiveness, and manipulation to get their way. McIntosh attributed the syndrome to "the failure of parents to enforce consistent, age-appropriate limits", but others, such as Aylward, note that temperament is probably a contributory factor. It is important to note that the temper tantrums are "recurrent". McIntosh observes that "many of the problem behaviors that cause parental concern are unrelated to spoiling as properly understood". Children may have occasional temper tantrums without them falling under the umbrella of "spoiled". Extreme cases of spoiled child syndrome, in contrast, will involve "frequent" temper tantrums, physical aggression, defiance, destructive behavior, and refusal to comply with even the simple demands of daily tasks. This can be similar to the profile of children diagnosed with Pathological Demand Avoidance, which is part of the autism spectrum.


Children with underlying medical or mental health problems may exhibit some of the symptoms. Indeed, where the difficulties are not predicated in the parental-child nexus, many loving parents may be judged as "spoiling" instead of affirmed. Speech or hearing disorders, and attention deficit disorder, may lead to children's failing to understand the limits set by parents. Children who have recently experienced a stressful event, such as the separation of the parents (divorce) or the birth or death of a close family relative, may also exhibit some or all of the symptoms. Children of parents who themselves have psychiatric disorders may manifest some of the symptoms, because the parents behave erratically, sometimes failing to perceive their children's behavior correctly, and thus fail to properly or consistently define limits of normal behavior for them.

Parents can seek advice, support, and encouragement to empower them in parenthood from diverse sources.

Treatment by a physician involves assessing parental competence, and whether the parents set limits correctly and consistently. Physicians should rule out dysfunction in the family, referring dysfunctional families for family therapy and dysfunctional parents for parenting skills training, and counsel parents in methods for modifying their child's behavior.

In early infancy, a baby signals desire for food, contact, and comfort by crying. This behavior should be viewed as a distress signal indicating that some biological need is not being met. Although parents sometimes worry about spoiling their children by giving them too much attention, specialists in child development maintain that babies cannot be spoiled in the first six months of life. During the first year, children are developing a sense of basic trust and attachment. In general, the more attention and care they receive from their parents, the better.

Alfred Adler (1870–1937) believed that "only children" were likely to experience a variety of problems from their situation. Adler theorized that because only children have no rivals for their parents' affection, they will become pampered and spoiled, particularly by their mother. He suggested that this could later cause interpersonal difficulties if the person is not universally liked and admired.

A 1987 quantitative review of 141 studies on 16 different personality traits contradicted Adler's theory. This research found no evidence of any "spoilage" or other pattern of maladjustment in only children. The major finding was that only children are not very different from children with siblings. The main exception to this was the finding that only children are generally higher in achievement motivation. A second analysis revealed that only children, first-borns, and children with only one sibling score higher on tests of verbal ability than later-borns and children with multiple siblings.

Spoiling in early childhood tends to create characteristic reactions that persist, fixed, into later life. These can cause significant social problems. Spoiled children may have difficulty coping with situations such as teachers scolding them or refusing to grant extensions on homework assignments, playmates refusing to allow them to play with their toys and playmates refusing playdates with them, a loss in friends, failure in employment, and failure with personal relationships. As adults, spoiled children may experience problems with anger management, professionalism, and personal relationships; a link with adult psychopathy has been observed.




</doc>
<doc id="149973" url="https://en.wikipedia.org/wiki?curid=149973" title="Procrastination">
Procrastination

Procrastination is the avoidance of doing a task that needs to be accomplished by a certain deadline. It could be further stated as a habitual or intentional delay of starting or finishing a task despite knowing it might have negative consequences. It is a common human experience involving delay in everyday chores or even putting off salient tasks such as attending an appointment, submitting a job report or academic assignment, or broaching a stressful issue with a partner. Although typically perceived as a negative trait due to its hindering effect on one's productivity often associated with depression, low self-esteem, guilt and inadequacy; it can also be considered a wise response to certain demands that could present risky or negative outcomes or require waiting for new information to arrive.

From a cultural perspective, students from both Western and non-Western cultures are found to exhibit academic procrastination, but for different reasons. Students from Western cultures tend to procrastinate in order to avoid doing worse than they have done before or from failing to learn as much as they should have, whereas students from non-Western cultures tend to procrastinate in order to avoid looking incompetent, or to avoid demonstrating a lack of ability in front of their peers. It is also important to consider how different cultural perspectives of time management can impact procrastination. For example, in cultures that have a multi-active view of time, people tend to place a higher value on making sure a job is done accurately before finishing. In cultures with a linear view of time, people tend to designate a certain amount of time on a task and stop once the allotted time has expired.

Various types of procrastination (such as academic/non-academic or behavioural/indecisive) have their own underlying causes and effects. The most prominent explanation in present literature draws upon "Intemporal discounting, task averseness and certain personality traits such as indecisiveness and distractibility" as the common causes of procrastination.

A study of behavioral patterns of pigeons through delayed reward suggests that procrastination is not unique to humans, but can also be observed in some other animals. There are experiments finding clear evidence for "procrastination" among pigeons, which show that pigeons tend to choose a complex but delayed task rather than an easy but hurry-up one.

Latin: "procrastinare", "pro-", 'forward', with "-crastinus", 'till next day' from "cras", 'tomorrow'.
In a study of academic procrastination from the University of Vermont, published in 1984, 46% of the subjects reported that they "always" or "nearly always" procrastinate writing papers, while approximately 30% reported procrastinating studying for exams and reading weekly assignments (by 28% and 30% respectively). Nearly a quarter of the subjects reported that procrastination was a problem for them regarding the same tasks. However, as many as 65% indicated that they would like to reduce their procrastination when writing papers, and approximately 62% indicated the same for studying for exams and 55% for reading weekly assignments.

A 1992 study showed that "52% of surveyed students indicated having a moderate to high need for help concerning procrastination." It is estimated that 80–95% of college students engage in procrastination, and approximately 75% consider themselves procrastinators.

In a study performed on university students, procrastination was shown to be greater on tasks that were perceived as unpleasant or as impositions than on tasks for which the student believed they lacked the required skills for accomplishing the task.

Another point of relevance is that of procrastination in industry. A study: The Impact of Organizational and Personal Factors on Procrastination in Employees of a Modern Russian Industrial Enterprise published in the Psychology in Russia: State of the Art journal, helped to identify the many factors that affected employees' procrastination habits. Some of which include intensity of performance evaluations, importance of their duty within a company, and their perception and opinions on management and/or upper level decisions.

Gregory Schraw, Theresa Wadkins, and Lori Olafson in 2007 proposed three criteria for a behavior to be classified as academic procrastination: it must be counterproductive, needless, and delaying. Steel reviewed all previous attempts to define procrastination, and concluded in a 2007 study that procrastination is "to voluntarily delay an intended course of action despite expecting to be worse off for the delay." Sabini & Silver argued that postponement and irrationality are the two key features of procrastination. Delaying a task is not deemed as procrastination, they argue, if there are rational reasons behind the delay.

An approach that integrates several core theories of motivation as well as meta-analytic research on procrastination is the temporal motivation theory. It summarizes key predictors of procrastination (expectancy, value, and impulsiveness) into a mathematical equation.

The pleasure principle may be responsible for procrastination; one may prefer to avoid negative emotions by delaying stressful tasks. As the deadline for their target of procrastination grows closer, they are more stressed and may, thus, decide to procrastinate more to avoid this stress. Some psychologists cite such behavior as a mechanism for coping with the anxiety associated with starting or completing any task or decision.
Piers Steel indicated in 2010 that anxiety is just as likely to induce people to start working early as late, and that the focus of studies on procrastination should be impulsiveness. That is, anxiety will cause people to delay only if they are impulsive.

Negative coping responses of procrastination tend to be avoidant or emotional rather than task-oriented or focused on problem-solving. Emotional and avoidant coping is employed to reduce stress (and cognitive dissonance) associated with delaying intended and important personal goals. This option provides immediate pleasure and is consequently very attractive to impulsive procrastinators, at the point of discovery of the achievable goals at hand. There are several emotion-oriented strategies, similar to Freudian defense mechanisms, coping styles and self-handicapping. 
Coping responses of procrastinators include the following.


Task- or problem-solving measures are taxing from a procrastinator's outlook. If such measures are pursued, it is less likely the procrastinator would remain a procrastinator. However, pursuing such measures requires actively changing one's behavior or situation to prevent and minimize the re-occurrence of procrastination.

In 2006, it was suggested that neuroticism has no direct links to procrastination and that any relationship is fully mediated by conscientiousness.
In 1982, it had been suggested that irrationality was an inherent feature of procrastination. "Putting things off even until the last moment isn't procrastination if there is a reason to believe that they will take only that moment". Steel "et al." explained in 2001, "actions must be postponed and this postponement must represent poor, inadequate, or inefficient planning".

According to Holly McGregor & Andrew Elliot (2002); Christopher Wolters (2003), academic procrastination among portions of undergraduate students has been correlated to "performance-avoidance orientation" which is one factor of the four factor model of achievement orientation. Andrew Elliot and Judith Harackiewicz (1996) showed that students with a performance-avoidance orientation tend to be concerned with comparisons to their peers. These students procrastinate as a result of not wanting to look incompetent, or to avoid demonstrating a lack of ability and adopt a facade of competence for a task in front of their peers.

Gregory Arief Liem and Youyan Nie (2008) found that cultural characteristics are shown to have a direct influence on achievement orientation because it is closely aligned with most students cultural values and beliefs. Sonja Dekker and Ronald Fischer's (2008) meta-analysis across thirteen different societies revealed that students from Western cultures tend to be motivated more by "mastery-approach orientation" because the degree of incentive value for individual achievement is strongly reflective of the values of Western culture. By contrast, most students from Eastern cultures have been found to be performance-avoidance orientated. They often make efforts to maintain a positive image of their abilities, which they display while in front of their peers. In addition, Hazel Rose Markus and Shinobu Kitayama (1991) showed that in non-Western cultures, rather than standing out through their achievements, people tend to be motivated to become part of various interpersonal relationships and to fit in with those that are relevant to them.

Research by Sushila Niles (1998) with Australian (Western) students and Sri Lankan (Eastern) students confirm these differences, revealing that Australian students often pursued more individual goals, whereas Sri Lankan students usually desired more collaborative and social goals. Multiple studies by Kuo-Shu Yang and An-Bang Yu (1987, 1988, 1990) have indicated that individual achievement among most Chinese and Japanese students are measured by a fulfillment of their obligation and responsibility to their family network, not to an individual accomplishment. Yang and Yu (1987) have also shown that Collectivism and Confucianism are very strong motivators for achievement in many non-Western cultures because of their emphasis on cooperation in the family unit and community. Guided by these cultural values, it is believed that the individual intuitively senses the degree of pressure that differentiates his or her factor of achievement orientation.

To a certain degree it is normal to procrastinate and it can be regarded as a useful way to prioritize between tasks, due to a lower tendency of procrastination on truly valued tasks (for most people). On the other hand, excessive procrastination can become a problem and impede normal functioning. When this happens, procrastination has been found to result in health problems, stress, anxiety, sense of guilt and crisis as well as loss of personal productivity and social disapproval for not meeting responsibilities or commitments. Together these feelings may promote further procrastination and for some individuals procrastination becomes almost chronic. Such procrastinators may have difficulties seeking support due to procrastination itself, but also social stigma and the belief that task-aversion is caused by laziness, lack of willpower or low ambition. In some cases problematic procrastination might be a sign of some underlying psychological disorder, but not necessarily.

Research on the physiological roots of procrastination have been concerned with the role of the prefrontal cortex, the area of the brain that is responsible for executive brain functions such as impulse control, attention and planning. This is consistent with the notion that procrastination is strongly related to such functions, or a lack thereof. The prefrontal cortex also acts as a filter, decreasing distracting stimuli from other brain regions. Damage or low activation in this area can reduce one's ability to avert diversions, which results in poorer organization, a loss of attention, and increased procrastination. This is similar to the prefrontal lobe's role in ADHD, where it is commonly underactivated.

In a 2014 U.S. study surveying procrastination and impulsiveness in fraternal- and identical twin pairs, both traits were found to be "moderately heritable". The two traits were not separable at the genetic level (r = 1.0), meaning no unique genetic influences of either trait alone was found. The authors confirmed three constructs developed from the evolutionary hypothesis that procrastination arose as a by-product of impulsivity: "(a) Procrastination is heritable, (b) the two traits share considerable genetic variation, and (c) goal-management ability is an important component of this shared variation."

Psychologist William J. Knaus estimated that more than 90% of college students procrastinate. Of these students, 25% are chronic procrastinators and typically abandon higher education (college dropouts).

Perfectionism is a prime cause for procrastination because pursuing unattainable goals (perfection) usually results in failure. Unrealistic expectations destroy self-esteem and lead to self-repudiation, self-contempt, and widespread unhappiness. To overcome procrastination, it is essential to recognize and accept the power of failure without condemning, to stop focusing on faults and flaws and to set goals that are easier to achieve.

Behaviors and practices that reduce procrastination:

Making a plan to complete tasks in a rigid schedule format might not work for everyone. There is no hard-and-fast rule to follow such a process if it turns out to be counter-productive. Instead of scheduling, it may be better to execute tasks in a flexible, unstructured schedule which has time slots for only necessary activities.

Piers Steel suggests that better time management is a key to overcoming procrastination, including being aware of and using one's "power hours" (being a "morning person" or "night owl"). A good approach is to creatively utilize one's internal circadian rhythms that are best suited for the most challenging and productive work. Steel states that it is essential to have realistic goals, to tackle one problem at a time and to cherish the "small successes". Brian O'Leary supports that "finding a work-life balance...may actually help us find ways to be more productive", suggesting that dedicating leisure activities as motivation can increase one's efficiency at handling tasks. Procrastination is not a lifelong trait. Those likely to worry can learn to let go, those who procrastinate can find different methods and strategies to help focus and avoid impulses.

After contemplating his own procrastination habits, philosopher John Perry authored an essay entitled "Structured Procrastination", wherein he proposes a "cheat" method as a safer approach for tackling procrastination: using a pyramid scheme to reinforce the unpleasant tasks needed to be completed in a quasi-prioritized order.

For some people, procrastination can be persistent and tremendously disruptive to everyday life. For these individuals, procrastination may be symptomatic of a psychological disorder. Procrastination has been linked to a number of negative associations, such as depression, irrational behaviour, low self-esteem, anxiety and neurological disorders such as ADHD. Others have found relationships with guilt and stress. Therefore, it is important for people whose procrastination has become chronic and is perceived to be debilitating to seek out a trained therapist or psychiatrist to investigate whether an underlying mental health issue may be present.

With a distant deadline, procrastinators report significantly less stress and physical illness than do non-procrastinators. However, as the deadline approaches, this relationship is reversed. Procrastinators report more stress, more symptoms of physical illness, and more medical visits, to the extent that, overall, procrastinators suffer more stress and health problems.

Procrastination has been linked to the complex arrangement of cognitive, affective and behavioral relationships from task desirability to low self esteem and anxiety to depression. A study found that procrastinators were less future-oriented than their non-procrastinator counterparts. This result was hypothesized to be in association with hedonistic perspectives on the present; instead it was found procrastination was better predicted by a fatalistic and hopeless attitude towards life.

A correlation between procrastination and eveningness was observed where individuals who had later sleeping and waking patterns were more likely to procrastinate. It has been shown that Morningness increases across lifespan and procrastination decreases with age.,

Traditionally, procrastination has been associated with perfectionism: a tendency to negatively evaluate outcomes and one's own performance, intense fear and avoidance of evaluation of one's abilities by others, heightened social self-consciousness and anxiety, recurrent low mood, and "workaholism". However, adaptive perfectionists—egosyntonic perfectionism—were "less" likely to procrastinate than non-perfectionists, while maladaptive perfectionists, who saw their perfectionism as a problem—egodystonic perfectionism—had high levels of procrastination and anxiety.
In a regression analysis study of Steel, from 2007, it is found that mild to moderate perfectionists typically procrastinate slightly less than others, with "the exception being perfectionists who were also seeking clinical counseling".

According to an Educational Science Professor, Hatice Odaci, academic procrastination is a significant problem during college years in part because many college students lack efficient time management skills in using the Internet. Also, Odaci notes that most colleges provide free and fast twenty-four-hour Internet service which some students are not usually accustomed to, and as a result of irresponsible use or lack of firewalls these students become engulfed in distractions, and thus in procrastination.

"Student syndrome" refers to the phenomenon where a student will begin to fully apply themself to a task only immediately before a deadline. This negates the usefulness of any buffers built into individual task duration estimates. Results from a 2002 study indicate that many students are aware of procrastination and accordingly set binding deadlines long before the date for which a task is due. These self-imposed binding deadlines are correlated with a better performance than without binding deadlines though performance is best for evenly spaced external binding deadlines. Finally, students have difficulties optimally setting self-imposed deadlines, with results suggesting a lack of spacing before the date at which results are due.
In one experiment, participation in online exercises was found to be five times higher in the final week before a deadline than in the summed total of the first three weeks for which the exercises were available. Procrastinators end up being the ones doing most of the work in the final week before a deadline.

Other reasons cited on why students procrastinate include fear of failure and success, perfectionist expectations, as well as legitimate activities that may take precedence over school work, such as a job.

Procrastinators have been found to receive worse grades than non-procrastinators. Tice et al. (1997) report that more than one-third of the variation in final exam scores could be attributed to procrastination. The negative association between procrastination and academic performance is recurring and consistent. Howell et al. (2006) found that, though scores on two widely used procrastination scales were not significantly associated with the grade received for an assignment, self-report measures of procrastination on the assessment itself were negatively associated with grade.

In 2005, a study conducted by Angela Chu and Jin Nam Choi and published in the "Journal of Social Psychology" intended to understand task performance among procrastinators with the definition of procrastination as the absence of self-regulated performance, from the 1977 work of Ellis & Knaus. In their study they identified two types of procrastination: the traditional procrastination which they denote as passive, and active procrastination where the person finds enjoyment of a goal-oriented activity only under pressure. The study calls this active procrastination positive procrastination, as it is a functioning state in a self-handicapping environment. In addition, it was observed that active procrastinators have more realistic perceptions of time and perceive more control over their time than passive procrastinators, which is considered a major differentiator between the two types. But surprisingly, active and passive procrastinators showed similar levels of academic performance. The population of the study was college students and the majority of the sample size were women and Asian in origin. Comparisons with chronic pathological procrastination traits were avoided.

Different findings emerge when observed and self-reported procrastination are compared. Steel et al. constructed their own scales based on Silver and Sabini's "irrational" and "postponement" criteria. They also sought to measure this behavior objectively. During a course, students could complete exam practice computer exercises at their own pace, and during the supervised class time could also complete chapter quizzes. A weighted average of the times at which each chapter quiz was finished formed the measure of observed procrastination, whilst observed irrationality was quantified with the number of practice exercises that were left uncompleted. Researchers found that there was only a moderate correlation between observed and self-reported procrastination (r = 0.35). There was a very strong inverse relationship between the number of exercises completed and the measure of postponement (r = −0.78). Observed procrastination was very strongly negatively correlated with course grade (r = −0.87), as was self-reported procrastination (though less so, r = −0.36). As such, self-reported measures of procrastination, on which the majority of the literature is based, may not be the most appropriate measure to use in all cases. It was also found that procrastination itself may not have contributed significantly to poorer grades. Steel et al. noted that those students who completed all of the practice exercises "tended to perform well on the final exam no matter how much they delayed."

Procrastination is considerably more widespread in students than in the general population, with over 70 percent of students reporting procrastination for assignments at some point. A 2014 panel study from Germany among several thousand university students found that increasing academic procrastination increases the frequency of seven different forms of academic misconduct, i.e., using fraudulent excuses, plagiarism, copying from someone else in exams, using forbidden means in exams, carrying forbidden means into exams, copying parts of homework from others, fabrication or falsification of data and the variety of academic misconduct. This study argues that academic misconduct can be seen as a means to cope with the negative consequences of academic procrastination such as performance impairment.





</doc>
<doc id="48917358" url="https://en.wikipedia.org/wiki?curid=48917358" title="Theory of narrative thought">
Theory of narrative thought

The Theory of Narrative Thought is a theory of thought designed to bridge the gap between the neurological functioning of the brain and the flow of everyday conscious experience. Proposed by Lee Roy Beach, the theory is expanded by Beach, Byron Bissell, and James Wise (2016).

 The theory of narrative thought (TNT) is a refinement of image theory which was developed as an alternative to rational choice theory. The essence of image theory is that decisions are shaped by long-term attempts to manage the future rather than short-term results. The image theory model of the decision process, called the compatibility test, held up in both laboratory and field tests (Beach, L. R., & Connolly, T. (2005). The Psychology of Decision Making: People in Organizations. Thousand Oaks, CA: Sage) and has been retained, but renamed, as the discrepancy test in TNT.

The theory of narrative thought (TNT) describes the brain as being composed of subsystems that act together to give rise to consciousness. Some of these subsystems, such as perception and memory, contribute content to consciousness. One subsystem in the left hemisphere codes the content in language while a corresponding subsystem in the right hemisphere links it with emotions. Working together as a single system, called the “interpreter,” they organize the content of consciousness. This order is the origin of narrative but at this point it is rudimentary, a proto-narrative. The proto-narrative is simple but up-to-date; it reflects what is happening at the moment. For infants, proto-narratives are pretty much the whole story, but as memories are formed, past proto-narratives become linked with present proto-narratives and, as caregivers lend a hand, more elaborate narratives take shape. As a store of elaborated narratives develops, proto-narratives continue to update them with current information from the perceptual system. This keeps the narratives aware of what is going on in the internal and external environments. Over time, two kinds of narratives develop.

Chronicle narratives are about what happened in the past, what is happening now, and what is expected to happen in the future. They are the ongoing story of one’s conscious experience. Procedural narratives are about how to do things. They are about the actions one can take to better inform one’s chronicle narratives. They also are about the actions one can take to shape the future by acting directly upon the internal or external environment to produce desired results. Procedural narratives are subordinate to chronicle narratives. When used alone, the word “narrative” refers to chronicle narratives. [Note that chronicle and procedural narratives are parallel to, but not identical to, Daniel Kahneman’s (2011) System 1 and System 2.]

Narratives are more than just updated memories about what has happened recently. They are the stuff of ongoing conscious experience, of moment-to-moment thinking, of the richness of mental life, and they are the foundations for informed guesses about the future. They are a mixture of memories and of visual, auditory, and other imagery as well as the accompanying emotions. Their elements are symbols that stand for real or imagined events and actors (including oneself), where the actors are animate beings or inanimate forces. The events and actors are linked by causality. Causality implies temporality and, for animate actors, purpose. That is, a narrative consists of a temporal arrangement of events that are purposefully caused by animate beings (including oneself) or are the result of inanimate forces. The narrative’s storyline is its meaning, which is created by a coherent arrangement of the events and actors.

Causality is the structural backbone of all narratives, both chronicle and procedural. This is because it is the source of narrative temporality. This temporal aspect of causality works retrospectively, from effect to cause, which allows one to account for what is happening now as a result of what has happened in the past. And it works prospectively, from cause to effect, which allows one to set expectations for what will happen in the future as a result of what is happening now and what came before. A good narrative is plausible if its actors’ actions contribute to the story line and are not uncharacteristic, i.e., are reasonably consistent across narratives. A good narrative is coherent if the actions of the actors and the effects of those actions conform to one’s causal rules. In short, a good narrative makes sense in that there are no loose ends.

Noncontingent rules are about what to expect to happen as a result of actions by other people or outside forces over which one has no control. Contingent rules are about what one expects to happen as a result of one’s own actions. These rules form the causal linkages among narratives’ elements and are the bases of expected and action projections respectively.

Causal rules tell one what caused something to happen or how to make something happen. Another kind of rule, normative rules, tell one why something should happen. Normative rules are about what qualifies as desirable and what does not. That is, normative rules are standards for what is ethical, right, proper, principled, reasonable, appropriate, preferred, and so on, all of which are one’s values and preferences.

The current narrative is the narrative one is focused upon at the moment. It is the narrative that makes sense of what has happened leading up to the present, what is happening right now, and what will happen next. The latter, results from extrapolating the past and present segments of current narrative to make educated guesses about the future. A guess about how the future might unfold if one does not make an effort to change it is called an expected projection and a guess about how the future might unfold as a result of one’s efforts to change it is called an action projection. Both projections are narratives. Both are extrapolations of the current narrative. Both are constructed using one’s store of causal rules. Both are subject to the scrutiny of normative rules. Both are imaginary.

When the future is projected, based upon the current narrative, the emotions associated with the relevant normative rules provide the criteria for evaluating the desirability of that future. Each feature of the projected future elicits an emotion that reflects how well it meets the relevant standard(s) set by one’s normative rule(s). A positive emotion is elicited if it meets the standard(s) and a negative emotion is elicited if it doesn’t. The strength of the elicited emotion is a function of the degree to which the feature exceeds or falls short of the standard(s). When evaluating the desirability of the projected future, the focus is on the features that fall short of one’s standards—called violations. If the violations are too abundant, too large, or too important, the associated negative emotions mount to a point at which one must conclude that the projected future will be undesirable. When this happens, survival, or simply unacceptable discomfort, dictates that something must be done to change the future so that when it arrives it will be more desirable than if things are simply allowed to happen as they will. The action agenda for changing the future is called a plan and execution of that plan is called implementation. The anticipated result of implementing the plan is the action projection. An acceptable plan is one that offers a sufficiently desirable future, a future that is compatible with one’s values and preferences.

A plan is a narrative about how one intends to go from the present to a more desirable future—how one intends to influence the course of ongoing events in a desirable direction. Like other narratives, plans must be plausible and coherent. Plausibility means that all the relevant elements, including oneself, are included and that their proposed actions are reasonable. Coherence means that the sequence of tactics creates a feasible causal chain from the present to the desired future. Plausibility is revealed by the ease with which one can imagine oneself and other actors engaging in a successful implementation of the plan, even if it will require hard work. Plausibility also requires that this activity seems natural and lifelike, and that success doesn’t rely on unlikely events or unrealistic amounts effort and resources. Coherence is revealed by the completeness of the causal chain of tactics, even though one knows one probably won’t end up doing things exactly as planned. A plan that lacks plausibility and/or coherence inspires little confidence (which is an emotion associated with a normative standard), which prompts its revision or replacement by another plan.

Briefly stated: (1) There are two kinds of narratives: chronicle narratives, which are stories about ongoing events, including the expected future, and procedural narratives, which are stories about how to do things; (2) both kinds of narrative are structured by time and causality and both are “good narratives” if they are plausible and coherent; (3) the undesirability of the expected future is assessed by its failure to meet standards set by one’s values and preferences; (4) action is prompted when the projected future is determined to be so undesirable as to be unacceptable.

Both Beach (2010) and Beach, Bissell, and Wise (2016) open with a historical description of the ebbing fortunes of “executive mind,” as a scientific concept but how, over the past 50 years, it has come back, albeit on firmer ground, as “the brain.” But while “the brain” may be the new name for “mind,” the fact that the old name continues to be used in both scientific and everyday discourse suggests that the new name doesn’t quite do the job. That is, rather than merely reflecting sloppy usage, the word “mind” reflects something more than merely neural activity in the brain.

Other than the computer analogy and the information processing metaphor, cognitive science has produce no encompassing theory. But it seems to be widely assumed that when such a theory appears, it will necessarily be in terms of brain function to which conscious experience is subsidiary and derivative. TNT challenges this assumption by recognizing that brain functioning and conscious experience are two qualitatively different things. Although subjective experience derives from brain activity, it is not reducible to it. Therefore, the theory recognizes the importance of the brain and how it works, but it also recognizes the importance of conscious experience and how it works. Putting the two together as equally legitimate and complementary provides the “something more” than neural activity in the brain that is implied by the word “mind.”
Much has been written about the characteristics of mind, mostly by philosophers. The commonalities across those discussions reveals three primary characteristics that a theory of mind should address and that are, in fact, addressed by TNT: Thinking that is both reflective and reflexive—that is able to consider both itself and things other than itself. Knowing that allows distinctions between truth and falsity, error and ignorance, and belief and opinion. Purpose that results in actions aimed at foreseen objectives. In addition, TNT addresses the most common questions raised in these philosophical discussions: How does the mind operate? What are its “intrinsic excellences or defects?” How is it related to matter, to bodily organs, to material conditions, and to other minds? Is it possessed in common with animals? Does it exist separate from corporeality? Insofar as the theory adequately deals with these characteristics and answers these questions, it qualifies as a modern theory of mind.



</doc>
<doc id="25692022" url="https://en.wikipedia.org/wiki?curid=25692022" title="Human ethology">
Human ethology

Human ethology is the study of human behavior.
Ethology as a discipline is generally thought of as a sub-category of biology, though psychological theories have been developed based on ethological ideas (e.g. sociobiology, evolutionary psychology, attachment theory, and theories about human universals such as gender differences, incest avoidance, mourning, hierarchy and pursuit of possession). The bridging between biological sciences and social sciences creates an understanding of human ethology.

Ethology has its roots in the study of evolution, especially after evolution's increasing popularity after Darwin's detailed observations. It became a distinct discipline in the 1930s with zoologists Konrad Lorenz, Niko Tinbergen and Karl Von Frisch1.

Many developmental psychologists were eager to incorporate ethological principles into their theories as a way of explaining observable phenomenon in babies that could not necessarily be explained by learning or other concepts. John Bowlby and Mary Ainsworth used ethology prominently to explain aspects of infant-caretaker‍‍ attachment theory‍‍ (Ainsworth & Bowlby, 1991). Some important attachment concepts related to evolution:


In later years, ethology played a large role in sociobiological theory and ultimately, in evolutionary psychology, which is a relatively new field of study. Evolutionary psychology combines ethology, primatology, anthropology, and other fields to study modern human behavior in relation to adaptive ancestral human behaviors.



Applied to human behavior, in the majority of cases, topical behavior results from motivational states and the intensity of a specific external stimulus. Organisms with a high inner motivational state for such a stimulus is called appetitive behavior. Other important concepts of zooethology, e.g., territoriality, hierarchy, sensitive periods in ontogenesis, etc., are also useful when discussing human behavior. Irenäus Eibl-Eibesfeldt's book "Human Ethology" is most important for how these concepts are applied to human behavior.

Human ethology has contributed in two particular ways to our understanding of the ontogeny of behavior in humans. This has resulted, first, from the application of techniques for the precise observation, description and classification of naturally occurring behavior and, secondly, from the ethological approach to the study of behavior, especially the development of behavior in terms of evolution. Of particular interest are questions relating to the function of a particular kind of behavior (e.g., attachment behavior) and its adaptive value. The description of the behavioral repertoire of a species, the recognition of patterns of behavioral development and the classification of established behavioral patterns are prerequisites for any comparison between different species or between organisms of a single species. The ethological approach is the study of the interaction between the organism with certain innate species-specific structures and the environment for which the organism is genetically programmed.

Invariant behavior patterns have a morphological basis, mainly in neuronal structures common to all members of a species and, depending on the kind of behavior, may also be common to a genus or family or a whole order, e.g., primates, or even to a whole class, e.g., mammals. In such structures we can retrace and follow the evolutionary process by which the environment produced structures, especially nervous systems and brains, which generate adaptive behavior. In organisms with a high level of organization, the processes in which the ethologist is especially interested are those genetically preprogrammed motor and perceptual processes that facilitate social interaction and communication, such as facial expression and vocalization. If we consider the most highly developed means of communication, language and speech, which is found in humans alone, the question arises as to the biological foundation of this species-specific behavior and perceptual skill. The ethologist examines this question primarily from the point of view of ontogenetic development.

The main strength of human ethology has been its application of established interpretive patterns to new problems. On the basis of theories, concepts and methods that have proved successful in animal ethology, it looks at human behavior from a new viewpoint. The essence of this is the evolutionary perspective. But since ethologists have been relatively unaffected by the long history of the humanities, they often refer to facts and interpretations neglected by other social sciences. If we look back at the history of the relationship between the life sciences and the social sciences, we find two prevailing modes of theoretical orientation: on the one hand, reductionism, i.e., attempts to reduce human action to non-cognitive behavior; and on the other, attempts to separate human action and human society completely from the animal world. The advent of the theory of evolution in the 19th century brought no easy solution to the problem of nature and nurture, since it could still be "solved" in either a continuous or discontinuous manner. Human ethology as much as any other discipline significantly contributes to the obsolescence of such simple dichotomies.

Human Ethology has an increasing influence on the dialogue between Human Sciences and Humanities as shown for example with the book "Being Human - Bridging the Gap between the Sciences of Body and Mind".

‍‍Ethologists‍‍ study behavior using two general methods: naturalistic observation and laboratory experimentation. Ethologist's insistence on observing organisms in their natural environment differentiates ethology from related disciplines such as evolutionary psychology and sociobiology, and their naturalistic observation "ranks as one of their main contributions to psychology" (Miller, 2001). Naturalistic Observation
Ethologist believe that in order to study species-specific behaviors, a species must be observed in its natural environment. One can only understand the function of a behavior by seeing how it specifically fits into the ‍‍species‍‍ natural environment in order to fulfill a specific need. Ethologist follow a specific set of steps when studying an organism:
These steps fall in line with Tinbergen's (1963) "On Aims of Methods of Ethology" in which he states that all studies of behavior must answer four questions to be considered legitimate.1. function (adaptation), 2.evolution (phylogeny), 3. causation (mechanism), and 4. development (ontogeny) needed to answer in a study.







</doc>
<doc id="43094" url="https://en.wikipedia.org/wiki?curid=43094" title="Gender role">
Gender role

A gender role, also known as a sex role, is a social role encompassing a range of behaviors and attitudes that are generally considered acceptable, appropriate, or desirable for people based on their actual or perceived sex. Gender roles are usually centered on conceptions of femininity and masculinity, although there are exceptions and variations. The specifics regarding these gendered expectations may vary substantially among cultures, while other characteristics may be common throughout a range of cultures. There is ongoing debate as to what extent gender roles and their variations are biologically determined, and to what extent they are socially constructed.

Various groups, most notably the feminist movement, have led efforts to change aspects of prevailing gender roles that they believe are oppressive or inaccurate.

The term "gender role" was first used by John Money and colleagues in 1954, during the course of his study of intersex individuals, to describe the manners in which these individuals expressed their status as a male or female in a situation where no clear biological assignment existed.

The World Health Organization (WHO) defines gender roles as "socially constructed roles, behaviors, activities and attributes that a given society considers appropriate for men and women". Debate continues as to what extent gender and gender roles are socially constructed (i.e. non-biologically influenced), and to what extent "socially constructed" may be considered synonymous with "arbitrary" or "malleable". Therefore, a concise authoritative definition of gender roles or gender itself is elusive.

In the sociology of gender, the process whereby an individual learns and acquires a gender role in society is termed "gender socialization".

Gender roles are culturally specific, and while most cultures distinguish only two (boy and girl or man and woman), others recognize more. Androgyny, for example, has been proposed as a third gender. Androgynous is simply a person with qualities pertaining to both the male and female gender. Other societies have claimed to identify more than five genders, and some non-Western societies have three genders – man, woman, and third gender. Some individuals (not necessarily being from such a culture) identify with no gender at all.

Many transgender people reject the idea that they are a separate third gender, and identify simply as men or women. However, biological differences between (some) trans women and cisgender women have historically been treated as relevant in certain contexts, especially those where biological traits may yield an unfair advantage such as sport.

Gender "role," which refers to the cultural expectations as understood by gender classification, is not the same thing as gender identity, which refers to the internal sense of one's own gender, whether or not it aligns with categories offered by societal norms. The point at which these internalized gender identities become externalized into a set of expectations is the genesis of a gender role.

Some theories – which are collectively termed "social construction" theories – claim that gender behavior is mostly due to social conventions, although opposing theories disagree, such as theories in evolutionary psychology.

Most children learn to categorize themselves by gender by the age of three. From birth, in the course of gender socialization, children learn gender stereotypes and roles from their parents and environment. In a traditional view, males learn to manipulate their physical and social environment through physical strength or dexterity, while girls learn to present themselves as objects to be viewed. Social constructionists state, for example, that gender-segregated children's activities create the appearance that gender differences in behavior reflect an essential nature of male and female behavior.

As an aspect of role theory, gender role theory "treats these differing distributions of women and men into roles as the primary origin of sex-differentiated social behavior, their impact on behavior is mediated by psychological and social processes." According to Gilbert Herdt, gender roles arose from correspondent inference, meaning that general labour division was extended to gender roles.

Gender roles are considered by social constructionists to be hierarchical, and are characterized as a male-advantaged gender hierarchy. The term patriarchy, according to researcher Andrew Cherlin, defines "a social order based on the domination of women by men, especially in agricultural societies".

According to Eagly "et al.", the consequences of gender roles and stereotypes are sex-typed social behavior because roles and stereotypes are both socially shared descriptive norms and prescriptive norms.

Judith Butler, in works such as "Gender Trouble" and "Undoing Gender", contends that being female is not "natural" and that it appears natural only through repeated performances of gender; these performances in turn, reproduce and define the traditional categories of sex and/or gender.

Working in the United States, Talcott Parsons developed a model of the nuclear family in 1955, which at that place and time was the prevalent family structure. It compared a strictly traditional view of gender roles (from an industrial-age American perspective) with a more liberal view. The Parsons model was used to contrast and illustrate extreme positions on gender roles.

However, these structured positions become less a liberal-individualist society, and the actual behavior of individuals is usually somewhere between these poles. According to the interactionist approach, roles (including gender roles) are not fixed but are constantly negotiated between individuals. In North America and southern South America, this is the most common approach among families whose business is agriculture.

Gender roles can influence all kinds of behaviors, such as choice of clothing, choice of work and personal relationships, e.g., parental status (See also Sociology of fatherhood).

Geert Hofstede, a Dutch researcher and social psychologist who dedicated himself to the study of culture, sees culture as "broad patterns of thinking, feeling and acting" in a society In Hofstede's view, masculinity and femininity differ in the social roles that are associated with the biological fact of the existence of the two sexes: masculinity and femininity refer to the dominant sex role pattern in the vast majority of both traditional and modern societies, males being more assertive and females more nurturing.

Femininity creates a society of overlapping gender roles, where "both men and women are supposed to be modest, tender, and concerned with the quality of life."

Masculinity creates a society of clearly distinct gender roles, where men should "be assertive, tough, and focused on material success," while women should "be more modest, tender, and concerned with the quality of life."

Hofstede's "Feminine and Masculine Culture Dimensions" states:

Masculine cultures expect men to be assertive, ambitious and competitive, to strive for material success, and to respect whatever is big, strong, and fast. Masculine cultures expect women to serve and care for the non-material quality of life, for children and for the weak. Feminine cultures, on the other hand, define relatively overlapping social roles for the sexes, in which, in particular, men need not be ambitious or competitive but may go for a different quality of life than material success; men may respect whatever is small, weak, and slow.

In feminine cultures, modesty and relationships are important characteristics. This differs from in masculine cultures, where self-enhancement leads to self-esteem. Masculine cultures are individualistic, and feminine cultures are more collective because of the significance of personal relationships.

'The dominant values in a masculine society are achievement and success; the dominant values in a feminine society are caring for others and quality of life'.

In the 1940s, Albert Ellis studied eighty-four cases of mixed births and concluded that 'while the power of the human sex drive may possibly be largely dependent on physiological factors... the direction of this drive does not seem to be directly dependent on constitutional element'. In the development of masculinity, femininity, and inclinations towards homosexuality or heterosexuality, nurture matters a great deal more than nature.

"In the 1950s, John Money, along with colleagues took up the study of intersex individuals, who, Money realized 'would provide invaluable material for the comparative study for bodily form and physiology, rearing, and psychosexual orientation'." "Money and his colleagues used their own studies to state in the extreme what these days seems extraordinary for its complete denial of the notion of natural inclination."

They concluded that gonads, hormones, and chromosomes did not automatically determine a child's gender role. Among the many terms he coined was "gender role" which he defined in a seminal 1955 paper as "all those things that a person says or does to disclose himself or herself as having the status of boy or man, girl or woman."

In recent years, the majority of Money's theories regarding the importance of socialization in the determination of gender have come under intense criticism, especially in connection with the false reporting of success in the "John/Joan" case, later revealed to be David Reimer.

West and Zimmerman developed an interactionist perspective on gender beyond its construction as "roles." For them, gender is "the product of social doings of some sort...undertaken by men and women whose competence as members of society is hostage to its production". They argue that the use of "role" to describe gender expectations conceals the production of gender through everyday activities. Furthermore, roles are situated identities, such as "nurse" and "student," developed as the situation demands while gender is a master identity with no specific site or organizational context. For them, "conceptualizing gender as a role makes it difficult to assess its influence on other roles and reduces its explanatory usefulness in discussions of power and inequality". West and Zimmerman consider gender an individual production that reflects and constructs interactional and institutional gender expectations.

Because of the influence of Simone de Beauvoir's feminist works and Michel Foucault's reflections on sexuality (among others), the idea that gender was unrelated to sex gained ground during the 1980s, especially in sociology and cultural anthropology. This view asserts that the relationship between gender and sex (presence of genitals/gonads) is not causally determinate. That is, that one may have the genitals of one sex while having the gender of another.

There continues to be debate on the subject. Simon Baron-Cohen, a Cambridge University professor of psychology and psychiatry claims 'the female brain is predominantly "hard-wired" for empathy, while the male brain is predominantly "hard-wired" for understanding and building systems'. However, Nash and Grossi describe his study of newborns as "fraught with methodological problems".

Several studies have been conducted looking at the gender roles of intersex children.

One such study looked at female infants with adrenal hyperplasia, and who had excess male hormone levels, but were thought to be females and raised as such by their parents. These girls were more likely to express masculine traits.

Another study looked at 18 infants with the intersex condition 5-alpha reductase deficiency, and XY chromosomes, assigned female at birth. At adult age only one individual maintained a female role, all the others being stereotypically male.

In a third study, 14 male children born with cloacal exstrophy and assigned female at birth, including through intersex medical interventions. Upon follow-up between the ages of 5 to 12, eight of them identified as boys, and all of the subjects had at least moderately male-typical attitudes and interests.
Dr. Sandra Lipsitz Bem is a psychologist who developed the gender schema theory, based on the combination of aspects of the social learning theory and the cognitive-development theory of sex role acquisition, to explain how individuals come to use gender as an organizing category in all aspects of their life. In 1971, she created the Bem Sex-Role Inventory to measure how well an individual conformed to a traditional gender role, characterizing those tested as having masculine, feminine, androgynous, or undifferentiated personality. She believed that through gender-schematic processing, a person spontaneously sorts attributes and behaviors into masculine and feminine categories, and that therefore individuals processes information and regulate their behavior based on whatever definitions of femininity and masculinity their culture provides.

While there are differences in average capabilities of various kinds (E.g. better average balance in females or greater average physical size and endurance in males) between the sexes the capabilities of some members of one sex will fall within the range of capabilities needed for tasks conventionally assigned to the other sex. Eve Shapiro, author of "Gender Circuits", explains that "gender, like other social categories, is both a personal identity and a culture set of behaviors, beliefs and values."

Research at the Yerkes National Primate Research Center has also shown that gender roles may be biological among primates. Yerkes researchers studied the interactions of 11 male and 23 female Rhesus monkeys with human toys, both wheeled and plush. The males played mostly with the wheeled toys while the females played with both types equally. Study co-author Kim Wallen has, however, warned against overinterpreting the results as the color and size of the toys may also have been factors.

Ideas of appropriate behavior according to gender vary among cultures and era, although some aspects receive more widespread attention than others. R.W. Connell in "Men, Masculinities and Feminism" claims:

There are huge areal differences in attitudes towards appropriate gender roles. In the "World Values Survey", responders were asked if they thought that wage work should be restricted to only men in the case of shortage in jobs: in Iceland the proportion that agreed with the proposition was 3.6%; while in Egypt it was 94.9%.

Attitudes have also varied historically, for example, in Europe, during the Middle Ages, women were commonly associated with roles related to medicine and healing. Because of the rise of witch-hunts across Europe and the institutionalization of medicine, these roles became exclusively associated with men but in the last few decades these roles have become largely gender-neutral in Western society.

Vern Bullough stated that homosexual communities are generally more tolerant of switching gender roles. For instance, someone with a masculine voice, a five o'clock shadow (or a fuller beard), an Adam's apple, wearing a woman's dress and high heels, carrying a purse would most likely draw ridicule or other unfriendly attention in ordinary social contexts.

Because the dominant class sees this form of gender expression as unacceptable, inappropriate, or perhaps threatening, these individuals are significantly more likely to experience discrimination and harassment both in their personal lives and from their employer, according to a 2011 report from the Center for American Progress.

Gender roles may be a means through which one expresses their gender identity, but they may also be employed as a means of exerting social control, and individuals may experience negative social consequences for violating them.

Different religious and cultural groups within one country may have different norms that they attempt to "police" within their own group, including gender norms.

"I Corinthians," 11:14 and 15 indicates that it is inappropriate for a man to wear his hair long, and good for a woman to wear her hair long.

The roles of women in Christianity can vary considerably today as they have varied historically since the first century New Testament church. This is especially true in marriage and in formal ministry positions within certain Christian denominations, churches, and parachurch organizations.

Many leadership roles in the organized church have been restricted to males. In the Roman Catholic and Eastern Orthodox churches, only men may serve as priests or deacons; only males serve in senior leadership positions such as pope, patriarch, and bishop. Women may serve as abbesses. Most mainstream Protestant denominations are beginning to relax their longstanding constraints on ordaining women to be ministers, though some large groups are tightening their constraints in reaction. Charismatic and Pentecostal churches have embraced the ordination of women since their founding.

Christian traditions that officially recognize "saints", as persons of exceptional holiness of life having attained the beatific vision (heaven), do list women in that group. Most prominent is Mary, mother of Jesus who is highly revered throughout Christianity, particularly in the Catholic and Orthodox churches where she is considered the "Theotokos", i.e. "Mother of God", the son born of her, Jesus, being God Incarnate, that is, both God and Man. 
Women prominent in Christianity have included contemporaries of Jesus, subsequent theologians, abbesses, mystics, doctors of the church, founders of religious orders, military leaders, monarchs and martyrs, evidencing the variety of roles played by women within the life of Christianity. Paul the Apostle held women in high regard and worthy of prominent positions in the church, though he was careful not to encourage disregard for the New Testament household codes, also known as New Testament Domestic Codes or "Haustafelen", of Greco-Roman law in the first century.

According to Dhami and Sheikh, gender roles in Muslim countries are centered on the importance of the family unit, which is viewed as the basis of a balanced and healthy society. Islamic views on gender roles and family are traditionally conservative.

Many Muslim-majority countries, most prominently Saudi Arabia, have interpretations of religious doctrine regarding gender roles embedded in their laws. In the United Arab Emirates, non-Muslim Western women can wear crop tops, whereas Muslim women are expected to dress much more modestly, due to the injunction on women in Islam to dress modestly at all times when in public. In some Muslim countries, these differences are sometimes even codified in law.

In some Muslim-majority countries, however, even non-Muslim women are expected to follow Muslim female gender norms and Islamic law to a certain extent, such as by covering their hair. This norm may sometimes be objected to by women visiting from other countries - but they may nevertheless decide to comply on pragmatic grounds, in the interests of their own safety. For example, in Egypt, women who do not dress "modestly" - whether they are Muslims or not - may be perceived to be akin to prostitutes by men.

Muhammad described the high status of mothers in both of the major hadith Collections (Bukhari and Muslim). One famous account is:

Hindu deities are more ambiguously gendered than deities of other world religions, such as Christianity, Islam, and others. This informs female and males relations, and informs how the differences between males and females are understood However, in a religious cosmology like Hinduism, which prominently features female and androgynous deities, some gender transgression is allowed. This group is known as the hijras, and has a long tradition of performing in important rituals, such as the birth of sons and weddings. Despite this allowance for transgression, Hindu cultural traditions portray women in contradictory ways. On one hand, women's fertility is given great value, and on the other, female sexuality is depicted as potentially dangerous and destructive.

Marriage is an institution that influences gender roles, inequality, and change. In the United States, gender roles are communicated by the media, social interaction, and language. Through these platforms society has influenced individuals to fulfill the stereotypical gender roles within a heterosexual marriage starting out at a young age. Although traditionally, society claims that roles within a heterosexual marriage should be decided based on one's biological sex, today individuals are determining their own roles for themselves, ultimately creating equal partnerships.

In the U.S., marriage roles are generally decided based on gender. For approximately the past seven decades, heterosexual marriage roles have been defined for men and women based on society's expectations and the influence of the media. Men and women are typically associated with certain social roles dependent upon the personality traits associated with those roles. Traditionally, the role of the homemaker is associated with a woman and the role of a breadwinner is associated with a male.

In the U.S., single men are outnumbered by single women at a ratio of 100 single women to 86 single men, though never-married men over the age of 15 outnumber women by a 5:4 ratio (33.9% to 27.3%) according to the 2006 U.S. Census American Community Survey. The results are varied between age groups, with 118 single men per 100 single women in their 20s, versus 33 single men to 100 single women over 65.

The numbers also vary between countries. For example, China has many more young men than young women, and this disparity is expected to increase. In regions with recent conflict such as Chechnya, women greatly outnumber men.

In a cross-cultural study by David Buss, men and women were asked to rank the importance of certain traits in a long term partner. Both men and women ranked "kindness" and "intelligence" as the two most important factors. Men valued beauty and youth more highly than women, while women valued financial and social status more highly than men.

In today's society, media saturates nearly every aspect of one's life. It seems inevitable for society to be influenced by the media and what it is portraying. Roles are gendered, meaning that both males and females are viewed and treated differently according to their biological sex, and because gendered roles are learned, the media has a direct impact on individuals. Thinking about the way in which couples act on romantic television shows or movies and the way women are portrayed as passive in magazine ads, reveals a lot about how gender roles are viewed in society and in heterosexual marriages. Traditional gendered roles view the man as a “pro-creator, a protector, and a provider,” and the woman as “pretty and polite but not too aggressive, not too outspoken and not too smart.” Media aids in society conforming to these traditional gendered views. People learn through imitation and social-interaction both in the physical world and through the media; television, magazines, advertisements, newspapers, the Internet, etc. Michael Messner argues that "gendered interactions, structure, and cultural meanings are intertwined, in both mutually reinforcing and contradictory ways."

Television's influence on society, specifically the influence of television advertisements, is shown in studies such as that of Jörg Matthes, Michael Prieler, and Karoline Adam. Their study into television advertising has shown that women are much more likely to be shown in a setting in the home compared to men. The study also shows that women are shown much less in work-like settings. This underrepresentation in television advertising is seen in many countries around the world but is very present in developed countries. In another study in the "Journal of Social Psychology," many television advertisements in countries around the world are seen targeting women at different times of the day than men. Advertisements for products directed towards female viewers are shown during the day on weekdays, while products for men are shown during weekends. The same article shows that a study on adults and television media has also seen that the more television adults watch, the more likely they are to believe or support the gender roles that are illustrated. The support of the presented gender stereotypes can lead to a negative view of feminism or sexual aggression.

It has been presented in the journal article by Emerald Group Publishing Limited that adolescent girls have been effected by the stereotypical view of women in media. Girls feel pressurised and stressed to achieve a particular appearance and there have been highly worrying consequences for the young girls if they fail to achieve this look. These consequences have ranged from anxiety to eating disorders. Young girls in an experiment of this journal article describe pictures on women in advertisements as unrealistic and fake. They are dressed in little and revealing clothing which sexualised the women and expose their thin figures, that are gazed upon by the public, creating an issue with stereotyping in the media.

It has also been presented that children are affected by gender roles in the media. Children's preferences in television characters are most likely to be to characters of the same gender. Because children favor characters of the same gender, the characteristics of the character are also looked to by children. In another journal article by Emerald Group Publishing Limited, the underrepresentation of women in children's television shows between 1930 and 1960 is examined. While studies between 1960 and 1990 show an increase in the representation of women in television, studies conducted between 1990 and 2005, a time when women were considered to be equal to men by some, show no change in the representation of women in children's television shows. Women, being underrepresented in children's television shows, are also often portrayed as married or in a relationship, while men are more likely to be single. This reoccurring theme in relationship status can be reflected in the ideals of children that only see this type of representation.

Gendered roles in heterosexual marriages are learned through imitation. People learn what society views as appropriate gender behaviors from imitating the repetition of actions by one's role-model or parent of the same biological sex. Imitation in the physical world that impacts one's gendered roles often comes from role-modeling parents, peers, teachers, and other significant figures in one's life. In a marriage, oftentimes each person's gendered roles are determined by their parents. If the wife grew up imitating the actions of traditional parents, and the husband non-traditional parents, their views on marital roles would be different. One way people can acquire these stereotypical roles through a reward and punishment system. When a little girl imitates her mother by performing the traditional domestic duties she is often rewarded by being told she is doing a good job. Nontraditionally, if a little boy was performing the same tasks he would more likely be punished due to acting feminine. Because society holds these expected roles for men and women within a marriage, it creates a mold for children to follow.

Over the years, gender roles have continued to change and have a significant impact on the institution of marriage. Gender roles can be defined as the behaviors, values, and attitudes that a society considers appropriate for both male and female. Motivated by the women's rights movement and various other movements gender roles have begun to change, resulting in the changing economic landscape, women entering the workplace and many more. Traditionally, men and women had completely opposing roles, men were seen as the provider for the family and women were seen as the caretakers of both the home and the family. However, in today's society the division of roles are starting to blur. More and more individuals are adapting non-traditional gender roles into their marriage in order to share responsibilities. This revolutionary view on gender roles seeks out equality between sexes. In today's society it is more likely that a man and woman are both providers for their family. More and more women are entering the workforce while more men are contributing to household duties. Despite the fact that there is still a gap between gender roles, today, roles are less gendered and more equal in comparison to how they were traditionally.

Throughout history spouses have been charged with certain societal functions. With the rise of the New World came the expected roles that each spouse was to carry out specifically. Husbands were typically working farmers - the providers. Wives typically cared for the home and the children. However, the roles are now changing, and even reversing.

Societies can change such that the gender roles rapidly change. The 21st century has seen a shift in gender roles due to multiple factors such as new family structures, education, media, and several others. A 2003 survey by the Bureau of Labor Statistics indicated that about 1/3 of wives earn more than their husbands.

With the importance of education emphasized nationwide, and the access of college degrees (online, for example), women have begun furthering their education. Women have also started to get more involved in recreation activities such as sports, which in the past were regarded to be for men. Family structures are changing, and the number of single-mother or single-father households is increasing. Fathers are also becoming more involved with raising their children, instead of the responsibility resting solely with the mother.

According to the Pew Research Center, the number of stay-at-home fathers in the US nearly doubled in the period from 1989 to 2012, from 1.1 million to 2.0 million. This trend appears to be mirrored in a number of countries including the UK, Canada and Sweden. However, Pew also found that, at least in the US, public opinion in general appears to show a substantial bias toward favoring a mother as a care-taker versus a father, regardless of any shift in actual roles each plays.

Gender equality allows gender roles to become less distinct and according to Donnalyn Pompper, is the reason "men no longer own breadwinning identities and, like women, their bodies are objectified in mass media images." The LGBT rights movement has played a role increasing pro-gay attitudes, which according to Brian McNair, are expressed by many metrosexual men.

According to Professor Lei Chang, gender attitudes within the domains of work and domestic roles, can be measured using a cross-cultural gender role attitudes test. Psychological processes of the East have historically been analysed using Western models (or "instruments") that have been translated, which potentially, is a more far-reaching process than linguistic translation. Some North American instruments for assessing gender role attitudes include:
Through such tests, it is known that American southerners exhibit less egalitarian gender views than their northern counterparts, demonstrating that gender views are inevitably affected by an individual's culture. This also may differ among compatriots whose 'cultures' are a few hundred miles apart.

Although existing studies have generally focused on gender views or attitudes that are work-related, there has so far not been a study on specific domestic roles. Supporting Hofstede's 1980 findings, that "high masculinity cultures are associated with low percentages of women holding professional and technical employment", test values for work-related egalitarianism were lower for Chinese than for Americans. This is supported by the proportion of women that held professional jobs in China (far less than that of America), the data clearly indicating the limitations on opportunities open to women in contemporary Eastern society. In contrast, there was no difference between the viewpoint of Chinese and Americans regarding domestic gender roles.

A study by Richard Bagozzi, Nancy Wong and Youjae Yi, examines the interaction between culture and gender that produces distinct patterns of association between positive and negative emotions. The United States was considered a more 'independence-based culture', while China was considered 'interdependence-based'. In the US people tend to experience emotions in terms of opposition whereas in China, they do so in dialectical terms (i.e., those of logical argumentation and contradictory forces). The study continued with sets of psychological tests among university students in Beijing and in Michigan. The fundamental goals of the research were to show that "gender differences in emotions are adaptive for the differing roles that males and females play in the culture". The evidence for differences in gender role was found during the socialization in work experiment, proving that "women are socialized to be more expressive of their feelings and to show this to a greater extent in facial expressions and gestures, as well as by verbal means". The study extended to the biological characteristics of both gender groups — for a higher association between PA and NA hormones in memory for women, the cultural patterns became more evident for women than for men.

Gender communication is viewed as a form of intercultural communication; and gender is both an influence on and a product of communication.

Communication plays a large role in the process in which people become male or female because each gender is taught different linguistic practices. Gender is dictated by society through expectations of behavior and appearances, and then is shared from one person to another, by the process of communication. Gender does not create communication, communication creates gender.

For example, females are often more expressive and intuitive in their communication, but males tend to be instrumental and competitive. In addition, there are differences in accepted communication behaviors for males and females. To improve communication between genders, people who identify as either male or female must understand the differences between each gender.

As found by Cara Tigue (McMaster University in Hamilton, Canada) the importance of powerful vocal delivery for women could not be underestimated, as famously described in accounts of Margaret Thatcher's years in power.)

Hall published an observational study on nonverbal gender differences and discussed the cultural reasons for these differences. In her study, she noted women smile and laugh more and have a better understanding of nonverbal cues. She believed women were encouraged to be more emotionally expressive in their language, causing them to be more developed in nonverbal communication.

Men, on the other hand, were taught to be less expressive, to suppress their emotions, and to be less nonverbally active in communication and more sporadic in their use of nonverbal cues. Most studies researching nonverbal communication described women as being more expressively and judgmentally accurate in nonverbal communication when it was linked to emotional expression; other nonverbal expressions were similar or the same for both genders.

McQuiston and Morris also noted a major difference in men and women's nonverbal communication. They found that men tend to show body language linked to dominance, like eye contact and interpersonal distance, more than women.

According to Julia Wood, there are distinct communication 'cultures' for women and men in the US. Wood believes that in addition to female and male communication cultures, there are also specific communication cultures for African Americans, older people, Indian Native Americans, gay men, lesbians, and people with disabilities. According to Wood, it is generally thought that biological sex is behind the distinct ways of communicating, but in reality the root is "gender".

Maltz and Broker's research suggested that the games children play may contribute to socializing children into masculine and feminine gender roles: for example, girls being encouraged to play "house" may promotes stereotypically feminine traits, and may promote interpersonal relationships as playing house does not necessarily have fixed rules or objectives; boys tended to play more competitive and adversarial team sports with structured, predetermined goals and a range of confined strategies.

Metts, et al. explain that sexual desire is linked to emotions and communicative expression. Communication is central in expressing sexual desire and 'complicated emotional states', and is also the 'mechanism for negotiating the relationship implications of sexual activity and emotional meanings'.

Gender differences appear to exist in communicating sexual desire, for example, masculine people are generally perceived to be more interested in sex than feminine people, and research suggests that masculine people are more likely than feminine people to express their sexual interest.

This may be greatly affected by masculine people being less inhibited by social norms for expressing their desire, being more aware of their sexual desire or succumbing to the expectation of their gender culture. When feminine people employ tactics to show their sexual desire, they are typically more indirect in nature. On the other hand, it is known masculinity is associated with aggressive behavior in all mammals, and most likely explains at least part of the fact that masculine people are more likely to express their sexual interest. This is known as the Challenge hypothesis.

Various studies show different communication strategies with a feminine person refusing a masculine person's sexual interest. Some research, like that of Murnen, show that when feminine people offer refusals, the refusals are verbal and typically direct. When masculine people do not comply with this refusal, feminine people offer stronger and more direct refusals. However, research from Perper and Weis showed that rejection includes acts of avoidance, creating distractions, making excuses, departure, hinting, arguments to delay, etc. These differences in refusal communication techniques are just one example of the importance of communicative competence for both masculine and feminine gender cultures.

A 1992 study tested gender stereotypes and labeling within young children.

Fagot "et al." divided this into two different studies, the first investigated how children identified the differences between gender labels of boys and girls, the second study looked at both gender labeling and stereotyping in the relationship of mother and child.

Within the first study, 23 children between the ages of 2 and 7 underwent a series of gender labelling and gender stereotyping tests consisting of showing the children either pictures of males and females or objects such as a hammer or a broom then identifying or labeling those to a certain gender. The results of these tests showed that children under 3 years could make gender-stereotypic associations.

The second study looked at gender labelling and stereotyping in the relationship of mother and child using three separate methods. The first consisted of identifying gender labeling and stereotyping, essentially the same method as the first study. The second consisted of behavioral observations, which looked at ten-minute play sessions with mother and child using gender specific toys.

The third was a series of questionnaires such as an "Attitude Toward Women Scale", "Personal Attributes Questionnaire", and "Schaefer and Edgerton Scale" which looked at the family values of the mother.

The results of these studies showed the same as the first study with regards to labelling and stereotyping.

They also identified in the second method that the mothers positive reactions and responses to same-sex or opposite-sex toys played a role in how children identified them. Within the third method the results found that the mothers of the children who passed the “Gender Labeling Test”, had more traditional family values. These two studies, conducted by Beverly I. Fagot, Mar D. Leinbach and Cherie O'Boyle, showed that gender stereotyping and labeling is acquired at a very young age, and that social interactions and associations play a large role in how genders are identified.

Virginia Woolf, in the 1920s, made the point: 'It is obvious that the values of women differ very often from the values which have been made by the other sex. Yet it is the masculine values that prevail' remade sixty years later by psychologist Carol Gilligan who used it to show that psychological tests of maturity have generally been based on masculine parameters, and so tended to show that women were less 'mature'. Gilligan countered this in her ground-breaking work, "In a Different Voice", holding that maturity in women is shown in terms of different, but equally important, human values.

Gender stereotypes are extremely common in society. One of the reasons this may be is simply because it is easier on the brain to stereotype (see Heuristics).

The brain has limited perceptual and memory systems, so it categorizes information into fewer and simpler units which allows for more efficient information processing. Gender stereotypes appear to have an effect at an early age. In one study, the effects of gender stereotypes on children's mathematical abilities were tested. In this study of American children between the ages of six and ten, it was found that the children, as early as the second grade, demonstrated the gender stereotype that mathematics is a 'boy's subject'. This may show that the mathematical self-belief is influenced before the age in which there are discernible differences in mathematical achievement.

According to the 1972 study by Jean Lipman-Blumen, women who grew up following traditional gender roles from childhood were less likely to want to be highly educated while women brought up with the view that men and women are equal were more likely to want higher education. This result indicates that gender roles that have been passed down traditionally can influence stereotypes about gender.

In a later study, Deaux and her colleagues (1984) found that most people think women are "more" "nurturant", but "less self-assertive" than men. and that this belief is indicated universally, but that this awareness is related to women's "role". To put it another way, women do not have an "inherently" nurturant personality, rather that a nurturing personality is acquired by whoever happens to be doing the housework.

In a study of gender stereotypes by Jacobs (1991) it was found that parents' stereotypes interact with the sex of their child to directly influence the parents' beliefs about the child's abilities. In turn, parents' beliefs about their child directly influence their child's self-perceptions, and both the parents' stereotypes and the child's self-perceptions influence the child's performance.

Stereotype threat is being at risk of confirming, as self-characteristic, a negative stereotype about one's group. In the case of gender it is the implicit belief in gender stereotype that women perform worse than men in mathematics, which is proposed to lead to lower performance by women.

A review article of stereotype threat research (2012) relating to the relationship between gender and mathematical abilities concluded "that although stereotype threat may affect some women, the existing state of knowledge does not support the current level of enthusiasm for this [as a] mechanism underlying the gender gap in mathematics".

In 2018 Jolien A. van Breen and colleagues conducted research into subliminal gender stereotyping. Researchers took participants through a fictional Moral Choice Dilemma Task, which consisted of eight scenarios "in which sacrificing one person can save several others of unspecified gender... In four scenarios, participants are asked to sacrifice a man to save several others (of unspecified gender), and in four other scenarios they are asked to sacrifice a woman." The results showed that women who identified as feminists were more willing to 'sacrifice' men than women who did not identify as feminists. "If a person wanted to counteract that and ‘level the playing field’, that can be done either by boosting women or by downgrading men”, said van Breen. “So I think that this effect on evaluations of men arises because our participants are trying to achieve an underlying aim: counteracting gender stereotypes."

Gender stereotypes are frequently brought up as one disadvantage to women during the hiring process, and as one explanation of the lack of women in key organizational positions.
Management and similar leader positions are often perceived to be "masculine" in type, meaning they are assumed to require aggressiveness, competitiveness, strength and independence. These traits do not line up with the perceived traditional female gender role stereotype. (This is often referred to as the "lack of fit" model which describes the dynamics of the gender bias.) Therefore, the perception that women do not possess these "masculine" qualities, limits their ability to be hired or promoted into managerial positions.

One's performance at work is also evaluated based on one's gender. If a female and a male worker show the same performance, the implications of that performance vary depending on the person's gender and on who observes the performance; if a man performs exceedingly well he is perceived as driven or goal-oriented and generally seen in a positive light while a woman showing a similar performance is often described using adjectives with negative connotations.
Female performance is therefore not evaluated neutrally or unbiased and stereotyped in ways to deem their equivalent levels and quality of work as instead of lesser value.

Consequently, that gender stereotype filter leads to a lack of fair evaluation and, in turn, to fewer women occupying higher paying positions. Gender stereotypes contain women at certain, lower levels; getting trapped within the glass ceiling. While the number of women in the workforce occupying management positions is slowly increasing, women currently fill only 2.5% of the higher managerial positions in the United States. The fact that most women are being allocated to occupations that pay less, is often cited as a contributor to the existing gender pay gap.

In relation to white women, women of color are disproportionally affected by the negative influence their gender has on their chances in the labor market. In 2005, women held only 14.7% of Fortune 500 board seats with 79% of them being white and 21% being women of color. This difference is understood through intersectionality, a term describing the multiple and intersecting oppressions and individual might experience. Activists during second-wave feminism have also used the term "horizontal oppressions" to describe this phenomenon. It has also been suggested that women of color in addition to the glass ceiling, face a "concrete wall" or a "sticky floor" to better visualize the barriers.

Liberal feminist theory states that due to these systemic factors of oppression and discrimination, women are often deprived of equal work experiences because they are not provided equal opportunities on the basis of legal rights. Liberal feminists further propose that an end needs to be put to discrimination based on gender through legal means, leading to equality and major economic redistributions.

While activists have tried calling on Title VII of the Civil Rights Act of 1964 to provide an equal hiring and promotional process, that practice has had limited success. The pay gap between men and women is slowly closing. Women make approximately 21% less than her male counterpart according to the Department of Labor. This number varies by age, race, and other perceived attributes of hiring agents. A proposed step towards solving the problem of the gender pay gap and the unequal work opportunities is the ratification of the Equal Rights Amendment which would constitutionally guarantee equal rights for women. This is hoped to end gender-based discrimination and provide equal opportunities for women.

If a woman "does" act according to female stereotypes, she is likely to receive backlash for not being competent enough; if she does "not" act according to the stereotypes connected to her gender and behaves more androgynous, or even masculine, it is likely to cause backlash through third-party punishment or further job discrimination. Therefore, women are expected to behave in a way that aligns with female gender stereotypes while these stereotypes are simultaneously used to justify their lack of success in an economic context, putting women in the workforce in a precarious, "double bind" situation. A proposed step to relieve women from this issue is the above-mentioned ratification of the Equal Rights Amendment, as it would legally further gender equality and prohibit gender-based discrimination regardless if a woman is acting according to female gender stereotypes or in defiance of them.

Rosabeth Moss Kanter identified four types of stereotypes given to professional women via the media. The four stereotypes are, iron maiden, pet, mother, and seductress/sex object. Iron maiden refers to women who are deemed to display too many masculine traits and not enough feminine traits according to her audience. This leads audiences to question the trustworthiness of an iron maiden, because she is seen as strategically playing the field to appease voters. The pet stereotype is given to women who are identified as helpmates, cheerleaders, or mascots, which then leads the audience to see these women as naive or weak and unable to lead without a man's help. If a professional woman is seen as a mother, she is more likely to be seen as compassionate and caring, but also has the capacity to be shrew, punishing, and scolding. Additionally, it is possible for her leadership abilities to be called into question due to perceived conflicts with her maternal responsibilities. The fourth stereotype, seductress, is assigned to women who speak and act rather femininely, or have been victims of sexual harassment. The media tends to focus on the seductress woman's sex appeal and physical appearance in opposition to her policy stances and rhetoric.

A proposed step to relieve women from that double bind is the above-mentioned ratification of the Equal Rights Amendment, as it would further legal gender equality and prohibit gender-based discrimination, regardless if a woman is acting according to female gender stereotypes or in defiance of them.

Gender stereotypes and roles can also be supported implicitly. Implicit stereotypes are the unconscious influence of attitudes a person may or may not be aware that they hold. A person is influenced by these attitudes even though they are not aware. Gender stereotypes can also be held in this manner.

These implicit stereotypes can often be demonstrated by the Implicit-association test (IAT).

One example of an implicit gender stereotype is that males are seen as better at mathematics than females. It has been found that men have stronger positive associations with mathematics than women, while women have stronger negative associations with mathematics and the more strongly a woman associates herself with the female gender identity, the more negative her association with mathematics.

These associations have been disputed for their biological connection to gender and have been attributed to social forces that perpetuate stereotypes such as aforementioned stereotype that men are better at mathematics than women.

This particular stereotype has been found in American children as early as second grade.

The same test with Singaporean children found that the strength of their mathematics-gender stereotype and their gender identity predicted the association between individuals and mathematical ability.

It has been shown that this stereotype also reflects mathematical performance: a study was done on the worldwide scale and it was found that the strength of this mathematics-gender stereotype in varying countries correlates with 8th graders' scores on the TIMSS, a standardized math and science achievement test that is given worldwide. The results were controlled for general gender inequality and yet were still significant.

An example of gender stereotypes assumes those of the male gender are more 'tech savvy' and happier working online, however, a study done by Hargittai & Shafer, shows that many women also typically have lower self-perceived abilities when it comes to use of the World Wide Web and online navigation skills. Because this stereotype is so well known many women assume they lack such technical skills when in reality, the gap in technological skill level between men and women is significantly less than many women assume.

In the journal article written by Elizabeth Behm-Morawitz video games have been guilty of using sexualised female characters, who wear revealing clothing with an 'ideal' figure. It has been shown, female gamers can experience lower self-efficacy when playing a game with a sexualized female character. Women have been stereotyped in online games and have shown to be quite sexist in their appearance. It has been shown these kind of character appearances have influenced peoples' beliefs about gender capabilities by assigning certain qualities to the male and female characters in different games.

The concept of gender inequality is often perceived as something that is non-existent within the online community, because of the anonymity possible online. Remote or home-working greatly reduces the volume of information one individual gives another compared to face-to-face encounters, providing fewer opportunities for unequal treatment but it seems real-world notions of power and privilege are being duplicated: people who choose to take up different identities (avatars) in the online world are (still) routinely discriminated against, evident in online gaming where users are able to create their own characters. This freedom allows the user to create characters and identities with a different appearance than their own in reality, essentially allowing them to create a new identity, confirming regardless of actual gender those who are perceived to be female are treated differently because of their on-line gender identity.

In marked contrast to the traditional male-dominated stereotype a study shows that 52% of the gaming audience is made up of women and a minority of gaming characters are women. Only 12% of game designers in Britain and 3% of all programmers are women.

Despite the growing number of women who partake in online communities, and the anonymous space provided by the Internet, issues such as gender inequality, the issue has simply been transplanted into the online world.

Even though the number of women running for elected office has increased over the last decades, they still only make up 20% of U.S. senators, 19.4% of U.S. congressional representatives and 24% of statewide executives. Additionally, many of these political campaigns appear to focus on the aggressiveness of the female candidate which is often still perceived as a masculine trait. Therefore, female candidates are running based on gender-opposing stereotypes because that predicts higher likelihood of success than appearing to be a stereotypical woman.

Elections of increasing numbers of women into office serves as a basis for many scholars to claim that voters are not biased towards a candidate's gender. However, it has been shown that female politicians are perceived as only being superior when it comes to handling women's rights and poverty, whereas male politicians are perceived to be better at dealing with crime and foreign affairs. That view lines up with the most common gender stereotypes.

It has also been predicted that gender does only highly matter for female candidates that have not been politically established. These predictions apply further to established candidates, stating that gender would not be a defining factor for their campaign or the focal point of media coverage. This has been disproven by multiple scholars, often based on Hillary Clinton's multiple campaigns for the office of President of the United States.

Additionally, when voters don't have a lot of information about a female candidate, they are likely to view her as being a stereotypical woman which they often take as a basis for not electing her because they consider typical male qualities as being crucial for someone holding a political office.

Throughout the 20th century, women in the United States saw a dramatic shift in social and professional aspirations and norms. Following the Women's Suffrage Movement of the late-nineteenth century, which resulted in the passage of the Nineteenth Amendment allowing women to vote, and in combination with conflicts in Europe, WWI and WWII, women found themselves shifted into the industrial workforce. During this time, women were expected to take up industrial jobs and support the troops abroad through the means of domestic industry. Moving from "homemakers" and "caregivers", women were now factory workers and "breadwinners" for the family.

However, after the war, men returned home to the United States and women, again, saw a shift in social and professional dynamics. With the reuniting of the nuclear family, the ideals of American Suburbia boomed. Throughout the 1950s and 1960s, middle-class families moved in droves from urban living into newly developed single-family homes on former farmland just outside major cities. Thus established what many modern critics describe as the "private sphere". Though frequently sold and idealized as “perfect living”, many women had difficulty adjusting to the new “private sphere.” Writer Betty Friedan described this discontent as “the feminine mystique.” The “mystique” was derived from women equipped with the knowledge, skills, and aspirations of the workforce, the “public sphere”, who felt compelled whether socially or morally to devote themselves to the home and family.

One major concern of feminism, is that women occupy lower-ranking job positions than men, and do most of the housework. A recent (October 2009) report from the Center for American Progress, "The Shriver Report: A Woman's Nation Changes Everything" tells us that women now make up 48% of the US workforce and "mothers are breadwinners or co-breadwinners in a majority of families" (63.3%, see figure 2, page 19 of the Executive Summary of The Shriver Report).
Another recent article in "The New York Times" indicates that young women today are closing the pay gap. Luisita Lopez Torregrosa has noted, "Women are ahead of men in education (last year, 55 percent of U.S. college graduates were female). And a study shows that in most U.S. cities, single, childless women under 30 are making an average of 8 percent more money than their male counterparts, with Atlanta and Miami in the lead at 20 percent.". While this study concerned American cities, a global trend is developing, and has now been termed "the reverse gender gap."

Feminist theory generally defines gender as a social construct that includes ideologies governing feminine/masculine (female/male) appearances, actions, and behaviors. An example of these gender roles would be that males were supposed to be the educated breadwinners of the family, and occupiers of the public sphere whereas, the female's duty was to be a homemaker, take care of her husband and children, and occupy the private sphere. According to contemporary gender role ideology, gender roles are continuously changing. This can be seen in Londa Schiebinger's "Has Feminism Changed Science", in which she states, "Gendered characteristics - typically masculine or feminine behaviors, interests, or values-are not innate, nor are they arbitrary. They are formed by historical circumstances. They can also change with historical circumstances."

One example of the contemporary definition of gender was depicted in Sally Shuttleworth’s "Female Circulation" in which the, “abasement of the woman, reducing her from an active participant in the labor market to the passive bodily existence to be controlled by male expertise is indicative of the ways in which the ideological deployment of gender roles operated to facilitate and sustain the changing structure of familial and market relations in Victorian England.” In other words, this shows what it meant to grow up into the roles (gender roles) of a female in Victorian England, which transitioned from being a homemaker to being a working woman and then back to being passive and inferior to males. In conclusion, gender roles in the contemporary sex gender model are socially constructed, always changing, and do not really exist since, they are ideologies that society constructs in order for various benefits at various times in history.

The men's rights movement (MRM) is a part of the larger men's movement. It branched off from the men's liberation movement in the early-1970s. The men's rights movement is made up of a variety of groups and individuals who are concerned about what they consider to be issues of male disadvantage, discrimination and oppression. The movement focuses on issues in numerous areas of society (including family law, parenting, reproduction, domestic violence) and government services (including education, compulsory military service, social safety nets, and health policies) that they believe discriminate against men.

Scholars consider the men's rights movement or parts of the movement to be a backlash to feminism. The men's rights movement denies that men are privileged relative to women. The movement is divided into two camps: those who consider men and women to be harmed equally by sexism, and those who view society as endorsing the degradation of men and upholding female privilege.

Men's rights groups have called for male-focused governmental structures to address issues specific to men and boys including education, health, work and marriage. Men's rights groups in India have called for the creation of a Men's Welfare Ministry and a National Commission for Men, as well as the abolition of the National Commission for Women. In the United Kingdom, the creation of a Minister for Men analogous to the existing Minister for Women, have been proposed by David Amess, MP and Lord Northbourne, but were rejected by the government of Tony Blair. In the United States, Warren Farrell heads a commission focused on the creation of a "White House Council on Boys and Men" as a counterpart to the "White House Council on Women and Girls" which was formed in March 2009.

Related to this is the Father's Rights Movement, whose members seek social and political reforms that affect fathers and their children. These individuals contest that societal institutions such as family courts, and laws relating to child custody and child support payments, are gender biased in favor of mothers as the default caregiver. They therefore are systemically discriminatory against males regardless of their actual caregiving ability, because males are typically seen as the bread-winner, and females as the care-giver.

Gender neutrality is the movement to end discrimination of gender altogether in society through means of gender neutral language, the end of sex segregation and other means.

Transgender is the state of one's gender identity or gender expression not matching one's assigned sex. Transgender is independent of sexual orientation; transgender people may identify as heterosexual, homosexual, bisexual, etc.; some may consider conventional sexual orientation labels inadequate or inapplicable to them. The definition of "transgender" includes:

While people self-identify as transgender, the transgender identity umbrella includes sometimes-overlapping categories. These include transsexual; transvestite or cross-dresser; genderqueer; androgyne; and bigender. Usually not included are transvestic fetishists (because it is considered to be a paraphilia rather than gender identification), and drag kings and drag queens, who are performers who cross-dress for the purpose of entertaining. In an interview, celebrity drag queen RuPaul talked about society's ambivalence to the differences in the people who embody these terms. "A friend of mine recently did the "Oprah" show about transgender youth", said RuPaul. "It was obvious that we, as a culture, have a hard time trying to understand the difference between a drag queen, transsexual, and a transgender, yet we find it very easy to know the difference between the American baseball league and the National baseball league, when they are both so similar."

Sexual orientation is defined by the interplay between a person's emotional and physical attraction toward others. Generally, sexual orientation is broken into the three categories: heterosexual, homosexual and bisexual. By basic definition, the term heterosexual is typically used in reference to someone who is attracted to people of the opposite sex, the term homosexual is used to classify people who are attracted to those of the same sex, and the term bisexual is used to identify those who are attracted to both the same and opposite sexes. However, some argue that sexual orientation is better defined as a continuum with those three categories represented. This idea was first proposed by sexologist Alfred Kinsey in 1948. After conducting a series of interviews, Kinsey and his team of researchers concluded that most people fell somewhere on a spectrum between strictly heterosexual and strictly homosexual. Their findings suggested that sexual orientation was more fluid than once believed.

Sexual orientation is developed based on the three components of sexual identity, sexual behavior and sexual attraction. Each component is independent so no other conclusions can be drawn based on one another.

An active conflict over the cultural acceptability of non-heterosexuality rages worldwide. The belief or assumption that heterosexual relationships and acts are "normal" is described as heterosexism or in queer theory, heteronormativity. Gender identity and sexual orientation are two separate aspects of individual identity, although they are often mistaken in the media.

Perhaps it is an attempt to reconcile this conflict that leads to a common assumption that one same-sex partner assumes a pseudo-male gender role and the other assumes a pseudo-female role. For a gay male relationship, this might lead to the assumption that the "wife" handled domestic chores, was the receptive sexual partner, adopted effeminate mannerisms, and perhaps even dressed in women's clothing. This assumption is flawed because homosexual couples tend to have more equal roles, and the effeminate behavior of some gay men is usually not adopted consciously, and is often more subtle.

Cohabitating same-sex partners are typically egalitarian when they assign domestic chores. Sometimes these couples assign traditional female responsibilities to one partner and traditional male responsibilities to the other. Same-sex domestic partners challenge traditional gender roles in their division of household responsibilities, and gender roles within homosexual relationships are flexible. For instance, cleaning and cooking, traditionally regarded by many as both female responsibilities, might be assigned to different people. Carrington observed the daily home lives of 52 gay and lesbian couples and found that the length of the work week and level of earning power substantially affected the assignment of housework, regardless of gender or sexuality.
In many cultures, gender roles, especially for men, simultaneously act as an indicator for heterosexuality, and as a boundary of acceptable behavior for straight people. Therefore, lesbians, gay men and bisexual people may be viewed as exempt from some or all components of gender roles or as having different "rules" they are expected to follow by society.

These modified "rules" for lesbian, gay and bisexual people may also be oppressive. Morgan examines the plight of homosexuals seeking asylum from homophobic persecution who have been turned away by US customs for "not being gay enough"; not conforming sufficiently to standard (Western) conceptions of the gender roles occupied by gays and lesbians.

Conversely, heterosexual men and women who are not perceived as being sufficiently masculine or feminine, respectively, may be assumed to be, or suspected to be, homosexual, and persecuted for their perceived homosexuality.

A number of studies conducted since the mid-90s have found direct correlation between a female criminal's ability to conform to gender role stereotypes, particularly murder, and the severity of their sentencing. "...In terms of the social realities of justice in America, the experiences of diverse groups of people in society have contributed to the shaping of the types of criminals and victims that we have had. Like Andersen and Hill Collins (1998: 4) in their discussion of what they refer to as a 'matrix of domination,' we too conceive that class, race, and gender represent "multiple, interlocking levels of domination that stem from the societal configurations of these structural relationships. These patterned actions, in turn, affect [ing] individual consciousness, group interaction, and individual and group access to institutional power and privileges.'" "Patterns of offending by men and by women are notable both for their similarities and for their differences. Both men and women are more heavily involved in minor property and substance abuse offenses than in serious crimes like robbery or murder. However, men offend at much higher rates than women for all crime categories except prostitution. This gender gap in crime is greatest for serious crime and least for mild forms of lawbreaking such as minor property crimes." 

The ‘Family Violence Framework’ applies gender dynamics to family violence. “Families are constructed around relationships that involve obligations and responsibilities, but also status and power”. According to Hattery and Smith, when “masculinity and femininity are constructed…to generate these rigid and narrow gender roles, it contributes to a culture of violence against women” “People with more resources are more likely to be abusive towards those without resources”, meaning that the stronger member of the relationship abuses their weaker partner or family member to exert their powerful roles. However, the fight for power and equality remains – “Intimate partner violence in same-sex couples reveals that the rates are similar to those in the heterosexual community”.



</doc>
<doc id="33742208" url="https://en.wikipedia.org/wiki?curid=33742208" title="Models of communication">
Models of communication

Models of communication are conceptual models used to explain the human communication process. The first major model for communication was developed in 1948 by Claude Elwood Shannon and published with an introduction by Warren Weaver for Bell Laboratories. Following the basic concept, communication is the process of sending and receiving messages or transferring information from one part (sender) to another (receiver).

In 1960, David Berlo expanded the linear transmission model with the Sender-Message-Channel-Receiver(SMCR) Model of Communication. Later, Wilbur Schramm introduced a model that identified multiple variables in communication which includes the transmitter, encoding, media, decoding, and receiver. 

Elwood Shannon and Warren Weaver were engineers that worked for Bell Telephone Labs in the United States. Their goal was to make sure that the telephone cables and radio waves were working at the maximum efficiency. Therefore, they developed the Shannon-Weaver model which had an intention to expand a mathematical theory of communication. The Shannon–Weaver model was developed in 1949 which is referred as the 'mother of all models'. The model is well accepted as a main initial model for Communication Studies which has grown since then.

As well, the Shannon-Weaver model was designed to mirror the functioning of radio and telephone technology. Their initial model consisted of four primary parts: sender, message, channel, and receiver. The sender was the part of a telephone a person speaks into, the channel was the telephone itself, and the receiver was the part of the phone through which one can hear the person on the other end of the line. Shannon and Weaver also recognized that there may often be static or background sounds that interfere with the process of the other partner in a telephone conversation; they referred to this as noise. Certain types of background sounds can also indicate the absence of a signal.

The original model of Shannon and Weaver has five elements: information source, transmitter, channel, receiver, and destination. To illustrate the process of the communication the first step is the information source where the information is stored. Next, in order to send the information, the message is encoded into signals, so it can travel to its destination. After the message is encoded, it goes through the channel which the signals are adapted for the transmission. In addition, the channel carried the noise course which is any interference that might happen to lead to the signal receive a different information from the source. After the channel, the message arrives in the receiver step where the message reconstruct (decode) from the signal. Finally, the message arrives at the destination.

In a simple model, often referred to as "the transmission model" or "standard view of communication", information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emissor/ sender/ encoder to a destination/ receiver/ decoder. According to this common communication-related conception, communication is viewed as a means of sending and receiving information. The strengths of this model are its simplicity, generality, and quantifiability. The mathematicians Claude Shannon and Warren Weaver structured this model on the basis of the following elements:


Shannon and Weaver argued that this concept entails three levels of problems for communication:


Daniel Chandler criticizes the transmission model in the following terms:


In 1960, David Berlo expanded Shannon and Weaver's 1949 linear model of communication and created the Sender-Message-Channel-Receiver (SMCR) Model of Communication. The SMCR Model of Communication separated the model into clear parts and has been expanded upon by other scholars.

The Berlo's communication process is a simple application for person-to-person communication, which include communication source, encoder, message, channel, decoder, and communication receiver. In addition, David Berlo presented some factors that influence the communication process between two people. The factors include communication skills, awareness level, social system, cultural system, and attitude.

The Berlo's Model of Communication process starts at the source. This is the part which determines the communication skills, attitude, knowledge, social system, and culture of the people involved in the communication. After the message is developed, which are elements in a set of symbols, the encoder step begins. The encoder process is where the motor skills take place by speaking or writing. The message goes through the channel which carries the message by hearing, seeing, touching, smelling, or tasting. Then the decoding process takes place. In this process, the receiver interprets the message with her or his sensory skills. Finally, the communication receiver gets the whole message understood.

Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emissor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schramm (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).

Communication can be seen as processes of information transmission governed by three levels of semiotic rules:

Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.

In light of these weaknesses, Barnlund (1970) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.

In a slightly more complex form, a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of "noise" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a [code-book], and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.

Theories of co-regulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.

There is an additional working definition of communication to consider that authors like Richard A. Lanham (2003) and as far back as Erving Goffman (1959) have highlighted. This is a progression from Lasswell's attempt to define human communication through to this century and revolutionized into the constructionist model. Constructionists believe that the process of communication is in itself the only messages that exist. The packaging can not be separated from the social and historical context from which it arose, therefore the substance to look at in communication theory is style for Richard Lanham and the performance of self for Erving Goffman.

Lanham chose to view communication as the rival to the over encompassing use of CBS model (which pursued to further the transmission model). CBS model argues that clarity, brevity, and sincerity are the only purpose to prose discourse, therefore communication. Lanham wrote: "If words matter too, if the whole range of human motive is seen as animating prose discourse, then rhetoric analysis leads us to the essential questions about prose style" (Lanham 10). This is saying that rhetoric and style are fundamentally important; they are not errors to what we actually intend to transmit. The process which we construct and deconstruct meaning deserves analysis.

Erving Goffman sees the performance of self as the most important frame to understand communication. Goffman wrote: "What does seem to be required of the individual is that he learn enough pieces of expression to be able to 'fill in' and manage, more or less, any part that he is likely to be given" (Goffman 73), highlighting the significance of expression.

The truth in both cases is the articulation of the message and the package as one. The construction of the message from social and historical context is the seed as is the pre-existing message is for the transmission model. Therefore, any look into communication theory should include the possibilities drafted by such great scholars as Richard A. Lanham and Goffman that style and performance is the whole process. lun

Communication stands so deeply rooted in human behaviors and the structures of society that scholars have difficulty thinking of it while excluding social or behavioral events. Because communication theory remains a relatively young field of inquiry and integrates itself with other disciplines such as philosophy, psychology, and sociology, one probably cannot expect a consensus conceptualization of communication across disciplines.

Communication Model Terms as provided by Rothwell (11-15):

Humans act toward people or things on the basis of the meanings they assign to those people or things.
-"Language is the source of meaning". 
-Meaning arises out of the social interaction people have with each other.

-Meaning is not inherent in objects but it is negotiated through the use of language, hence the term symbolic interactionism.
As human beings, we have the ability to name things.
Symbols, including names, are arbitrary signs.
By talking with others, we ascribe meaning to words and develop a universe of discourse
A symbol is a stimulus that has a learned/shared meaning and a value for people
Significant symbols can be nonverbal as well as linguistic.

-Negative responses can consequently reduce a person to nothing.
-Our expectations evoke responses that confirm what we originally anticipated, resulting in a self-fulfilling prophecy.

This is a one-way model to communicate with others. It consists of the sender encoding a message and channeling it to the receiver in the presence of noise. In this model there is no feedback or response which may allow for a continuous 
exchange of information (F.N.S. Palma, 1993).

The linear model was first introduced by Shannon & Weaver in 1949. In the linear communication model, the message travels one direction from the start point to the endpoint. In other words, once the sender sends the message to the receiver the communication process ends. Many communications online use the linear communication model. For example, when you send an email, post a blog, or share something on social media. However, the linear model does not explain many other forms of communication including face-to-face conversation.

It is two linear models stacked on top of each other. The sender channels a message to the receiver and the receiver then becomes the sender and channels a message to the original sender. This model has added feedback, indicating that communication is not a one way but a two way process. It also has "field of experience" which includes our cultural background, ethnicity geographic location, extent of travel, and general personal experiences accumulated over the course of your lifetime. Draw backs – there is feedback but it is not simultaneous.


Communication theory can be seen from one of the following viewpoints:


Inspection of a particular theory on this level will provide a framework on the nature of communication as seen within the confines of that theory.

Theories can also be studied and organized according to the ontological, epistemological, and axiological framework imposed by the theorist.

Ontology essentially poses the question of what, exactly, the theorist is examining. One must consider the very nature of reality. The answer usually falls in one of three realms depending on whether the theorist sees the phenomena through the lens of a realist, nominalist, or social constructionist. Realist perspective views the world objectively, believing that there is a world outside of our own experience and cognitions. Nominalists see the world subjectively, claiming that everything outside of one's cognitions is simply names and labels. Social constructionists straddle the fence between objective and subjective reality, claiming that reality is what we create together.

Epistemology is an examination of the approaches and beliefs which inform particular modes of study of phenomena and domains of expertise. In positivist approaches to epistemology, objective knowledge is seen as the result of the empirical observation and perceptual experience. In the history of science, empirical evidence collected by way of pragmatic-calculation and the scientific method is believed to be the most likely to reflect truth in the findings. Such approaches are meant to predict a phenomenon. Subjective theory holds that understanding is based on situated knowledge, typically found using interpretative methodology such as ethnography and also interviews. Subjective theories are typically developed to explain or understand phenomena in the social world.

Axiology is concerned with how values inform research and theory development. Most communication theory is guided by one of three axiological approaches. The first approach recognizes that values will influence theorists' interests but suggests that those values must be set aside once actual research begins. Outside replication of research findings is particularly important in this approach to prevent individual researchers' values from contaminating their findings and interpretations. The second approach rejects the idea that values can be eliminated from any stage of theory development. Within this approach, theorists do not try to divorce their values from inquiry. Instead, they remain mindful of their values so that they understand how those values contextualize, influence or skew their findings. The third approach not only rejects the idea that values can be separated from research and theory, but rejects the idea that they should be separated. This approach is often adopted by critical theorists who believe that the role of communication theory is to identify oppression and produce social change. In this axiological approach, theorists embrace their values and work to reproduce those values in their research and theory development.




</doc>
<doc id="697354" url="https://en.wikipedia.org/wiki?curid=697354" title="Social proof">
Social proof

Social proof, a term coined by Robert Cialdini in his 1984 book, "Influence", is also known as informational social influence. It describes a psychological and social phenomenon wherein people copy the actions of others in an attempt to undertake behavior in a given situation. 

Social proof is considered prominent in ambiguous social situations where people are unable to determine the appropriate "mode of behavior", and is driven by the assumption that the surrounding people possess more knowledge about the current situation.

The effects of social influence can be seen in the tendency of large groups to conform to choices which are either correct or mistaken. This is referred to in some publications as the "herd behavior". Although social proof reflects a rational motive to take into account the information possessed by others, formal analysis shows that it can cause people to converge too quickly upon a single distinct choice, so that decisions of even larger groups of individuals may be grounded in very little information (see information cascades).

Social proof is one type of conformity. When a person is in a situation where they are unsure of the correct way to behave, they will often look to others for clues concerning the correct behavior. When "we conform because we believe that others' interpretation of an ambiguous situation is more accurate than ours and will help us choose an appropriate course of action", it is informational social influence. This is contrasted with normative social influence wherein a person conforms to be liked or accepted by others.

Social proof often leads not only to public compliance (conforming to the behavior of others publicly without necessarily believing it is correct) but also private acceptance (conforming out of a genuine belief that others are correct). 
Social proof is more powerful when being accurate is more important and when others are perceived as especially knowledgeable.

Uncertainty is a major factor that encourages the use of social proof. One study found that when evaluating a product, consumers were more likely to incorporate the opinions of others through the use of social proof when their own experiences with the product were ambiguous, leaving uncertainty as to the correct conclusion that they should make.

Similarity also motivates the use of social proof; when a person perceives themselves as similar to the people around them, they are more likely to adopt and perceive as correct the observed behavior of these people. This has been noted in areas such as the use of laugh tracks, where participants will laugh longer and harder when they perceive the people laughing to be similar to themselves.

Social proof is also one of Robert Cialdini's six principles of persuasion, (along with reciprocity, commitment/consistency, authority, liking, and scarcity) which maintains that people are especially likely to perform certain actions if they can relate to the people who performed the same actions before them. One experiment which exemplifies this claim was conducted by researchers who joined a door-to-door charity campaign, who found that if a list of prior donators was longer, the next person solicited was more likely to donate as well. This trend was even more pronounced when the names on the donor list were people that the prospective donor knew, such as friends and neighbors. Cialdini's principle also asserts that peer power is effective because people are more likely respond to influence tactics applied horizontally rather than vertically, so people are more likely to be persuaded by a colleague than a superior.

The most famous study of social proof is Muzafer Sherif's 1935 experiment. In this experiment subjects were placed in a dark room and asked to look at a dot of light about 15 feet away. They were then asked how much, in inches, the dot of light was moving. In reality it was not moving at all, but due to the autokinetic effect it appeared to move. How much the light appears to move varies from person to person but is generally consistent over time for each individual. A few days later a second part of the experiment was conducted. Each subject was paired with two other subjects and asked to give their estimate of how much the light was moving out loud. Even though the subjects had previously given different estimates, the groups would come to a common estimate. To rule out the possibility that the subjects were simply giving the group answer to avoid looking foolish while still believing their original estimate was correct, Sherif had the subjects judge the lights again by themselves after doing so in the group. They maintained the group's judgment. Because the movement of the light is ambiguous the participants were relying on each other to define reality.

Another study looked at informational social influence in eyewitness identification. Subjects were shown a slide of the "perpetrator". They were then shown a slide of a line-up of four men, one of whom was the perpetrator they had seen, and were asked to pick him out. The task was made difficult to the point of ambiguity by presenting the slides very quickly. The task was done in a group that consisted of one actual subject and three confederates (a person acting as a subject but actually working for the experimenter). The confederates answered first and all three gave the same wrong answer. In a high-importance condition of the experiment subjects were told that they were participating in a real test of eyewitness identification ability that would be used by police departments and courts, and their scores would establish the norm for performance. In a low-importance condition subjects were told that the slide task was still being developed and that the experimenters had no idea what the norm for performance was—they were just looking for useful hints to improve the task. It was found that when subjects thought the task was of high importance they were more likely to conform, giving the confederate's wrong answer 51% of the time as opposed to 35% of the time in the low-importance condition.

The strength of social proof also varies across different cultures. For instance, studies have shown that subjects in collectivist cultures conform to others' social proof more often than those in individualist cultures. Although this trend seems reoccurring, there is evidence which suggests that these results are a simplification, and that an independent subject's personal individualistic-collectivist tendency also makes an impact upon their decisions. Additional variables, such as the subject's sense of social responsibility, need to be taken into account to better understand the mechanisms of social proof across cultures; for example, more collectivist individuals will often have an increased compulsion to help others because of their prominent awareness of social responsibility, and this in turn will increase the likelihood they will comply to requests, regardless of their peers' previous decisions.

Social proof has been proposed as an explanation for copycat suicide, where suicide rates increase following media publication about suicides. One study using agent-based modeling showed that copycat suicides are more likely when there are similarities between the person involved in the publicized suicide and the potential copycats. In addition, research performed by David Phillips between 1947 and 1968 further supports the existence of copycat suicides.

A person who has been unemployed for a long time may have a hard time finding a new job—even if they are highly skilled and qualified. Potential employers attribute wrongly the person's lack of employment to the person rather than the situation. This causes the potential employers to search more intensively for flaws or other negative characteristics that are "congruent" with or explain the person's failure and to discount the applicant's virtues.

Similarly, a person who is in high demand—for example a CEO—may continue to get many attractive job offers and can, as a result, extract a considerable wage premium—even if his/her objective performance has been poor. When people appear successful, potential employers and others who evaluate them tend to search more intensively for virtues or positive characteristics that are "congruent" with or explain the person's success, and
to ignore or underestimate the person's faults. People who experience positive social proof may also benefit from a halo effect. Other attributes are deemed to be more positive than they actually are. Additionally, the person's attributes may be viewed with a positive framing bias. For example, a person might be viewed as arrogant if they have negative social proof, and bold if they have positive social proof. For these reasons, social proof is important in determining a potential employer's consideration set.

Social proof naturally also applies to products and is used extensively in marketing and sales.

Situations that violate social proof can cause cognitive dissonance, and can cause people to have a sense of loss of control or failure of the "just world hypothesis".

Theaters sometimes use specially planted audience members who are instructed to give ovations at pre-arranged times. Usually, these people are the ones who clap initially, and the rest of the audience follows. Such ovations may be perceived by non-expert audience members as signals of the performance's quality.

Contrary to common annoyance of canned laughter in television shows, television studios have discovered that they can increase the perceived "funniness" of a show by merely playing canned laughter at key "funny" moments. They have found that even though viewers find canned laughter highly annoying, they perceive shows that happen to use canned laughter more funny than the shows that do not use canned laughter.

Social proof is becoming more prominent in digital marketing as a way to build buyer trust and confidence. The use of social proof is can be seen as written reviews, sliding banners featuring company logos, and video testimonials from industry influencers. 

A common trend among digital marketers is the use of real-time social proof notifications on websites.

Social proof is also prominent on social networks such as Twitter, Facebook, Instagram and YouTube. The number of followers, fans, views, likes, favorites and even comments that a user has made, positively affects how other users perceive them. A user on Twitter with a million followers is perceived as more trustworthy and reputable than a similar user with a thousand followers, resulting in faster growth of followers and higher engagement and click-through-rates.. Although, these fake followers will never help meet business objectives or generate sales directly.

An entire multimillion-dollar industry, known as ghost followers, exist for the sole purpose of increasing social proof on social media.

Social norms are often not clearly articulated for sustainable or pro-environmental conduct.

If one perceives that s/he is better advised about a situation than the surrounding group, then s/he is less likely to follow the group's behavior.

If one perceives themselves as a relevant authority figure in the situation, they are less likely to follow the surrounding group's behavior. This is a combination of "Identification of the surrounding group with self" and "Possession of special knowledge". People in authority positions tend to place themselves in different categories than other people and usually they have special training or knowledge that allows them to conclude that they are better informed than the surrounding group.

One might perceive particular groups of others, identified by their behavior or other characteristics, to be more reliable guides to the situation than the average person. One might think truck drivers to be more frequent, and therefore more experienced drivers than others, and therefore weigh more heavily the number of trucks than the number of cars parked when judging the quality of a restaurant. One might identify the movement of betting odds or securities prices at certain times as revealing the preferences of "smart money"—those more likely to be in the know.


</doc>
<doc id="246043" url="https://en.wikipedia.org/wiki?curid=246043" title="Kindness">
Kindness

Kindness is a behavior marked by ethical characteristics, a pleasant disposition, and concern and consideration for others. It is considered a virtue, and is recognized as a value in many cultures and religions (see ethics in religion). 

In Book II of "Rhetoric", Aristotle defines kindness as "helpfulness towards someone in need, not in return for anything, nor for the advantage of the helper himself, but for that of the person helped". Nietzsche considered kindness and love to be the "most curative herbs and agents in human intercourse". Kindness is considered to be one of the Knightly Virtues. In Meher Baba's teachings, God is synonymous with kindness: "God is so kind that it is impossible to imagine His unbounded kindness!"

In human mating choice, studies suggest that both men and women value kindness and intelligence in their prospective mates, along with physical appearance, attractiveness, social status, and age. 

The signs of she likes him is when she faces her feet towards a man even when looking away, this is the sign she is interested, if shes impressed she moves her feet more toward him. He or she will crease next to their eyes when smiling to indicate higher happiness and embarrassment, crows feet and wide smile called the Duchenne Smile is another indicator of authentic human happiness, these are two examples of happy smiling. "Good men like to make their women happy.". 

A "nice guy" is an informal and usually stereotypical term for an (often young) adult male who portrays himself as gentle, compassionate, sensitive, and/or vulnerable. The term is used both positively and negatively. One side of vulnerable is seeking approval whenever her affection is increasing and when her love is at stake, this is a nice guy attitude in tune with her thoughts more. A nice guy will share more of what hes interested in, if she responds with more affection shortly, kindness and happiness a nice guy will respond with more warm affection. When used positively, and particularly when used as a preference or description by someone else, it is intended to imply a male who puts the needs of others before his own, avoids confrontations, does favors, gives emotional support, tries to stay out of trouble, and generally acts nicely towards others. Nice guys accommodate a women's needs, nice guys co-create relationships, a formal kindness is when a woman shares her feelings or acts lovingly. In the context of a relationship, it may also refer to traits of honesty, loyalty, romanticism, courtesy and respect. When used negatively, a nice guy implies a male who is unassertive, does not express his true feelings and, in the context of dating (in which the term is often used), uses acts of friendship with the unstated aim of progressing to a romantic or sexual relationship.

Based on experiments at Yale University using games with babies, some studies concluded that kindness is inherent to human beings. There are similar studies about the root of empathy in infancy - motor mirroring developing in the early months of life, to lead (optimally) to the easy concern shown by children for their peers in distress.

Barbara Taylor and Adam Phillips have stressed the element of necessary realism in adult kindness, as well as the way “real kindness changes people in the doing of it, often in unpredictable ways”.


Based on the novel of the same name written in 1999 by author Catherine Ryan Hyde, the motion picture "Pay it Forward", which starred Kevin Spacey, Helen Hunt, Haley Joel Osment and Jon Bon Jovi, illustrates the power one person can have to make an impact on a chain reaction of kind deeds. The philosophy of "Pay It Forward" is that through acts of kindness among strangers, we all foster a more caring society. In the book and film, Reuben St. Clair, a social studies teacher in Atascadero, California, challenges his students to "change the world". One of his students, Trevor, takes the challenge to heart. He starts by showing kindness to a stranger which ripples further than he could have ever imagined.

In October 2011, Life Vest Inside posted a video called "Kindness Boomerang".It shows how one act of kindness passes seamlessly from one person to the next and boomerangs back to the person who set it into motion. Orly Wahba, Life Vest Inside Founder and Director of Kindness Boomerang explains that each scene was based on real life experiences she personally went through; moments of kindness that left a lasting impression on her life. Within several months after its release, Kindness Boomerang went viral; reaching over 20 million people globally and eventually landing Wahba spot on the TED2013 stage to speak about the power of kindness.




</doc>
<doc id="3357557" url="https://en.wikipedia.org/wiki?curid=3357557" title="Social influences on fitness behavior">
Social influences on fitness behavior

Physical fitness is maintained by a range of physical activities. Physical activity is defined by the World Health Organization as "any bodily movement produced by skeletal muscles that requires energy expenditure." Human factors and social influences are important in starting and maintaining such activities. Social environments can influence motivation and persistence, through pressures towards social conformity.

Obesity is a physical marker of poor health, increasing the likelihood of various diseases. Due to social constructs surrounding health, the belief that being skinny is healthy and discrimination against those perceived to be 'unhealthy', people who are considered overweight or obese on the BMI scale face many social challenges. Challenges can range from basic things such as buying clothes, pressure from society to change their body, and being unable to get a job. This can lead to various problems such as eating disorders, self-esteem issues, and misdiagnosis and improper treatment of physical ailments due to discrimination.

Obesity has become a serious health risk throughout the developed world. More than 1 billion adults are overweight, and more than 300 million of them are clinically obese.
Furthermore, 40% of adults in the United States in 1997 reported engaging in no physical activity, while 59% do not engage in vigorous leisure-time physical activity. Although these percentages and occurrences of inactivity among older adults may vary by racial and ethnic group, and by gender, it's stated that inactivity appears, "From 47 percent among woman age 75 and older to 59 percent in older black males and 61 percent in older black females" (Hughes et al. 55).

Nearly one in five children in the United States are overweight. Ogden et al., studied 3,958 children and adolescents between the ages of 2–19 from 2003–2004. The study was a part of the NHANES which is the National Health and Nutrition Examination Survey. It showed that 17.1% of the 3,958 children were overweight, with over 35% percent of children ages 6–19 being seriously overweight. Ogden et al., had results showing 37.4% of this group was considered to be seriously overweight. Another age range where there are a high percentage of children who are overweight, are the range from 6–11. Ogden et al., was a study which was conducted in 2003–2004. The data collected was arranged into different age ranges. It showed that 18.8% of children ages 6–11 are overweight. Research has shown that obesity in children has increased from 1999 to 2004. The 1999 study showed that 28.2% of children from the ages of 2–19 were overweight. In the 2004 study, that amount was 33.6%. The age range that was the most affected was children from the ages of 6–11, for which the percentage rose 7.4 points from 29.8% to 37.2%.

Obesity can be responsible for lowering an individual's views on themselves as well as their self-respect. Sweeting et al., conducted a test on children ages 11–15. 2,127 students were surveyed on who was obese, who needed to slim down, and who was becoming obese. Of these students, 9.6% of males and 10.5% of females were considered obese at age 11. At age 15 10.5% of males and 11.6% of females were obese. 3.5% of the total students surveyed needed to slim down and 4.5% are becoming obese. The test revealed that obesity has a great effect on changing moods and lowering of self-esteem.

Inactivity is one of the biggest reasons for obesity in children. Berkey et al., conducted a study on 11,887 children from the ages of 10–15 to test whether or not an increase in a person's activity level would reduce a person's body fat. A test done on girls concluded that an increase in physical activity brought their body mass index (BMI) down .11. The male test resulted in a .33 decrease in their BMI. An increase of inactivity showed an increase in BMI for girls by .02. A way to improve this inactivity would be by changing a child's daily physical regime. Switching an overweight child's daily physical activity could help a child lose weight and get into better physical condition. Rodearmel et al., studied a couple of Latino families who had at least one child between the ages of 7–14. This was done over a 6-month span. There were two groups that the children were put into. The first group was the "America on the Move" group and the other group was the "self monitor only" group. The America on the Move group had to walk an additional 2,000 steps per day from their baseline which was already measured through pedometers and to eliminate 100 kcal/day from their normal diet by replacing dietary sugar with no caloric sweetener. The self monitor group did not have to change their physical activity or their diet. This group only had to record their physical activity with pedometers. Both groups had results showing they had decreased their BMIs with the America on the Move group decreasing their BMI much more significantly.

In the West, there is a high prevalence of overweight and obese children and adults. In the US, only 26% of adults engage in vigorous leisure-time activity (which includes a sport) or exercising three or more times per week. In an effort to increase adult involvement and decrease the percentage of adult inactivity, the US Department of Health and Human Services has set a national health objective for 2010 that hopes to "Reduce the prevalence of no leisure time activity from more than 25 percent to 20 percent of US adults" (Berlin, Storti, and Brach 1137). In Australia, the Australian Bureau of Statistics found that in 2011/12 adults spent an average of 33 minutes per day doing physical activity with 60% of the population doing less than 30 minutes and fewer than 20% doing an hour or more per day on average. The survey also showed almost 30% of the adult population reporting more than five hours of sedentary leisure activity each day.

Inactivity can contribute to a range of health related problems including: obesity, heart disease, metabolic syndrome, and other disorders. Researches have shown, "49 percent of heart disease in sedentary patients is due to lack of exercise" (Perkins, Whitehead, Steptoe 725) and the relative risk of metabolic syndrome was, "1.7 (95% CI 0.9–2.8) for lack of exercise, 1.5 (95% CI 1.1–2.1) for a positive family history and 2.0 (95% CI 1.2–3.4) in individuals with none or only an elementary school education versus university graduation" (Lee et al. 48).

Inactivity in young people has been seen to be rising in recent years, and the prevalence of sedentary leisure activities for children is significant. Video games and the internet may play a part in this. It has been found that "26 percent of children and adolescents in the United States spend more than four hours a day watching television, and they have become even more sedentary with access to computers and video games" (Damlo 1434). Along with, "62 percent of children nine to thirteen years of age do not participate in organized physical activities, and 23 percent do not participate in non-organized physical activities outside of school hours" (1434).

There may be many reasons why people may remain inactive. One reason may be due to laziness. Individuals completed a Physical Activity Questionnaire and research showed "Over one third (34 percent) of female and 12 percent of male adolescents had no leisure physical activity during a one-week period. Self-efficacy was found to have the highest correlation to leisure physical activity among all selected determinants, while 'laziness' had the highest correlation among perceived barriers" (Analysis). Along with the questionnaire other research suggested that fitness groups and fitness programs positively altered the exercise behavior of families and youths.

Another reason for physical inactivity is the perception by populations that there is nowhere safe to do so. As cities become more populated, the increased need for housing overtakes the desire for parks, cycle and walk ways therefore increasing the amount of physical inactivity in the population. Ways that can help increase the amount of physical activity is to plan and build the environment in a way that makes the population of the community feel safe to be physically active in the area. This could be done for example by slowing speed limits to safer speeds and providing safe street crossings and also by building infrastructure close to the street and pathways with safe pedestrian and cycle access and safe bike parking. By implementing these simple changes into communities, the residents will have an increased feel in security and therefore increase their daily physical activity.

Given the social and economic costs of low levels of physical activity there have been a number of public policy initiatives to raise the level, particularly focusing on children and adolescents.

United States government agencies, at both Federal and State levels, have initiated a number of programs which include encouraging workers to bike to work rather than to drive. The Fitness Portal provides a range of tools, examples, and case studies of organization ideas. A few examples given by the Fitness Portal include: using the push mower, going for a walk, taking the stairs instead of an elevator, biking, running errands, visiting friends, cleaning out the garage or the attic, volunteering to become a coach or referee, signing up for a group exercise class, joining a softball league, or parking at the farthest end of the parking lot. For children they suggest: taking a dog for a walk, starting up a kickball game, joining a sports team, going to the park, helping their parents with yard work, playing tag, riding their bike to school, walking to the store, seeing how many jumping jacks they can do, or racing a friend to the end of the block.

The Wear Valley District Council along with its local Durham Dales Primary Care Trust in England developed an innovative scheme in an effort to combat the high levels of poor health and obesity in the area. They created a mobile gym with electronic fitness monitoring equipment, which is called "WOW" (Wellness on Wheels). This effort was to persuade as many people as possible to enroll in regular workouts as part of a wider campaign in the district. Barry Nelson, a health editor for the "Northern Echo", in an interview noted a high level of interest generated. The strategy was to take exercise to people's homes rather than waiting for them to use existing leisure facilities. Children and adults, who otherwise wouldn't have exercised, came out to exercise in the mobile gym. Clearly, from this instance, the availability and nearness of a fitness program in a community positively affected the fitness behavior of the residents. As history explains, twenty-three years ago, Mr. Hackleman was named benefits manager. At this time the county had just three health and fitness programs: blood pressure screenings performed by public health nurses, a tennis tournament and some aerobics classes. However, soon after the roster grew to include at least five preventive health screening programs, eight healthy lifestyle and wellness programs, twelve health improvement and risk reduction programs and five family-life education programs. Finally in 2005, Ms. Gibson documented a total of 3,382 participants in the county's health and fitness programs, or 2,283 individuals, representing 41% of the county's population.

Most research that defends the idea of fitness programs increasing exercise practices among children and adults stems out of San Diego. Heritage Elementary school implemented its first walking school bus. This initiative had students walking to their designated stops, but not waiting for the school bus. Rather, waiting for a train of people on foot to pick them up. This allowed the students to walk to school with their peers. Although it is not directly stated, a major reason children choose to walk is because they fall victim to mob mentality. Perhaps the phrase, "fall victim to mob mentality" has a negative connotation, but that is, in essence, what the children do. Because so many other students were walking, all joined in. In this instance, mob mentality and mass behavior have a positive influence on fitness behavior, and encourage the students to exercise. The opinion of children, teachers and volunteers has been in general very favorable toward the project. Research states, "91.4 percent of participating students stated they liked the initiative very much and 87.4 percent among them prefer to go to school by the Piedibus than by any other means. Teacher and volunteers outlined the social value of the project and the increase in physical activity as positive aspects of the project". The group environment of the walking bus provides comfort in exercising, which in turn leads children to exercise more often. Although not exactly identical to the "Forrest Gump phenomenon", this walking school bus exhibits the same results: "If I can do it, you can do it".

Several video game companies have developed ways to mix the two spectrums of electronic and exercise. Dance Dance Revolution, perhaps the most well-known exercise game, had players earn points by dancing to a beat. Players earn more points for tapping dance pads on the dancing platform at precise times and in proper sequences, thereby incorporating physical exercise.

In 2006 Nintendo introduced the Wii, a next generation game console the features a motion sensitive controller. Many players have noticed the benefits of increased physical activity due to playing games on the Wii system, but that does not count as serious exercise.

According to Biddle (2007) the social influence of technology, such as electronic gaming and screen time are the main causes towards actions of sedentary behaviour, with TV viewing and computer use being the most prevalent benefactors. However, sedentary behaviours, are not simply “opposites” of physical activity, but instead suggests that they "displace time that would otherwise be used for physical activity". (Biddle, 2007)

Children and adolescents, are deemed most at risk for these sedentary behaviours with estimates for youth TV viewing being around "1.8 - 2.8 hours per day". Also Biddle (2007) states that for young people "television took up 40% of the time spent in the five most prevalent sedentary behaviours during the week and 37% at weekends" which stresses the negative impact of these social and technological advances on physical activity and fitness behaviour.

The study Hardy, Dobbins, Booth, Denney, Wilson and Okely (2006) stated that, "there are powerful societal inducements to be inactive and there are increasing concerns of an emerging preference among young people to adopt sedentary lifestyles." Based on Australian adolescents, results were received which indicated that many young people are engaging in sedentary behaviour, with grade 6's spending 34 hours per week, grade 8's with 41 hours and grade 10's with 45 hours. (Hardy et al., 2006)

Another study Zimmit (2010), found a strong, positive association between sedentary behaviours, in particular TV viewing, with obesity and low participation levels. The study stated that in the last 20 years (1990-2010), the prevalence of obesity in Australia has more than doubled. It stresses "public health initiatives targeting the reduction of sedentary pursuits may be necessary to curb the obesity epidemic."

According to the study Martínez-González, Alfredo Martínez, Hu, Gibney, & Kearney, (1999) "Obesity is the most prevalent nutrition-related problem in Western societies, and it is associated with an important burden of suffering in terms of mortality, morbidity and psychological stress". The study stresses that people suffering from obesity place a severe burden on health care systems, and that obesity could become the leading public health problem in the next century.





</doc>
<doc id="1683043" url="https://en.wikipedia.org/wiki?curid=1683043" title="Behavior-shaping constraint">
Behavior-shaping constraint

A behavior-shaping constraint, also sometimes referred to as a forcing function or poka-yoke, is a technique used in error-tolerant design to prevent the user from making common errors or mistakes. One example is the reverse lockout on the transmission of a moving automobile.

The microwave oven provides another example of a forcing function. In all modern microwave ovens, it is impossible to start the microwave while the door is still open. Likewise, the microwave will shut off automatically if the door is opened by the user. By forcing the user to close the microwave door while it is in use, it becomes impossible for the user to err by leaving the door open. Forcing functions are very effective in safety critical situations such as this, but can cause confusion in more complex systems that do not inform the user of the error that has been made.

When automobiles first started shipping with on-board GPS systems, it was not uncommon to use a forcing function which prevented the user from interacting with the GPS (such as entering in a destination) while the car was in motion. This ensures that the driver's attention is not distracted by the GPS. However, many drivers found this feature irksome, and the forcing function has largely been abandoned. This reinforces the idea that forcing functions are not always the best approach to shaping behavior.

These forcing functions are being used in the service industry as well. Call centers concerned with credit card fraud and friendly fraud are using agent-assisted automation to prevent the agent from seeing or hearing the credit card information so that it cannot be stolen. The customer punches the information into their phone keypad, the tones are masked to the agent and are not visible in the customer relationship management software.



</doc>
<doc id="46112" url="https://en.wikipedia.org/wiki?curid=46112" title="Violence">
Violence

Violence is "the use of physical force so as to injure, abuse, damage, or destroy." . Less conventional definitions are also used, such as the World Health Organization's definition of violence as "the intentional use of physical force or power, threatened or actual, against oneself, another person, or against a group or community, which either results in or has a high likelihood of resulting in injury, death, psychological harm, maldevelopment, or deprivation."
Globally, violence resulted in the deaths of an estimated 1.28 million people in 2013 up from 1.13 million in 1990. Of the deaths in 2013, roughly 842,000 were attributed to self-harm (suicide), 405,000 to interpersonal violence, and 31,000 to collective violence (war) and legal intervention. In Africa, out of every 100,000 people, each year an estimated 60.9 die a violent death. For each single death due to violence, there are dozens of hospitalizations, hundreds of emergency department visits, and thousands of doctors' appointments. Furthermore, violence often has lifelong consequences for physical and mental health and social functioning and can slow economic and social development.
In 2013, assault by firearm was the leading cause of death due to interpersonal violence, with 180,000 such deaths estimated to have occurred. The same year, assault by sharp object resulted in roughly 114,000 deaths, with a remaining 110,000 deaths from personal violence being attributed to other causes.
Violence in many forms can be preventable. There is a strong relationship between levels of violence and modifiable factors in a country such as concentrated (regional) poverty, income and gender inequality, the harmful use of alcohol, and the absence of safe, stable, and nurturing relationships between children and parents. Strategies addressing the underlying causes of violence can be relatively effective in preventing violence, although mental and physical health and individual responses, personalities, etc. have always been decisive factors in the formation of these behaviors.

The World Health Organization divides violence into three broad categories:

This initial categorization differentiates between violence a person inflicts upon himself or herself, violence inflicted by another individual or by a small group of individuals, and violence inflicted by larger groups such as states, organized political groups, militia groups and terrorist organizations. These three broad categories are each divided further to reflect more specific types of violence:

Alternatively, violence can primarily be classified as either instrumental or reactive / hostile.

Self-directed violence is subdivided into suicidal behaviour and self-abuse. The former includes suicidal thoughts, attempted suicides – also called "para suicide" or "deliberate self-injury" in some countries – and completed suicides. Self-abuse, in contrast, includes acts such as self-mutilation.

Collective violence is subdivided into structural violence and economic violence. Unlike the other two broad categories, the subcategories of collective violence suggest possible motives for violence committed by larger groups of individuals or by states. Collective violence that is committed to advance a particular social agenda includes, for example, crimes of hate committed by organized groups, terrorist acts and mob violence. Political violence includes war and related violent conflicts, state violence and similar acts carried out by larger groups. Economic violence includes attacks by larger groups motivated by economic gain – such as attacks carried out with the purpose of disrupting economic activity, denying access to essential services, or creating economic division and fragmentation. Clearly, acts committed by larger groups can have multiple motives.

This typology, while imperfect and far from being universally accepted, does provide a useful framework for understanding the complex patterns of violence taking place around the world, as well as violence in the everyday lives of individuals, families and communities. It also overcomes many of the limitations of other typologies by capturing the nature of violent acts, the relevance of the setting, the relationship between the perpetrator and the victim, and – in the case of collective violence – possible motivations for the violence. However, in both research and practice, the dividing lines between the different types of violence are not always so clear. State violence also involves upholding, forms of violence of a structural nature, such as poverty, through dismantling welfare, creating strict policies such as 'welfare to work', in order to cause further stimulation and disadvantage Poverty as a form of violence may involve oppressive policies that specifically target minority or low socio-economic groups. The 'war on drugs', for example, rather than increasing the health and well-being of at risk demographics, most often results in violence committed against these vulnerable demographics through incarceration, stigmatization and police brutality

War is a state of prolonged violent large-scale conflict involving two or more groups of people, usually under the auspices of government. It is the most extreme form of collective violence.
War is fought as a means of resolving territorial and other conflicts, as war of aggression to conquer territory or loot resources, in national self-defence or liberation, or to suppress attempts of part of the nation to secede from it. There are also ideological, religious and revolutionary wars.

Since the Industrial Revolution the lethality of modern warfare has grown. World War I casualties were over 40 million and World War II casualties were over 70 million.

Violence includes those acts that result from a power relationship, including threats and intimidation, neglect or acts of omission. Such non-physical violence has a broad range of outcomes – including psychological harm, deprivation and maldevelopment. Violence may not necessarily result in injury or death, but nonetheless poses a substantial burden on individuals, families, communities and health care systems worldwide. Many forms of violence can result in physical, psychological and social problems that do not necessarily lead to injury, disability or death. These consequences can be immediate, as well as latent, and can last for years after the initial abuse. Defining outcomes solely in terms of injury or death thus limits the understanding of the full impact of violence.

Interpersonal violence is divided into two subcategories: Family and intimate partner violence – that is, violence largely between family members and intimate partners, usually, though not exclusively, taking place in the home. Community violence – violence between individuals who are unrelated, and who may or may not know each other, generally taking place outside the home. The former group includes forms of violence such as child abuse, intimate partner violence and abuse of the elderly. The latter includes youth violence, random acts of violence, rape or sexual assault by strangers, and violence in institutional settings such as schools, workplaces, prisons and nursing homes. When interpersonal violence occurs in families, its psychological consequences can affect parents, children, and their relationship in the short- and long-terms.

Child maltreatment is the abuse and neglect that occurs to children under 18 years of age. It includes all types of physical and/or emotional ill-treatment, sexual abuse, neglect, negligence and commercial or other child exploitation, which results in actual or potential harm to the child’s health, survival, development or dignity in the context of a relationship of responsibility, trust, or power. Exposure to intimate partner violence is also sometimes included as a form of child maltreatment.

Child maltreatment is a global problem with serious lifelong consequences, which is, however, complex and difficult to study.

There are no reliable global estimates for the prevalence of child maltreatment. Data for many countries, especially low- and middle-income countries, are lacking. Current estimates vary widely depending on the country and the method of research used. Approximately 20% of women and 5–10% of men report being sexually abused as children, while 25–50% of all children report being physically abused.

Consequences of child maltreatment include impaired lifelong physical and mental health, and social and occupational functioning (e.g. school, job, and relationship difficulties). These can ultimately slow a country's economic and social development. Preventing child maltreatment before it starts is possible and requires a multisectoral approach. Effective prevention programmes support parents and teach positive parenting skills. Ongoing care of children and families can reduce the risk of maltreatment reoccurring and can minimize its consequences.

Following the World Health Organization, youth are defined as people between the ages of 10 and 29 years. Youth violence refers to violence occurring between youths, and includes acts that range from bullying and physical fighting, through more severe sexual and physical assault to homicide.

Worldwide some 250,000 homicides occur among youth 10–29 years of age each year, which is 41% of the total number of homicides globally each year ("Global Burden of Disease", World Health Organization, 2008). For each young person killed, 20-40 more sustain injuries requiring hospital treatment. Youth violence has a serious, often lifelong, impact on a person's psychological and social functioning. Youth violence greatly increases the costs of health, welfare and criminal justice services; reduces productivity; decreases the value of property; and generally undermines the fabric of society.

Prevention programmes shown to be effective or to have promise in reducing youth violence include life skills and social development programmes designed to help children and adolescents manage anger, resolve conflict, and develop the necessary social skills to solve problems; schools-based anti-bullying prevention programmes; and programmes to reduce access to alcohol, illegal drugs and guns. Also, given significant neighbourhood effects on youth violence, interventions involving relocating families to less poor environments have shown promising results. Similarly, urban renewal projects such as business improvement districts have shown a reduction in youth violence.

Different types of youth on youth violence include witnessing or being involved in physical, emotional and sexual abuse (e.g. physical attacks, bullying, rape), and violent acts like gang shootings and robberies. According to researchers in 2018, "More than half of children and adolescents living in cities have experienced some form of community violence." The violence "can also all take place under one roof, or in a given community or neighborhood and can happen at the same time or at different stages of life." Youth violence has immediate and long term adverse impact whether the individual was the recipient of the violence or a witness to it.

Youth violence impacts individuals, their families, and society. Victims can have lifelong injuries which means ongoing doctor and hospital visits, the cost of which quickly add up. Since the victims of youth-on-youth violence may not be able to attend school or work because of their physical and/or mental injuries, it is often up to their family members to take care of them, including paying their daily living expenses and medical bills. Their caretakers may have to give up their jobs or work reduced hours to provide help to the victim of violence. This causes a further burden on society because the victim and maybe even their caretakers have to obtain government assistance to help pay their bills. Recent research has found that psychological trauma during childhood can change a child's brain. "Trauma is known to physically affect the brain and the body which causes anxiety, rage, and the ability to concentrate. They can also have problems remembering, trusting, and forming relationships." Since the brain becomes used to violence it may stay continually in an alert state (similar to being stuck in the fight or flight mode). "Researchers claim that the youth who are exposed to violence may have emotional, social, and cognitive problems. They may have trouble controlling emotions, paying attention in school, withdraw from friends, or show signs of post-traumatic stress disorder".

It is important for youth exposed to violence to understand how their bodies may react so they can take positive steps to counteract any possible short- and long-term negative effects (e.g., poor concentration, feelings of depression, heightened levels of anxiety). By taking immediate steps to mitigate the effects of the trauma they’ve experienced, negative repercussions can be reduced or eliminated. As an initial step, the youths need to understand why they may be feeling a certain way and to understand how the violence they have experienced may be causing negative feelings and making them behave differently. Pursuing a greater awareness of their feelings, perceptions, and negative emotions is the first step that should be taken as part of recovering from the trauma they have experienced. “Neuroscience research shows that the only way we can change the way we feel is by becoming aware of our inner experience and learning to befriend what is going on inside ourselves”. 
Some of the ways to combat the adverse effects of exposure to youth violence would be to try various mindfulness and movement activities, deep breathing exercises and other actions that enable youths to release their pent up emotions. Using these techniques will teach body awareness, reduce anxiety and nervousness, and reduce feelings of anger and annoyance. Over time these types of activities will help these younger victims of violence to have greater control over their feelings and behaviors and avoid unhealthy ways of coping. Another way to help trauma victims of youth violence is through the arts. This can be accomplished by giving them the opportunity to engage in drawing, painting, music, and singing which will give them an outlet to express themselves and their emotions in a positive way.

Youth who have experienced violence benefit from having a close relationship with one or more people. This is important because the trauma victims need to have people who are safe and trustworthy that they can relate and talk to about their horrible experiences. Some youth do not have adult figures at home or someone they can count on for guidance and comfort. Schools in bad neighborhoods where youth violence is prevalent should assign counselors to each student so that they receive regular guidance. In addition to counseling/therapy sessions and programs, it has been recommended that schools offer mentoring programs where students can interact with adults who can be a positive influence on them. Another way is to create more neighborhood programs to ensure that each child has a positive and stable place to go when school in not in session. Many children have benefited from formal organizations now which aim to help mentor and provide a safe environment for the youth especially those living in neighborhoods with higher rates of violence. This includes organizations such as Becoming a Man, CeaseFire Illinois, Chicago Area Project, Little Black Pearl, and Rainbow House". These programs are designed to help give the youth a safe place to go, stop the violence from occurring, offering counseling and mentoring to help stop the cycle of violence. If the youth do not have a safe place to go after school hours they will likely get into trouble, receive poor grades, drop out of school and use drugs and alcohol. The gangs look for youth who do not have positive influences in their life and need protection. This is why these programs are so important for the youth to have a safe environment rather than resorting to the streets. For example, Derek grew up amongst the violence in Chicago in the 1980's and was even a former gang leader. It took him thirty years in a gang and time in jail to realize he was on the wrong path. He created a boxing program called "Boxing Out Negativity" which provides youth in high crime areas a safe place to get out their anger and energy. It helps them in a positive way and keeps them off the street. With the help of programs to help victims of youth violence there is a greater opportunity for these youth to turn their lives around.

Intimate partner violence refers to behaviour in an intimate relationship that causes physical, sexual or psychological harm, including physical aggression, sexual coercion, psychological abuse and controlling behaviours.

Population-level surveys based on reports from victims provide the most accurate estimates of the prevalence of intimate partner violence and sexual violence in non-conflict settings. A study conducted by WHO in 10 mainly developing countries found that, among women aged 15 to 49 years, between 15% (Japan) and 70% (Ethiopia and Peru) of women reported physical and/or sexual violence by an intimate partner.

Intimate partner and sexual violence have serious short- and long-term physical, mental, sexual and reproductive health problems for victims and for their children, and lead to high social and economic costs. These include both fatal and non-fatal injuries, depression and post-traumatic stress disorder, unintended pregnancies, sexually transmitted infections, including HIV.

Factors associated with the perpetration and experiencing of intimate partner violence are low levels of education, history of violence as a perpetrator, a victim or a witness of parental violence, harmful use of alcohol, attitudes that are accepting of violence as well as marital discord and dissatisfaction. Factors associated only with perpetration of intimate partner violence are having multiple partners, and antisocial personality disorder.

A recent theory named "The Criminal Spin" suggests a mutual flywheel effect between partners that is manifested by an escalation in the violence. A violent spin may occur in any other forms of violence, but in Intimate partner violence the added value is the mutual spin, based on the unique situation and characteristics of intimate relationship.

The primary prevention strategy with the best evidence for effectiveness for intimate partner violence is school-based programming for adolescents to prevent violence within dating relationships. Evidence is emerging for the effectiveness of several other primary prevention strategies – those that: combine microfinance with gender equality training; promote communication and relationship skills within communities; reduce access to, and the harmful use of alcohol; and change cultural gender norms.

Sexual violence is any sexual act, attempt to obtain a sexual act, unwanted sexual comments or advances, or acts to traffic, or otherwise directed against a person’s sexuality using coercion, by any person regardless of their relationship to the victim, in any setting. It includes rape, defined as the physically forced or otherwise coerced penetration of the vulva or anus with a penis, other body part or object.

Population-level surveys based on reports from victims estimate that between 0.3–11.5% of women reported experiencing sexual violence. Sexual violence has serious short- and long-term consequences on physical, mental, sexual and reproductive health for victims and for their children as described in the section on intimate partner violence. If perpetrated during childhood, sexual violence can lead to increased smoking, drug and alcohol misuse, and risky sexual behaviors in later life. It is also associated with perpetration of violence and being a victim of violence.

Many of the risk factors for sexual violence are the same as for domestic violence. Risk factors specific to sexual violence perpetration include beliefs in family honor and sexual purity, ideologies of male sexual entitlement and weak legal sanctions for sexual violence.

Few interventions to prevent sexual violence have been demonstrated to be effective. School-based programmes to prevent child sexual abuse by teaching children to recognize and avoid potentially sexually abusive situations are run in many parts of the world and appear promising, but require further research. To achieve lasting change, it is important to enact legislation and develop policies that protect women; address discrimination against women and promote gender equality; and help to move the culture away from violence.

Elder maltreatment is a single or repeated act, or lack of appropriate action, occurring within any relationship where there is an expectation of trust which causes harm or distress to an older person. This type of violence constitutes a violation of human rights and includes physical, sexual, psychological, emotional; financial and material abuse; abandonment; neglect; and serious loss of dignity and respect.

While there is little information regarding the extent of maltreatment in elderly populations, especially in developing countries, it is estimated that 4–6% of elderly people in high-income countries have experienced some form of maltreatment at home However, older people are often afraid to report cases of maltreatment to family, friends, or to the authorities. Data on the extent of the problem in institutions such as hospitals, nursing homes and other long-term care facilities are scarce. Elder maltreatment can lead to serious physical injuries and long-term psychological consequences. Elder maltreatment is predicted to increase as many countries are experiencing rapidly ageing populations.

Many strategies have been implemented to prevent elder maltreatment and to take action against it and mitigate its consequences including public and professional awareness campaigns, screening (of potential victims and abusers), caregiver support interventions (e.g. stress management, respite care), adult protective services and self-help groups. Their effectiveness has, however, not so far been well-established.

Several rare but painful episodes of assassination, attempted assassination and school shootings at elementary, middle, high schools, as well as colleges and universities in the United States, led to a considerable body of research on ascertainable behaviors of persons who have planned or carried out such attacks. These studies (1995–2002) investigated what the authors called "targeted violence," described the "path to violence" of those who planned or carried out attacks and laid out suggestions for law enforcement and educators. A major point from these research studies is that targeted violence does not just "come out of the blue".

As an anthropological concept, "everyday violence" may refer to the incorporation of different forms of violence (mainly political violence) into daily practices.

Violence cannot be attributed to a single factor. Its causes are complex and occur at different levels. To represent this complexity, the ecological, or social ecological model is often used. The following four-level version of the ecological model is often used in the study of violence:

The first level identifies biological and personal factors that influence how individuals behave and increase their likelihood of becoming a victim or perpetrator of violence: demographic characteristics (age, education, income), genetics, brain lesions, personality disorders, substance abuse, and a history of experiencing, witnessing, or engaging in violent behaviour.

The second level focuses on close relationships, such as those with family and friends. In youth violence, for example, having friends who engage in or encourage violence can increase a young person’s risk of being a victim or perpetrator of violence. For intimate partner violence, a consistent marker at this level of the model is marital conflict or discord in the relationship. In elder abuse, important factors are stress due to the nature of the past relationship between the abused person and the care giver.

The third level explores the community context—i.e., schools, workplaces, and neighbourhoods. Risk at this level may be affected by factors such as the existence of a local drug trade, the absence of social networks, and concentrated poverty. All these factors have been shown to be important in several types of violence.

Finally, the fourth level looks at the broad societal factors that help to create a climate in which violence is encouraged or inhibited: the responsiveness of the criminal justice system, social and cultural norms regarding gender roles or parent-child relationships, income inequality, the strength of the social welfare system, the social acceptability of violence, the availability of weapons, the exposure to violence in mass media, and political instability.

Cross-cultural studies have shown that greater prevalence of corporal punishment of children tends to predict higher levels of violence in societies. For instance, a 2005 analysis of 186 pre-industrial societies found that corporal punishment was more prevalent in societies which also had higher rates of homicide, assault, and war. In the United States, domestic corporal punishment has been linked to later violent acts against family members and spouses. While studies showing associations between physical punishment of children and later aggression cannot prove that physical punishment causes an increase in aggression, a number of longitudinal studies suggest that the experience of physical punishment has a direct causal effect on later aggressive behaviors. The American family violence researcher Murray A. Straus believes that disciplinary spanking forms "the most prevalent and important form of violence in American families", whose effects contribute to several major societal problems, including later domestic violence and crime.

The causes of violent behavior in people are often a topic of research in psychology. Neurobiologist Jan Vodka emphasizes that, for those purposes, "violent behavior is defined as overt and intentional physically aggressive behavior against another person."

Based on the idea of human nature, scientists do agree violence is inherent in humans. Among prehistoric humans, there is archaeological evidence for both contentions of violence and peacefulness as primary characteristics.

Since violence is a matter of perception as well as a measurable phenomenon, psychologists have found variability in whether people perceive certain physical acts as "violent". For example, in a state where execution is a legalized punishment we do not typically perceive the executioner as "violent", though we may talk, in a more metaphorical way, of the state acting violently. Likewise, understandings of violence are linked to a perceived aggressor-victim relationship: hence psychologists have shown that people may not recognise defensive use of force as violent, even in cases where the amount of force used is significantly greater than in the original aggression.

The "violent male ape" image is often brought up in discussions of human violence. Dale Peterson and Richard Wranghamin "Demonic Males: Apes and the Origins of Human Violence" write that violence is inherent in humans, though not inevitable.
However, William L. Ury, editor of a book called "We Must Fight! From the Battlefield to the Schoolyard—A New Perspective on Violent Conflict and Its Prevention" criticizes the "killer ape" myth in his book which brings together discussions from two Harvard Law School symposiums. The conclusion is that "we also have lots of natural mechanisms for cooperation, to keep conflict in check, to channel aggression, and to overcome conflict. These are just as natural to us as the aggressive tendencies."

The psychiatrist James Gilligan argues that most violent behavior represents an effort to eliminate feelings of shame and humiliation, which he calls "the death of self". The use of violence often is a source of pride and a defence of honor, especially among males who believe violence defines manhood.

In an article entitled "The History of Violence" in "The New Republic", Steven Pinker posits that, on average, the amount and cruelty of violence to humans and animals has decreased over the last few centuries.

Pinker's observation of the decline in interpersonal violence echoes the work of Norbert Elias, who attributes the decline to a "civilizing process", in which the state's monopolization of violence, the maintenance of socioeconomic interdependencies or "figurations", and the maintenance of behavioural codes in culture all contribute to the development of individual sensibilities, which increase the repugnance of individuals towards violent acts.

Some scholars disagree with the argument that all violence is decreasing arguing that not all types of violent behaviour are lower now than in the past. They suggest that research typically focuses on lethal violence, often looks at homicide rates of death due to warfare, but ignore the less obvious forms of violence. However, non-lethal violence, such as assaults or bullying appear to be declining as well.
In his article "The Coming Anarchy", Robert D. Kaplan introduces the notion of liberating violence. According to Kaplan, we will observe more violent civil wars in the future, which will be fought due to economic inequalities around the world.

The concept of violence normalization, is known as socially sanctioned or structural violence, and is a topic of increasing interest to researchers trying to understand violent behavior. It has been discussed at length by researchers in sociology, medical anthropology, psychology, philosophy, and bioarchaeology.

Evolutionary psychology offers several explanations for human violence in various contexts, such as sexual jealousy in humans, child abuse, and homicide. Goetz (2010) argues that humans are similar to most mammal species and use violence in specific situations. He writes that "Buss and Shackelford (1997a) proposed seven adaptive problems our ancestors recurrently faced that might have been solved by aggression: co-opting the resources of others, defending against attack, inflicting costs on same-sex rivals, negotiating status and hierarchies, deterring rivals from future aggression, deterring mate from infidelity, and reducing resources expended on genetically unrelated children."

Goetz writes that most homicides seem to start from relatively trivial disputes between unrelated men who then escalate to violence and death. He argues that such conflicts occur when there is a status dispute between men of relatively similar status. If there is a great initial status difference, then the lower status individual usually offers no challenge and if challenged the higher status individual usually ignores the lower status individual. At the same an environment of great inequalities between people may cause those at the bottom to use more violence in attempts to gain status.

Research into the media and violence examines whether links between consuming media violence and subsequent aggressive and violent behaviour exists. Although some scholars had claimed media violence may increase aggression, this view is coming increasingly in doubt both in the scholarly community and was rejected by the US Supreme Court in the Brown v EMA case, as well as in a review of video game violence by the Australian Government (2010) which concluded evidence for harmful effects were inconclusive at best and the rhetoric of some scholars was not matched by good data.

The threat and enforcement of physical punishment has been a tried and tested method of preventing some violence since civilisation began. It is used in various degrees in most countries.

A review of scientific literature by the World Health Organization on the effectiveness of strategies to prevent interpersonal violence identified the seven strategies below as being supported by either strong or emerging evidence for effectiveness. These strategies target risk factors at all four levels of the ecological model.

Among the most effective such programmes to prevent child maltreatment and reduce childhood aggression are the Nurse Family Partnership home-visiting programme and the Triple P (Parenting Program). There is also emerging evidence that these programmes reduce convictions and violent acts in adolescence and early adulthood, and probably help decrease intimate partner violence and self-directed violence in later life.

Evidence shows that the life skills acquired in social development programmes can reduce involvement in violence, improve social skills, boost educational achievement and improve job prospects. Life skills refer to social, emotional, and behavioural competencies which help children and adolescents effectively deal with the challenges of everyday life.

Evaluation studies are beginning to support community interventions that aim to prevent violence against women by promoting gender equality. For instance, evidence suggests that programmes that combine microfinance with gender equity training can reduce intimate partner violence. School-based programmes such as Safe Dates programme in the United States of America and the Youth Relationship Project in Canada have been found to be effective for reducing dating violence.

Rules or expectations of behaviour – norms – within a cultural or social group can encourage violence. Interventions that challenge cultural and social norms supportive of violence can prevent acts of violence and have been widely used, but the evidence base for their effectiveness is currently weak. The effectiveness of interventions addressing dating violence and sexual abuse among teenagers and young adults by challenging social and cultural norms related to gender is supported by some evidence.

Interventions to identify victims of interpersonal violence and provide effective care and support are critical for protecting health and breaking cycles of violence from one generation to the next. Examples for which evidence of effectiveness is emerging includes: screening tools to identify victims of intimate partner violence and refer them to appropriate services; psychosocial interventions – such as trauma-focused cognitive behavioural therapy – to reduce mental health problems associated with violence, including post-traumatic stress disorder; and protection orders, which prohibit a perpetrator from contacting the victim, to reduce repeat victimization among victims of intimate partner violence.

Not surprisingly, scientific evidence about the effectiveness of interventions to prevent collective violence is lacking. However, policies that facilitate reductions in poverty, that make decision-making more accountable, that reduce inequalities between groups, as well as policies that reduce access to biological, chemical, nuclear and other weapons have been recommended. When planning responses to violent conflicts, recommended approaches include assessing at an early stage who is most vulnerable and what their needs are, co-ordination of activities between various players and working towards global, national and local capabilities so as to deliver effective health services during the various stages of an emergency.

One of the main functions of law is to regulate violence. Sociologist Max Weber stated that the state claims the monopoly of the legitimate use of force practised within the confines of a specific territory. Law enforcement is the main means of regulating nonmilitary violence in society. Governments regulate the use of violence through legal systems governing individuals and political authorities, including the police and military. Civil societies authorize some amount of violence, exercised through the police power, to maintain the status quo and enforce laws.

However, German political theorist Hannah Arendt noted: "Violence can be justifiable, but it never will be legitimate ... Its justification loses in plausibility the farther its intended end recedes into the future. No one questions the use of violence in self-defence, because the danger is not only clear but also present, and the end justifying the means is immediate". Arendt made a clear distinction between violence and power. Most political theorists regarded violence as an extreme manifestation of power whereas Arendt regarded the two concepts as opposites.
In the 20th century in acts of democide governments may have killed more than 260 million of their own people through police brutality, execution, massacre, slave labour camps, and sometimes through intentional famine.

Violent acts that are not carried out by the military or police and that are not in self-defense are usually classified as crimes, although not all crimes are violent crimes. Damage to property is classified as violent crime in some jurisdictions but not in all. The Federal Bureau of Investigation (FBI) classifies violence resulting in homicide into criminal homicide and justifiable homicide (e.g. self-defense).

The criminal justice approach sees its main task as enforcing laws that proscribe violence and ensuring that "justice is done". The notions of individual blame, responsibility, guilt, and culpability are central to criminal justice's approach to violence and one of the criminal justice system's main tasks is to "do justice", i.e. to ensure that offenders are properly identified, that the degree of their guilt is as accurately ascertained as possible, and that they are punished appropriately. To prevent and respond to violence, the criminal justice approach relies primarily on deterrence, incarceration and the punishment and rehabilitation of perpetrators.

The criminal justice approach, beyond justice and punishment, has traditionally emphasized indicated interventions, aimed at those who have already been involved in violence, either as victims or as perpetrators. One of the main reasons offenders are arrested, prosecuted, and convicted is to prevent further crimes – through deterrence (threatening potential offenders with criminal sanctions if they commit crimes), incapacitation (physically preventing offenders from committing further crimes by locking them up) and through rehabilitation (using time spent under state supervision to develop skills or change one's psychological make-up to reduce the likelihood of future offences).

In recent decades in many countries in the world, the criminal justice system has taken an increasing interest in preventing violence before it occurs. For instance, much of community and problem-oriented policing aims to reduce crime and violence by altering the conditions that foster it – and not to increase the number of arrests. Indeed, some police leaders have gone so far as to say the police should primarily be a crime prevention agency. Juvenile justice systems – an important component of criminal justice systems – are largely based on the belief in rehabilitation and prevention. In the US, the criminal justice system has, for instance, funded school- and community-based initiatives to reduce children's access to guns and teach conflict resolution. In 1974, the US Department of Justice assumed primary responsibility for delinquency prevention programmes and created the Office of Juvenile Justice and Delinquency Prevention, which has supported the "Blueprints for violence prevention" programme at the University of Colorado Boulder.

The public health approach is a science-driven, population-based, interdisciplinary, intersectoral approach based on the ecological model which emphasizes primary prevention. Rather than focusing on individuals, the public health approach aims to provide the maximum benefit for the largest number of people, and to extend better care and safety to entire populations. The public health approach is interdisciplinary, drawing upon knowledge from many disciplines including medicine, epidemiology, sociology, psychology, criminology, education and economics. Because all forms of violence are multi-faceted problems, the public health approach emphasizes a multi-sectoral response. It has been proved time and again that cooperative efforts from such diverse sectors as health, education, social welfare, and criminal justice are often necessary to solve what are usually assumed to be purely "criminal" or "medical" problems. The public health approach considers that violence, rather than being the result of any single factor, is the outcome of multiple risk factors and causes, interacting at four levels of a nested hierarchy (individual, close relationship/family, community and wider society) of the Social ecological model.

From a public health perspective, prevention strategies can be classified into three types:

A public health approach emphasizes the primary prevention of violence, i.e. stopping them from occurring in the first place. Until recently, this approach has been relatively neglected in the field, with the majority of resources directed towards secondary or tertiary prevention. Perhaps the most critical element of a public health approach to prevention is the ability to identify underlying causes rather than focusing upon more visible "symptoms". This allows for the development and testing of effective approaches to address the underlying causes and so improve health.

The public health approach is an evidence-based and systematic process involving the following four steps:

In many countries, violence prevention is still a new or emerging field in public health. The public health community has started only recently to realize the contributions it can make to reducing violence and mitigating its consequences. In 1949, Gordon called for injury prevention efforts to be based on the understanding of causes, in a similar way to prevention efforts for communicable and other diseases. In 1962, Gomez, referring to the WHO definition of health, stated that it is obvious that violence does not contribute to "extending life" or to a "complete state of well-being". He defined violence as an issue that public health experts needed to address and stated that it should not be the primary domain of lawyers, military personnel, or politicians.

However, it is only in the last 30 years that public health has begun to address violence, and only in the last fifteen has it done so at the global level. This is a much shorter period of time than public health has been tackling other health problems of comparable magnitude and with similarly severe lifelong consequences.

The global public health response to interpersonal violence began in earnest in the mid-1990s. In 1996, the World Health Assembly adopted Resolution WHA49.25 which declared violence "a leading worldwide public health problem" and requested that the World Health Organization (WHO) initiate public health activities to (1) document and characterize the burden of violence, (2) assess the effectiveness of programmes, with particular attention to women and children and community-based initiatives, and (3) promote activities to tackle the problem at the international and national levels. The World Health Organization's initial response to this resolution was to create the Department of Violence and Injury Prevention and Disability and to publish the World report on violence and health (2002).

The case for the public health sector addressing interpersonal violence rests on four main arguments. First, the significant amount of time health care professionals dedicate to caring for victims and perpetrators of violence has made them familiar with the problem and has led many, particularly in emergency departments, to mobilize to address it. The information, resources, and infrastructures the health care sector has at its disposal are an important asset for research and prevention work. Second, the magnitude of the problem and its potentially severe lifelong consequences and high costs to individuals and wider society call for population-level interventions typical of the public health approach. Third, the criminal justice approach, the other main approach to addressing violence (link to entry above), has traditionally been more geared towards violence that occurs between male youths and adults in the street and other public places – which makes up the bulk of homicides in most countries – than towards violence occurring in private settings such as child maltreatment, intimate partner violence and elder abuse – which makes up the largest share of non-fatal violence. Fourth, evidence is beginning to accumulate that a science-based public health approach is effective at preventing interpersonal violence.

The human rights approach is based on the obligations of states to respect, protect and fulfill human rights and therefore to prevent, eradicate and punish violence. It recognizes violence as a violation of many human rights: the rights to life, liberty, autonomy and security of the person; the rights to equality and non-discrimination; the rights to be free from torture and cruel, inhuman and degrading treatment or punishment; the right to privacy; and the right to the highest attainable standard of health. These human rights are enshrined in international and regional treaties and national constitutions and laws, which stipulate the obligations of states, and include mechanisms to hold states accountable. The Convention on the Elimination of All Forms of Discrimination Against Women, for example, requires that countries party to the Convention take all appropriate steps to end violence against women. The Convention on the Rights of the Child in its Article 19 states that States Parties shall take all appropriate legislative, administrative, social and educational measures to protect the child from all forms of physical or mental violence, injury or abuse, neglect or negligent treatment, maltreatment or exploitation, including sexual abuse, while in the care of parent(s), legal guardian(s) or any other person who has the care of the child.

Violence, as defined in the dictionary of human geography, "appears whenever power is in jeopardy" and "in and of itself stands emptied of strength and purpose: it is part of a larger matrix of socio-political power struggles". Violence can be broadly divided into three broad categories – direct violence, structural violence and cultural violence. Thus defined and delineated, it is of note, as Hyndman says, that "geography came late to theorizing violence" in comparison to other social sciences. Social and human geography, rooted in the humanist, Marxist, and feminist subfields that emerged following the early positivist approaches and subsequent behavioral turn, have long been concerned with social and spatial justice.
Along with critical geographers and political geographers, it is these groupings of geographers that most often interact with violence. Keeping this idea of social/spatial justice via geography in mind, it is worthwhile to look at geographical approaches to violence in the context of politics.

Derek Gregory and Alan Pred assembled the influential edited collection "Violent Geographies: Fear, Terror, and Political Violence", which demonstrates how place, space, and landscape are foremost factors in the real and imagined practices of organized violence both historically and in the present. Evidently, political violence often gives a part for the state to play. When "modern states not only claim a monopoly of the legitimate means of violence; they also routinely use the threat of violence to enforce the rule of law", the law not only becomes a form of violence but is violence. Philosopher Giorgio Agamben's concepts of state of exception and "homo sacer" are useful to consider within a geography of violence. The state, in the grip of a perceived, potential crisis (whether legitimate or not) takes preventative legal measures, such as a suspension of rights (it is in this climate, as Agamben demonstrates, that the formation of the Social Democratic and Nazi government's lager or concentration camp can occur). However, when this "in limbo" reality is designed to be in place "until further notice…the state of exception thus ceases to be referred to as an external and provisional state of factual danger and comes to be confused with juridical rule itself". For Agamben, the physical space of the camp "is a piece of land placed outside the normal juridical order, but it is nevertheless not simply an external space". At the scale of the body, in the state of exception, a person is so removed from their rights by "juridical procedures and deployments of power" that "no act committed against them could appear any longer as a crime"; in other words, people become only "homo sacer". Guantanamo Bay could also be said to represent the physicality of the state of exception in space, and can just as easily draw man as homo sacer.

In the 1970s, genocides in Cambodia under the Khmer Rouge and Pol Pot resulted in the deaths of over two million Cambodians (which was 25% of the Cambodian population), forming one of the many contemporary examples of state-sponsored violence. About fourteen thousand of these murders occurred at Choeung Ek, which is the best-known of the extermination camps referred to as the Killing Fields. The killings were arbitrary; for example, a person could be killed for wearing glasses, since that was seen as associating them with intellectuals and therefore as making them part of the enemy. People were murdered with impunity because it was no crime; Cambodians were made "homo sacer" in a condition of bare life. The Killing Fields—manifestations of Agamben's concept of camps beyond the normal rule of law—featured the state of exception. As part of Pol Pot's "ideological intent…to create a purely agrarian society or cooperative", he "dismantled the country's existing economic infrastructure and depopulated every urban area". Forced movement, such as this forced movement applied by Pol Pot, is a clear display of structural violence. When "symbols of Cambodian society were equally disrupted, social institutions of every kind…were purged or torn down", cultural violence (defined as when "any aspect of culture such as language, religion, ideology, art, or cosmology is used to legitimize direct or structural violence") is added to the structural violence of forced movement and to the direct violence, such as murder, at the Killing Fields. Vietnam eventually intervened and the genocide officially ended. However, ten million landmines left by opposing guerillas in the 1970s continue to create a violent landscape in Cambodia.

Human geography, though coming late to the theorizing table, has tackled violence through many lenses, including anarchist geography, feminist geography, Marxist geography, political geography, and critical geography. However, Adriana Cavarero notes that, "as violence spreads and assumes unheard-of forms, it becomes difficult to name in contemporary language". Cavarero proposes that, in facing such a truth, it is prudent to reconsider violence as "horrorism"; that is, "as though ideally all the…victims, instead of their killers, ought to determine the name". With geography often adding the forgotten spatial aspect to theories of social science, rather than creating them solely within the discipline, it seems that the self-reflexive contemporary geography of today may have an extremely important place in this current (re)imaging of violence, exemplified by Cavarero.

As of 2010, all forms of violence resulted in about 1.34 million deaths up from about 1 million in 1990. Suicide accounts for about 883,000, interpersonal violence for 456,000 and collective violence for 18,000. Deaths due to collective violence have decreased from 64,000 in 1990.

By way of comparison, the 1.5 millions deaths a year due to violence is greater than the number of deaths due to tuberculosis (1.34 million), road traffic injuries (1.21 million), and malaria (830'000), but slightly less than the number of people who die from HIV/AIDS (1.77 million).

For every death due to violence, there are numerous nonfatal injuries. In 2008, over 16 million cases of non-fatal violence-related injuries were severe enough to require medical attention. Beyond deaths and injuries, forms of violence such as child maltreatment, intimate partner violence, and elder maltreatment have been found to be highly prevalent.

In the last 45 years, suicide rates have increased by 60% worldwide. Suicide is among the three leading causes of death among those aged 15–44 years in some countries, and the second leading cause of death in the 10–24 years age group. These figures do not include suicide attempts which are up to 20 times more frequent than completed suicide. Suicide was the 16th leading cause of death worldwide in 2004 and is projected to increase to the 12th in 2030. Although suicide rates have traditionally been highest among the male elderly, rates among young people have been increasing to such an extent that they are now the group at highest risk in a third of countries, in both developed and developing countries.

Rates and patterns of violent death vary by country and region. In recent years, homicide rates have been highest in developing countries in Sub-Saharan Africa and Latin America and the Caribbean and lowest in East Asia, the western Pacific, and some countries in northern Africa. Studies show a strong, inverse relationship between homicide rates and both economic development and economic equality. Poorer countries, especially those with large gaps between the rich and the poor, tend to have higher rates of homicide than wealthier countries. Homicide rates differ markedly by age and sex. Gender differences are least marked for children. For the 15 to 29 age group, male rates were nearly six times those for female rates; for the remaining age groups, male rates were from two to four times those for females.

Studies in a number of countries show that, for every homicide among young people age 10 to 24, 20 to 40 other young people receive hospital treatment for a violent injury.

Forms of violence such as child maltreatment and intimate partner violence are highly prevalent. Approximately 20% of women and 5–10% of men report being sexually abused as children, while 25–50% of all children report being physically abused. A WHO multi-country study found that between 15–71% of women reported experiencing physical and/or sexual violence by an intimate partner at some point in their lives.

Wars grab headlines, but the individual risk of dying violently in an armed conflict is today relatively low—much lower than the risk of violent death in many countries that are not suffering from an armed conflict. For example, between 1976 and 2008, African Americans were victims of 329,825 homicides. Although there is a widespread perception that war is the most dangerous form of armed violence in the world, the average person living in a conflict-affected country had a risk of dying violently in the conflict of about 2.0 per 100,000 population between 2004 and 2007. This can be compared to the average world homicide rate of 7.6 per 100,000 people. This illustration highlights the value of accounting for all forms of armed violence rather than an exclusive focus on conflict related violence. Certainly, there are huge variations in the risk of dying from armed conflict at the national and subnational level, and the risk of dying violently in a conflict in specific countries remains extremely high. In Iraq, for example, the direct conflict death rate for 2004–07 was 65 per 100,000 people per year and, in Somalia, 24 per 100,000 people. This rate even reached peaks of 91 per 100,000 in Iraq in 2006 and 74 per 100,000 in Somalia in 2007.

Organized, large-scale, militaristic, or regular human-on-human violence was absent for the vast majority of the human timeline, and is first documented to have started only relatively recently in the Holocene, an epoch that began about 11,700 years ago, probably with the advent of higher population densities due to sedentism. Social anthropologist Douglas P. Fry writes that scholars are divided on the origins of this greater degree of violence—in other words, war-like behavior: 

Jared Diamond in his books "Guns, Germs and Steel" and "The Third Chimpanzee" posits that the rise of large-scale warfare is the result of advances in technology and city-states. For instance, the rise of agriculture provided a significant increase in the number of individuals that a region could sustain over hunter-gatherer societies, allowing for development of specialized classes such as soldiers, or weapons manufacturers.

In academia, the idea of the peaceful pre-history and non-violent tribal societies gained popularity with the post-colonial perspective. The trend, starting in archaeology and spreading to anthropology reached its height in the late half of the 20th century. However, some newer research in archaeology and bioarchaeology may provide evidence that violence within and among groups is not a recent phenomenon. According to the book "The Bioarchaeology of Violence" violence is a behavior that is found throughout human history.

Lawrence H. Keeley at the University of Illinois writes in "War Before Civilization" that 87% of tribal societies were at war more than once per year, and that 65% of them were fighting continuously. He writes that the attrition rate of numerous close-quarter clashes, which characterize endemic warfare, produces casualty rates of up to 60%, compared to 1% of the combatants as is typical in modern warfare. "Primitive Warfare" of these small groups or tribes was driven by the basic need for sustenance and violent competition.

Fry explores Keeley's argument in depth and counters that such sources erroneously focus on the ethnography of hunters and gatherers in the present, whose culture and values have been infiltrated externally by modern civilization, rather than the actual archaeological record spanning some two million years of human existence. Fry determines that all present ethnographically studied tribal societies, "by the very fact of having been described and published by anthropologists, have been irrevocably impacted by history and modern colonial nation states" and that "many have been affected by state societies for at least 5000 years."

Steven Pinker's 2011 book, "The Better Angels of Our Nature", roused both acclaim and controversy by asserting that modern society is less violent than in periods of the past, whether on the short scale of decades or long scale of centuries or millennia.

Steven Pinker argues that by every possible measure, every type of violence has drastically decreased since ancient and medieval times. A few centuries ago, for example, genocide was a standard practice in all kinds of warfare and was so common that historians did not even bother to mention it. According to Pinker, rape, murder, warfare and animal cruelty have all seen drastic declines in the 20th century. However, Pinker's analyses have met with much criticism; for example, Pinker himself, on his FAQ page, states that he does not include catastrophic ecological violence (including violence against wild or domesticated non-human animals or plants, or against ecosystems) or the violence of economic inequality and of coercive working conditions in his definition; he controversially regards these forms of violence as "metaphorical". Some critics have therefore argued that Pinker suffers from "a reductive vision of what it means to be violent."

Beyond deaths and injuries, highly prevalent forms of violence (such as child maltreatment and intimate partner violence) have serious lifelong non-injury health consequences. Victims may engage in high-risk behaviours such as alcohol and substance misuse and smoking, which in turn can contribute to cardiovascular disorders, cancers, depression, diabetes and HIV/AIDS, resulting in premature death. The balances of prevention, mitigation, mediation and exacerbation are complex, and vary with the underpinnings of violence.

In countries with high levels of violence, economic growth can be slowed down, personal and collective security eroded, and social development impeded. Families edging out of poverty and investing in schooling their sons and daughters can be ruined through the violent death or severe disability of the main breadwinner. Communities can be caught in poverty traps where pervasive violence and deprivation form a vicious circle that stifles economic growth. For societies, meeting the direct costs of health, criminal justice, and social welfare responses to violence diverts many billions of dollars from more constructive societal spending. The much larger indirect costs of violence due to lost productivity and lost investment in education work together to slow economic development, increase socioeconomic inequality, and erode human and social capital.

Additionally, communities with high level of violence do not provide the level of stability and predictability vital for a prospering business economy. Individuals will be less likely to invest money and effort towards growth in such unstable and violent conditions. One of the possible proves might be the study of Baten and Gust that used “regicide” as measurement unit to approximate the influence of interpersonal violence and depict the influence of high interpersonal violence on economic development and level of investments. The results of the research prove the correlation of the human capital and the interpersonal violence.

In 2016, the Institute for Economics and Peace, released the Economic Value of Peace report, which estimates the economic impact of violence and conflict on the global economy, the total economic impact of violence on the world economy in 2015 was estimated to be $13.6 trillion in purchasing power parity terms.

Religious and political ideologies have been the cause of interpersonal violence throughout history. Ideologues often falsely accuse others of violence, such as the ancient blood libel against Jews, the medieval accusations of casting witchcraft spells against women, and modern accusations of satanic ritual abuse against day care center owners and others.

Both supporters and opponents of the 21st-century War on Terrorism regard it largely as an ideological and religious war.

Vittorio Bufacchi describes two different modern concepts of violence, one the "minimalist conception" of violence as an intentional act of excessive or destructive force, the other the "comprehensive conception" which includes violations of rights, including a long list of human needs.

Anti-capitalists assert that capitalism is violent. They believe private property and profit survive only because police violence defends them and that capitalist economies need war to expand. They may use the term "structural violence" to describe the systematic ways in which a given social structure or institution kills people slowly by preventing them from meeting their basic needs, for example the deaths caused by diseases because of lack of medicine.

Frantz Fanon critiqued the violence of colonialism and wrote about the counter violence of the "colonized victims."

Throughout history, most religions and individuals like Mahatma Gandhi have preached that humans are capable of eliminating individual violence and organizing societies through purely nonviolent means. Gandhi himself once wrote: "A society organized and run on the basis of complete non-violence would be the purest anarchy." Modern political ideologies which espouse similar views include pacifist varieties of voluntarism, mutualism, anarchism and libertarianism.

Terence Fretheim writing about the Old Testament:
For many people, ... only physical violence truly qualifies as violence. But, certainly, violence is more than killing people, unless one includes all those words and actions that kill people slowly. The effect of limitation to a “killing fields” perspective is the widespread neglect of many other forms of violence. We must insist that violence also refers to that which is psychologically destructive, that which demeans, damages, or depersonalizes others. In view of these considerations, violence may be defined as follows: any action, verbal or nonverbal, oral or written, physical or psychical, active or passive, public or private, individual or institutional/societal, human or divine, in whatever degree of intensity, that abuses, violates, injures, or kills. Some of the most pervasive and most dangerous forms of violence are those that are often hidden from view (against women and children, especially); just beneath the surface in many of our homes, churches, and communities is abuse enough to freeze the
blood. Moreover, many forms of systemic violence often slip past our attention because they are so much a part of the infrastructure of life (e.g., racism, sexism, ageism).




</doc>
<doc id="364338" url="https://en.wikipedia.org/wiki?curid=364338" title="Respect">
Respect

Respect also called esteem is a positive feeling or action shown towards someone or something considered important, or held in high esteem or regard; it conveys a sense of admiration for good or valuable qualities; and it is also the process of honoring someone by exhibiting care, concern, or consideration for their needs or feelings.

Some people may earn the respect of individuals by assisting others or by playing important social roles. In many cultures, individuals are considered to be worthy of respect until they prove otherwise. Being silent is another misinterpreted sign of respect. Often, people think that if someone doesn't talk to them, it means they are giving them attitude, but in reality, the silent one doesn't want to make anyone feel uncomfortable by saying something. Occasionally, people think it is rude to be ignored, but the person being ignored has such an impression, that people don't want to say or do the wrong thing. It is one of the greater acts of respect, because people want to mind their own business and not malevolent a wrong impression. Especially if that person being ignored is a close family member or the household. Courtesies that show respect include simple words and phrases like "thank you" in The West, simple physical signs like a slight bow in the East a smile, or direct eye contact, or a simple handshake.

Respect is shown in many different languages by following certain grammatical conventions, especially in referring to individuals.

An honorific is a word or expression (often a pronoun) that shows respect when used in addressing or referring to a person or animal.

Typically honorifics are used for second and third persons; use for first person is less common. Some languages have anti-honorific first person forms (like "your most humble servant" or "this unworthy person") whose effect is to enhance the relative honor accorded a second or third person.

For instance, it is disrespectful to not use polite language and honorifics when speaking in Japanese with someone having a higher social status. The Japanese honorific "san" can be used when speaking English.

In China it is rude to call someone by their first name unless you have known them for a long period of time. In work-related situations, people address each other by their title. At home people often refer to each other by nicknames or terms of kinship.
In the Chinese culture, individuals often address their friends as juniors and seniors even if they are just a few months younger or older. When a Chinese person asks someone their age they often do this so they know how to address the person.

In Islamic cultures around the world, there are many ways to show respect to people. For example, it is recommended to kiss the hands of parents, grandparents and teachers. Also, it is narrated in the sayings of Muhammad that if a person looks at the faces of parents and teachers with a smile, he will definitely be rewarded by Allah with success and happiness.

In India, it is customary that, out of respect, when a person's foot accidentally touches a book or any written material (considered to be a manifestations of Saraswati, the goddess of knowledge) or another person's leg, it will be followed by an apology in the form of a single hand gesture (Pranāma) with the right hand, where the offending person first touches the object with the finger tips and then the forehead and/or chest. This also counts for money, which is considered to be a manifestation of the goddess of wealth Lakshmi. Pranāma, or the touching of feet in Indian culture is a sign of respect. For instance, when a child is greeting his or her grandparent, they typically will touch their hands to their grandparents' feet. In Indian culture, it is believed that the feet are a source of power and love.

In many African/West Indian descent communities and some non-African//West Indian descent communities, respect can be signified by the touching of fists.

Many gestures or physical acts that are common in the West can be considered disrespectful in Japan. For instance, one should not point directly at someone. When greeting someone or thanking them, it may be insulting if the person of lower status does not bow lower than the person with higher status. The duration and level of the bow depends on many factors such as age and status. Some signs of physical respect apply to women only. If a woman does not wear cosmetics or a brassiere, it is possible that she will be considered unprofessional or others may think she does not care about the situation.

Unlike Japanese culture, it is not necessary in Chinese culture to bow to one another as a greeting or parting gesture. Bowing is generally reserved as a sign of respect for elders and ancestors. When bowing, they place the fist of the right hand in the palm of their left at stomach level. The deeper the bow, the more respect they are showing.

In Chinese culture, there is not much participation in physical contact, especially when doing business because this can be seen as too casual, thus disrespectful. It is considered rude to slap, pat, or put one's arm around the shoulders of another. However, affection in same-sex friendships in East Asia is much more pronounced than in the West. Same-sex friends will often be seen with their arms around one another, holding hands, and other signs of physical affection.

It is uncommon to see very many hand gestures being used in Chinese culture because this is often considered to be excessive.

Traditionally, there was not much hand-shaking in Chinese culture. However, this gesture is now widely practiced among men, especially when greeting Westerners or other foreigners. Many Westerners may find Chinese handshakes to be too long or too weak, but this is because a weaker handshake is a gesture of humility and respect.

Kowtowing, or kneeling and bowing so deeply that one's forehead is touching the floor, is practiced during worship at temples. Kowtowing is a powerful gesture reserved mainly for honoring the dead or offering deep respect at a temple.

Many codes of behavior revolve around young people showing respect to older people. Like in many cultures, younger Chinese individuals are expected to defer to older people, let them speak first, sit down after them and not contradict them. Sometimes when an older person enters a room, everyone stands. People are often introduced from oldest to youngest. Often time, younger people will go out of their way to open doors for their elders and not cross their legs in front of them. The older you are the more respect you are expected to be treated with.

In many indigenous American societies, respect serves as an important concept valued in indigenous American culture. In addition to esteem or deference, respect is viewed as a moral value that teaches indigenous individuals about their culture. This moral value is treated as a process that influences participation in the community and also helps individuals develop and become integrated into their culture's community. The value of respect is taught during childhood because the process of indigenous children participating in and learning about their community is an important aspect of the culture.

Respect as a form of behavior and participation is especially important in childhood as it serves as a basis of how children must conduct themselves in their community. Children engage in mature activities such as cooking for the family, cleaning and sweeping the house, caring for infant peers, and crop work. Indigenous children learn to view their participation in these activities as a representation of respect. Through this manner of participation in activities of respect, children not only learn about culture but also practice it as well.





</doc>
<doc id="15179951" url="https://en.wikipedia.org/wiki?curid=15179951" title="Human sexuality">
Human sexuality

Human sexuality is the way people experience and express themselves sexually. This involves biological, erotic, physical, emotional, social, or spiritual feelings and behaviors. Because it is a broad term, which has varied over time, it lacks a precise definition. The biological and physical aspects of sexuality largely concern the human reproductive functions, including the human sexual response cycle. Someone's sexual orientation can influence that person's sexual interest and attraction for another person. Physical and emotional aspects of sexuality include bonds between individuals that are expressed through profound feelings or physical manifestations of love, trust, and care. Social aspects deal with the effects of human society on one's sexuality, while spirituality concerns an individual's spiritual connection with others. Sexuality also affects and is affected by cultural, political, legal, philosophical, moral, ethical, and religious aspects of life.

Interest in sexual activity typically increases when an individual reaches puberty. Opinions differ on the origins of an individual's sexual orientation and sexual behavior. Some argue that sexuality is determined by genetics, while others believe it is molded by the environment, or that both of these factors interact to form the individual's sexual orientation. This pertains to the nature versus nurture debate. In the former, one assumes that the features of a person innately correspond to their natural inheritance, exemplified by drives and instincts; the latter refers to the assumption that the features of a person continue to change throughout their development and nurturing, exemplified by ego ideals and formative identifications.

Evolutionary perspectives on human coupling, reproduction and reproduction strategies, and social learning theory provide further views of sexuality. Socio-cultural aspects of sexuality include historical developments and religious beliefs. Examples of these include Jewish views on sexual pleasure within marriage and some views of other religions on avoidance of sexual pleasures. Some cultures have been described as sexually repressive. The study of sexuality also includes human identity within social groups, sexually transmitted infections (STIs/STDs), and birth control methods.

Certain characteristics may be innate in humans; these characteristics may be modified by the physical and social environment in which people interact. Human sexuality is driven by genetics and mental activity. The sexual drive affects the development of personal identity and social activities. An individual's normative, social, cultural, educational, and environmental characteristics moderate the sexual drive. Two well-known schools in psychology took opposing positions in the nature-versus-nurture debate: the Psychoanalytic school led by Sigmund Freud and the Behaviorist school which traces its origins to John Locke.

Freud believed sexual drives are instinctive. He was a firm supporter of the nature argument; he said there are a large number of instincts but they are reduced into two broad groups: Eros (the life instinct), which comprises the self-preserving and erotic instincts, and Thanatos (the death instinct), which comprises instincts invoking aggression, self-destruction, and cruelty. He gave sexual drives a centrality in human life, actions, and behaviors that had not been accepted before his proposal. His instinct theory said humans are driven from birth by the desire to acquire and enhance bodily pleasures, thus supporting the nature debate. Freud redefined the term "sexuality" to make it cover any form of pleasure that can be derived from the human body. He also said pleasure lowers tension while displeasure raises it, influencing the sexual drive in humans. His developmentalist perspective was governed by inner forces, especially biological drives and maturation, and his view that humans are biologically inclined to seek sexual gratification demonstrates the nature side of the debate.
The nurture debate traces back to John Locke and his theory of the mind as a "tabula rasa" or blank slate. Later, behaviorists would apply this notion in support of the idea that the environment is where one develops one's sexual drives.

Psychological theories exist regarding the development and expression of gender differences in human sexuality. A number of them, including neo-analytic theories, sociobiological theories, social learning theory, social role theory, and script theory, agree in predicting that men should be more approving of casual sex (sex happening outside a stable, committed relationship such as marriage) and should also be more promiscuous (have a higher number of sexual partners) than women. These theories are mostly consistent with observed differences in males' and females' attitudes toward casual sex before marriage in the United States; other aspects of human sexuality, such as sexual satisfaction, incidence of oral sex, and attitudes toward homosexuality and masturbation, show little to no observed difference between males and females. Observed gender differences regarding the number of sexual partners are modest, with males tending to have slightly more than females.

Like other mammals, humans are primarily grouped into either the male or female sex, with a small proportion (around 1%) of intersex individuals, for whom sexual classification may not be as clear. The biological aspects of humans' sexuality deal with the reproductive system, the sexual response cycle, and the factors that affect these aspects. They also deal with the influence of biological factors on other aspects of sexuality, such as organic and neurological responses, heredity, hormonal issues, gender issues, and sexual dysfunction.

Males and females are anatomically similar; this extends to some degree to the development of the reproductive system. As adults, they have different reproductive mechanisms that enable them to perform sexual acts and to reproduce. Men and women react to sexual stimuli in a similar fashion with minor differences. Women have a monthly reproductive cycle, whereas the male sperm production cycle is more continuous.

The hypothalamus is the most important part of the brain for sexual functioning. This is a small area at the base of the brain consisting of several groups of nerve cell bodies that receives input from the limbic system. Studies have shown that within lab animals, destruction of certain areas of the hypothalamus causes the elimination of sexual behavior. The hypothalamus is important because of its relationship to the pituitary gland, which lies beneath it. The pituitary gland secretes hormones that are produced in the hypothalamus and itself. The four important sexual hormones are oxytocin, prolactin, follicle-stimulating hormone, and luteinizing hormone. Oxytocin, sometimes referred to as the "love hormone," is released in both sexes during sexual intercourse when an orgasm is achieved. Oxytocin has been suggested as critical to the thoughts and behaviors required to maintain close relationships. The hormone is also released in women when they give birth or are breastfeeding. Both prolactin and oxytocin stimulate milk production in women. Follicle-stimulating hormone (FSH) is responsible for ovulation in women, which acts by triggering egg maturity; in men it stimulates sperm production. Luteinizing hormone (LH) triggers ovulation, which is the release of a mature egg.

Males also have both internal and external genitalia that are responsible for procreation and sexual intercourse. Production of spermatozoa (sperm) is also cyclic, but unlike the female ovulation cycle, the sperm production cycle is constantly producing millions of sperm daily.

The male genitalia are the penis and the scrotum. The penis provides a passageway for sperm and urine. An average-sized flaccid penis is about in length and in diameter. When erect, the average penis is between to in length and in diameter. The penis's internal structures consist of the shaft, glans, and the root.

The shaft of the penis consists of three cylindrical bodies of spongy tissue filled with blood vessels along its length. Two of these bodies lie side-by-side in the upper portion of the penis called corpora cavernosa. The third, called the corpus spongiosum, is a tube that lies centrally beneath the others and expands at the end to form the tip of the penis (glans).

The raised rim at the border of the shaft and glans is called the corona. The urethra runs through the shaft, providing an exit for sperm and urine. The root consists of the expanded ends of the cavernous bodies, which fan out to form the crura and attach to the pubic bone and the expanded end of the spongy body (bulb). The root is surrounded by two muscles; the bulbocavernosus muscle and the ischiocavernosus muscle, which aid urination and ejaculation. The penis has a foreskin that typically covers the glans; this is sometimes removed by circumcision for medical, religious or cultural reasons. In the scrotum, the testicles are held away from the body, one possible reason for this is so sperm can be produced in an environment slightly lower than normal body temperature.

Male internal reproductive structures are the testicles, the duct system, the prostate and seminal vesicles, and the Cowper's gland.

The testicles are the male gonads where sperm and male hormones are produced. Millions of sperm are produced daily in several hundred seminiferous tubules. Cells called the Leydig cells lie between the tubules; these produce hormones called androgens; these consist of testosterone and inhibin. The testicles are held by the spermatic cord, which is a tubelike structure containing blood vessels, nerves, the vas deferens, and a muscle that helps to raise and lower the testicles in response to temperature changes and sexual arousal, in which the testicles are drawn closer to the body.

Sperm are transported through a four-part duct system. The first part of this system is the epididymis. The testicles converge to form the seminiferous tubules, coiled tubes at the top and back of each testicle. The second part of the duct system is the vas deferens, a muscular tube that begins at the lower end of the epididymis. The vas deferens passes upward along the side of the testicles to become part of the spermatic cord. The expanded end is the ampulla, which stores sperm before ejaculation. The third part of the duct system is the ejaculatory ducts, which are -long paired tubes that pass through the prostate gland, where semen is produced. The prostate gland is a solid, chestnut-shaped organ that surrounds the first part of the urethra, which carries urine and semen. Similar to the female G-spot, the prostate provides sexual stimulation and can lead to orgasm through anal sex.

The prostate gland and the seminal vesicles produce seminal fluid that is mixed with sperm to create semen. The prostate gland lies under the bladder and in front of the rectum. It consists of two main zones: the inner zone that produces secretions to keep the lining of the male urethra moist and the outer zone that produces seminal fluids to facilitate the passage of semen. The seminal vesicles secrete fructose for sperm activation and mobilization, prostaglandins to cause uterine contractions that aid movement through the uterus, and bases that help neutralize the acidity of the vagina. The Cowper's glands, or bulbourethral glands, are two pea sized structures beneath the prostate.

The mons veneris, also known as the Mound of Venus, is a soft layer of fatty tissue overlaying the pubic bone. Following puberty, this area grows in size. It has many nerve endings and is sensitive to stimulation.

The labia minora and labia majora are collectively known as the lips. The labia majora are two elongated folds of skin extending from the mons to the perineum. Its outer surface becomes covered with hair after puberty. In between the labia majora are the labia minora, two hairless folds of skin that meet above the clitoris to form the clitoral hood, which is highly sensitive to touch. The labia minora become engorged with blood during sexual stimulation, causing them to swell and turn red. The labia minora are composed of connective tissues that are richly supplied with blood vessels which cause the pinkish appearance. Near the anus, the labia minora merge with the labia majora. In a sexually unstimulated state, the labia minora protects the vaginal and urethral opening by covering them. At the base of the labia minora are the Bartholin's glands, which add a few drops of an alkaline fluid to the vagina via ducts; this fluid helps to counteract the acidity of the outer vagina since sperm cannot live in an acidic environment.

The clitoris is developed from the same embryonic tissue as the penis; it or its glans alone consists of as many (or more in some cases) nerve endings as the human penis or glans penis, making it extremely sensitive to touch. The clitoral glans, which is a small, elongated erectile structure, has only one known function—sexual sensations. It is the main source of orgasm in women. Thick secretions called smegma collect in the clitoris.

The vaginal opening and the urethral opening are only visible when the labia minora are parted. These opening have many nerve endings that make them sensitive to touch. They are surrounded by a ring of sphincter muscles called the bulbocavernosus muscle. Underneath this muscle and on opposite sides of the vaginal opening are the vestibular bulbs, which help the vagina grip the penis by swelling with blood during arousal. Within the vaginal opening is the hymen, a thin membrane that partially covers the opening in many virgins. Rupture of the hymen has been historically considered the loss of one's virginity, though by modern standards, loss of virginity is considered to be the first sexual intercourse. The hymen can be ruptured by activities other than sexual intercourse. The urethral opening connects to the bladder with the urethra; it expels urine from the bladder. This is located below the clitoris and above the vaginal opening.

The breasts are external organs used for sexual pleasure in some cultures. Western culture is one of the few in which they are considered erotic. The breasts are the subcutaneous tissues on the front thorax of the female body. Breasts are modified sweat glands made up of fibrous tissues and fat that provide support and contain nerves, blood vessels and lymphatic vessels. Their purpose is to provide milk to a developing infant. Breasts develop during puberty in response to an increase in estrogen. Each adult breast consists of 15 to 20 milk-producing mammary glands, irregularly shaped lobes that include alveolar glands and a lactiferous duct leading to the nipple. The lobes are separated by dense connective tissues that support the glands and attach them to the tissues on the underlying pectoral muscles. Other connective tissue, which forms dense strands called suspensory ligaments, extends inward from the skin of the breast to the pectoral tissue to support the weight of the breast. Heredity and the quantity of fatty tissue determine the size of the breasts.

The female internal reproductive organs are the vagina, uterus, Fallopian tubes, and ovaries. The vagina is a sheath-like canal that extends from the vulva to the cervix. It receives the penis during intercourse and serves as a depository for sperm. The vagina is also the birth canal; it can expand to during labor and delivery. The vagina is located between the bladder and the rectum. The vagina is normally collapsed, but during sexual arousal it opens, lengthens, and produces lubrication to allow the insertion of the penis. The vagina has three layered walls; it is a self-cleaning organ with natural bacteria that suppress the production of yeast. The G-spot, named after the Ernst Gräfenberg who first reported it in 1950, may be located in the front wall of the vagina and may cause orgasms. This area may vary in size and location between women; in some it may be absent. Various researchers dispute its structure or existence, or regard it as an extension of the clitoris.

The uterus or womb is a hollow, muscular organ where a fertilized egg (ovum) will implant itself and grow into a fetus. The uterus lies in the pelvic cavity between the bladder and the bowel, and above the vagina. It is usually positioned in a 90-degree angle tilting forward, although in about 20% of women it tilts backwards. The uterus has three layers; the innermost layer is the endometrium, where the egg is implanted. During ovulation, this thickens for implantation. If implantation does not occur, it is sloughed off during menstruation. The cervix is the narrow end of the uterus. The broad part of the uterus is the fundus.

During ovulation, the ovum travels down the Fallopian tubes to the uterus. These extend about from both sides of the uterus. Finger-like projections at the ends of the tubes brush the ovaries and receive the ovum once it is released. The ovum then travels for three to four days to the uterus. After sexual intercourse, sperm swim up this funnel from the uterus. The lining of the tube and its secretions sustain the egg and the sperm, encouraging fertilization and nourishing the ovum until it reaches the uterus. If the ovum divides after fertilization, identical twins are produced. If separate eggs are fertilized by different sperm, the mother gives birth to non-identical or fraternal twins.

The ovaries are the female gonads; they develop from the same embryonic tissue as the testicles. The ovaries are suspended by ligaments and are the source where ova are stored and developed before ovulation. The ovaries also produce female hormones progesterone and estrogen. Within the ovaries, each ovum is surrounded by other cells and contained within a capsule called a primary follicle. At puberty, one or more of these follicles are stimulated to mature on a monthly basis. Once matured, these are called Graafian follicles. The female reproductive system does not produce the ova; about 60,000 ova are present at birth, only 400 of which will mature during the woman's lifetime.

Ovulation is based on a monthly cycle; the 14th day is the most fertile. On days one to four, menstruation and production of estrogen and progesterone decreases, and the endometrium starts thinning. The endometrium is sloughed off for the next three to six days. Once menstruation ends, the cycle begins again with an FSH surge from the pituitary gland. Days five to thirteen are known as the pre-ovulatory stage. During this stage, the pituitary gland secretes follicle-stimulating hormone (FSH). A negative feedback loop is enacted when estrogen is secreted to inhibit the release of FSH. Estrogen thickens the endometrium of the uterus. A surge of Luteinizing Hormone (LH) triggers ovulation. On day 14, the LH surge causes a Graafian follicle to surface the ovary. The follicle ruptures and the ripe ovum is expelled into the abdominal cavity. The fallopian tubes pick up the ovum with the fimbria. The cervical mucus changes to aid the movement of sperm. On days 15 to 28—the post-ovulatory stage, the Graafian follicle—now called the corpus luteum—secretes estrogen. Production of progesterone increases, inhibiting LH release. The endometrium thickens to prepare for implantation, and the ovum travels down the Fallopian tubes to the uterus. If the ovum is not fertilized and does not implant, menstruation begins.

The sexual response cycle is a model that describes the physiological responses that occur during sexual activity. This model was created by William Masters and Virginia Johnson. According to Masters and Johnson, the human sexual response cycle consists of four phases; excitement, plateau, orgasm, and resolution, also called the EPOR model. During the excitement phase of the EPOR model, one attains the intrinsic motivation to have sex. The plateau phase is the precursor to orgasm, which may be mostly biological for men and mostly psychological for women. Orgasm is the release of tension, and the resolution period is the unaroused state before the cycle begins again.

The male sexual response cycle starts in the excitement phase; two centers in the spine are responsible for erections. Vasoconstriction in the penis begins, the heart rate increases, the scrotum thickens, the spermatic cord shortens, and the testicles become engorged with blood. In the plateau phase, the penis increases in diameter, the testicles become more engorged, and the Cowper's glands secrete pre-seminal fluid. The orgasm phase, during which rhythmic contractions occur every 0.8 seconds, consists of two phases; the emission phase, in which contractions of the vas deferens, prostate, and seminal vesicles encourage ejaculation, which is the second phase of orgasm. Ejaculation is called the expulsion phase; it cannot be reached without an orgasm. In the resolution phase, the male is now in an unaroused state consisting of a refactory (rest) period before the cycle can begin. This rest period may increase with age.

The female sexual response begins with the excitement phase, which can last from several minutes to several hours. Characteristics of this phase include increased heart and respiratory rate, and an elevation of blood pressure. Flushed skin or blotches of redness may occur on the chest and back; breasts increase slightly in size and nipples may become hardened and erect. The onset of vasocongestion results in swelling of the clitoris, labia minora, and vagina. The muscle that surrounds the vaginal opening tightens and the uterus elevates and grows in size. The vaginal walls begin to produce a lubricating liquid. The second phase, called the plateau phase, is characterized primarily by the intensification of the changes begun during the excitement phase. The plateau phase extends to the brink of orgasm, which initiates the resolution stage; the reversal of the changes begun during the excitement phase. During the orgasm stage the heart rate, blood pressure, muscle tension, and breathing rates peak. The pelvic muscle near the vagina, the anal sphincter, and the uterus contract. Muscle contractions in the vaginal area create a high level of pleasure, though all orgasms are centered in the clitoris.

From rodent to human, the corticalization of the brain induces several changes in the control of sexual behavior, including lordosis behavior. These changes induce a "difference between the stereotyped sexual behaviors in non-human mammals and the astounding variety of human sexual behaviors".

Sexual reflexes, such as the motor reflex of lordosis, become secondary. In particular, lordosis behavior, which is a motor reflex complex and essential to carry out copulation in non-primate mammals (rodents, canines, bovids ...), is apparently no longer functional in women. Sexual stimuli on women do not trigger any more neither immobilization nor the reflex position of lordosis. On the level of olfactory systems, the vomeronasal organ is altered in hominids and 90% of the pheromone receptor genes become pseudogenes in humans. Concerning hormonal control, sexual activities are gradually dissociated from hormonal cycles. Humans can have sex anytime during the year and hormonal cycles. On the contrary, the importance of rewards / reinforcements and cognition became major. Especially in humans, the extensive development of the neocortex allows the emergence of culture, which has a major influence on behavior. For all these reasons, the dynamics of sexual behavior was modified.
In human beings, sexuality is multifactorial, with several factors that interact (genes, hormones, conditioning, sexual preferences, emotions, cognitive processes, cultural context). The relative importance of each of these factors is dependent both on individual physiological characteristics, personal experience and aspects of the sociocultural environment.

Sexual disorders, according to the DSM-IV-TR, are disturbances in sexual desire and psycho-physiological changes that characterize the sexual response cycle and cause marked distress and interpersonal difficulty. The sexual dysfunctions is a result of physical or psychological disorders. The physical causes include hormonal imbalance, diabetes, heart disease and more. The psychological causes includes but are not limited to stress, anxiety, and depression. The sexual dysfunction affects men and women. There are four major categories of sexual problems for women: desire disorders, arousal disorders, orgasmic disorders, and sexual pain disorders. The sexual desire disorder occurs when an individual lacks the sexual desire because of hormonal changes, depression, and pregnancy.The arousal disorder is a female sexual dysfunction. Arousal disorder means lack of vaginal lubrication. In addition, blood flow problems may affect arousal disorder. Lack of orgasm, also known as, anorgasmia is another sexual dysfunction in women. The anorgasmia occurs in women with psychological disorders such as guilt and anxiety that was caused by sexual assault. The last sexual disorder is the painful intercourse. The sexual disorder can be result of pelvic mass, scar tissue, sexually transmitted disease and more.
There are also three common sexual disorders for men including, sexual desire, ejaculation disorder, and erectile dysfunction. The lack of sexual desire in men is because of loss of libido, low testosterone. There are also psychological factors such as anxiety, and depression.
The ejaculation disorder has three types: retrograde ejaculation, retarded ejaculation, premature ejaculation. The erectile dysfunction is a disability to have and maintain an erection during intercourse.

Sexuality in humans generates profound emotional and psychological responses. Some theorists identify sexuality as the central source of human personality. Psychological studies of sexuality focus on psychological influences that affect sexual behavior and experiences. Early psychological analyses were carried out by Sigmund Freud, who believed in a psychoanalytic approach. He also proposed the concepts of psychosexual development and the Oedipus complex, among other theories.

Gender identity is a person's sense of self-identification as female, male, both, neither, or somewhere in between. The social construction of gender has been discussed by many scholars, including Judith Butler. More recent research has focused upon the influence of feminist theory and courtship.

Sexual behavior and intimate relationships are strongly influenced by a person's sexual orientation. Sexual orientation is an enduring pattern of romantic or sexual attraction (or a combination of these) to persons of the opposite sex, same sex, or both sexes. Heterosexual people are romantically/sexually attracted to the members of the opposite sex, gay and lesbian people are romantically/sexually attracted to people of the same sex, and those who are bisexual are romantically/sexually attracted to both sexes.

Before the High Middle Ages, homosexual acts appear to have been ignored or tolerated by the Christian church. During the 12th century, hostility toward homosexuality began to spread throughout religious and secular institutions. By the end of the 19th century, it was viewed as a pathology. Havelock Ellis and Sigmund Freud adopted more accepting stances; Ellis said homosexuality was inborn and therefore not immoral, not a disease, and that many homosexuals made significant contributions to society. Freud wrote that all human beings as capable of becoming either heterosexual or homosexual; neither orientation was assumed to be innate. According to Freud, a person's orientation depended on the resolution of the Oedipus complex. He said male homosexuality resulted when a young boy had an authoritarian, rejecting mother and turned to his father for love and affection, and later to men in general. He said female homosexuality developed when a girl loved her mother and identified with her father, and became fixated at that stage.

Freud and Ellis said homosexuality resulted from reversed gender roles. In the early 21st century, this view is reinforced by the media's portrayal of male homosexuals as effeminate and female homosexuals as masculine. A person's conformity or non-conformity to gender stereotypes does not always predict sexual orientation. Society believes that if a man is masculine he is heterosexual, and if a man is feminine he is homosexual. There is no strong evidence that a homosexual or bisexual orientation must be associated with atypical gender roles. By the early 21st century, homosexuality was no longer considered to be a pathology. Theories have linked many factors, including genetic, anatomical, birth order, and hormones in the prenatal environment, to homosexuality.

Other than the need to procreate, there are many other reasons people have sex. According to one study conducted on college students (Meston & Buss, 2007), the four main reasons for sexual activities are; physical attraction, as a means to an end, to increase emotional connection, and to alleviate insecurity.

In the past, children were often assumed not to have sexuality until later development. Sigmund Freud was one of the first researchers to take child sexuality seriously. His ideas, such as psychosexual development and the Oedipus conflict, have been much debated but acknowledging the existence of child sexuality was an important development. Freud gave sexual drives an importance and centrality in human life, actions, and behavior; he said sexual drives exist and can be discerned in children from birth. He explains this in his theory of infantile sexuality, and says sexual energy (libido) is the most important motivating force in adult life. Freud wrote about the importance of interpersonal relationships to one's sexual and emotional development. From birth, the mother's connection to the infant affects the infant's later capacity for pleasure and attachment. Freud described two currents of emotional life; an affectionate current, including our bonds with the important people in our lives; and a sensual current, including our wish to gratify sexual impulses. During adolescence, a young person tries to integrate these two emotional currents.

Alfred Kinsey also examined child sexuality in his Kinsey Reports. Children are naturally curious about their bodies and sexual functions. For example, they wonder where babies come from, they notice the differences between males and females, and many engage in genital play, which is often mistaken for masturbation. Child sex play, also known as playing doctor, includes exhibiting or inspecting the genitals. Many children take part in some sex play, typically with siblings or friends. Sex play with others usually decreases as children grow, but they may later possess romantic interest in their peers. Curiosity levels remain high during these years, but the main surge in sexual interest occurs in adolescence.

Adult sexuality originates in childhood. However, like many other human capacities, sexuality is not fixed, but matures and develops. A common stereotype associated with old people is that they tend to lose interest and the ability to engage in sexual acts once they reach late adulthood. This misconception is reinforced by Western popular culture, which often ridicules older adults who try to engage in sexual activities. Age does not necessarily change the need or desire to be sexually expressive or active. A couple in a long-term relationship may find that the frequency of their sexual activity decreases over time and the type of sexual expression may change, but many couples experience increased intimacy and love.

Human sexuality can be understood as part of the social life of humans, which is governed by implied rules of behavior and the status quo. This narrows the view to groups within a society. The socio-cultural context of society, including the effects of politics and the mass media, influences and forms social norms. Before the early 21st century, people fought for their civil rights. The civil rights movements helped to bring about massive changes in social norms; examples include the sexual revolution and the rise of feminism.

The link between constructed sexual meanings and racial ideologies has been studied. Sexual meanings are constructed to maintain racial-ethnic-national boundaries by denigration of "others" and regulation of sexual behavior within the group. According to Joane Nagel, "both adherence to and deviation from such approved behaviors, define and reinforce racial, ethnic, and nationalist regimes". Scholars also study the ways in which colonialism has affected sexuality today and argue that due to racism and slavery it has been dramatically changed from the way it had previously been understood. These changes to sexuality are argued to be largely effected by the enforcement of the gender binary and heteropatriarchy as tools of colonization on colonized communities as seen in nations such as India, Samoa, and the First Nations in the Americas, resulting in the deaths and erasure of non-western genders and sexualities. In the United States people of color face the effects of colonialism in different ways with stereotypes such as the Mammy, and Jezebel for Black women; lotus blossom, and dragon lady for Asian women; and the "spicy" Latina.

The age and manner in which children are informed of issues of sexuality is a matter of sex education. The school systems in almost all developed countries have some form of sex education, but the nature of the issues covered varies widely. In some countries, such as Australia and much of Europe, age-appropriate sex education often begins in pre-school, whereas other countries leave sex education to the pre-teenage and teenage years. Sex education covers a range of topics, including the physical, mental, and social aspects of sexual behavior. Geographic location also plays a role in society's opinion of the appropriate age for children to learn about sexuality. According to "TIME" magazine and CNN, 74% of teenagers in the United States reported that their major sources of sexual information were their peers and the media, compared to 10% who named their parents or a sex education course.

In some religions, sexual behavior is regarded as primarily spiritual. In others it is treated as primarily physical. Some hold that sexual behavior is only spiritual within certain kinds of relationships, when used for specific purposes, or when incorporated into religious ritual. In some religions there are no distinctions between the physical and the spiritual, whereas some religions view human sexuality as a way of completing the gap that exists between the spiritual and the physical.

Many religious conservatives, especially those of Abrahamic religions and Christianity in particular, tend to view sexuality in terms of behavior ("i.e." homosexuality or heterosexuality is what someone does) and certain sexualities such as bisexuality tend to be ignored as a result of this. These conservatives tend to promote celibacy for gay people, and may also tend to believe that sexuality can be changed through conversion therapy or prayer to become an ex-gay. They may also see homosexuality as a form of mental illness, something that ought to be criminalised, an immoral abomination, caused by ineffective parenting, and view same-sex marriage as a threat to society.

On the other hand, most religious liberals define sexuality-related labels in terms of sexual attraction and self-identification. They may also view same-sex activity as morally neutral and as legally acceptable as opposite-sex activity, unrelated to mental illness, genetically or environmentally caused (but not as the result of bad parenting), and fixed. They also tend to be more in favor of same-sex marriage.

According to Judaism, sex between man and woman within marriage is sacred and should be enjoyed; celibacy is considered sinful.

The Roman Catholic Church teaches that sexuality is "noble and worthy" but that it must be used in accordance with natural law. For this reason, all sexual activity must occur in the context of a marriage between a man and a woman, and must not be divorced from the possibility of conception. Most forms of sex without the possibility of conception are considered intrinsically disordered and sinful, such as the use of contraceptives, masturbation, and homosexual acts.

In Islam, sexual desire is considered to be a natural urge that should not be suppressed, although the concept of free sex is not accepted; these urges should be fulfilled responsibly. Marriage is considered to be a good deed; it does not hinder spiritual wayfaring. The term used for marriage within the Quran is "nikah", which literally means sexual intercourse. Although Islamic sexuality is restrained via Islamic sexual jurisprudence, it emphasizes sexual pleasure within marriage. It is acceptable for a man to have more than one wife, but he must take care of those wives physically, mentally, emotionally, financially, and spiritually. Muslims believe that sexual intercourse is an act of worship that fulfils emotional and physical needs, and that producing children is one way in which humans can contribute to God's creation, and Islam discourages celibacy once an individual is married. However, homosexuality is strictly forbidden in Islam, and some Muslim lawyers have suggested that gay people should be put to death. On the other hand, some have argued that Islam has an open and playful approach to sex so long as it is within marriage, free of lewdness, fornication and adultery. For many Muslims, sex with reference to the Quran indicates that—bar anal intercourse and adultery—a Muslim marital home bonded by "Nikah" marital contract between husband and his wife(s) should enjoy and even indulge, within the privacy of their marital home, in limitless scope of heterosexual sexual acts within a monogamous or polygamous marriage.

Hinduism emphasizes that sex is only appropriate between husband and wife, in which satisfying sexual urges through sexual pleasure is an important duty of marriage. Any sex before marriage is considered to interfere with intellectual development, especially between birth and the age of 25, which is said to be brahmacharya and this should be avoided. Kama (sensual pleasures) is one of the four purusharthas or aims of life (dharma, artha, kama, and moksha). The Hindu "Kama Sutra" deals partially with sexual intercourse; it is not exclusively a sexual or religious work.

Sikhism views chastity as important, as Sikhs believe that the divine spark of "Waheguru" is present inside every individual's body, therefore it is important for one to keep clean and pure. Sexual activity is limited to married couples, and extramarital sex is forbidden. Marriage is seen as a commitment to "Waheguru" and should be viewed as part of spiritual companionship, rather than just sexual intercourse, and monogamy is deeply emphasised in Sikhism. Any other way of living is discouraged, including celibacy and homosexuality. However, in comparison to other religions, the issue of sexuality in Sikhism is not considered one of paramount importance.

Sexuality has been an important, vital part of human existence throughout history. All civilizations have managed sexuality through sexual standards, representations, and behavior.

Before the rise of agriculture, groups of hunter/gatherers (H/G) and nomads inhabited the world. Within these groups, some implications of male dominance existed, but there were signs that women were active participants in sexuality, with bargaining power of their own. These hunter/gatherers had less restrictive sexual standards that emphasized sexual pleasure and enjoyment, but with definite rules and constraints. Some underlying continuities or key regulatory standards contended with the tension between recognition of pleasure, interest, and the need to procreate for the sake of social order and economic survival. H/G groups also placed high value on certain types of sexual symbolism. Two common tensions in H/G societies are expressed in their art, which emphasizes male sexuality and prowess, with equally common tendencies to blur gender lines in sexual matters. One example of these male-dominated portrayals is the Egyptian creation myth, in which the sun god Atum masturbates in the water, creating the Nile River. In Sumerian myth, the Gods' semen filled the Tigris.

Once agricultural societies emerged, the sexual framework shifted in ways that persisted for many millennia in much of Asia, Africa, Europe, and parts of the Americas. One common characteristic new to these societies was the collective supervision of sexual behavior due to urbanization, and the growth of population and population density. Children would commonly witness parents having sex because many families shared the same sleeping quarters. Due to landownership, determination of children's paternity became important, and society and family life became patriarchal. These changes in sexual ideology were used to control female sexuality and to differentiate standards by gender. With these ideologies, sexual possessiveness and increases in jealousy emerged. With the domestication of animals, new opportunities for bestiality arose. Males mostly performed these types of sexual acts and many societies acquired firm rules against it. These acts also explain the many depictions of half-human, half-animal mythical creatures, and the sports of gods and goddesses with animals. While retaining the precedents of earlier civilizations, each classical civilization established a somewhat distinctive approach to gender, artistic expression of sexual beauty, and to behaviors such as homosexuality. Some of these distinctions are portrayed in sex manuals, which were also common among civilizations in China, Greece, Rome, Persia, and India; each has its own sexual history.

During the beginning of the industrial revolution of the 18th and 19th centuries, many changes in sexual standards occurred. New, dramatic, artificial birth control devices such as the condom and diaphragm were introduced. Doctors started claiming a new role in sexual matters, urging that their advice was crucial to sexual morality and health. New pornographic industries grew and Japan adopted its first laws against homosexuality. In western societies, the definition of homosexuality was constantly changing; western influence on other cultures became more prevalent. New contacts created serious issues around sexuality and sexual traditions. There were also major shifts in sexual behavior. During this period, puberty began occurring at younger ages, so a new focus on adolescence as a time of sexual confusion and danger emerged. There was a new focus on the purpose of marriage; it was increasing regarded as being for love rather than only for economics and reproduction.

Alfred Kinsey initiated the modern era of sex research. He collected data from questionnaires given to his students at Indiana University, but then switched to personal interviews about sexual behaviors. Kinsey and his colleagues sampled 5,300 men and 5,940 women. He found that most people masturbated, that many engaged in oral sex, that women are capable of having multiple orgasms, and that many men had had some type of homosexual experience in their lifetimes. Many believe he was the major influence in changing 20th century attitudes about sex. Kinsey Institute for Research in Sex, Gender, and Reproduction at Indiana University continues to be a major center for the study of human sexuality. Before William Masters, a physician, and Virginia Johnson, a behavioral scientist, the study of anatomy and physiological studies of sex was still limited to experiments with laboratory animals. Masters and Johnson started to directly observe and record the physical responses in humans that are engaged in sexual activity under laboratory settings. They observed 10,000 episodes of sexual acts between 312 men and 382 women. This led to methods of treating clinical problems and abnormalities. Masters and Johnson opened the first sex therapy clinic in 1965. In 1970, they described their therapeutic techniques in their book, "Human Sexual Inadequacy".

Reproductive and sexual rights encompass the concept of applying human rights to issues related to reproduction and sexuality. This concept is a modern one, and remains controversial, especially outside the West, since it deals, directly and indirectly, with issues such as contraception, LGBT rights, abortion, sex education, freedom to choose a partner, freedom to decide whether to be sexually active or not, right to bodily integrity, freedom to decide whether or not, and when, to have children. According to the Swedish government, "sexual rights include the right of all people to decide over their own bodies and sexuality" and "reproductive rights comprise the right of individuals to decide on the number of children they have and the intervals at which they are born." Such rights are not accepted in all cultures, with practices such criminalization of consensual sexual activities (such as those related to homosexual acts and sexual acts outside marriage), acceptance of forced marriage and child marriage, failure to criminalize all non-consensual sexual encounters (such as marital rape), female genital mutilation, or restricted availability of contraception, being common around the world.

In humans, sexual intercourse and sexual activity in general have been shown to have health benefits, such as an improved sense of smell, stress and blood pressure reduction, increased immunity, and decreased risk of prostate cancer. Sexual intimacy and orgasms increase levels of oxytocin, which helps people bond and build trust. A long-term study of 3,500 people between ages 30 and 101 by clinical neuropsychologist David Weeks, MD, head of old-age psychology at the Royal Edinburgh Hospital in Scotland, said he found that "sex helps you look between four and seven years younger", according to impartial ratings of the subjects' photographs. Exclusive causation, however, is unclear, and the benefits may be indirectly related to sex and directly related to significant reductions in stress, greater contentment, and better sleep that sex promotes.

Sexual intercourse can also be a disease vector. There are 19 million new cases of sexually transmitted diseases (STD) every year in the U.S., and worldwide there are over 340 million STD infections each year. More than half of these occur in adolescents and young adults aged 15–24 years. At least one in four U.S. teenage girls has a sexually transmitted disease. In the U.S., about 30% of 15- to 17-year-olds have had sexual intercourse, but only about 80% of 15- to 19-year-olds report using condoms for their first sexual intercourse. In one study, more than 75% of young women age 18–25 years felt they were at low risk of acquiring an STD.

People both consciously and subconsciously seek to attract others with whom they can form deep relationships. This may be for companionship, procreation, or an intimate relationship. This involves interactive processes whereby people find and attract potential partners and maintain a relationship. These processes, which involve attracting one or more partners and maintaining sexual interest, can include:

Sexual attraction is attraction on the basis of sexual desire or the quality of arousing such interest. Sexual attractiveness or sex appeal is an individual's ability to attract the sexual or erotic interest of another person, and is a factor in sexual selection or mate choice. The attraction can be to the physical or other qualities or traits of a person, or to such qualities in the context in which they appear. The attraction may be to a person's aesthetics or movements or to their voice or smell, besides other factors. The attraction may be enhanced by a person's adornments, clothing, perfume, hair length and style, and anything else which can attract the sexual interest of another person. It can also be influenced by individual genetic, psychological, or cultural factors, or to other, more amorphous qualities of the person. Sexual attraction is also a response to another person that depends on a combination of the person possessing the traits and also on the criteria of the person who is attracted.

Though attempts have been made to devise objective criteria of sexual attractiveness, and measure it as one of several bodily forms of capital asset ("see erotic capital"), a person's sexual attractiveness is to a large extent a subjective measure dependent on another person's interest, perception, and sexual orientation. For example, a gay or lesbian person would typically find a person of the same sex to be more attractive than one of the other sex. A bisexual person would find either sex to be attractive. In addition, there are asexual people, who usually do not experience sexual attraction for either sex, though they may have romantic attraction (homoromantic, biromantic or heteroromantic). Interpersonal attraction includes factors such as physical or psychological similarity, familiarity or possessing a preponderance of common or familiar features, similarity, complementarity, reciprocal liking, and reinforcement.

The ability of a person's physical and other qualities to create a sexual interest in others is the basis of their use in advertising, music video, pornography, film, and other visual media, as well as in modeling, sex work and other occupations.

Globally, laws regulate human sexuality in several ways, including criminalizing particular sexual behaviors, granting individuals the privacy or autonomy to make their own sexual decisions, protecting individuals with regard to equality and non-discrimination, recognizing and protecting other individual rights, as well as legislating matters regarding marriage and the family, and creating laws protecting individuals from violence, harassment, and persecution.

In the United States, there are two fundamentally different approaches, applied in different states, regarding the way the law is used to attempt to govern a person's sexuality. The “black letter” approach to law focuses on the study of pre-existing legal precedent, and attempts to offer a clear framework of rules within which lawyers and others can work. In contrast, the socio-legal approach focuses more broadly on the relationship between the law and society, and offers a more contextualized view of the relationship between legal and social change. Both approaches are used to guide changes in the legal system of states, and both have an effect.

Issues regarding human sexuality and human sexual orientation have come to the forefront in Western law in the latter half of the twentieth century, as part of the gay liberation movement's encouragement of LGBT individuals to "come out of the closet" and engaging with the legal system, primarily through courts. Therefore, many issues regarding human sexuality and the law are found in the opinions of the courts.

While the issue of privacy has been useful to sexual rights claims, some scholars have criticized its usefulness, saying that this perspective is too narrow and restrictive. The law is often slow to intervene in certain forms of coercive behavior that can limit individuals' control over their own sexuality (such as female genital mutilation, forced marriages or lack of access to reproductive health care). Many of these injustices are often perpetuated wholly or in part by private individuals rather than state agents, and as a result, there is an ongoing debate about the extent of state responsibility to prevent harmful practices and to investigate such practices when they do occur.

State intervention with regards to sexuality also occurs, and is considered acceptable by some, in certain instances (e.g. same-sex sexual activity or prostitution).




</doc>
<doc id="11399296" url="https://en.wikipedia.org/wiki?curid=11399296" title="John Henryism">
John Henryism

John Henryism (JH) is a strategy for coping with prolonged exposure to stresses such as social discrimination by expending high levels of effort which results in accumulating physiological costs.
The term was conceived in the 1970s by African-American epidemiologist and public health researcher Sherman James while he was investigating racial health disparities between blacks and others in North Carolina.

One of the people he interviewed was a black man, who, despite being born into an impoverished sharecropper family and having only a second grade education, could read and write. The man had freed himself and his offspring from the sharecropper system, had of farmed land by age 40, but by his 50s, he had hypertension, arthritis, and severe peptic ulcer disease.

His name, John Henry Martin, and his circumstances were evocative of folk hero John Henry, an African American who worked vigorously enough to compete successfully with a steam powered machine but died as a result of his effort.

James' hypothesis was that African Americans sometimes attempted to control their environment through similar attempts at superhuman performance. The expression of this superhuman performance may not necessarily involve a steel hammer. It may involve working harder at the office or working long to prove one’s worth. The end results, however, may still involve the same negative consequences that befell John Henry. 

James developed a scale for measuring JH based on agreement with a series of statements such as these:


In his seminal 1983 study, 132 southern, working class Black men between the ages of 17 and 60 years were administered the John Henryism scale. The scale was used to measure the extent to which these men believed that they could control their environment through hard work and determination. In accordance with the author's hypothesis, subjects who scored low on educational variables and high on John Henryism had significantly higher levels of diastolic blood pressure than those who scored above the median on both measures. James believed that educational achievement and the John Henryism construct score may have a positive correlation with autonomic arousal in African Americans when these individuals have encounters with everyday stressors.

Men who scored higher on the John Henry scale were not found to have statistically significant differences in mean systolic blood pressure or mean diastolic blood pressure when compared to their lower scoring counterparts, however a significant effect did emerge in variation in percentage of hypertension. African American males who were categorized as low or medium SES (socio economic status) and had high levels of John Henryism, had a significantly higher % of hypertension than their counterparts with low levels of John Henryism , however high SES individuals with high levels of John Henryism were found to have lower levels of hypertension than their low John Henryism, high SES counterparts. 

Studies have found that African-Americans with high scores are less likely to be current or former smokers than African-Americans with low JH scores. African-American college students with high JH scores were less likely to have carried a weapon on campus for self-defence, more likely to have been arrested for driving under the influence, and more likely to have missed a class due to alcohol use.




</doc>
<doc id="822164" url="https://en.wikipedia.org/wiki?curid=822164" title="Personal advertisement">
Personal advertisement

A personal or personal ad is an item or notice traditionally in the newspaper, similar to a classified advertisement but personal in nature. In British English it is also commonly known as an advert in a lonely hearts column. With its rise in popularity, the World Wide Web has also become a common medium for personals, commonly referred to as online dating. Personals are generally meant to generate romance, friendship, or casual (sometimes sexual) encounters, and usually include a basic description of the person posting it, and their interests.

Newspapers and magazines that take personal advertisements often provide a reply forwarding service; in this case, the text of the advert will include a unique box number and anyone wishing to reply to the advert sends or delivers their reply to the publisher's address in an envelope bearing that number. The publisher forwards replies in bulk to the advertiser at a given interval, for example each week. 

Another method of replying to Lonely Hearts adverts is via telephone; this took off with the introduction of premium-rate telephone numbers, providing an additional way for the publisher to generate money. The usual business model is for the advertiser to be enticed to place an advert free of charge (using an 0800 number or equivalent); those replying (and also the advertiser, when they want to check for any replies) must use a premium-rate line.

Due to newspaper prices being based on characters or lines of text, a jargon of abbreviations, acronyms and code words arose in personals and have often carried over to the internet.

The following are examples of single-letter abbreviations used in three-letter acronyms (TLAs).

The first letter often describes the relationship state or sexuality of the person:


The middle letter generally represents the ethnicity or nationality of the person posting the ad. Can be replaced by 4, standing for the word for ("seeking", "desires", etc.).


The third letter commonly describes the gender of the person (or couple, if that is what is seeking or sought).


As well as three-letter abbreviations of the format described above, a number of other acronyms and abbreviated words have been popular in personal adverts at different times and in different places. This list is far from complete:





</doc>
<doc id="58616052" url="https://en.wikipedia.org/wiki?curid=58616052" title="Probabilistic epigenesis">
Probabilistic epigenesis

Probabilistic epigenesis is a way of understanding human behavior based on the relationship between experience and biology. It is a variant form of epigenetics, proposed by American psychologist Gilbert Gottlieb in 1991. Gottlieb’s model is based on Conrad H. Waddington’s idea of developmental epigenesis. Both theories examine the complexity of the ways in which the brain develops and explore factors that occur outside the genome. However, probabilistic epigenesis differs from Waddington’s model as it relies much more heavily on the potential developmental impacts of experience and environment and how they interact with an individual’s genes. Probabilistic epigenesis takes into account developmental, hormonal, environmental, neuropsychological, and genetic factors in order to explain various forms of behavior.

In developmental psychology, probabilistic epigenesis is a theory of human behavior that assumes that different neural structures develop and activate either based on an individual's biology or interactions with their environment. It relies on the idea that there are multi-directional interactions between biological and psychological factors, meaning probabilistic epigenesis is a non-hierarchical model of understanding development. 

The biological factors, also known as genetic determinants, involve an individual's genetic makeup, and how it influences their behavior. These factors must be looked at alongside environment variables, however, as the number of genes in a human brain is insufficient in regards to explaining all aspects of the human mind–there is simply not enough storage. According to the theory of probabilistic epigenesis, the environment (socioeconomic, household, etc.) in which a person lives and interacts with contributes to overall behavioral development by triggering certain genetic information to activate.

The relationship between socioeconomically disadvantaged youth and poor physical health is a topic notably evaluated by James Hamblin. Research has proven there is a correlation between disadvantaged youth and high blood pressure, excess body fat, and high levels of cortisol. The pressures associated with a low socioeconomic background have proven to produce chronic stress in individuals who strive for upward mobility, likely due to the increased familial and social pressures. Subsequently, chronic stress has been linked to a break down in bodily functions, and is thus a stimulant for disease. In other words, this means that the extrinsic factor of socioeconomic class can cause an individual to be less physically capable of handling stress and heavy workloads than someone born into a wealthier environment. This means that their self-control and work ethic comes with the price of their health, as people from low socioeconomic backgrounds are more likely to age faster at the cellular level.

An unpredictable and chaotic household structure can be linked to socioemotional development. Socioemotional development, if occurring in an environment that lacks a regular pattern, can result in behavioral difficulties and symptoms of internalization.

An example of this can be seen in a study conducted by Urie Bronfenbrenner, in which he examines how the exchange of energy between the developing child and the persons and objects in their close settings effects their development. His research ultimately suggests that the more regular and positive these household interactions are, the better the child will perform academically and the less likely they will be to internalize problems.

Gottlieb's model of probabilistic epigenesis is based on the idea that certain areas of the brain are operational before they are completely developed. This means that, while the development process is occurring, neural structures are prone to influence–either internal or external. The multi-directional relationship between these influences is again important when regarding brain development into maturation, as brain maturation and functional experience are both the cause and effect of one another.

“Nature versus nurture,” a term coined by Francis Galton in the late 1800's, was an early and simple way of explaining human behavior. In this model, child development into adolescence and adulthood can be explained either by intrinsic aspects of the child or by extrinsic factors influencing the child.

Probabilistic epigenesis draws from this model in that it emphasizes the importance of factors that could be categorized as nature and/or nurture. However, it expands upon and complicates the idea that it is either nature or nurture that causes a person to act a certain way. In probabilistic epigenesis, nature and nurture interact so that every variable is both a cause and an effect. As developmental and neurological understandings have progressed, the idea that intrinsic and extrinsic factors interact with one another rather than independently, as suggested in the probabilistic epigenesis model, has become the predominate way of understanding behavior.


</doc>
<doc id="43562466" url="https://en.wikipedia.org/wiki?curid=43562466" title="Mockery">
Mockery

Mockery or mocking is the act of insulting or making light of a person or other thing, sometimes merely by taunting, but often by making a caricature, purporting to engage in imitation in a way that highlights unflattering characteristics. Mockery can be done in a lighthearted and gentle way, but can also be cruel and hateful, such that it "conjures images of corrosion, deliberate degradation, even subversion; thus, 'to laugh at in contempt, to make sport of' (OED)". Mockery appears to be unique to humans, and serves a number of psychological functions, such as reducing the perceived imbalance of power between authority figures and common people. Examples of mockery can be found in literature and the arts.

The root word "mock" traces to the Old French "mocquer" (later "moquer"), meaning to scoff at, laugh at, deride, or fool, although the origin of "mocquer" is itself unknown. Labeling a person or thing as a mockery may also be used to imply that it or they are a poor quality or counterfeit version of some genuine other, such as the case in the usages: "mockery of man" or "the trial was a mockery of justice".

Australian linguistics professor Michael Haugh differentiated between teasing and mockery by emphasizing that, while the two do have substantial overlap in meaning, mockery does not connote repeated provocation or the intentional withholding of desires, and instead implies a type of imitation or impersonation where a key element is that the nature of the act places a central importance on the expectation that it not be taken seriously. Specifically in examining non-serious forms of jocular mockery, Haugh summarized the literature on the features of mockery as consisting of the following:


In turn, the audience of the mockery may reply with a number of additional cues to indicate that the actions are understood as non-serious, including laughter, explicit agreement, or a continuation or elaboration of the mockery.

Jayne Raisborough and Matt Adams alternatively identified mockery as a type of disparagement humour mainly available as a tool of privileged groups, which ensures normative responses from non-privileged groups. They emphasize that mockery may be used ironically and comedically, to identify moral stigma and signal moral superiority, but also as a form of social encouragement, allowing those who are providing social cues, to do so in a way that provides a level of social distance between the criticism and critic through use of parody and satire. In this way, mockery can function as a "more superficially 'respectable', morally sensitive way of doing class-based distinction than less civil disgust."

The philosopher Baruch Spinoza took a dim view of mockery, contending that it rests "upon a false opinion and proclaim[s] the imperfection of the mocker". He reasoned that either the object of the mockery is not ridiculous, in which case the mocker is wrong in treating it in such a way, or it is ridiculous, in which case mockery is not an effective tool for improvement. Though the mocker reveals that they recognize the imperfection, they do nothing to resolve it using good reason. Writing in his "Tractatus Politicus", Spinoza declared that mockery was a form of hatred and sadness "which can never be converted into joy".

Catholic Bishop Francis de Sales, in his 1877 "Introduction to the Devout Life", decried mockery as a sin:

Alternatively, while philosophers John Locke and Anthony Ashley-Cooper, 3rd Earl of Shaftesbury agreed on the importance of critical inquiry regarding the views of authority figures, Shaftesbury saw an important role specifically for mockery in this process. Shaftesbury held that "a moderate use of mockery could correct vices," and that mockery was among the most important challenges for truth, because "if an opinion cannot stand mockery" then it similarly would be "revealed to be ridiculous". As such all serious claims of knowledge should be subjected to it. This was a view echoed by René Descartes, who saw mockery as a "trait of a good man" which "bears witness to the cheerfulness of his temper ... tranquility of his soul ... [and] the ingenuity of his mind."

Mockery is one form of the literary genre of satire, and it has been noted that "[t]he mock genres and the practice of literary mockery goes back at least as far as the sixth century BCE". Mockery, as a genre, can also be directed towards other artistic genres:

The English comedy troupe, Monty Python, was considered to be particularly adept at the mockery of both authority figures and people making a pretense to competence beyond their abilities. One such sketch, involving a nearly-deaf hearing aid salesman and a nearly-blind contact lens salesman, depicts them as "both desperately unsuccessful, and exceedingly hilarious. The comicality of such characters is largely due to the fact that the objects of mockery themselves create a specific context in which we find that they deserve being ridiculed". In the United States, the television show, "Saturday Night Live" has been noted as having "a history of political mockery", and it has been proposed that "[h]istorical and rhetorical analyses argue that this mockery matters" with respect to political outcomes.

Mockery appears to be a uniquely human activity. Although several species of animal are observed to engage in laughter, humans are the only animal observed to use laughter to mock one another.

An examination of the appearance of the capacity for mockery during childhood development indicates that mockery "does not appear as an expectable moment in early childhood, but becomes more prominent as the latency child enters the social world of sibling rivalry, competition, and social interaction". As it develops, it is "displayed in forms of schoolyard bullying and certainly in adolescence with the attempt to achieve independence while negotiating the conflicts arising out of encounters with authority." One common element of mockery is caricature, a wide-ranging practice of imitating and exaggerating aspects of the subject being mocked. It has been suggested that caricature produced "survival advantages of rapid decoding of facial information", and at the same time that it provides "some of our best humor and, when suffused with too much aggression, may reach the form of mockery". Mockery serves a number of social functions:

Richard Borshay Lee reported mockery as a facet of Bushmen culture designed to keep individuals who are successful in certain regards from becoming arrogant. When weaker people are mocked by stronger people, this can constitute a form of bullying.




</doc>
<doc id="39807" url="https://en.wikipedia.org/wiki?curid=39807" title="Nature versus nurture">
Nature versus nurture

The nature versus nurture debate involves whether human behavior is determined by the environment, either prenatal or during a person's life, or by a person's genes. The alliterative expression "nature and nurture" in English has been in use since at least the Elizabethan period and goes back to medieval French.
The combination of the two concepts as complementary is ancient (Greek: ). Nature is what we think of as pre-wiring and is influenced by genetic inheritance and other biological factors. Nurture is generally taken as the influence of external factors after conception e.g. the product of exposure, experience and learning on an individual.

The phrase in its modern sense was popularized by the English Victorian polymath Francis Galton, the modern founder of eugenics and behavioral genetics, discussing the influence of heredity and environment on social advancement. Galton was influenced by the book "On the Origin of Species" written by his half-cousin, Charles Darwin.

The view that humans acquire all or almost all their behavioral traits from "nurture" was termed "tabula rasa" ("blank slate") by John Locke in 1690. A "blank slate view" in human developmental psychology assuming that human behavioral traits develop almost exclusively from environmental influences, was widely held during much of the 20th century (sometimes termed "blank-slatism").
The debate between "blank-slate" denial of the influence of heritability, and the view admitting both environmental and heritable traits, has often been cast in terms of nature "versus" nurture. These two conflicting approaches to human development were at the core of an ideological dispute over research agendas throughout the second half of the 20th century. As both "nature" and "nurture" factors were found to contribute substantially, often in an inextricable manner, such views were seen as naive or outdated by most scholars of human development by the 2000s.

The strong dichotomy of nature "versus" nurture has thus been claimed to have limited relevance in some fields of research. Close feedback loops have been found in which "nature" and "nurture" influence one another constantly, as seen in self-domestication. In ecology and behavioral genetics, researchers think nurture has an essential influence on nature. Similarly in other fields, the dividing line between an inherited and an acquired trait becomes unclear, as in epigenetics or fetal development.

John Locke's "An Essay Concerning Human Understanding" (1690) is often cited as the foundational document of the "blank slate" view. Locke was criticizing René Descartes's claim of an innate idea of God universal to humanity.
Locke's view was harshly criticized in his own time. Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, complained that by denying the possibility of any innate ideas, 
Locke "threw all order and virtue out of the world", leading to total moral relativism.
Locke's was not the predominant view in the 19th century, which on the contrary tended to focus on "instinct".
Leda Cosmides and John Tooby noted that William James (1842–1910) argued that humans have "more" instincts than animals, and that greater freedom of action is the result of having more psychological instincts, not fewer.

The question of "innate ideas" or "instincts" were of some importance in the discussion of free will in moral philosophy. In 18th-century philosophy, this was cast in terms of "innate ideas" establishing the presence of a universal virtue, prerequisite for objective morals. In the 20th century, this argument was in a way inverted, as some philosophers now argued that the evolutionary origins of human behavioral traits forces us to concede that there is no foundation for ethics (J. L. Mackie), while others treat ethics as a field in complete isolation from evolutionary considerations (Thomas Nagel).

In the early 20th century, there was an increased interest in the role of the environment, as a reaction to the strong focus on pure heredity in the wake of the triumphal success of Darwin's theory of evolution.

During this time, the social sciences developed as the project of studying the influence of culture in clean isolation from questions related to "biology".
Franz Boas's "The Mind of Primitive Man" (1911) established a program that would dominate American anthropology for the next fifteen years. In this study he established that in any given population, biology, language, material and symbolic culture, are autonomous; that each is an equally important dimension of human nature, but that no one of these dimensions is reducible to another.

The tool of twin studies was developed as a research design intended to exclude all confounders based on inherited behavioral traits. Such studies are designed to decompose the variability of a given trait in a given population into a genetic and an environmental component.

John B. Watson in the 1920s and 1930s established the school of purist behaviorism that would become dominant over the following decades. Watson is often said to have been convinced of the complete dominance of cultural influence over anything that heredity might contribute, based on the following quote which is frequently repeated without context:

The last sentence of the above quote is frequently omitted, leading to confusion about Watson's position. 

During the 1940s to 1960s, Ashley Montagu was a notable proponent of this purist form of behaviorism which allowed no contribution from heredity whatsoever: 

In 1951, Calvin Hall suggested that the dichotomy opposing nature to nurture is ultimately fruitless.

Robert Ardrey in the 1960s argued for innate attributes of human nature, especially concerning territoriality, in the widely read "African Genesis" (1961) and "The Territorial Imperative". Desmond Morris in "The Naked Ape" (1967) expressed similar views.
Organised opposition to Montagu's kind of purist "blank-slatism" began to pick up in the 1970s, notably led by E. O. Wilson ("On Human Nature" 1979).
Twin studies established that there was, in many cases, a significant heritable component. 
These results did not in any way point to overwhelming contribution of heritable factors, with heritability typically ranging around 40% to 50%, so that the controversy may not be cast in terms of purist behaviorism vs. purist nativism. Rather, it was purist behaviorism which was gradually replaced by the now-predominant view that both kinds of factors usually contribute to a given trait, anecdotally phrased by Donald Hebb as an answer to the question "which, nature or nurture, contributes more to personality?" by asking in response, "Which contributes more to the area of a rectangle, its length or its width?" 
In a comparable avenue of research, anthropologist Donald Brown in the 1980s surveyed hundreds of anthropological studies from around the world and collected a set of cultural universals. He identified approximately 150 such features, coming to the conclusion there is indeed a "universal human nature", and that these features point to what that universal human nature is.

At the height of the controversy, during the 1970s to 1980s, the debate was highly ideologised. In "" (1984), Richard Lewontin, Steven Rose and Leon Kamin criticise "genetic determinism" from a Marxist framework, arguing that "Science is the ultimate legitimator of bourgeois ideology ... If biological determinism is a weapon in the struggle between classes, then the universities are weapons factories, and their teaching and research faculties are the engineers, designers, and production workers." The debate thus shifted away from whether heritable traits exist to whether it was politically or ethically permissible to admit their existence. The authors deny this, requesting that evolutionary inclinations be discarded in ethical and political discussions regardless of whether they exist or not.

Heritability studies became much easier to perform, and hence much more numerous, with the advances of genetic studies during the 1990s. By the late 1990s, an overwhelming amount of evidence had accumulated that amounts to a refutation of the extreme forms of "blank-slatism" advocated by Watson or Montagu.

This revised state of affairs was summarized in books aimed at a popular audience from the late 1990s. In "" (1998), Judith Rich Harris was heralded by Steven Pinker as a book that "will come to be seen as a turning point in the history of psychology".
but Harris was criticized for exaggerating the point of "parental upbringing seems to matter less than previously thought" to the implication that "parents do not matter".

The situation as it presented itself by the end of the 20th century was summarized in "" (2002) by Steven Pinker. The book became a best-seller, and was instrumental in bringing to the attention of a wider public the paradigm shift away from the behaviourist purism of the 1940s to 1970s that had taken place over the preceding decades. Pinker portrays the adherence to pure blank-slatism as an ideological dogma linked to two other dogmas found in the dominant view of human nature in the 20th century, which he termed "noble savage" (in the sense that people are born good and corrupted by bad influence) and 
"ghost in the machine" (in the sense that there is a human soul capable of moral choices completely detached from biology). Pinker argues that all three dogmas were held onto for an extended period even in the face of evidence because they were seen as "desirable" in the sense that if any human trait is purely conditioned by culture, any undesired trait (such as crime or aggression) may be engineered away by purely cultural (political means). Pinker focuses on reasons he assumes were responsible for unduly repressing evidence to the contrary, notably the fear of (imagined or projected) political or ideological consequences.

It is important to note that the term "heritability" refers only to the degree of genetic variation between people on a trait. It does not refer to the degree to which a trait of a particular individual is due to environmental or genetic factors. The traits of an individual are always a complex interweaving of both. For an individual, even strongly genetically influenced, or "obligate" traits, such as eye color, assume the inputs of a typical environment during ontogenetic development (e.g., certain ranges of temperatures, oxygen levels, etc.).

In contrast, the "heritability index" statistically quantifies the extent to which variation "between individuals" on a trait is due to variation in the genes those individuals carry. In animals where breeding and environments can be controlled experimentally, heritability can be determined relatively easily. Such experiments would be unethical for human research. This problem can be overcome by finding existing populations of humans that reflect the experimental setting the researcher wishes to create.

One way to determine the contribution of genes and environment to a trait is to study twins. In one kind of study, identical twins reared apart are compared to randomly selected pairs of people. The twins share identical genes, but different family environments. In another kind of twin study, identical twins reared together (who share family environment and genes) are compared to fraternal twins reared together (who also share family environment but only share half their genes). Another condition that permits the disassociation of genes and environment is adoption. In one kind of adoption study, biological siblings reared together (who share the same family environment and half their genes) are compared to adoptive siblings (who share their family environment but none of their genes).

In many cases, it has been found that genes make a substantial contribution, including psychological traits such as intelligence and personality. Yet heritability may differ in other circumstances, for instance environmental deprivation. Examples of low, medium, and high heritability traits include:

Twin and adoption studies have their methodological limits. For example, both are limited to the range of environments and genes which they sample. Almost all of these studies are conducted in Western, first-world countries, and therefore cannot be extrapolated globally to include poorer, non-western populations. Additionally, both types of studies depend on particular assumptions, such as the equal environments assumption in the case of twin studies, and the lack of pre-adoptive effects in the case of adoption studies.

Since the definition of "nature" in this context is tied to "heritability", the definition of "nurture" has necessarily become very wide, including any type of causality that is not heritable. The term has thus moved away from its original connotation of "cultural influences" to include all effects of the environment, including; indeed, a substantial source of environmental input to human nature may arise from stochastic variations in prenatal development and is thus in no sense of the term "cultural".

Heritability refers to the origins of differences between people. Individual development, even of highly heritable traits, such as eye color, depends on a range of environmental factors, from the other genes in the organism, to physical variables such as temperature, oxygen levels etc. during its development or ontogenesis.

The variability of trait can be meaningfully spoken of as being due in certain proportions to genetic differences ("nature"), or environments ("nurture"). For highly penetrant Mendelian genetic disorders such as Huntington's disease virtually all the incidence of the disease is due to genetic differences. Huntington's animal models live much longer or shorter lives depending on how they are cared for .

At the other extreme, traits such as native language are environmentally determined: linguists have found that any child (if capable of learning a language at all) can learn any human language with equal facility. With virtually all biological and psychological traits, however, genes and environment work in concert, communicating back and forth to create the individual.

At a molecular level, genes interact with signals from other genes and from the environment. While there are many thousands of single-gene-locus traits, so-called complex traits are due to the additive effects of many (often hundreds) of small gene effects. A good example of this is height, where variance appears to be spread across many hundreds of loci.

Extreme genetic or environmental conditions can predominate in rare circumstances—if a child is born mute due to a genetic mutation, it will not learn to speak any language regardless of the environment; similarly, someone who is practically certain to eventually develop Huntington's disease according to their genotype may die in an unrelated accident (an environmental event) long before the disease will manifest itself.

Steven Pinker likewise described several examples:

When traits are determined by a complex interaction of genotype and environment it is possible to measure the heritability of a trait within a population. However, many non-scientists who encounter a report of a trait having a certain percentage heritability imagine non-interactional, additive contributions of genes and environment to the trait. As an analogy, some laypeople may think of the degree of a trait being made up of two "buckets," genes and environment, each able to hold a certain capacity of the trait. But even for intermediate heritabilities, a trait is always shaped by both genetic dispositions and the environments in which people develop, merely with greater and lesser plasticities associated with these heritability measures.

Heritability measures always refer to the degree of "variation between individuals in a population". That is, as these statistics cannot be applied at the level of the individual, it would be incorrect to say that while the heritability index of personality is about 0.6, 60% of one's personality is obtained from one's parents and 40% from the environment. To help to understand this, imagine that all humans were genetic clones. The heritability index for all traits would be zero (all variability between clonal individuals must be due to environmental factors). And, contrary to erroneous interpretations of the heritability index, as societies become more egalitarian (everyone has more similar experiences) the heritability index goes up (as environments become more similar, variability between individuals is due more to genetic factors).

One should also take into account the fact that the variables of heritability and environmentality are not precise and vary within a chosen population and across cultures. It would be more accurate to state that the degree of heritability and environmentality is measured in its reference to a particular phenotype in a chosen group of a population in a given period of time. The accuracy of the calculations is further hindered by the number of coefficients taken into consideration, age being one such variable. The display of the influence of heritability and environmentality differs drastically across age groups: the older the studied age is, the more noticeable the heritability factor becomes, the younger the test subjects are, the more likely it is to show signs of strong influence of the environmental factors.

Some have pointed out that environmental inputs affect the expression of genes (see the article on epigenetics). This is one explanation of how environment can influence the extent to which a genetic disposition will actually manifest. The interactions of genes with environment, called gene–environment interactions, are another component of the nature–nurture debate. A classic example of gene–environment interaction is the ability of a diet low in the amino acid phenylalanine to partially suppress the genetic disease phenylketonuria. Yet another complication to the nature–nurture debate is the existence of gene–environment correlations. These correlations indicate that individuals with certain genotypes are more likely to find themselves in certain environments. Thus, it appears that genes can shape (the selection or creation of) environments. Even using experiments like those described above, it can be very difficult to determine convincingly the relative contribution of genes and environment.

A study conducted by T. J. Bouchard, Jr. showed data that has been evidence for the importance of genes when testing middle-aged twins reared together and reared apart. The results shown have been important evidence against the importance of environment when determining, happiness, for example. In the Minnesota study of twins reared apart, it was actually found that there was higher correlation for monozygotic twins reared apart (0.52)than monozygotic twins reared together (0.44). Also, highlighting the importance of genes, these correlations found much higher correlation among monozygotic than dizygotic twins that had a correlation of 0.08 when reared together and −0.02 when reared apart.

The social pre-wiring hypothesis refers to the ontogeny of social interaction. Also informally referred to as, "wired to be social." The theory questions whether there is a propensity to socially oriented action already present "before" birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social.

Circumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be contributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics.

Principal evidence of this theory is uncovered by examining twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed.

The social pre-wiring hypothesis was proved correct, "The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions.".

Traits may be considered to be adaptations (such as the umbilical cord), byproducts of adaptations (the belly button) or due to random variation (convex or concave belly button shape).
An alternative to contrasting nature and nurture focuses on "obligate vs. facultative" adaptations. Adaptations may be generally more obligate (robust in the face of typical environmental variation) or more facultative (sensitive to typical environmental variation). For example, the rewarding sweet taste of sugar and the pain of bodily injury are obligate psychological adaptations—typical environmental variability during development does not much affect their operation. On the other hand, facultative adaptations are somewhat like "if-then" statements. An example of a facultative psychological adaptation may be adult attachment style. The attachment style of adults, (for example, a "secure attachment style," the propensity to develop close, trusting bonds with others) is proposed to be conditional on whether an individual's early childhood caregivers could be trusted to provide reliable assistance and attention. An example of a facultative physiological adaptation is tanning of skin on exposure to sunlight (to prevent skin damage). Facultative social adaptation have also been proposed. For example, whether a society is warlike or peaceful has been proposed to be conditional on how much collective threat that society is experiencing .

Quantitative studies of heritable traits throw light on the question.

Developmental genetic analysis examines the effects of genes over the course of a human lifespan. Early studies of intelligence, which mostly examined young children, found that heritability measured 40–50%. Subsequent developmental genetic analyses found that variance attributable to additive environmental effects is less apparent in older individuals, with estimated heritability of IQ increasing in adulthood.

Multivariate genetic analysis examines the genetic contribution to several traits that vary together. For example, multivariate genetic analysis has demonstrated that the genetic determinants of all specific cognitive abilities (e.g., memory, spatial reasoning, processing speed) overlap greatly, such that the genes associated with any specific cognitive ability will affect all others. Similarly, multivariate genetic analysis has found that genes that affect scholastic achievement completely overlap with the genes that affect cognitive ability.

Extremes analysis examines the link between normal and pathological traits. For example, it is hypothesized that a given behavioral disorder may represent an extreme of a continuous distribution of a normal behavior and hence an extreme of a continuous distribution of genetic and environmental variation. Depression, phobias, and reading disabilities have been examined in this context.

For a few highly heritable traits, studies have identified loci associated with variance in that trait, for instance in some individuals with schizophrenia.

Through studies of identical twins separated at birth, one-third of their creative thinking abilities come from genetics and two-thirds come from learning. Research suggests that between 37 and 42 percent of the explained variance can be attributed to genetic factors. The learning primarily comes in the form of human capital transfers of entrepreneurial skills through parental role modeling. Other findings agree that the key to innovative entrepreneurial success comes from environmental factors and working “10,000 hours” to gain mastery in entrepreneurial skills.

Evidence from behavioral genetic research suggests that family environmental factors may have an effect upon childhood IQ, accounting for up to a quarter of the variance. The American Psychological Association's report "" (1995) states that there is no doubt that normal child development requires a certain minimum level of responsible care. Here, environment is playing a role in what is believed to be fully genetic (intelligence) but it was found that severely deprived, neglectful, or abusive environments have highly negative effects on many aspects of children's intellect development. Beyond that minimum, however, the role of family experience is in serious dispute. On the other hand, by late adolescence this correlation disappears, such that adoptive siblings no longer have similar IQ scores.

Moreover, adoption studies indicate that, by adulthood, adoptive siblings are no more similar in IQ than strangers (IQ correlation near zero), while full siblings show an IQ correlation of 0.6. Twin studies reinforce this pattern: monozygotic (identical) twins raised separately are highly similar in IQ (0.74), more so than dizygotic (fraternal) twins raised together (0.6) and much more than adoptive siblings (~0.0). Recent adoption studies also found that supportive parents can have a positive effect on the development of their children.

Personality is a frequently cited example of a heritable trait that has been studied in twins and adoptees using behavioral genetic study designs. The most famous categorical organization of heritable personality traits were created by Goldberg (1990) in which he had college students rate their personalities on 1400 dimensions to begin, and then narrowed these down into ""The Big Five"" factors of personality—openness, conscientiousness, extraversion, agreeableness, and neuroticism. The close genetic relationship between positive personality traits and, for example, our happiness traits are the mirror images of comorbidity in psychopathology. These personality factors were consistent across cultures, and many studies have also tested the heritability of these traits.

Identical twins reared apart are far more similar in personality than randomly selected pairs of people. Likewise, identical twins are more similar than fraternal twins. Also, biological siblings are more similar in personality than adoptive siblings. Each observation suggests that personality is heritable to a certain extent. A supporting article had focused on the heritability of personality (which is estimated to be around 50% for subjective well-being) in which a study was conducted using a representative sample of 973 twin pairs to test the heritable differences in subjective well-being which were found to be fully accounted for by the genetic model of the Five-Factor Model’s personality domains. However, these same study designs allow for the examination of environment as well as genes.

Adoption studies also directly measure the strength of shared family effects. Adopted siblings share only family environment. Most adoption studies indicate that by adulthood the personalities of adopted siblings are little or no more similar than random pairs of strangers. This would mean that shared family effects on personality are zero by adulthood.

In the case of personality traits, non-shared environmental effects are often found to out-weigh shared environmental effects. That is, environmental effects that are typically thought to be life-shaping (such as family life) may have less of an impact than non-shared effects, which are harder to identify. One possible source of non-shared effects is the environment of pre-natal development. Random variations in the genetic program of development may be a substantial source of non-shared environment. These results suggest that "nurture" may not be the predominant factor in "environment". Environment and our situations, do in fact impact our lives, but not the way in which we would typically react to these environmental factors. We are preset with personality traits that are the basis for how we would react to situations. An example would be how extraverted prisoners become less happy than introverted prisoners and would react to their incarceration more negatively due to their preset extraverted personality. Behavioral genes are somewhat proven to exist when we take a look at fraternal twins. When fraternal twins are reared apart, they show the same similarities in behavior and response as if they have been reared together.

The relationship between personality and people's own well-being is influenced and mediated by genes (Weiss, Bates, & Luciano, 2008). There has been found to be a stable set point for happiness that is characteristic of the individual (largely determined by the individual's genes). Happiness fluctuates around that setpoint (again, genetically determined) based on whether good things or bad things are happening to us ("nurture"), but only fluctuates in small magnitude in a normal human. The midpoint of these fluctuations is determined by the "great genetic lottery" that people are born with, which leads them to conclude that how happy they may feel at the moment or over time is simply due to the luck of the draw, or gene. This fluctuation was also not due to educational attainment, which only accounted for less than 2% of the variance in well-being for women, and less than 1% of the variance for men.

They consider that the individualities measured together with personality tests remain steady throughout an individual’s lifespan. They further believe that human beings may refine their forms or personality but can never change them entirely. Darwin's Theory of Evolution steered naturalists such as George Williams and William Hamilton to the concept of personality evolution. They suggested that physical organs and also personality is a product of natural selection.

With the advent of genomic sequencing, it has become possible to search for and identify specific gene polymorphisms that affect traits such as IQ and personality. These techniques work by tracking the association of differences in a trait of interest with differences in specific molecular markers or functional variants. An example of a visible human trait for which the precise genetic basis of differences are relatively well known is eye color. For traits with many genes affecting the outcome, a smaller portion of the variance is currently understood: For instance for height known gene variants account for around 5–10% of height variance at present.
When discussing the significant role of genetic heritability in relation to one's level of happiness, it has been found that from 44% to 52% of the variance in one's well-being is associated with genetic variation. Based on the retest of smaller samples of twins studies after 4,5, and 10 years, it is estimated that the heritability of the genetic stable component of subjective well-being approaches 80%. Other studies that have found that genes are a large influence in the variance found in happiness measures, exactly around 35–50%.

In contrast to views developed in 1960s that gender identity is primarily learned (which led to policy-based surgical sex changed in children such as David Reimer), genomics has provided solid evidence that both sex and gender identities are primarily influenced by genes:

In their attempts to locate the genes responsible for configuring certain phenotypes, researches resort to two different techniques.
Linkage study facilitates the process of determining a specific location in which a gene of interest is located. This methodology is applied only among individuals that are related and does not serve to pinpoint specific genes. It does, however, narrow down the area of search, making it easier to locate one or several genes in the genome which constitute a specific trait.

Association studies, on the other hand, are more hypothetic and seek to verify whether a particular genetic variable really influences the phenotype of interest. In association studies it is more common to use case-control approach, comparing the subject with relatively higher or lower hereditary determinants with the control subject.



</doc>
<doc id="60619734" url="https://en.wikipedia.org/wiki?curid=60619734" title="Interactionism (nature versus nurture)">
Interactionism (nature versus nurture)

In the context of the nature-nurture debate, interactionism is the view that all human behavioral traits develop from the interaction of both "nature" and "nurture", that is, from both genetic and environmental factors. This view further holds that genetic and environmental influences on organismal development are so closely interdependent that they are inseparable from one another. Historically, it has often been confused with the statistical concept of gene-environment interaction. Historically, interactionism has presented a limited view of the manner in which behavioral traits develop, and has simply demonstrated that "nature" and "nurture" are both necessary. Among the first biologists to propose an interactionist theory of development was Daniel Lehrman. Since then, numerous interactionist perspectives have been proposed, and the contradictions between many of these perspectives has led to much controversy in evolutionary psychology. Proponents of various forms of interactionist perspectives include Philip Kitcher, who refers to his view as "causal democracy", and Susan Oyama, who describes her perspective as "constructive interactionism".


</doc>
<doc id="53782753" url="https://en.wikipedia.org/wiki?curid=53782753" title="Behavior change (individual)">
Behavior change (individual)

A behavioral change can be a temporary or permanent effect that is considered a change in an individual's behavior when compared to previous behavior. It is sometimes considered a mental disorder, yet it is also a strategy used to improve such disorders. This change is generally characterized by changes in thinking, interpretations, emotions, or relationships. These changes can be either good or bad, depending on which behavior is being affected. Often, it takes much more work to change behavior for the better than it does to experience a negative change. Medications can cause this change as a side effect. The interaction between physiological processes and their effect on individual behavior is the basis of psychophysiology. Several theories exist as to why and how behavioral change can be affected, including behaviorism, Self-efficacy theory, and the stages of change model.

Behavioral change can be very beneficial to an individual, an annoying side-effect, or a life-altering condition which takes considerable effort to repair. Two such theories on the subject include behavior modification theory and cognitive behavioral theory. Both of these seek to help a patient engage in a positive behavioral change. Both legal and illegal drugs have been shown to alter behavior, both acutely and chronically. In both cases, following common sense harm reduction strategies can potentially reduce these side-effects. With mental illness, behavioral change is a menace, with drugs it is expected, and with the right techniques it can be a method to improve quality of life. In recent decades we have gained knowledge on common causes of these changes, such as mental illness and drug use, while also developing and applying several psychological fields to the study of inducing beneficial changes in individuals, resulting in a variety of novel solutions.

While some behavioral changes can be beneficial, others can cause serious harm to the individual experiencing them. Sometimes, a change can be due to something as small as an environmental cue, whereas other cases may take a more multifaceted approach. In those cases, there are several treatment options available. Behavior modification is one method used to correct harmful behaviors. This method is centered around the concept of using rewards and punishments in order to condition the patient out of the behavior. By focusing on any specific issues the patient may have, this type of therapy allows for a wide range of applications. With applications that range from job performance to success in marriage, a lot of varied research has been done within this topic. Cognitive behavioral therapy can be used in the effort to change the behavior of an individual. This type of therapy focuses in on challenging and changing maladaptive behaviors by utilizing emotional self-regulation, while also developing beneficial coping mechanisms. It requires the willing participation of the individual being counseled to be effective. Training of counselors and their interventions can increase the likelihood of behavioral changes in those they counsel.

Similarly, medication can cause changes in people’s behavior. In some instances, the reaction to a medication is an expected effect. For example, someone who is taking an opiate for pain should expect to feel sleepy and relaxed. In other instances, a behavior change may indicate that the dosage of medication is at a toxic level, or is an indication of hypersensitivity to the medication. For example, someone taking a stimulant medication should not experience depression. While this can be a very serious consequence, it is a concern that needs to be balanced with the effective dose of that medication. Geriatric patients are more susceptible to these effects. As a related note, various illicit drugs can also impact behavior change, without the benefit of having a medical professional to monitor the user. Drugs such as cannabis, opiates, stimulants, hallucinogens and barbiturates can have a very serious impact on one’s behavior, with both acute and chronic usage leading to change.

While behavioral change is often associated with issues of medical importance, there are many non medical reasons that behavioral change may occur. One example is the noted change that happens in an individual as they go through the stages of grief. Despite a prolonged alteration in the way that one behaves, normalcy does usually return to an individual without any type of medical intervention. Another reason for such a change could be an altered schedule, or work-related stress. Such levels of stress may not qualify as medically necessary, and thus may be handled with preventative care. Such preventative care can include exercise, a good social support group, and a nutritious diet. While this type of behavioral change does not always require medical attention, individuals should seek professional help if they notice that these behavioral changes are especially maladaptive, or if they last longer than usual.


</doc>
<doc id="60439005" url="https://en.wikipedia.org/wiki?curid=60439005" title="Sexual guilt">
Sexual guilt

<br>Sexual Guilt is a negative emotional response associated with the feeling of anxiety, guilt or shame in relation to sexual activity. It is also known as “Sexual shame”. 

It is linked with the negative social stigma and cultural expectations that are held towards sex as well as the historical religious opposition of all “immoral” sexual acts. Participation in sexual intercourse does not need to occur to experience sexual guilt, however self sexual pleasure or sexual activities with others are major causes. Sexual Guilt can be felt by an individual who feels guilty about the idea of sex.Sexual guilt is derived from the negative pressures placed upon individuals throughout a lifetime from parental messages surrounding sexual activity and expression, religious teachings..

Sexual guilt can severely impact the affected individual and deteriorate the relationships of those close to them. It has been linked to cases of sexual dysfunction, clinical depression and other mental illnesses.Sexual guilt can also cause physical impacts and illnesses. If the individual feels shame or guilt towards sexual participation they may be less likely to seek protective and contraceptive measures or seek medical attention if they encounter symptoms from sexual intercourse.

Within the modern era of sexual expressiveness and instant sexual gratification, sexual education plays an important role in reducing the impacts and risk of sexual guilt as its incidence increases. Past historical research into the cause of sexual guilt has shown to require more study.

Participation in sexual activity or intercourse does not need to take place in order for someone to encounter sexual guilt. Sexual Guilt can come from participating in sexual acts, thinking about participating in sexual acts or from critically judging sexual acts and attitudes of yourself or others.

Sexual guilt can originate from


Studies have suggested that sexual guilt can be a predictor for individuals past relationships, sexual attitudes and experiences such as 


Sexual Guilt can be caused by "messages about approved or disapproved attitudes toward sexual issues" that individuals faces from external sources such as family and friends about and religious groups. cultural ‘norms’ or identifying as a non-binary sexual orientation.


There are two main psychologically recognised types of sexual guilt which are “Latent” guilt and “Morning After” guilt. Each type of guilt can be found in different scenarios and can cause different effects upon the individual.

Latent guilt is an intrinsic feeling of shame or guilt that comes from the negative association of sexual activity or desire as a base or animal instinct. Individuals with Latent guilt may believe that sexual activity shows a weakness that breaks down the individual’s strength of character. People who encounter this form of sexual guilt do not have to physically participate in sexual activities to feel it. Individuals can feel shame towards their own inner desire, or they may possess a lowered libido, inability to climax and which could impact their relationships.

This form of Sexual Guilt is the most common. “Morning after guilt” is derived from the feeling of guilt, sin or shame felt by the individual after they have committed an act that is not in line with their own internal values or within the expectations set out by people in a relationship or within a certain group, nationality or religion. This type of sexual guilt can most commonly be found within individuals who regret performing an act of sexual activity e.g. cheating within a relationship, partaking in premarital sex or having sex with someone that makes the induvial feel remorse. As the colloquial name suggests it is commonly experienced the “Morning after” or post coitus.
Ancient Religions and cultures shapes how society behaves today. The sexual attitudes of religions in the past can be seen as having an effect upon the sexual attitudes and pressures felt in the modern day. Each religion looks at sexual activity independently and hold different rules and moral expectations while many have overlapping values and ideas about the role sex should play.

Religious beliefs and writings have passed through generations to create expectations upon people to behave and interact a certain way. Marriage between a male and female is seen commonly between multiple religions as the only relationship in which sexual activity should occur, this can place pressure upon people within non heterosexual relationship as members of their community may not agree with their choices. 

The Catholic Church has a traditional view towards sexual activity, teaching followers that it should be done within the confines of marriage as a "noble and worthy" act of between a man and a woman. This religion considers any other sexual acts such as homosexuality, masturbation and contraception use to be sinful. 

Religions such as Judaism consider restraining from sex to be an immoral act. Whilst Islam looks as sex as an act that should be responsibly acted upon through marriage. This is shown in the Quran where the term "nikah" is used for marriage which translates to sexual intercourse. Sexual pleasure is emphasised within marriage when care and love is present. However homosexuality is also strictly demonised and has been recommended to be punished by death

Hinduism is a religion that has a strong binding to sexual pleasure or kama however this pleasure is thought to be a responsibility of marriage and should be avoided until the age of 25 in search of virtuous living, intellectual, financial and spiritual development. The "Kama Sutra" is a text thought to be of sacred religious meaning in relation to sex however it aims to show the three pillars of Hinduism dharma, artha and kama, Vatsyayana's text is supposed to signify the significance of sexual activity in relation to the priorities of virtuous living and financial gain,Indra Sinha.. 

A research paper was done by Mark. P Gunderson and James Leslie McCary in which 373 college students completed a 173 item questionnaire to determine whether Sexual Guilt or religion was a better indicator of the individuals "level of sex information obtained, sexual attitudes held and sexual behaviour expressed". They found that. "Sexual guilt is a far better and more powerful predictor of level of sex information obtained, sexual attitudes held, and sexual behavior expressed than religion. The conclusion is that religion is an intervening variable with sexual guilt such that the more frequently students attend church, the more likely they are to have high sexual guilt which interferes with their sexuality." 

Social and cultural attitudes and expectations upon members of their community can be felt by the individual and cause sexual guilt, embarrassment, anxiety or even sexual abstinence. Some of these values and behaviours may have been derived from sexual myths and legends which have amalgamated into societal expectations and social stigmas towards acts and forms of sex.

Sexual Orientation and Identification is a major cause of sexual guilt, anxiety and feelings of non-inclusivity for people who identify in non traditional orientations. E.g. Homosexuality, Bisexuality, Pansexuality, Asexuality, etc. Each country and territory has its own LGBT laws and rights which are based on the cultural values and beliefs of that region. In some cultures it is illegal to be in a same sex relationship and is punishable by imprisonment or death. Some of the laws and rights surrounding sex have been shaped by myths and legends which may deter, support more traditional or spiritual forms of sex. Some myths and art show evidence for the presence of LGBT themes in mythology and ancient cultures.

Individuals who experience sexual guilt can experience a range of effects that can have a severe and highly detrimental influence upon their wellbeing and the health and wellbeing of partners and close relationships. These can be seen as Psychological or mentally impacting or have physical manifestations and effects.

Individuals who experience sexual guilt or who have experienced sexual guilt previously can be affected mentally by the challenges which this attitude can lead to. Possessing sexual guilt can lead individuals who are sexually active to be hyper aware or critical of their sexual performance which could lead to sexual disfunction, depression and performance anxiety and other illnesses. These mental effects can have compounding physical and behavioral impacts such as a fear of sex or loss of sexual desire in which the individual may abstain from sex completely. Individuals who feel a shame towards sexual acts can also be sexually inactive. Sexually inactive individuals can also feel a reluctance, disinterest or anxiety towards sexual acts due to the pressures from religion, media and people around them which can depict sex as a pursuit of the animalistic urges from the id. "People can also feel sexual guilt about the nature of the erotic fantasies"/sexual fantasies. Individuals who have symptoms may require professional psychological advice in order to work through the effects of sexual guilt.

Sexual guilt may leave the affected individual in a state of crippling anxiety in which they do not want to seek out help or practice safe sexual practices. Freud linked the feeling of guilt, and its related emotion of anxiety. People who are less informed or practiced about safe sex practices are more likely to transfer sexually transmitted diseases, be involved in an unwanted pregnancy. Someone suffering from sexual guilt is less likely to seek medical assistance due to a feeling of shame or anxiety, this can then lead to more severe symptoms or infection.


</doc>
<doc id="4254743" url="https://en.wikipedia.org/wiki?curid=4254743" title="Chorography">
Chorography

Chorography (from χῶρος "khōros", "place" and γράφειν "graphein", "to write") is the art of describing or mapping a region or district, and by extension such a description or map. This term derives from the writings of the ancient geographer Pomponius Mela and Ptolemy, where it meant the geographical description of regions. However, its resonances of meaning have varied at different times. Richard Helgerson states that "chorography defines itself by opposition to chronicle. It is the genre devoted to place, and chronicle is the genre devoted to time". Darrell Rohl prefers a broad definition of "the representation of space or place".

In his text of the "Geographia" (2nd century CE), Ptolemy defined geography as the study of the entire world, but chorography as the study of its smaller parts—provinces, regions, cities, or ports. Its goal was "an impression of a part, as when one makes an image of just an ear or an eye"; and it dealt with "the qualities rather than the quantities of the things that it sets down". Ptolemy implied that it was a "graphic" technique, comprising the making of views (not simply maps), since he claimed that it required the skills of a draftsman or landscape artist, rather than the more technical skills of recording "proportional placements". Ptolemy's most recent English translators, however, render the term as "regional cartography".

Ptolemy's text was rediscovered in the west at the beginning of the fifteenth century, and the term "chorography" was revived by humanist scholars. An early instance is a small-scale map of Britain in an early fifteenth-century manuscript, which is labelled a "tabula chorographica". John Dee in 1570 regarded the practice as "an underling, and a twig of "Geographie"", by which the "plat" [plan or drawing] of a particular place would be exhibited to the eye.
The term also came to be used, however, for "written" descriptions of regions. These regions were extensively visited by the writer, who then combined local topographical description, summaries of the historical sources, and local knowledge and stories, into a text. The most influential example (at least in Britain) was probably William Camden's "Britannia" (first edition 1586), which described itself on its title page as a "Chorographica descriptio". William Harrison in 1587 similarly described his own "Description of Britaine" as an exercise in chorography, distinguishing it from the historical/chronological text of Holinshed's "Chronicles" (to which the "Description" formed an introductory section). Peter Heylin in 1652 defined chorography as "the exact description of some Kingdom, Countrey, or particular Province of the same", and gave as examples Pausanias's "Description of Greece" (2nd century AD); Camden's "Britannia" (1586); Lodovico Guicciardini's "Descrittione di tutti i Paesi Bassi" (1567) (on the Low Countries); and Leandro Alberti's "Descrizione d'Italia" (1550).

Camden's "Britannia" was predominantly concerned with the history and antiquities of Britain, and, probably as a result, the term chorography in English came to be particularly associated with antiquarian texts. William Lambarde, John Stow, John Hooker, Michael Drayton, Tristram Risdon, John Aubrey and many others used it in this way, arising from a gentlemanly topophilia and a sense of service to one's county or city, until it was eventually often applied to the genre of county history. A late example was William Grey's "Chorographia" (1649), a survey of the antiquities of the city of Newcastle upon Tyne. Even before Camden's work appeared, Andrew Melville in 1574 had referred to chorography and chronology as the "twa lights" [two lights] of history.
However, the term also continued to be used for maps and map-making, particularly of sub-national or county areas. William Camden praised the county mapmakers Christopher Saxton and John Norden as "most skilfull Chorographers"; and Robert Plot in 1677 and Christopher Packe in 1743 both referred to their county maps as chorographies.

By the beginning of the eighteenth century the term had largely fallen out of use in all these contexts, being superseded for most purposes by either "topography" or "cartography". Samuel Johnson in his "Dictionary" (1755) made a distinction between geography, chorography and topography, arguing that geography dealt with large areas, topography with small areas, but chorography with intermediary areas, being "less in its object than geography, and greater than topography". In practice, however, the term is only rarely found in English by this date.

In more technical geographical literature, the term had been abandoned as city views and city maps became more and more sophisticated and demanded a set of skills that required not only skilled draftsmanship but also some knowledge of scientific surveying. However, its use was revived for a second time in the late nineteenth century by the geographer Ferdinand von Richthofen. He regarded chorography as a specialization within geography, comprising the description through field observation of the particular traits of a given area.

The term is also now widely used by historians and literary scholars to refer to the early modern genre of topographical and antiquarian literature.




</doc>
<doc id="11023939" url="https://en.wikipedia.org/wiki?curid=11023939" title="Public humanities">
Public humanities

Public humanities is the work of engaging diverse publics in reflecting on heritage, traditions, and history, and the relevance of the humanities to the current conditions of civic and cultural life. Public humanities is often practiced within federal, state, nonprofit and community-based cultural organizations that engage people in conversations, facilitate and present lectures, exhibitions, performances and other programs for the general public on topics such as history, philosophy, popular culture and the arts. 

Workers within the public humanities endeavor to create physical and virtual spaces where the public can engage in conversation, learning and reflection about issues and ideas. Public humanities projects include exhibitions and programming related to historic preservation, oral history, archives, material culture, public art, cultural heritage, and cultural policy. One example of this type of project is the Humanities Truck, an experimental mobile platform for collecting, exhibiting, preserving, and expanding dialogue around the humanities in and around the Washington, D.C. area, and is sponsored by the Henry Luce Foundation and American University. The National Endowment for the Humanities notes that public humanities projects it has supported in the past include "interpretation at historic sites, television and radio productions, museum exhibitions, podcasts, short videos, digital games, websites, mobile apps, and other digital media." Many practitioners of public humanities are invested in ensuring the accessibility and relevance of the humanities to the general public or community groups.

The American Council of Learned Societies' National Task Force on Scholarship and the Public Humanities suggests that the nature of public humanities work is to teach the public the findings of academic scholarship: it sees "scholarship and the public humanities not as two distinct spheres but as parts of a single process, the process of taking private insight, testing it, and turning it into public knowledge." Others suggest a more balanced understanding of the ways in which history, heritage and culture are shared between the academy and the public, drawing on the notion of shared historical authority.

Subfields of the public humanities include public history, public sociology, public folklore, public anthropology, public philosophy, historic preservation, museum studies, museum education, cultural heritage management, community archaeology, public art, and public science.

Several universities have established programs in the public humanities (or have otherwise expressed commitments to public humanities via the creation of centers, degrees, or certificate programs with investments in various forms of "public" work), including:




</doc>
<doc id="12769389" url="https://en.wikipedia.org/wiki?curid=12769389" title="Humanities in the United States">
Humanities in the United States

Humanities in the United States refers to the study of humanities disciplines, such as literature, history, language, performing and visual arts or philosophy, in the United States of America.

Many American colleges and universities seek to provide a broad "liberal arts education", in which all college students to study the humanities in addition to their specific area of study. Prominent proponents of liberal arts in the United States have included Mortimer J. Adler and E.D. Hirsch. A liberal arts focus is often coupled with curricular requirements; colleges including Saint Anselm College and Providence College have mandatory two-year core curricula in the humanities for their students.

The 1980 United States Rockefeller Commission on the Humanities described the humanities in its report, "The Humanities in American Life":
Through the humanities we reflect on the fundamental question: What does it mean to be human? The humanities offer clues but never a complete answer. They reveal how people have tried to make moral, spiritual, and intellectual sense of a world in which irrationality, despair, loneliness, and death are as conspicuous as birth, friendship, hope, and reason.

The very concept of the ‘humanities’ as a class or kind, distinct from the ’sciences’, has come under repeated attack in the twentieth century. T.S. Kuhn’s "The Structure of Scientific Revolutions" argued that the forces driving scientific progress often have less to do with objective inference from unbiased observation than with much more value-laden sociological and cultural factors. More recently, Richard Rorty has argued that the distinction between the sciences and the humanities is harmful to both pursuits, placing the former on an undeserved pedestal and condemning the latter to irrationality. Rorty’s position requires a wholesale rejection of such traditional philosophical distinctions as those between appearance and reality, subjective and objective, replacing them with what he endorses as a new ‘fuzziness’. This leads to a kind of pragmatism where "the oppositions between the humanities, the arts, and the sciences, might gradually fade away... In this situation, ‘the humanities’ would no longer think of themselves as such..."

In the United States, the late 20th century saw a challenge to the "elitism" of the humanities, which Edward Said has characterized as a "conservative philosophy of gentlemanly refinement, or sensibility." Such postmodernists argued that the humanities should go beyond the study of "dead white males" to include work by women and people of color, and without religious bias. The French philosopher Michel Foucault has been a very influential part of this movement, stating in "The Order of Things" that "we can study only individuals, not human nature." However some in the humanities believed that such changes could be detrimental; the result is said to be what E. D. Hirsch Jr. refers to as declining cultural literacy.

President Lyndon Johnson signed the National Foundation on the Arts and Humanities Act in 1965, creating the National Council on the Humanities, and funded the National Endowment for the Humanities (NEH) in 1969. NEH is an independent grant-making agency of the United States government dedicated to supporting research, education, preservation, and public programs in the humanities (see Public humanities).

NEH facilitated the creation of State Humanities Councils in the 56 U.S. states and territories. Each council operates independently, defining the "humanities" in relationship to the disciplines, subjects, and values valued in the regions they serve. Councils give grant funds to individuals, scholars, and nonprofit organizations dedicated to the humanities in their region. Councils also offer diverse programs and services that respond to the needs of their communities and according to their own definitions of the humanities.

Criticism of the traditional humanities/liberal arts degree program has been leveled by critics who see them as both expensive and relatively "useless" in the modern American job market, where several years of specialized study is required in most job fields. According to a 2018 report by the Humanities Indicators, unemployment rates for humanities majors were slightly higher and their earnings were slightly lower than the averages for college degree recipients with similar degree levels (though both were still substantially better than for those without a college degree). Their overall levels of satisfaction with their jobs and their lives, however, were essentially the same as graduates from other fields, with more than 85% of humanities graduates reporting they were satisfied with their jobs. As of 2015, approximately five million people employed in management and professional jobs had bachelor’s degrees in the humanities.




</doc>
<doc id="16799902" url="https://en.wikipedia.org/wiki?curid=16799902" title="Hprints">
Hprints

hprints (pronounced in English as aitch prints) is an archive for electronic preprints of academic papers in the fields of arts and humanities. It can be accessed freely via the Internet since it is an open access repository aiming at making scholarly documents publicly available to the widest possible audience.

The aim of hprints is to make Nordic research available through an open access online electronic full text archive, but the limitation to Nordic countries is claimed to be mainly an initial restriction for funding reasons. The archive will primarily contain electronic research documents in the form of preprints, reprints, working papers, book chapters, conference reports, invited lecture manuscripts etc. The archive is set up, maintained and promoted by Copenhagen University Library and consortium members. The consortium original members were:
Submissions of electronic text material to the archive is decentral and take place at the local individual researcher, or research group level.

Hence hprints is a tool for scientific communication between academic scholars, who can upload full-text research material such as articles, papers, conference papers, book chapters etc. The content of the deposited material should be comparable to that of a scientific paper that a scholar would consider suitable for publication in for example a peer reviewed scientific journal.

It is possible to search and find the paper by defined topics through an Internet search. Secondly, all submitted papers are stored permanently and receive a stable web address, as for example the paper in this example:

May 2007 the Nordic funding agency for libraries, Nordbib, granted the hprints project 287,000 DKK as part of its financing programme "Work Package 2: Focus area on Content and Accessibility". The plan was to launch an archive one year from this date i.e. approximately June 2008: The hprints project wishes to provide a policy and a technical infrastructure that permits open access to research within the arts and humanities. The assumption was that this will result in a number of advantages with respect to the electronic accessibility and visibility of the arts and humanities research area.

October 2007, the Advisory Board of the Nordbib "hprints project" chose the system to be used for the Nordic arts and humanities e-print archive. Three possible alternatives existed: EPrints from the University of Southampton, LUR from Lund University Libraries, and HAL from the French national research council (CNRS). Both EPrints and LUR are free open source software that can be set up locally or hosted commercially, while HAL is a functioning archive, to which portals can be set up.

At the hprints Advisory Board meeting in October, it was decided to collaborate with the French research council, Centre National de la Recherche Scientifique (CNRS). Hence, the hprints e-print archive for Nordic arts and humanities is set up as a Nordic HAL portal with its own layout and adapted to the requirements of hprints.

March 2008 hprints opened for public access. Since the archive is a part of HAL and papers will be shared with the French national archive.




</doc>
<doc id="17956828" url="https://en.wikipedia.org/wiki?curid=17956828" title="List of people considered a founder in a Humanities field">
List of people considered a founder in a Humanities field

Those known as the father, mother, or considered a founder in a Humanities field are those who have made important contributions to that field. In some fields several people are considered the founders, while in others the title of being the "father" is debatable. Some of the people who have humanity are given in REFERENCES.


</doc>
<doc id="19027841" url="https://en.wikipedia.org/wiki?curid=19027841" title="Open Humanities Press">
Open Humanities Press

Open Humanities Press is an international open access publishing initiative in the humanities, specializing in critical and cultural theory. OHP's editorial board includes leading scholars such as Alain Badiou, Jonathan Culler, Stephen Greenblatt, Jean-Claude Guédon, J. Hillis Miller, Antonio Negri, Peter Suber and Gayatri Spivak among others.

The Open Humanities Press (OHP) is a scholar-led publishing initiative founded by Paul Ashton (Australia), Gary Hall (UK), Sigi Jöttkandt (Australia) and David Ottina (US). Its aim is to raise awareness of open access publishing in the humanities and to provide promotional and technical support to open access journals that have been invited by OHP's editorial oversight group to join the collective.

OHP launched in May 2008 with seven open access journals and was named a "beacon of hope" by the Public Library of Science. In August, 2009 OHP announced it will begin publishing open access book series edited by senior members of OHP's board. 

The monograph series are:

Journals

Open Humanities Press also host several open access journals, including the following:




</doc>
