<doc id="5178" url="https://en.wikipedia.org/wiki?curid=5178" title="Classics">
Classics

Classics or classical studies is the study of classical antiquity. It encompasses the study of the Greco-Roman world, particularly of its languages and literature (Ancient Greek and Classical Latin) but also of Greco-Roman philosophy, history, and archaeology. Traditionally in the West, the study of the Greek and Roman classics is considered one of the cornerstones of the humanities and a fundamental element of a rounded education. The study of classics has therefore traditionally been a cornerstone of a typical elite education.

Study encompasses specifically a time-period of history from the mid-2nd millennium BC to the 6th century AD.

The word "classics" is derived from the Latin adjective "classicus", meaning "belonging to the highest class of citizens". The word was originally used to describe the members of the highest class in ancient Rome. By the 2nd century AD the word was used in literary criticism to describe writers of the highest quality. For example, Aulus Gellius, in his "Attic Nights", contrasts "classicus" and "proletarius" writers. By the 6th century AD, the word had acquired a second meaning, referring to pupils at a school. Thus the two modern meanings of the word, referring both to literature considered to be of the highest quality, and to the standard texts used as part of a curriculum, both derive from Roman use.

In the Middle Ages, classics and education were tightly intertwined; according to Jan Ziolkowski, there is no era in history in which the link was tighter. Medieval education taught students to imitate earlier classical models, and Latin continued to be the language of scholarship and culture, despite the increasing difference between literary Latin and the vernacular languages of Europe during the period.

While Latin was hugely influential, however, Greek was barely studied, and Greek literature survived almost solely in Latin translation. The works of even major Greek authors such as Hesiod, whose names continued to be known by educated Europeans, were unavailable in the Middle Ages. In the thirteenth century, the English philosopher Roger Bacon wrote that "there are not four men in Latin Christendom who are acquainted with the Greek, Hebrew, and Arabic grammars."

Along with the unavailability of Greek authors, there were other differences between the classical canon known today and the works valued in the Middle Ages. Catullus, for instance, was almost entirely unknown in the medieval period. The popularity of different authors also waxed and waned throughout the period: Lucretius, popular during the Carolingian period, was barely read in the twelfth century, while for Quintilian the reverse is true.

The Renaissance led to the increasing study of both ancient literature and ancient history, as well as a revival of classical styles of Latin. From the 14th century, first in Italy and then increasingly across Europe, Renaissance Humanism, an intellectual movement that "advocated the study and imitation of classical antiquity", developed. Humanism saw a reform in education in Europe, introducing a wider range of Latin authors as well as bringing back the study of Greek language and literature to Western Europe. This reintroduction was initiated by Petrarch (1304–1374) and Boccaccio (1313–1375) who commissioned a Calabrian scholar to translate the Homeric poems. This humanist educational reform spread from Italy, in Catholic countries as it was adopted by the Jesuits, and in countries that became Protestant such as England, Germany, and the Low Countries, in order to ensure that future clerics were able to study the New Testament in the original language.

The late 17th and 18th centuries are the period in Western European literary history which is most associated with the classical tradition, as writers consciously adapted classical models. Classical models were so highly prized that the plays of William Shakespeare were rewritten along neoclassical lines, and these "improved" versions were performed throughout the 18th century.

From the beginning of the 18th century, the study of Greek became increasingly important relative to that of Latin.
In this period Johann Winckelmann's claims for the superiority of the Greek visual arts influenced a shift in aesthetic judgements, while in the literary sphere, G.E. Lessing "returned Homer to the centre of artistic achievement".
In the United Kingdom, the study of Greek in schools began in the late 18th century. The poet Walter Savage Landor claimed to have been one of the first English schoolboys to write in Greek during his time at Rugby School.

The 19th century saw the influence of the classical world, and the value of a classical education, decline, especially in the US, where the subject was often criticised for its elitism. By the 19th century, little new literature was still being written in Latin – a practice which had continued as late as the 18th century – and a command of Latin declined in importance. Correspondingly, classical education from the 19th century onwards began to increasingly de-emphasise the importance of the ability to write and speak Latin. In the United Kingdom this process took longer than elsewhere. Composition continued to be the dominant classical skill in England until the 1870s, when new areas within the discipline began to increase in popularity.
In the same decade came the first challenges to the requirement of Greek at the universities of Oxford and Cambridge, though it would not be finally abolished for another 50 years.

Though the influence of classics as the dominant mode of education in Europe and North America was in decline in the 19th century, the discipline was rapidly evolving in the same period. Classical scholarship was becoming more systematic and scientific, especially with the "new philology" created at the end of the 18th and beginning of the 19th century. Its scope was also broadening: it was during the 19th century that ancient history and classical archaeology began to be seen as part of classics rather than separate disciplines.

During the 20th century, the study of classics became less common. In England, for instance, Oxford and Cambridge universities stopped requiring students to have qualifications in Greek in 1920, and in Latin at the end of the 1950s. When the National Curriculum was introduced in England, Wales, and Northern Ireland in 1988, it did not mention the classics. By 2003, only about 10% of state schools in Britain offered any classical subjects to their students at all. In 2016, AQA, the largest exam board for A-Levels and GCSE's in England, Wales and Northern Ireland, announced that it would be scrapping A-Level subjects in Classical Civilization, Archaeology, and Art History. This left just one out of five exam boards in England which still offered Classical Civilization as a subject. The decision was immediately denounced by archaeologists and historians, with Natalie Haynes of the "Guardian" stating that the loss of the A-Level would deprive state school students, 93% of all students, the opportunity to study classics while making it once again the exclusive purview of wealthy private-school students.

However, the study of classics has not declined as fast elsewhere in Europe. In 2009, a review of "Meeting the Challenge", a collection of conference papers about the teaching of Latin in Europe, noted that though there is opposition to the teaching of Latin in Italy, it is nonetheless still compulsory in most secondary schools. The same can be said in the case of France or Greece, too. Indeed, Ancient Greek is one of the compulsory subjects in Greek secondary education, whereas in France, Latin is one of the optional subjects that can be chosen in a majority of middle schools and high schools. Ancient Greek is also still being taught, but not as much as Latin.

One of the most notable characteristics of the modern study of classics is the diversity of the field. Although traditionally focused on ancient Greece and Rome, the study now encompasses the entire ancient Mediterranean world, thus expanding the studies to Northern Africa as well as parts of the Middle East.

Philology is the study of language preserved in written sources; classical philology is thus concerned with understanding any texts from the classical period written in the classical languages of Latin and Greek.
The roots of classical philology lie in the Renaissance, as humanist intellectuals attempted to return to the Latin of the classical period, especially of Cicero, and as scholars attempted to produce more accurate editions of ancient texts.
Some of the principles of philology still used today developed during this period. For instance, the observation that if a manuscript could be shown to be a copy of an earlier extant manuscript, then it provides no further evidence of the original text, was made as early as 1489 by Angelo Poliziano.
Other philological tools took longer to be developed: the first statement, for instance, of the principle that a more difficult reading should be preferred over a simpler one, was in 1697 by Jean Le Clerc.

The modern discipline of classical philology began in Germany at the turn of the nineteenth century. It was during this period that scientific principles of philology began to be put together into a coherent whole, in order to provide a set of rules by which scholars could determine which manuscripts were most accurate. This "new philology", as it was known, centred around the construction of a genealogy of manuscripts, with which a hypothetical common ancestor, closer to the original text than any existing manuscript, could be reconstructed.

Classical archaeology is the oldest branch of archaeology, with its roots going back to J.J. Winckelmann's work on Herculaneum in the 1760s. It was not until the last decades of the 19th century, however, that classical archaeology became part of the tradition of Western classical scholarship. It was included as part of Cambridge University's Classical Tripos for the first time after the reforms of the 1880s, though it did not become part of Oxford's Greats until much later.

The second half of the 19th century saw Schliemann's excavations of Troy and Mycenae; the first excavations at Olympia and Delos; and Arthur Evans' work in Crete, particularly on Knossos. This period also saw the foundation of important archaeological associations (e.g. the Archaeological Institute of America in 1879), including many foreign archaeological institutes in Athens and Rome (the American School of Classical Studies at Athens in 1881, British School at Athens in 1886, American Academy in Rome in 1895, and British School at Rome in 1900).

More recently, classical archaeology has taken little part in the theoretical changes in the rest of the discipline, largely ignoring the popularity of "New Archaeology", which emphasised the development of general laws derived from studying material culture, in the 1960s. New Archaeology is still criticized by traditional minded scholars of classical archaeology despite a wide acceptance of its basic techniques.

Some art historians focus their study on the development of art in the classical world. Indeed, the art and architecture of Ancient Rome and Greece is very well regarded and remains at the heart of much of our art today. For example, Ancient Greek architecture gave us the Classical Orders: Doric, Ionic, and Corinthian. The Parthenon is still the architectural symbol of the classical world.

Greek sculpture is well known and we know the names of several Ancient Greek artists: for example, Phidias.

With philology, archaeology, and art history, scholars seek understanding of the history and culture of a civilisation, through critical study of the extant literary and physical artefacts, in order to compose and establish a continual historic narrative of the Ancient World and its peoples. The task is difficult due to a dearth of physical evidence: for example, Sparta was a leading Greek city-state, yet little evidence of it survives to study, and what is available comes from Athens, Sparta's principal rival; likewise, the Roman Empire destroyed most evidence (cultural artefacts) of earlier, conquered civilizations, such as that of the Etruscans.

The English word "philosophy" comes from the Greek word φιλοσοφία, meaning "love of wisdom", probably coined by Pythagoras. Along with the word itself, the discipline of philosophy as we know it today has its roots in ancient Greek thought, and according to Martin West "philosophy as we understand it is a Greek creation". Ancient philosophy was traditionally divided into three branches: logic, physics, and ethics. However, not all of the works of ancient philosophers fit neatly into one of these three branches. For instance, Aristotle's "Rhetoric" and "Poetics" have been traditionally classified in the West as "ethics", but in the Arabic world were grouped with logic; in reality, they do not fit neatly into either category.

From the last decade of the eighteenth century, scholars of ancient philosophy began to study the discipline historically. Previously, works on ancient philosophy had been unconcerned with chronological sequence and with reconstructing the reasoning of ancient thinkers; with what Wolfgang-Ranier Mann calls "New Philosophy", this changed.

A relatively recent new discipline within the classics is "reception studies", which developed in the 1960s at the University of Konstanz.
Reception studies is concerned with how students of classical texts have understood and interpreted them.
As such, reception studies is interested in a two-way interaction between reader and text, taking place within a historical context.

Though the idea of an "aesthetics of reception" was first put forward by Hans Robert Jauss in 1967, the principles of reception theory go back much earlier than this.
As early as 1920, T.S. Eliot wrote that "the past [is] altered by the present as much as the present is directed by the past"; Charles Martindale describes this as a "cardinal principle" for many versions of modern reception theory.

Ancient Greece was the civilization belonging to the period of Greek history lasting from the Archaic period, beginning in the eighth century BC, to the Roman conquest of Greece after the Battle of Corinth in 146 BC. The Classical period, during the fifth and fourth centuries BC, has traditionally been considered the height of Greek civilisation. The Classical period of Greek history is generally considered to have begun with the first and second Persian invasions of Greece at the start of the Greco-Persian wars, and to have ended with the death of Alexander the Great.

Classical Greek culture had a powerful influence on the Roman Empire, which carried a version of it to many parts of the Mediterranean region and Europe; thus Classical Greece is generally considered to be the seminal culture which provided the foundation of Western civilization.

Ancient Greek is the historical stage in the development of the Greek language spanning the Archaic (c. 8th to 6th centuries BC), Classical (c. 5th to 4th centuries BC), and Hellenistic (c. 3rd century BC to 6th century AD) periods of ancient Greece and the ancient world. It is predated in the 2nd millennium BC by Mycenaean Greek. Its Hellenistic phase is known as Koine ("common") or Biblical Greek, and its late period mutates imperceptibly into Medieval Greek. Koine is regarded as a separate historical stage of its own, although in its earlier form it closely resembles Classical Greek. Prior to the Koine period, Greek of the classical and earlier periods included several regional dialects.

Ancient Greek was the language of Homer and of classical Athenian historians, playwrights, and philosophers. It has contributed many words to the vocabulary of English and many other European languages, and has been a standard subject of study in Western educational institutions since the Renaissance. Latinized forms of Ancient Greek roots are used in many of the scientific names of species and in other scientific terminology.

The earliest surviving works of Greek literature are epic poetry. Homer's "Iliad" and "Odyssey" are the earliest to survive to us today, probably composed in the eighth century BC. These early epics were oral compositions, created without the use of writing.
Around the same time that the Homeric epics were composed, the Greek alphabet was introduced; the earliest surviving inscriptions date from around 750 BC.
European drama was invented in ancient Greece. Traditionally this was attributed to Thespis, around the middle of the sixth century BC, though the earliest surviving work of Greek drama is Aeschylus' tragedy "The Persians", which dates to 472 BC. Early Greek tragedy was performed by a chorus and two actors, but by the end of Aeschylus' life, a third actor had been introduced, either by him or by Sophocles. The last surviving Greek tragedies are the "Bacchae" of Euripides and Sophocles' Oedipus at Colonus, both from the end of the fifth century BC.

Surviving Greek comedy begins later than tragedy; the earliest surviving work, Aristophanes' "Acharnians", comes from 425 BC. However, comedy dates back as early as 486 BC, when the Dionysia added a competition for comedy to the much earlier competition for tragedy. The comedy of the fifth century is known as Old Comedy, and it comes down to us solely in the eleven surviving plays of Aristophanes, along with a few fragments. Sixty years after the end of Aristophanes' career, the next author of comedies to have any substantial body of work survive is Menander, whose style is known as New Comedy.

Two historians flourished during Greece's classical age: Herodotus and Thucydides. Herodotus is commonly called the father of history, and his "History" contains the first truly literary use of prose in Western literature. Of the two, Thucydides was the more careful historian. His critical use of sources, inclusion of documents, and laborious research made his History of the Peloponnesian War a significant influence on later generations of historians. The greatest achievement of the 4th century was in philosophy. There were many Greek philosophers, but three names tower above the rest: Socrates, Plato, and Aristotle. These have had a profound influence on Western society.

Greek mythology is the body of myths and legends belonging to the ancient Greeks concerning their gods and heroes, the nature of the world, and the origins and significance of their own cult and ritual practices. They were a part of religion in ancient Greece. Modern scholars refer to the myths and study them in an attempt to throw light on the religious and political institutions of Ancient Greece and its civilization, and to gain understanding of the nature of myth-making itself.

Greek religion encompassed the collection of beliefs and rituals practiced in ancient Greece in the form of both popular public religion and cult practices. These different groups varied enough for it to be possible to speak of Greek religions or "cults" in the plural, though most of them shared similarities. Also, the Greek religion extended out of Greece and out to neighbouring islands.

Many Greek people recognized the major gods and goddesses: Zeus, Poseidon, Hades, Apollo, Artemis, Aphrodite, Ares, Dionysus, Hephaestus, Athena, Hermes, Demeter, Hestia and Hera; though philosophies such as Stoicism and some forms of Platonism used language that seems to posit a transcendent single deity. Different cities often worshipped the same deities, sometimes with epithets that distinguished them and specified their local nature.

The earliest surviving philosophy from ancient Greece dates back to the 6th century BC, when according to Aristotle Thales of Miletus was considered to have been the first Greek philosopher. Other influential pre-Socratic philosophers include Pythagoras and Heraclitus. The most famous and significant figures in classical Athenian philosophy, from the 5th to the 3rd centuries BC, are Socrates, his student Plato, and Aristotle, who studied at Plato's Academy before founding his own school, known as the Lyceum. Later Greek schools of philosophy, including the Cynics, Stoics, and Epicureans, continued to be influential after the Roman annexation of Greece, and into the post-Classical world.

Greek philosophy dealt with a wide variety of subjects, including political philosophy, ethics, metaphysics, ontology, and logic, as well as disciplines which are not today thought of as part of philosophy, such as biology and rhetoric.

The language of ancient Rome was Latin, a member of the Italic family of languages. The earliest surviving inscription in Latin comes from the 7th century BC, on a brooch from Palestrina. Latin from between this point and the early 1st century BC is known as Old Latin. Most surviving Latin literature is Classical Latin, from the 1st century BC to the 2nd century AD. Latin then evolved into Late Latin, in use during the late antique period. Late Latin survived long after the end of classical antiquity, and was finally replaced by written Romance languages around the 9th century AD. Along with literary forms of Latin, there existed various vernacular dialects, generally known as Vulgar Latin, in use throughout antiquity. These are mainly preserved in sources such as graffiti and the Vindolanda tablets.

The earliest surviving Latin authors, writing in Old Latin, include the playwrights Plautus and Terence. Much of the best known and most highly thought of Latin literature comes from the classical period, with poets such as Virgil, Horace, and Ovid; historians such as Julius Caesar and Tacitus; orators such as Cicero; and philosophers such as Seneca the Younger and Lucretius. Late Latin authors include many Christian writers such as Lactantius, Tertullian and Ambrose; non-Christian authors, such as the historian Ammianus Marcellinus, are also preserved.

According to legend, the city of Rome was founded in 753 BC; in reality, there had been a settlement on the site since around 1000 BC, when the Palatine Hill was settled. The city was originally ruled by kings, first Roman, and then Etruscan – according to Roman tradition, the first Etruscan king of Rome, Tarquinius Priscus, ruled from 616 BC. Over the course of the 6th century BC, the city expanded its influence over the entirety of Latium. Around the end of the 6th century – traditionally in 510 BC – the kings of Rome were driven out, and the city became a republic.

Around 387 BC, Rome was sacked by the Gauls following the Battle of the Allia. It soon recovered from this humiliating defeat, however, and in 381 the inhabitants of Tusculum in Latium were made Roman citizens. This was the first time Roman citizenship was extended in this way. Rome went on to expand its area of influence, until by 269 the entirety of the Italian peninsula was under Roman rule. Soon afterwards, in 264, the First Punic War began; it lasted until 241. The Second Punic War began in 218, and by the end of that year, the Carthaginian general Hannibal had invaded Italy. The war saw Rome's worst defeat to that point at Cannae; the largest army Rome had yet put into the field was wiped out, and one of the two consuls leading it was killed. However, Rome continued to fight, annexing much of Spain and eventually defeating Carthage, ending her position as a major power and securing Roman preeminence in the Western Mediterranean.
The classical languages of the Ancient Mediterranean world influenced every European language, imparting to each a learned vocabulary of international application. Thus, Latin grew from a highly developed cultural product of the Golden and Silver eras of Latin literature to become the "international lingua franca" in matters diplomatic, scientific, philosophic and religious, until the 17th century. Long before this, Latin had evolved into the Romance languages and Ancient Greek into Modern Greek and its dialects. In the specialised science and technology vocabularies, the influence of Latin and Greek is notable. Ecclesiastical Latin, the Roman Catholic Church's official language, remains a living legacy of the classical world in the contemporary world.

Latin had an impact far beyond the classical world. It continued to be the pre-eminent language for serious writings in Europe long after the fall of the Roman empire. The modern Romance languages – such as French, Spanish, and Italian – all derive from Latin. Latin is still seen as a foundational aspect of European culture.

The legacy of the classical world is not confined to the influence of classical languages. The Roman empire was taken as a model by later European empires, such as the Spanish and British empires. Classical art has been taken as a model in later periods – medieval Romanesque architecture and Enlightenment-era neoclassical literature were both influenced by classical models, to take but two examples, while Joyce's "Ulysses" is one of the most influential works of twentieth century literature.





</doc>
<doc id="22101040" url="https://en.wikipedia.org/wiki?curid=22101040" title="Caucasology">
Caucasology

Caucasology, or Caucasiology refers to the historical and geopolitical studies of Caucasus region. The branch has more than 150 years history. In 1972, the Caucasiological Center (renamed to International Caucasiological Center in 2000) was founded under the auspices of the Israel President Zalman Shazar.




</doc>
<doc id="1828083" url="https://en.wikipedia.org/wiki?curid=1828083" title="Linguistic turn">
Linguistic turn

The linguistic turn was a major development in Western philosophy during the early 20th century, the most important characteristic of which is the focusing of philosophy and the other humanities primarily on the relationship between philosophy and language.

Very different intellectual movements were associated with the "linguistic turn", although the term itself is commonly thought popularised by Richard Rorty's 1967 anthology "The Linguistic Turn", in which it means the turn towards linguistic philosophy. According to Rorty, who later dissociated himself from linguistic philosophy and analytic philosophy generally, the phrase "the linguistic turn" originated with philosopher Gustav Bergmann.

Traditionally, the linguistic turn is taken to also mean the birth of analytic philosophy. One of the results of the linguistic turn was an increasing focus on logic and philosophy of language, and the cleavage between ideal language philosophy and ordinary language philosophy.

According to Michael Dummett, the linguistic turn can be dated to Gottlob Frege's 1884 work "The Foundations of Arithmetic", specifically paragraph 62 where Frege explores the identity of a numerical proposition. 

In order to answer a Kantian question about numbers, "How are numbers given to us, granted that we have no idea or intuition of them?" Frege invokes his "context principle", stated at the beginning of the book, that only in the context of a proposition do words have meaning, and thus finds the solution to be in defining "the sense of a proposition in which a number word occurs." Thus an ontological and epistemological problem, traditionally solved along idealist lines, is instead solved along linguistic ones.

This concern for the logic of propositions and their relationship to "facts" was later taken up by the notable analytic philosopher Bertrand Russell in "On Denoting", and played a weighty role in his early work in logical atomism.

Ludwig Wittgenstein, an associate of Russell, was one of the progenitors of the linguistic turn. This follows from his ideas in his "Tractatus Logico-Philosophicus" that philosophical problems arise from a misunderstanding of the logic of language, and from his remarks on language games in his later work. His later work (specifically "Philosophical Investigations") significantly departs from the common tenets of analytic philosophy and might be viewed as having some resonance in the post-structuralist tradition. 

W. V. O. Quine describes the historical continuity of the linguistic turn with earlier philosophy in "Two Dogmas of Empiricism": "Meaning is what essence becomes when it is divorced from the object of reference and wedded to the word."

Later in the twentieth century, philosophers like Saul Kripke in "Naming and Necessity" drew metaphysical conclusions from closely analyzing language.

Decisive for the linguistic turn in the humanities were the works of yet another tradition, namely the Continental structuralism of Ferdinand de Saussure. Structuralism was the initial outcome of Saussure's linguistic turn, which later led to poststructuralism with the input of Friedrich Nietzsche's ideas. Influential poststructuralist theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Gilles Deleuze, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain rhetorical tropes, in historical discourse was explored by Hayden White.

These various movements often lead to the notion that language 'constitutes' reality, a position contrary to intuition and to most of the Western tradition of philosophy. The traditional view (what Derrida called the 'metaphysical' core of Western thought) saw words as functioning labels attached to concepts. According to this view, there is something like 'the "real" chair', which exists in some external reality and corresponds roughly with a concept in human thought, "chair", to which the linguistic word "chair" refers. However, the founder of structuralism, Ferdinand de Saussure, held that definitions of concepts cannot exist independently from a linguistic system defined by difference, or, to put it differently, that a concept of something cannot exist without being named. Thus differences between meanings structure our perception; there is no "real" chair except insofar as we are manipulating symbolic systems. We would not even be able to recognize a chair "as" a chair without simultaneously recognising that a chair is "not" everything else - in other words a chair is defined as being a specific collection of characteristics which are themselves defined in certain ways, and so on, and all of this within the symbolic system of language. Thus, a large part of what we think of as "reality" is really a convention of naming and characterising, a convention which is itself called "language".





</doc>
<doc id="1072404" url="https://en.wikipedia.org/wiki?curid=1072404" title="World community">
World community

The term world community is used primarily in political and humanitarian contexts to describe an international aggregate of nation states of widely varying types. In most connotations, the term is used to convey meanings attached to consensus or inclusion of all people in all lands and their governments.

World community often is a semi-personal rhetorical connotation that represents Humanity in a singular context as in "…for the sake of the World Community" or "…with the approval of the World Community".

The term sometimes is used to reference the United Nations or its affiliated agencies as bodies of governance. Other times it is a generic term with no explicit ties to states or governments but retaining a political connotation.

In terms of human needs, humanitarian aid, human rights, and other discourse in the humanities, the world community is akin to the conceptual Global village aimed at the inclusion of non-aligned countries, aboriginal peoples, the Third World into the "connected" world via the communications infrastructure or at least representative ties to it.

In terms of the World economy, the "world community" is seen by some economists as an inter-dependent system of goods and services with semi-permeable boundaries and flexible sets of import/export rules. Proponents of Globalization may tend to establish or impose more rigidity to this framework. Controversy has arisen as to whether this paradigm will strengthen or weaken the world as a community. See World Trade Organization

When considering Sustainable development and Ecology, the inter-dependence angle generally expands quickly to a Global context. In this paradigm, the planet as a whole represents a single Biome and the World's population represents the Ecological succession in a singular eco-system. This also can be recognized as the World Community.

Many religions have taken on the challenge of establishing a world community through the propagation of their doctrine, faith and practice.

In the Bahá'í Faith, `Abdu'l-Bahá, successor and son of Bahá'u'lláh, produced a series of documents called the Tablets of the Divine Plan. Now in a book form, after their publication in 1917 and their 'unveiling' in New York in 1919, these tablets contained an outline and a plan for the expansion of the Bahá'í community into Asia, Asia minor, Europe and the Americas, indeed, throughout the planet.

The formal implementation of this plan, known as 'Abdu'l-Baha's Divine Plan, began in 1937 in the first of a series of sub-plans, the Seven Year Plan of 1937 to 1944. Shoghi Effendi, the leader of the Baha'i community until 1957 and then the Universal House of Justice from 1963, were instrumental in the organization and design of future sub-plans. This led to the creation and establishment of a world community, with members of the faith estimated to have reached 5 to 6 million in the early 21st century, while also being the second most geographically widespread religion in the world.

In Buddhism "the conventional Sangha of monks has been entrusted by the Buddha with the task of leading all people in creating the ideal world community of noble disciples or truly civilized people."

A Benedictine monk, Friar John Main, inspired the World Community for Christian meditation through the practice of meditation centered around the Maranatha mantra, meaning "Come Lord."

The Lutheran Church in America had issued a social statement - "World Community: Ethical Imperatives in an Age of Interdependence" Adopted by the Fifth Biennial Convention, Minneapolis, Minnesota, June 25-July 2, 1970. Since then The Evangelical Lutheran Church in America has formed and retained the statement as a 'historical document'.

The term world community is often used in the context of establishing and maintaining world peace through a peace process or through a resolvable end to local-regional wars and global-world wars. Many social movements and much political theory deals with issues revolving around the institutionalization of the process of propagating the ideal of a world community. A world community is one which has a global vision, is established throughout the world, that is it has a membership that exists in most of the countries on the planet and that involves the participation of its members in a variety of ways.




</doc>
<doc id="51319" url="https://en.wikipedia.org/wiki?curid=51319" title="Intellectual history">
Intellectual history

Intellectual history refers to the history of ideas and thinkers. This history cannot be considered without the knowledge of the humans who created, discussed, wrote about, and in other ways were concerned with ideas. Intellectual history as practiced by historians is parallel to the history of philosophy as done by philosophers, and is more akin to the history of ideas. Its central premise is that ideas do not develop in isolation from the people who developed and use them, and that one must study ideas not only as abstract propositions but also in terms of the culture, lives, and historical contexts.

Intellectual history aims to understand ideas from the past by putting them in context. The term "context" in the preceding sentence is ambiguous: it can be political, cultural, intellectual, and social. One can read a text both in terms of a chronological context (for example, as a contribution to a discipline or tradition as it extended over time) or in terms of a contemporary intellectual moment (for example, as participating in a debate particular to a certain time and place). Both of these acts of contextualization are typical of what intellectual historians do, nor are they exclusive. Generally speaking, intellectual historians seek to place concepts and texts from the past in multiple contexts.

It is important to realize that intellectual history is not just the history of intellectuals. It studies ideas as they are expressed in texts, and as such is different from other forms of cultural history which deal also with visual and other non-verbal forms of evidence. Any written trace from the past can be the object of intellectual history. The concept of the "intellectual" is relatively recent, and suggests someone professionally concerned with thought. Instead, anyone who has put pen to paper to explore his or her thoughts can be the object of intellectual history. A famous example of an intellectual history of a non-canonical thinker is Carlo Ginzburg's study of a 16th-century Italian miller, Menocchio, in his seminal work "The Cheese and the Worms".

Although the field emerged from European disciplines of "Kulturgeschichte" and "Geistesgeschichte", the historical study of ideas has engaged not only western intellectual traditions but others as well, including those in other parts of the world. Increasingly, historians are calling for a Global intellectual history that will show the parallels and interrelations in the history of thought of all human societies. Another important trend has been the history of the book and of reading, which has drawn attention to the material aspects of how books were designed, produced, distributed, and read.

Intellectual history as a self-conscious discipline is a relatively recent phenomenon. It has precedents, however, in the history of philosophy, the history of ideas, and in cultural history as practiced since Burckhardt or indeed since Voltaire. The history of the human mind, as it was called in the eighteenth century, was of great concern to scholars and philosophers, and their efforts can in part be traced to Francis Bacon’s call for what he termed a literary history in his The Advancement of Learning. In economics, John Maynard Keynes (1883-1946) was both a historian of economic thought himself, and the subject of study by historians of economic thought because of the significance of the Keynesian revolution. However, the discipline of intellectual history as it is now understood emerged only in the immediate postwar period, in its earlier incarnation as "the history of ideas" under the leadership of Arthur Lovejoy, the founder of the Journal of the History of Ideas. Since that time, Lovejoy's formulation of "unit-ideas" has been developed in different and often diverging directions, some of which more historically sensitive accounts of intellectual activity as historically situated (contextualism), and this shift is reflected in the replacement of the phrase history of ideas by "intellectual history".

Intellectual history includes the history of thought in many disciplines, such as the history of philosophy, and the history of economic thought. Analytical concepts - such as the nature of paradigms and the causes of paradigm shifts - have been borrowed from the study of other disciplines, exemplified by the use of the ideas of Thomas Kuhn about The Structure of Scientific Revolutions to explain revolutions in thought in economics and other disciplines.

In Britain the history of political thought has been a particular focus since the late 1960s and is associated especially with historians at Cambridge, such as John Dunn and Quentin Skinner. They studied European political thought in its historical context, emphasizing the emergence and development of such concepts as the state and freedom. Skinner in particular is renowned for his provocative methodological essays, which were and are widely read by philosophers and practitioners of other humanistic disciplines, and did much to give prominence to the practice of intellectual history. 

In the United States, intellectual history is understood more broadly to encompass many different forms of intellectual output, not just the history of political ideas, and it includes such fields as the history of historical thought, associated especially with Anthony Grafton of Princeton University and J.G.A. Pocock of Johns Hopkins University. Formalized in 2010, the History and Culture Ph.D. at Drew University is one of a few graduate programs in the US currently specializing in intellectual history, both in its American and European contexts. Despite the prominence of early modern intellectual historians (those studying the age from the Renaissance to the Enlightenment), the intellectual history of the modern period has also been the locus of intense and creative output on both sides of the Atlantic. Prominent examples of such work include Louis Menand's "" and Martin Jay's "The Dialectical Imagination".

In continental Europe, equivalents of intellectual history can be found. An example is Reinhart Koselleck’s "Begriffsgeschichte" (history of concepts), though there are methodological differences between the work of Koselleck and his followers and the work of Anglo-American intellectual historians.







</doc>
<doc id="26711608" url="https://en.wikipedia.org/wiki?curid=26711608" title="School of Letters">
School of Letters

The School of Letters was a summer institute and degree-granting (M.A. and Ph.D. minor) program at Indiana University, Bloomington. The School moved from Kenyon College in 1951 following the withdrawal of funding of the School of English by the Rockefeller Foundation. I.U. President Herman B. Wells obtained funding from the University and located the School under the administration of Dean John W. Ashton of the College of Arts and Sciences. The School opened under the direction of Prof. Richard B. Hudson and then transitioned to Prof. Newton P. 'Stalky' Stallknecht until his retirement and the School's dissolution in 1972.

When Indiana University moved the School from Kenyon to Bloomington they maintained John Crowe Ransom, Lionel Trilling, Philip Rahv, Austin Warren, and Allen Tate as senior fellows, all well-known literary scholars. The Kenyon School of English was founded by three senior fellows, John Crowe Ransom, F. O. Matthiessen and Lionel Trilling and was held during a summer session at Kenyon College from 1948 until 1950. The first session of the School of Letters in Bloomington ran from June 21 to August 1, 1951.

Students at the School of Letters included James M. Cox '51 (later would become a faculty member at Indiana University and the School), Martha Banta '62, Bruce Jackson '62, Paul Lauter '55, and Geoffrey H. Hartman '51. The School existed during a period of major change within the field of literary studies from the dominance of New Criticism until the rise of post-structuralism. During each session of the School high profile academics, poets, and critics were brought to Bloomington to teach seminars and deliver weekly forum lectures. These instructors included Northrop Frye, William Empson, John Berryman, Robert Lowell, Leslie Fiedler, and R. P. Blackmur.


</doc>
<doc id="81724" url="https://en.wikipedia.org/wiki?curid=81724" title="Oral tradition">
Oral tradition

Oral tradition, or oral lore, is a form of human communication wherein knowledge, art, ideas and cultural material is received, preserved and transmitted orally from one generation to another. The transmission is through speech or song and may include folktales, ballads, chants, prose or verses. In this way, it is possible for a society to transmit oral history, oral literature, oral law and other knowledge across generations without a writing system, or in parallel to a writing system. Religions such as Buddhism, Hinduism and Jainism, for example, have used an oral tradition, in parallel to a writing system, to transmit their canonical scriptures, secular knowledge such as Sushruta Samhita, hymns and mythologies from one generation to the next.

Oral tradition is information, memories and knowledge held in common by a group of people, over many generations, and it is not the same as testimony or oral history. In a general sense, "oral tradition" refers to the recall and transmission of a specific, preserved textual and cultural knowledge through vocal utterance. As an academic discipline, it refers both to a set of objects of study and a method by which they are studied.

The study of oral tradition is distinct from the academic discipline of oral history, which is the recording of personal memories and histories of those who experienced historical eras or events. Oral tradition is also distinct from the study of orality defined as thought and its verbal expression in societies where the technologies of literacy (especially writing and print) are unfamiliar to most of the population. A folklore is a type of oral tradition, but knowledge other than folklore has been orally transmitted and thus preserved in human history.

According to John Foley, oral tradition has been an ancient human tradition found in "all corners of the world". Modern archaeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures:

In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques. Some scholars such as Jack Goody state that the Vedas are not the product strictly of an oral tradition, basing this view by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a "parallel products of a literate society".

In ancient Greece, the oral tradition was a dominant tradition. Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally. As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience, but making the historicity embedded in the oral tradition as unreliable. The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition. The Torah and other ancient Jewish literature, the Judeo-Christian Bible and texts of early centuries of Christianity are rooted in an oral tradition, and the term "People of the Book" is a medieval construct. This is evidenced, for example, by the multiple scriptural statements by Paul admitting "previously remembered tradition which he received" orally.

Writing systems are not known to exist among Native North Americans before contact with Europeans. Oral storytelling traditions flourished in a context without the use of writing to record and preserve history, scientific knowledge, and social practices. While some stories were told for amusement and leisure, most functioned as practical lessons from tribal experience applied to immediate moral, social, psychological, and environmental issues. Stories fuse fictional, supernatural, or otherwise exaggerated characters and circumstances with real emotions and morals as a means of teaching. Plots often reflect real life situations and may be aimed at particular people known by the story's audience. In this way, social pressure could be exerted without directly causing embarrassment or social exclusion. For example, rather than yelling, Inuit parents might deter their children from wandering too close to the water's edge by telling a story about a sea monster with a pouch for children within its reach. One single story could provide dozens of lessons. Stories were also used as a means to assess whether traditional cultural ideas and practices are effective in tackling contemporary circumstances or if they should be revised.

Native American storytelling is a collaborative experience between storyteller and listeners. Native American tribes generally have not had professional tribal storytellers marked by social status. Stories could and can be told by anyone, with each storyteller using their own vocal inflections, word choice, content, or form. Storytellers not only draw upon their own memories, but also upon a collective or tribal memory extending beyond personal experience but nevertheless representing a shared reality. Native languages have in some cases up to twenty words to describe physical features like rain or snow and can describe the spectra of human emotion in very precise ways, allowing storytellers to offer their own personalized take on a story based on their own lived experiences. Fluidity in story deliverance allowed stories to be applied to different social circumstances according to the storyteller's objective at the time. One's rendition of a story was often considered a response to another's rendition, with plot alterations suggesting alternative ways of applying traditional ideas to present conditions. Listeners might have heard the story told many times, or even may have told the same story themselves. This does not take away from a story's meaning, as curiosity about what happens next was less of a priority than hearing fresh perspectives on well-known themes and plots. Elder storytellers generally were not concerned with discrepancies between their version of historical events and neighboring tribes' version of similar events, such as in origin stories. Tribal stories are considered valid within the tribe's own frame of reference and tribal experience.

Stories are used to preserve and transmit both tribal history and environmental history, which are often closely linked. Native oral traditions in the Pacific Northwest, for example, describe natural disasters like earthquakes and tsunamis. Various cultures from Vancouver Island and Washington have stories describing a physical struggle between a Thunderbird and a Whale. One such story tells of the Thunderbird, which can create thunder by moving just a feather, piercing the Whale's flesh with its talons, causing the Whale to dive to the bottom of the ocean, bringing the Thunderbird with it. Another depicts the Thunderbird lifting the Whale from the Earth then dropping it back down. Regional similarities in themes and characters suggests that these stories mutually describe the lived experience of earthquakes and floods within tribal memory. According to one story from the Suquamish Tribe, Agate Pass was created when an earthquake expanded the channel as a result of an underwater battle between a serpent and bird. Other stories in the region depict the formation of glacial valleys and moraines and the occurrence of landslides, with stories being used in at least one case to identify and date earthquakes that occurred in CE 900 and 1700. Further examples include Arikara origin stories of emergence form an 'underworld' of persistent darkness, which may represent the remembrance of life in the Arctic Circle during the last ice age, and stories involving a 'deep crevice,' which may refer to the Grand Canyon. Despite such examples of agreement between geological and archeological records on one hand and Native oral records on the other, some scholars have cautioned against the historical validity of oral traditions because of their susceptibility to detail alteration over time and lack of precise dates. The Native American Graves Protection and Repatriation Act considers oral traditions as a viable source of evidence for establishing the affiliation between cultural objects and Native Nations.

Oral traditions face the challenge of accurate transmission and verifiability of the accurate version, particularly when the culture lacks written language or has limited access to writing tools. Oral cultures have employed various strategies that achieve this without writing. For example, a heavily rhythmic speech filled with mnemonic devices enhances memory and recall. A few useful mnemonic devices include alliteration, repetition, assonance, and proverbial sayings. In addition, the verse is often metrically composed with an exact number of syllables or morae - such as with Greek and Latin prosody and in Chandas found in Hindu and Buddhist texts. The verses of the epic or text are typically designed wherein the long and short syllables are repeated by certain rules, so that if an error or inadvertent change is made, an internal examination of the verse reveals the problem. Oral Traditions are able to be passed on through means of plays and acting which can be shown in the modern day Cameroon by the Graffis or Grasslanders how act out and deliver speeches to spread their history in the manner of Oral Tradition. Such strategies help facilitate transmission of information from individual to individual without a written intermediate, and they can also be applied to oral governance.

Rudyard Kipling's The Jungle Book provides an excellent demonstration of oral governance in the Law of the Jungle. Not only does grounding rules in oral proverbs allow for simple transmission and understanding, but it also legitimizes new rulings by allowing extrapolation. These stories, traditions, and proverbs are not static, but are often altered upon each transmission barring the overall meaning remains intact. In this way, the rules that govern the people are modified by the whole and not authored by a single entity.

Ancient texts of Hinduism, Buddhism and Jainism were preserved and transmitted by an oral tradition. For example, the śrutis of Hinduism called the Vedas, the oldest of which trace back to the second millennium BCE. Michael Witzel explains this oral tradition as follows:

Ancient Indians developed techniques for listening, memorization and recitation of their knowledge, in schools called Gurukul, while maintaining exceptional accuracy of their knowledge across the generations. Many forms of recitation or "paths" were designed to aid accuracy in recitation and the transmission of the "Vedas" and other knowledge texts from one generation to the next. All hymns in each Veda were recited in this way; for example, all 1,028 hymns with 10,600 verses of the Rigveda was preserved in this way; as were all other Vedas including the Principal Upanishads, as well as the Vedangas. Each text was recited in a number of ways, to ensure that the different methods of recitation acted as a cross check on the other. Pierre-Sylvain Filliozat summarizes this as follows:

These extraordinary retention techniques guaranteed an accurate Śruti, fixed across the generations, not just in terms of unaltered word order but also in terms of sound. That these methods have been effective, is testified to by the preservation of the most ancient Indian religious text, the "" (ca. 1500 BCE).

The following overview draws upon "Oral-Formulaic Theory and Research: An Introduction and Annotated Bibliography", (NY: Garland Publishing, 1985, 1986, 1989); additional material is summarized from the overlapping prefaces to the following volumes: "The Theory of Oral Composition: History and Methodology", (Indiana University Press, 1988, 1992); "Immanent Art: From Structure to Meaning in Traditional Oral Epic" (Bloomington: Indiana University Press, 1991); "The Singer of Tales in Performance" (Bloomington: Indiana University Press, 1995) and "Comparative Research on Oral Traditions: A Memorial for Milman Parry" (Columbus, Ohio: Slavica Publishers, 1987). in the work of the Serb scholar Vuk Stefanović Karadžić (1787–1864), a contemporary and friend of the Brothers Grimm. Vuk pursued similar projects of "salvage folklore" (similar to rescue archaeology) in the cognate traditions of the Southern Slavic regions which would later be gathered into Yugoslavia, and with the same admixture of romantic and nationalistic interests (he considered all those speaking the Eastern Herzegovinian dialect as Serbs). Somewhat later, but as part of the same scholarly enterprise of nationalist studies in folklore, the turcologist Vasily Radlov (1837–1918) would study the songs of the Kara-Kirghiz in what would later become the Soviet Union; Karadzic and Radloff would provide models for the work of Parry.

In a separate development, the media theorist Marshall McLuhan (1911–1980) would begin to focus attention on the ways that communicative media shape the nature of the content conveyed. He would serve as mentor to the Jesuit, Walter Ong (1912–2003), whose interests in cultural history, psychology and rhetoric would result in "Orality and Literacy" (Methuen, 1980) and the important but less-known "Fighting for Life: Contest, Sexuality and Consciousness" (Cornell, 1981) These two works articulated the contrasts between cultures defined by primary orality, writing, print, and the secondary orality of the electronic age.

Ong's works also made possible an integrated theory of oral tradition which accounted for both production of content (the chief concern of Parry-Lord theory) and its reception. This approach, like McLuhan's, kept the field open not just to the study of aesthetic culture but to the way physical and behavioral artifacts of oral societies are used to store, manage and transmit knowledge, so that oral tradition provides methods for investigation of cultural differences, other than the purely verbal, between oral and literate societies.

The most-often studied section of "Orality and Literacy" concerns the "psychodynamics of orality" This chapter seeks to define the fundamental characteristics of 'primary' orality and summarizes a series of descriptors (including but not limited to verbal aspects of culture) which might be used to index the relative orality or literacy of a given text or society.

In advance of Ong's synthesis, John Miles Foley began a series of papers based on his own fieldwork on South Slavic oral genres, emphasizing the dynamics of performers and audiences. Foley effectively consolidated oral tradition as an academic field when he compiled "Oral-Formulaic Theory and Research" in 1985. The bibliography gives a summary of the progress scholars made in evaluating the oral tradition up to that point, and includes a list of all relevant scholarly articles relating to the theory of Oral-Formulaic Composition. He also both established both the journal "Oral Tradition" and founded the "Center for Studies in Oral Tradition" (1986) at the University of Missouri. Foley developed Oral Theory beyond the somewhat mechanistic notions presented in earlier versions of Oral-Formulaic Theory, by extending Ong's interest in cultural features of oral societies beyond the verbal, by drawing attention to the agency of the bard and by describing how oral traditions bear meaning.

The bibliography would establish a clear underlying methodology which accounted for the findings of scholars working in the separate Linguistics fields (primarily Ancient Greek, Anglo-Saxon and Serbo-Croatian). Perhaps more importantly, it would stimulate conversation among these specialties, so that a network of independent but allied investigations and investigators could be established.

Foley's key works include "The Theory of Oral Composition" (1988); "Immanent Art" (1991); "Traditional Oral Epic: The Odyssey, Beowulf and the Serbo-Croatian Return-Song" (1993); "The Singer of Tales in Performance" (1995); "Teaching Oral Traditions" (1998); "How to Read an Oral Poem" (2002). His Pathways Project (2005–2012) draws parallels between the media dynamics of oral traditions and the Internet.

The theory of oral tradition would undergo elaboration and development as it grew in acceptance. While the number of formulas documented for various traditions proliferated, the concept of the formula remained lexically-bound. However, numerous innovations appeared, such as the "formulaic system" with structural "substitution slots" for syntactic, morphological and narrative necessity (as well as for artistic invention). Sophisticated models such as Foley's "word-type placement rules" followed. Higher levels of formulaic composition were defined over the years, such as "ring composition", "responsion" and the "type-scene" (also called a "theme" or "typical scene"). Examples include the "Beasts of Battle" and the "Cliffs of Death". Some of these characteristic patterns of narrative details, (like "the arming sequence;" "the hero on the beach"; "the traveler recognizes his goal") would show evidence of global distribution.

At the same time, the fairly rigid division between oral and literate was replaced by recognition of transitional and compartmentalized texts and societies, including models of diglossia (Brian Stock Franz Bäuml, and Eric Havelock). Perhaps most importantly, the terms and concepts of "orality" and "literacy" came to be replaced with the more useful and apt "traditionality" and "textuality". Very large units would be defined (The Indo-European Return Song) and areas outside of military epic would come under investigation: women's song, riddles and other genres.

The methodology of oral tradition now conditions a large variety of studies, not only in folklore, literature and literacy, but in philosophy, communication theory, Semiotics, and including a very broad and continually expanding variety of languages and ethnic groups, and perhaps most conspicuously in biblical studies, in which Werner Kelber has been especially prominent. The annual bibliography is indexed by 100 areas, most of which are ethnolinguistic divisions.

Present developments explore the implications of the theory for rhetoric and composition, interpersonal communication, cross-cultural communication, postcolonial studies, rural community development, popular culture and film studies, and many other areas. The most significant areas of theoretical development at present may be the construction of systematic hermeneutics and aesthetics specific to oral traditions.

The theory of oral tradition encountered early resistance from scholars who perceived it as potentially supporting either one side or another in the controversy between what were known as "unitarians" and "analysts" – that is, scholars who believed Homer to have been a single, historical figure, and those who saw him as a conceptual "author function," a convenient name to assign to what was essentially a repertoire of traditional narrative. A much more general dismissal of the theory and its implications simply described it as "unprovable" Some scholars, mainly outside the field of oral tradition, represent (either dismissively or with approval) this body of theoretical work as reducing the great epics to children's party games like "telephone" or "Chinese whispers". While games provide amusement by showing how messages distort content via uncontextualized transmission, Parry's supporters argue that the theory of oral tradition reveals how oral methods optimized the signal-to-noise ratio and thus improved the quality, stability and integrity of content transmission.

There were disputes concerning particular findings of the theory. For example, those trying to support or refute Crowne's hypothesis found the "Hero on the Beach" formula in numerous Old English poems. Similarly, it was also discovered in other works of Germanic origin, Middle English poetry, and even an Icelandic prose saga. J.A. Dane, in an article characterized as "polemics without rigor" claimed that the appearance of the theme in Ancient Greek poetry, a tradition without known connection to the Germanic, invalidated the notion of "an autonomous theme in the baggage of an oral poet."

Within Homeric studies specifically, Lord's "The Singer of Tales", which focused on problems and questions that arise in conjunction with applying oral-formulaic theory to problematic texts such as the "Iliad", "Odyssey", and even "Beowulf", influenced nearly all of the articles written on Homer and oral-formulaic composition thereafter. However, in response to Lord, Geoffrey Kirk published "The Songs of Homer", questioning Lord's extension of the oral-formulaic nature of Serbian and Croatian literature (the area from which the theory was first developed) to Homeric epic. Kirk argues that Homeric poems differ from those traditions in their "metrical strictness", "formular system[s]", and creativity. In other words, Kirk argued that Homeric poems were recited under a system that gave the reciter much more freedom to choose words and passages to get to the same end than the Serbo-Croatian poet, who was merely "reproductive". Shortly thereafter, Eric Havelock's "Preface to Plato" revolutionized how scholars looked at Homeric epic by arguing not only that it was the product of an oral tradition, but also that the oral-formulas contained therein served as a way for ancient Greeks to preserve cultural knowledge across many different generations. Adam Parry, in his 1966 work "Have we Homer's "Iliad"?", theorized the existence of the most fully developed oral poet to his time, a person who could (at his discretion) creatively and intellectually create nuanced characters in the context of the accepted, traditional story. In fact, he discounted the Serbo-Croatian tradition to an "unfortunate" extent, choosing to elevate the Greek model of oral-tradition above all others. Lord reacted to Kirk's and Parry's essays with "Homer as Oral Poet", published in 1968, which reaffirmed Lord's belief in the relevance of Yugoslav poetry and its similarities to Homer and downplayed the intellectual and literary role of the reciters of Homeric epic.

Many of the criticisms of the theory have been absorbed into the evolving field as useful refinements and modifications. For example, in what Foley called a "pivotal" contribution, Larry Benson introduced the concept of "written-formulaic" to describe the status of some Anglo-Saxon poetry which, while demonstrably written, contains evidence of oral influences, including heavy reliance on formulas and themes A number of individual scholars in many areas continue to have misgivings about the applicability of the theory or the aptness of the South Slavic comparison, and particularly what they regard as its implications for the creativity which may legitimately be attributed to the individual artist. However, at present, there seems to be little systematic or theoretically coordinated challenge to the fundamental tenets of the theory; as Foley put it, ""there have been numerous suggestions for revisions or modifications of the theory, but the majority of controversies have generated further understanding.




</doc>
<doc id="27900272" url="https://en.wikipedia.org/wiki?curid=27900272" title="Health humanities">
Health humanities

Health humanities refers to the application of the creative or fine arts (including visual arts, music, performing arts) and humanities disciplines (including literary studies, languages, law, history, philosophy, religion, etc.) to discourse about, express, and/or promote dimensions of human health and well being. This applied capacity of the humanities is not itself a novel idea; however, the construct of the health humanities has only recently begun to emerge over the first decade of the 21st Century. Historically, the roots informing the health humanities can be traced back to, and can now be considered to include, such multidisciplinary areas as the medical humanities and the expressive therapies/creative arts therapies.

In the health humanities, health (and the promotion of health) is understood according to the constructivist (and other non-positivist) principles indigenous to the humanities, as opposed to the positivism of science. The health humanities are rooted in dialogical (negotiated, intersubjective voices of multiple truths), versus monological (a singular, authoritative voice of "the" truth) perspectives on health. As such, evidence upon which health practices are based is generally considered axiological (based in meanings, values, and aesthetics), versus epistemological (based in factual knowledge), in orientation. The health humanities are not an alternative to the health sciences, but rather offer a contrasting paradigm and pragmatic approach with respect to health and its promotion, and can function in a manner that is complementary and simultaneous relative to the health sciences. 

The health humanities are a growing movement internationally. A conference on the health humanities was held October 13–15, 2006, at Green College, University of British Columbia. The conference was co-organized by Judy Segal (UBC English) and Alan Richardson (UBC Philosophy) and featured presentations by Jacalyn Duffin, Carl Elliott, Nicholas King, Lorelei Lingard, Robert Proctor, Susan Squier, Andrea Tone, and Kathleen Woodward. In January 2009, Paul Crawford became the world's first Professor of Health Humanities at The University of Nottingham, and with Dr Victoria Tischler, Charley Baker, Dr Brian Brown, Dr Lisa Mooney-Smith and Professor Ronald Carter created an international health humanities initiative that included the AHRC-funded International Health Humanities Conference (IHHC). The first field description for Health Humanities was presented in the key article "Health Humanities: The future of Medical Humanities" (Crawford, Brown, Tischler, & Baker, 2010), published in the Mental Health Review Journal.

The 1st International Health Humanities Conference was held 6–8 August 2010, at The University of Nottingham, United Kingdom. The conference opened with Professor Crawford's address entitled ‘Health humanities: Literature and Madness’ and included keynote lectures by Professor Kay Redfield Jamison and Professor Elaine Showalter. Mark A. Radcliffe, who also spoke at the conference, reported on 'health humanities' in his weekly column for the Nursing Times. The conference was also reported in the Bethlem Blog. The 2nd International Health Humanities Conference was hosted in the USA, 9–11 August 2012, at Montclair State University in New Jersey, with the theme of "Music, Health, and Humanity." The 3rd International Health Humanities Conference was held 5-7 September 2014, once again at the University of Nottingham, and featured the theme of "Traumatextualities: Trauma in the Clinical, Arts and Humanities Contexts." The 4th International Health Humanities Conference was held 30 April and 1-2 May 2015, at the Center for Bioethics and Humanities, Anschutz Medical Campus, University of Colorado, Denver, featuring the theme of "Health Humanities: The Next Decade (Pedagogies, Practices, Politics)." The 5th International Health Humanities Conference was held 15-17 September 2016, at the University of Seville, Seville, Spain, featuring the theme of "Arts and Humanities for Improving Social Inclusion, Education, and Health: Creative Practice and Mutuality." . The 6th Annual was held March 9-11, 2017 at the University of Texas Houston on the theme of "Diversity, Cultures, and the Health Humanities". The 8th International Health Humanities Conference will be held March 28-30, 2019 at DePaul University (Chicago) on "The Environments of the Health Humanities: Inquiry and Practice".

Textbooks on the health humanities include "Health Humanities Reader" and "Health Humanities," "Medical Humanities: An Introduction," and forthcoming "Research Methods in the Health Humanities (OUP)." 

In 2015 a Health Humanities Centre was established at University College London, dedicated to research and teaching in the Health Humanities, including an MA in Health Humanities.


Jones T, Blackie M, Garden R, and Wear D. (2016) The Almost Right Word: The Move From Medical to Health Humanities. Academic Medicine http://journals.lww.com/academicmedicine/Abstract/publishahead/The_Almost_Right_Word___The_Move_From_Medical_to.98313.aspx

SCOPE: The Health Humanities Learning Lab

http://sites.fhi.duke.edu/healthhumanitieslab/

https://healthhumanitiesconsortium.com

Northwest Narrative Medicine Collaborative - community of narrative medicine, medical humanities, and health humanities practitioners in the U.S. Pacific Northwest


</doc>
<doc id="18933569" url="https://en.wikipedia.org/wiki?curid=18933569" title="Library science">
Library science

Library science (often termed library studies, bibliothecography, library economy) is an interdisciplinary or multidisciplinary field that applies the practices, perspectives, and tools of management, information technology, education, and other areas to libraries; the collection, organization, preservation, and dissemination of information resources; and the political economy of information. Martin Schrettinger, a Bavarian librarian, coined the discipline within his work (1808–1828) "Versuch eines vollständigen Lehrbuchs der Bibliothek-Wissenschaft oder Anleitung zur vollkommenen Geschäftsführung eines Bibliothekars". Rather than classifying information based on nature-oriented elements, as was previously done in his Bavarian library, Schrettinger organized books in alphabetical order. The first American school for library science was founded by Melvil Dewey at Columbia University in 1887.

Historically, library science has also included archival science. This includes how information resources are organized to serve the needs of selected user groups, how people interact with classification systems and technology, how information is acquired, evaluated and applied by people in and outside libraries as well as cross-culturally, how people are trained and educated for careers in libraries, the ethics that guide library service and organization, the legal status of libraries and information resources, and the applied science of computer technology used in documentation and records management.

There is no generally agreed-upon distinction between the terms "library science",and "librarianship", and to a certain extent they are interchangeable, perhaps differing most significantly in connotation. The term "library science or library studies" (LIS) is most often used; most librarians consider it as only a terminological variation, intended to emphasize the scientific and technical foundations of the subject and its relationship with information science. LIS should not be confused with information theory, the mathematical study of the concept of information. "Library philosophy" has been contrasted with "library science" as the study of the aims and justifications of librarianship as opposed to the development and refinement of techniques.

The earliest text on library operations, "Advice on Establishing a Library" was published in 1627 by French librarian and scholar Gabriel Naudé.
Naudé wrote prolifically, producing works on many subjects including politics, religion, history, and the supernatural. He put into practice all the ideas put forth in "Advice" when given the opportunity to build and maintain the library of Cardinal Jules Mazarin.

Martin Schrettinger wrote the second textbook (the first in Germany) on the subject from 1808 to 1829.

Thomas Jefferson, whose library at Monticello consisted of thousands of books, devised a classification system inspired by the Baconian method, which grouped books more or less by subject rather than alphabetically, as it was previously done.

The Jefferson collection provided the start of what became the Library of Congress.

The first American school of librarianship opened at Columbia University under the leadership of Melvil Dewey, noted for his 1876 decimal classification, on 5 January 1887 as the School of Library Economy. The term "library economy" was common in the U.S. until 1942, with the "library science" predominant through much of the 20th century.

Later, the term was used in the title of S. R. Ranganathan's "The Five Laws of Library Science", published in 1931, and in the title of Lee Pierce Butler's 1933 book, "An introduction to library science" (University of Chicago Press).

S. R. Ranganathan conceived the five laws of library science and the development of the first major analytico-synthetic classification system, the colon classification. In India, he is considered to be the father of library science, documentation, and information science and is widely known throughout the rest of the world for his fundamental thinking in the field.

In the United States, Lee Pierce Butler's new approach advocated research using quantitative methods and ideas in the social sciences with the aim of using librarianship to address society's information needs. He was one of the first faculty at the University of Chicago Graduate Library School, which changed the structure and focus of education for librarianship in the twentieth century. This research agenda went against the more procedure-based approach of "library economy," which was mostly confined to practical problems in the administration of libraries.

William Stetson Merrill's "A Code for Classifiers", released in several editions from 1914 to 1939, is an example of a more pragmatic approach, where arguments stemming from in-depth knowledge about each field of study are employed to recommend a system of classification. While Ranganathan's approach was philosophical, it was also tied more to the day-to-day business of running a library. A reworking of Ranganathan's laws was published in 1995 which removes the constant references to books. Michael Gorman's "Our Enduring Values: Librarianship in the 21st Century" features his eight principles necessary by library professionals and incorporate knowledge and information in all their forms, allowing for digital information to be considered.

In more recent years, with the growth of digital technology, the field has been greatly influenced by information science concepts. In the English speaking world the term "library science" seems to have been used for the first time in India in the 1916 book "Punjab Library Primer", written by Asa Don Dickinson and published by the University of the Punjab, Lahore, Pakistan. This university was the first in Asia to begin teaching "library science". The "Punjab Library Primer" was the first textbook on library science published in English anywhere in the world. The first textbook in the United States was the "Manual of Library Economy", published in 1929. In 1923, C. C. Williamson, who was appointed by the Carnegie Corporation, published an assessment of library science education entitled "The Williamson Report," which designated that universities should provide library science training. This report had a significant impact on library science training and education. Library research and practical work, the area of information science, has remained largely distinct both in training and in research interests.

The digital age has transformed how information is accessed and retrieved. "The library is now a part of a complex and dynamic educational, recreational, and informational infrastructure." Mobile devices and applications with wireless networking, high-speed computers and networks, and the computing cloud have deeply impacted and developed information science and information services. The evolution of the library sciences maintains its mission of access equity and community space, as well as the new means for information retrieval called information literacy skills. All catalogues, databases, and a growing number of books are all available on the Internet. In addition, the expanding free access to open source journals and sources such as Wikipedia have fundamentally impacted how information is accessed. Information literacy is the ability to "determine the extent of information needed, access the needed information effectively and efficiently, evaluate information and its sources critically, incorporate selected information into one’s knowledge base, use information effectively to accomplish a specific purpose, and understand the economic, legal, and social issues surrounding the use of information, and access and use information ethically and legally."

Academic courses in library science include collection management, information systems and technology, research methods, information literacy, cataloging and classification, , reference, statistics and management. Library science is constantly evolving, incorporating new topics like database management, information architecture and information management, among others. With the mounting acceptance of Wikipedia as a valued and reliable reference source, many libraries, museums and archives have introduced the role of Wikipedian in residence. As a result, some universities are including coursework relating to Wikipedia and Knowledge Management in their MLIS programs.

Most schools in the US only offer a master's degree in library science or an MLIS and do not offer an undergraduate degree in the subject. About fifty schools have this graduate program, and seven are still being ranked. Many have online programs, which makes attending more convenient if the college is not in a student's immediate vicinity. According to "US News" online journal, University of Illinois is at the top of the list of best MLIS programs provided by universities. Second is University of North Carolina and third is University of Washington. All the listings can be found here.

Most professional library jobs require a professional post-baccalaureate degree in library science, or one of its equivalent terms. In the United States and Canada the certification usually comes from a master's degree granted by an ALA-accredited institution, so even non-scholarly librarians have an originally academic background. In the United Kingdom, however, there have been moves to broaden the entry requirements to professional library posts, such that qualifications in, or experience of, a number of other disciplines have become more acceptable. In Australia, a number of institutions offer degrees accepted by the ALIA (Australian Library and Information Association). Global standards of accreditation or certification in librarianship have yet to be developed.

In academic regalia in the United States, the color for library science is lemon.

The Master of Library Science (MLIS) is the master's degree that is required for most professional librarian positions in the United States and Canada. The MLIS is a relatively recent degree; an older and still common degree designation for librarians to acquire is the Master of Library Science (MLS), or Master of Science in Library Science (MSLS) degree. According to the American Library Association (ALA), "The master’s degree in library and information studies is frequently referred to as the MLS; however, ALA-accredited degrees have various names such as Master of Arts, Master of Librarianship, Master of Library and Information Studies, or Master of Science. The degree name is determined by the program. The [ALA] Committee for Accreditation evaluates programs based on their adherence to the Standards for Accreditation of Master's Programs in Library and Information Studies, not based on the name of the degree

According to 'U.S. News & World Report', library and information science ranked as one of the "Best Careers of 2008." The median annual salary for 2017 was reported by the U.S. Bureau of Labor Statistics as $58,520 in the United States. Additional salary breakdowns available by metropolitan area show that the San Jose-Sunnyvale-Santa Clara metropolitan area has the highest average salary at $86,380. In December 2016, the BLS projected growth for the field at "9 percent between 2016 and 2026", which is "as fast as the average for all occupations". The 2010-2011 Occupational Outlook Handbook states, "Workers in this occupation tend to be older than workers in the rest of the economy. As a result, there may be more workers retiring from this occupation than other occupations. However, relatively large numbers of graduates from MLS programs may cause competition in some areas and for some jobs."

Librarianship manifests a dual career structure for men and women in the United States. While the ratio of female to male librarians remains roughly 4:1, top positions are more often held by men. In large academic libraries, there is less of a discrepancy; however, overall, throughout the profession, men tend to hold higher or leadership positions. Women, however, have made continuous progress toward equality. Women have also been largely left out of standard histories of U.S. librarianship, but Suzanne Hildenbrand's scholarly assessment of the work done by women has expanded the historical record. See also "The Role of women in librarianship, 1876–1976: the entry, advancement, and struggle for equalization in one profession", by Kathleen Weibel, Kathleen de la Peña McCook, and Dianne J. Ellsworth (1979), Phoenix, Ariz: Oryx Press.

There was a "Women's Meeting" at the 1882 14th American Libraries Conference, where issues concerning the salaries of women librarians and what female patrons do in reading rooms were discussed.

During the first 35 years of the American Library Association its presidency was held by men. In 1911 Theresa Elmendorf became the first woman elected president of the ALA. She was ALA president from May 24, 1911, until July 2, 1912.

In 1919, an ALA resolution promoting equal pay and opportunities for women in librarianship was defeated by a large margin.

In 1970, Betty Wilson brought forth a resolution that would have had the ALA refrain from using facilities that discriminate against women. That resolution was also defeated by the membership.

In 1977, the ALA took a stand for the Equal Rights Amendment. The organization stated that they would no longer hold conferences in states that did not ratify the amendment, with the boycott measure set to take place in 1981. An ERA Task Force was formed in 1979 towards this goal and a sum of $25,000 was allocated towards task force operations in unratified states. At the time, a number of state library associations passed pro-ERA resolutions and formed committees on women in libraries.

In 2013–2014, 82% of graduates in Master of Library Science (MLS) programs were female.
In 2016, Carla Hayden became the first female Librarian of Congress.

There are multiple groups within the American Library Association, dedicated to discussing, critiquing, and furthering gender-related and feminist issues within the profession.

In 1969 the first women's rights task force was founded: the National Women's Liberation Front for Librarians (NWFFL or New-Waffle). It was also in 1969 that children's librarians, after being unable to find children's books that included working mothers, worked to remedy the situation and succeeded in their efforts.

The American Library Association's Social Responsibilities Round Table Feminist Task Force (FTF) was founded in 1970 by women who wished to address sexism in libraries and librarianship. FTF was the first ALA group to focus on women's issues. In recent years during Women's History Month (March), the FTF has dedicated their efforts to expanding women's library history online, using the website Women of Library History. The FTF also publishes the annual Amelia Bloomer Project list, which includes some of the best feminist young adult literature of the year.

The Committee on the Status of Women in Librarianship (COSWL) of the American Library Association, founded in 1976, represents the diversity of women's interest within ALA and ensures that the Association considers the rights of the majority (women) in the library field, and promotes and initiates the collection, analysis, dissemination, and coordination of information on the status of women in librarianship. The bibliographic history of women in U.S. librarianship and women librarians developing services for women has been well-documented in the series of publications initially issued by the Social Responsibilities Round Table Task Force on Women and later continued by COSWL.

The ALA also has the Women & Gender Studies Section (WGSS) of its Division "Association of College & Research Libraries"; this section was formed to discuss, promote, and support women's studies collections and services in academic and research libraries.

Finally, the ALA has the Gay, Lesbian, Bisexual, and Transgender Roundtable (GLBTRT). While the GLBTRT deals with sexuality, different than gender identity, much of the roundtable's work is arguably feminist in nature, and concerned with issues of gender. The GLBTRT is committed to serving the information needs of the GLBT professional library community, and the GLBT information and access needs of individuals at large.

Many scholars within the profession have taken up gender and its relationship to the discipline of library and information science. Scholars like Hope A. Olson and Sanford Berman have directed efforts at the problematic nature of cataloging and classification standards and schemes that are obscuring or exclusionary to marginalized groups. Others have written about the implications of gendered stereotypes in librarianship, particularly as they relate to library instruction. Library instruction also intersects with feminist pedagogy, and scholars such as Maria Accardi have written about feminist pedagogical practices in libraries. Library scholars have also dealt with issues of gender and leadership, having equitable gender representation in library collection development, and issues of gender and young adult and children's librarianship.

The ALA Policy Manual states under "B.2.1.15 Access to Library Resources and Services Regardless of Sex, Gender Identity, Gender Expression, or Sexual Orientation (Old Number 53.1.15):" "The American Library Association stringently and unequivocally maintains that libraries and librarians have an obligation to resist efforts that systematically exclude materials dealing with any subject matter, including sex, gender identity or expression, or sexual orientation. The Association also encourages librarians to proactively support the First Amendment rights of all library users, regardless of sex, sexual orientation, or gender identity or expression. Adopted 1993, amended 2000, 2004, 2008, 2010." It also states under "B.2.12 Threats to Library Materials Related to Sex, Gender Identity, or Sexual Orientation (Old Number 53.12)", "The American Library Association supports the inclusion in library collections of materials that reflect the diversity of our society, including those related to sex, sexual orientation, and gender identity or expression. ALA encourages all American Library Association chapters to take active stands against all legislative or other government attempts to proscribe materials related to sex, sexual orientation, and gender identity or expression; and encourages all libraries to acquire and make available materials representative of all the people in our society. Adopted 2005, Amended 2009, 2010." 

The field of library science seeks to provide a diverse working environment in libraries across the United States. Ways to change the status quo include diversifying the job field with regards to age, class, disabilities, ethnicity, gender identity, race, sex, and sexual orientation. The demographics of America are changing; those who were once minorities will become the majority. Library facilities can best represent their communities by hiring diverse staffs. The American Library Association and many libraries around the country realize the issue of diversity in the workplace and are addressing this problem.

The majority of librarians working in the U.S. are female, between the ages of 55–64, and Caucasian. A 2014 study by the American Library Association of research done from 2009 to 2010 shows that 98,273 of credentialed librarians were female while 20,393 were male. 15,335 of the total 111,666 were 35 and younger and only 6,222 were 65 or older. 104,393 were white; 6,160 African American, 3,260 American Pacific Islander; 185 Native American including Alaskan; 1,008 of two or more races, and 3,661 Latino. (ALA).

To help change the lack of diversity in library jobs in the U.S., more scholarships and grants are emerging. Most library and information science students do not belong to an underrepresented group and as a reaction to these research statistics, the field is creating ways to encourage more diversity in the classroom.

The ALA Annual Research Diversity Grant Program is a way to encourage innovation in scholars and professionals to provide insight into how to diversify the field. The ALA Grant is directed toward those who have valuable and original research ideas that can add to the knowledge of diversity in the field of librarianship. The program awards up to three individuals once a year with a grant of $2,500 each. The applicants have submission guidelines, are given a timeline, and are shown the evaluation process online.

One way to nurture cultural diversity in the library field is with cultural competencies. Scholars recommend defining skills needed to serve and work with others who belong to different cultures. It is suggested that these definitions be posted in job listings and be referred to when promoting and giving raises. In library and information science graduate programs, it is also suggested by scholars that there is a lack of classes teaching students cultural competences. It is important for more classes to teach about diversity and measure the outcomes.

Another strategy is to create interest in the field of library and information science from a young age. If minorities do not desire to become librarians, they will not seek to obtain an MLS or MLIS and therefore will not fill high job roles in libraries. A recommended solutions are to create a great experience for all racial group's early on in life. This may inspire more young children to become interested in this field.

ALA Office for Diversity

The Office for Diversity is a sector of the American Library Association whose purpose is to aid libraries in providing a diverse workforce, gathering data, and teaching others about the issue of diversity related to the field of library and information science.

American Indian Library Association

The American Indian Library Association (AILA) was created in 1979. It publishes a newsletter twice a year and educates individuals and groups about Indian culture.

Black Caucus of the American Library Association

BCALA promotes not only library services that can be enjoyed by the African American community but also the emergence of African American librarians and library professionals. By joining the association, patrons have access to newsletters, the entirety of their website, and networking boards.

CALA

The Chinese American Librarians Association (CALA) began March on 31, 1973. It was formerly known as the Mid-West Chinese American Librarians Association. It has members not only in America but in China, Hong Kong, Canada, and more. The organization promotes the Chinese culture through the outlet of libraries and communicates with others in the profession of librarianship.

Reforma

Reforma is the national library association to promote library and information services to Latino and the Spanish speaking, created in 1971. The association has pushed for Spanish collections in libraries, gives out yearly scholarships, and sends out quarterly newsletters. One of Reforma's main goals is to recruit Latinos into professional positions of the library.

Deaf people have the same needs as any other library visitors, and often have more difficulty accessing materials and services. Over the last few decades, libraries in the United States have begun to implement services and collections for deaf and HoH patrons and are working to make more of their collections, services, their communities, and even the world more accessible to this group of underserved people.

The history of the role of libraries in the Deaf community in the United States is a sordid one. The American Library Association readily admits that disabled people belong to a minority that is often overlooked and underrepresented by people in the library, and the Deaf community belongs in this minority group. However, in the last few decades, libraries across the United States have made great strides in the mission of making libraries more accessible to disabled people in general and to the Deaf community specifically. The Library Bill of Rights preamble states that "all libraries are forums for information and ideas" and as such libraries need to remove the physical and technological barriers which in turn would allow persons with disabilities full access to the resources available.

One notable American activist in the library community working toward accessibility for the deaf was Alice Lougee Hagemeyer.

Australian librarian Karen McQuigg stated in 2003 that "even ten years ago, when I was involved in a project looking at what public libraries could offer the deaf, it seemed as if the gap between the requirements of this group and what public libraries could offer was too great for public libraries to be able to serve them effectively." Clearly, not even so long ago, there was quite a dearth of information for or about the deaf community available in libraries across the nation and around the globe.

New guidelines from library organizations such as International Federation of Library Associations and Institutions (IFLA) and the ALA were written in order to help libraries make their information more accessible to people with disabilities, and in some cases, specifically the deaf community. IFLA's "Guidelines for Library Services to Deaf People" is one such set of guidelines, was published to inform libraries of the services that should be provided for deaf patrons. Most of the guidelines pertain to ensuring that deaf patrons have equal access to all available library services. Other guidelines include training library staff to provide services for the deaf community, availability of text telephones or TTYs not only to assist patrons with reference questions but also for making outside calls, using the most recent technology in order to communicate more effectively with deaf patrons, including closed captioning services for any television services, and developing a collection that would interest the members of the deaf community.

Over the years, library services have begun to evolve in order to accommodate the needs and desires of local deaf communities. There is now a Library Service to People Who Are Deaf or Hard of Hearing Forum for libraries to look at to find out what they can do to better serve their Deaf/HoH users. At the Queen Borough Public Library (QBPL) in New York, the staff implemented new and innovative ideas in order to involve the community and library staff with the deaf people in their community. The QBPL hired a deaf librarian, Lori Stambler, to train the library staff about deaf culture, to teach sign language classes for family members and people who are involved with deaf people, and to teach literacy classes for deaf patrons. In working with the library, Stambler was able to help the community reach out to its deaf neighbors, and helped other deaf people become more active in their outside community.

The library at Gallaudet University, the only deaf liberal arts university in the United States, was founded in 1876. The library's collection has grown from a small number of reference books to the world's largest collection of deaf-related materials, with over 234,000 books and thousands of other materials in different formats. The collection is so large that the library had to create a hybrid classification system based on the Dewey Decimal Classification System in order to make cataloging and location within the library easier for both library staff and users. The library also houses the university's archives, which holds some of the oldest deaf-related books and documents in the world.

In Nashville, Tennessee, Sandy Cohen manages the Library Services for the Deaf and Hard of Hearing (LSDHH). The program was created in 1979 in response to information accessibility issues for the deaf in the Nashville area. Originally, the only service provided was the news via a teletypewriter or TTY, but today, the program has expanded to serving the entire state of Tennessee by providing all different types of information and material on deafness, deaf culture, and information for family members of deaf people, as well as a historical and reference collection.

Many practicing librarians do not contribute to LIS scholarship, but focus on daily operations within their own libraries or library systems. Other practicing librarians, particularly in academic libraries, do perform original scholarly LIS research and contribute to the academic end of the field.

Whether or not individual professional librarians contribute to scholarly research and publication, many are involved with and contribute to the advancement of the profession and of library science through local, state, regional, national and international library or information organizations.

Library science is very closely related to issues of knowledge organization; however, the latter is a broader term which covers how knowledge is represented and stored (computer science/linguistics), how it might be automatically processed (artificial intelligence), and how it is organized outside the library in global systems such as the internet. In addition, library science typically refers to a specific community engaged in managing holdings as they are found in university and government libraries, while knowledge organization in general refers to this and also to other communities (such as publishers) and other systems (such as the Internet). The library system is thus one socio-technical structure for knowledge organization.

The terms information organization and knowledge organization are often used synonymously. The fundamentals of their study (particularly theory relating to indexing and classification) and many of the main tools used by the disciplines in modern times to provide access to digital resources (abstracting, metadata, resource description, systematic and alphabetic subject description, and terminology) originated in the 19th century and were developed, in part, to assist in making humanity's intellectual output accessible by recording, identifying, and providing bibliographic control of printed knowledge. 

Information has been published which analyses the relations between philosophy of information (PI), library and information science (LIS), and social epistemology (SE).

The study of librarianship for public libraries covers issues such as cataloging; collection development for a diverse community; information literacy; readers' advisory; community standards; public services-focused librarianship; serving a diverse community of adults, children, and teens; intellectual freedom; censorship; and legal and budgeting issues. The public library as a commons or public sphere based on the work of Jürgen Habermas has become a central metaphor in the 21st century.

Most people are familiar with municipal public libraries, but there are many different types of public libraries that exist. There are four different types of public libraries: association libraries, municipal public libraries, school district libraries and special district public libraries. It is very important to be able to distinguish between the four. Each receives its funding through different sources. Each is established by a different set of voters. And, not all are subject to municipal civil service governance. Listed below is a chart from the New York State Library's library development website. This chart lists all of the information about the different public libraries.

The study of school librarianship covers library services for children in schools through secondary school. In some regions, the local government may have stricter standards for the education and certification of school librarians (who are often considered a special case of teacher), than for other librarians, and the educational program will include those local criteria. School librarianship may also include issues of intellectual freedom, pedagogy, information literacy, and how to build a cooperative curriculum with the teaching staff.

The study of academic librarianship covers library services for colleges and universities. Issues of special importance to the field may include copyright; technology, digital libraries, and digital repositories; academic freedom; open access to scholarly works; as well as specialized knowledge of subject areas important to the institution and the relevant reference works. Librarians often divide focus individually as liaisons on particular schools within a college or university.

Some academic librarians are considered faculty, and hold similar academic ranks to those of professors, while others are not. In either case, the minimal qualification is a Master of Arts in Library Studies or Masters of Arts in Library Science. Some academic libraries may only require a master's degree in a specific academic field or a related field, such as educational technology.

The study of archives includes the training of archivists, librarians specially trained to maintain and build archives of records intended for historical preservation. Special issues include physical preservation, conservation and restoration of materials and mass deacidification; specialist catalogs; solo work; access; and appraisal. Many archivists are also trained historians specializing in the period covered by the archive.

The archival mission includes three major goals: To identify papers and records that have enduring value, to preserve the identified papers, and to make the papers available to others.

There are significant differences between libraries and archives, including differences in collections, records creation, item acquisition, and preferred behavior in the institution. The major difference in collections is that library collections typically comprise published items (books, magazines, etc.), while archival collections are usually unpublished works (letters, diaries, etc.) In managing their collections, libraries will categorize items individually, but archival items never stand alone. An archival record gains its meaning and importance from its relationship to the entire collection; therefore archival items are usually received by the archive in a group or batch. Library collections are created by many individuals, as each author and illustrator creates their own publication; in contrast, an archive usually collects the records of one person, family, institution, or organization, and so the archival items will have fewer source authors.

Another difference between a library and an archive, is that library materials are created explicitly by authors or others who are working intentionally. They choose to write and publish a book, for example, and that occurs. Archival materials are not created intentionally. Instead, the items in an archive are what remain after a business, institution, or person conducts their normal business practices.The collection of letters, documents, receipts, ledger books, etc. were created with intention to perform daily tasks, they were not created in order to populate a future archive.

As for item acquisition, libraries receive items individually, but archival items will usually become part of the archive's collection as a cohesive group.

Behavior in an archive differs from behavior in a library, as well. In most libraries, patrons are allowed and encouraged to browse the stacks, because the books are openly available to the public. Archival items almost never circulate, and someone interested in viewing documents must request them of the archivist and may only view them in a closed reading room. Those who wish to visit an archive will usually begin with an entrance interview. This is an opportunity for the archivist to register the researcher, confirm their identity, and determine their research needs. This is also the opportune time for the archivist to review reading room rules, which vary but typically include policies on privacy, photocopying, the use of finding aids, and restrictions on food, drinks, and other activities or items that could damage the archival materials.

Special libraries and special librarians include almost any other form of librarianship, including those who serve in medical libraries (and hospitals or medical schools), corporations, news agencies, government organizations, or other special collections. The issues at these libraries are specific to the industries they inhabit, but may include solo work, corporate financing, specialized collection development, and extensive self-promotion to potential patrons. Special librarians have their own professional organization, the Special Library Association.

National Center for Atmospheric Research (NCAR) is considered a special library. Its mission is to support, preserve, make accessible, and collaborate in the scholarly research and educational outreach activities of UCAR/NCAR.

Another is the Federal Bureau of Investigations Library. According to its website, "The FBI Library supports the FBI in its statutory mission to uphold the law through investigation of violations of federal criminal law; to protect the United States from foreign intelligence and terrorist activities; and to provide leadership and law enforcement assistance to federal, state, local, and international agencies.

Preservation librarians most often work in academic libraries. Their focus is on the management of preservation activities that seek to maintain access to content within books, manuscripts, archival materials, and other library resources. Examples of activities managed by preservation librarians include binding, conservation, digital and analog reformatting, digital preservation, and environmental monitoring.





</doc>
<doc id="489094" url="https://en.wikipedia.org/wiki?curid=489094" title="Area studies">
Area studies

Area studies (also regional studies) are interdisciplinary fields of research and scholarship pertaining to particular geographical, national/federal, or cultural regions. The term exists primarily as a general description for what are, in the practice of scholarship, many heterogeneous fields of research, encompassing both the social sciences and the humanities. Typical area study programs involve international relations, strategic studies, history, political science, political economy, cultural studies, languages, geography, literature, and other related disciplines. In contrast to cultural studies, area studies often include diaspora and emigration from the area.

Interdisciplinary area studies became increasingly common in the United States of America and in Western scholarship after World War II. Before that war American universities had just a few faculty who taught or conducted research on the non-Western world. Foreign-area studies were virtually nonexistent. After the war, liberals and conservatives alike were concerned about the U.S. ability to respond effectively to perceived external threats from the Soviet Union and China in the context of the emerging Cold War, as well as to the fall-out from the Decolonization of Africa and Asia.

In this context, the Ford Foundation, the Rockefeller Foundation, and the Carnegie Corporation of New York convened a series of meetings producing a broad consensus that to address this knowledge deficit, the U.S. must invest in international studies. Therefore, the foundations of the field are strongly rooted in America. Participants argued that a large brain trust of internationally oriented political scientists and economists was an urgent national priority. There was a central tension, however, between those who felt strongly that, instead of applying Western models, social scientists should develop culturally and historically contextualized knowledge of various parts of the world by working closely with humanists, and those who thought social scientists should seek to develop overarching macrohistorial theories that could draw connections between patterns of change and development across different geographies. The former became area-studies advocates, the latter proponents of modernization theory.

The Ford Foundation would eventually become the dominant player in shaping the area-studies program in the United States.

In 1950 the foundation established the prestigious Foreign Area Fellowship Program (FAFP), the first large-scale national competition in support of area-studies training in the United States. From 1953 to 1966 it contributed $270 million to 34 universities for area and language studies. Also during this period, it poured millions of dollars into the committees run jointly by the Social Science Research Council and the American Council of Learned Societies for field-development workshops, conferences, and publication programs. Eventually, the SSRC-ACLS joint committees would take over the administration of FAFP.

Other large and important programs followed Ford's. Most notably, the National Defense Education Act of 1957, renamed the Higher Education Act in 1965, allocated funding for some 125 university-based area-studies units known as National Resource Center programs at U.S. universities, as well as for Foreign Language and Area Studies fellowships for graduate students.

Meanwhile, area studies were also developed in the Soviet Union.

Since their inception, area studies have been subject to criticism—including by area specialists themselves. Many of them alleged that because area studies were connected to the Cold War agendas of the CIA, the FBI, and other intelligence and military agencies, participating in such programs was tantamount to serving as an agent of the state. Some argue that there is the notion that U.S concerns and research priorities will define the intellectual terrain of area studies. Others insisted, however, that once they were established on university campuses, area studies began to encompass a much broader and deeper intellectual agenda than the one foreseen by government agencies, thus not American centric.

Arguably, one of the greatest threats to the area studies project was the rise of rational choice theory in political science and economics. To mock one of the most outspoken rational choice theory critics, Japan scholar Chalmers Johnson asked: Why do you need to know Japanese or anything about Japan's history and culture if the methods of rational choice will explain why Japanese politicians and bureaucrats do the things they do?

Following the demise of the Soviet Union, philanthropic foundations and scientific bureaucracies moved to attenuate their support for area studies, emphasizing instead interregional themes like "development and democracy". When the Social Science Research Council and the American Council of Learned Societies, which had long served as the national nexus for raising and administering funds for area studies, underwent their first major restructuring in thirty years, closing down their area committees, scholars interpreted this as a massive signal about the changing research environment.

Fields are defined differently from university to university, and from department to department, but common area-studies fields include:
Due to an increasing interest in studying translocal, transregional, transnational and transcontinental phenomena, a Potsdam-based research network has recently coined the term "TransArea Studies" (POINTS – Potsdam International Network for TransArea Studies).

Other interdisciplinary research fields such as women's studies (also known as gender studies), disability studies, and ethnic studies (including African American studies, Asian American studies, Latino/a studies, and Native American studies) are not part of area studies but are sometimes included in discussion along with it.

Area studies is sometimes known as regional studies. The Regional Studies Association is an international association focusing on these interdisciplinary fields. 

Some entire institutions of higher education (tertiary education) are devoted solely to area studies such as School of Oriental and African Studies, part of the University of London, or the Tokyo University of Foreign Studies in Japan. At the University of Oxford, the School of Interdisciplinary Area Studies (SIAS)School of Interdiscplinary Area Studies, Oxford and St Antony's College specialise in area studies, and hosts a number of post-graduate teaching programmes and research centres covering various regions of the world.
Jawaharlal Nehru University, New Delhi, is the only institution with immense contribution towards popularising area studies in India.
An institution which exclusively deals with Area Studies is the GIGA (German Institute of Global Area Studies) in Germany. Additionally, Lund University in Sweden offers the largest Asian Studies masters program in Northern Europe and is dedicated to promoting studies related to South Asia through its SASNet.





</doc>
<doc id="34375832" url="https://en.wikipedia.org/wiki?curid=34375832" title="Vorlage">
Vorlage

A Vorlage (; from the German for "prototype" or "template") is a prior version or manifestation of a text under consideration. It may refer to such a version of a text itself, a particular manuscript of the text, or a more complex manifestation of the text (e.g., a group of copies, or a group of excerpts). Thus, the original-language version of a text which a translator then works into a translation is called the "Vorlage" of that translation. For example, the Luther Bible is a translation of the Textus Receptus, so the Textus Receptus is the "Vorlage" of the Luther Bible. 

Sometimes the "Vorlage" of a translation may be lost to history. In some of these cases the "Vorlage" may be reconstructed from the translation. Such a reconstructed "Vorlage" may be called a "retroversion", and it invariably is made with some amount of uncertainty. Nevertheless, the "Vorlage" may still be reconstructed in some parts at such a level of confidence that the translation and its retroversion can be used as a witness for the purposes of textual criticism. This reconstructed "Vorlage" may stand on its own as the sole witness of the original-language text, or it may be compared and used along with other witnesses. Thus, for example, scholars use the reconstructed "Vorlage" of the Greek Septuagint translation of the Hebrew Bible at parts to correct the Hebrew Masoretic version when trying to determine oldest version of the Hebrew Bible that they can infer. Or, as another example, the Coptic fragments of Plato's Republic included among the Nag Hammadi library are used to help attest to the original Greek text which Plato himself wrote. For the bulk of the Gospel of Thomas, the "Vorlage" exists only as a retroversion of the Coptic translation, as no other witness to the original Greek text for most of the sayings recorded therein is known.


</doc>
<doc id="35036752" url="https://en.wikipedia.org/wiki?curid=35036752" title="Outline of the humanities">
Outline of the humanities

The following outline is provided as an overview of and topical guide to the humanities:

Humanities – academic disciplines that study the human condition, using methods that are primarily analytical, critical, or speculative, as distinguished from the mainly empirical approaches of the natural sciences.

The humanities can be described as all of the following:














List of humanities journals





</doc>
<doc id="35885810" url="https://en.wikipedia.org/wiki?curid=35885810" title="Somatic theory">
Somatic theory

Somatic theory is a theory of human social behavior based loosely on the somatic marker hypothesis of António Damásio, which proposes a mechanism by which emotional processes can guide (or bias) behavior, particularly decision-making, as well as the attachment theory of John Bowlby and the self psychology of Heinz Kohut, especially as consolidated by Allan Schore. 

It draws on various philosophical models from "On the Genealogy of Morals" of Friedrich Nietzsche through Martin Heidegger on "das Man", Maurice Merleau-Ponty on the lived body, and Ludwig Wittgenstein on social practices to Michel Foucault on discipline, as well as theories of performativity emerging out of the speech act theory of J. L. Austin, especially as developed by Judith Butler and Shoshana Felman; some somatic theorists have also tied somaticity to performance in the schools of actor training developed by Konstantin Stanislavski and Bertolt Brecht.

Barbara Sellers-Young applies Damasio’s somatic-marker hypothesis to critical thinking as an embodied performance, and provides a review of the theoretical literature in performance studies that supports something like Damasio’s approach:


Edward Slingerland applies Damasio's somatic-marker hypothesis to the cognitive linguistics of Gilles Fauconnier and Mark Turner and George Lakoff and Mark Johnson, especially Fauconnier and Turner's theory of conceptual blending and Lakoff and Johnson's embodied mind theory of metaphor. His goal in importing somatic theory into cognitive linguistics is to show that

Douglas Robinson first began developing a somatic theory of language for a keynote presentation at the 9th American Imagery Conference in Los Angeles, October, 1985, based on Ahkter Ahsen's theory of somatic response to images as the basis for therapeutic transformations; in contradistinction to Ahsen's model, which rejected Freud's "talking cure" on the grounds that words do not awaken somatic responses, Robinson argued that there is a very powerful somatics of language. He later incorporated this notion into "The Translator's Turn" (1991), drawing on the (passing) somatic theories of William James, Ludwig Wittgenstein, and Kenneth Burke in order to argue that somatic response may be "idiosomatic" (somatically idiosyncratic) but typically is "ideosomatic" (somatically ideological, or shaped and guided by society), and that the ideosomatics of language explains how language remains stable enough for communication to be possible. This work preceded the Damasio group's first scientific publication on the somatic-marker hypothesis in 1991, and Robinson did not begin to incorporate Damasio's somatic-marker hypothesis into his somatic theory until later in the 1990s.

In "Translation and Taboo" (1996) Robinson drew on the protosomatic theories of Sigmund Freud, Jacques Lacan, and Gregory Bateson to explore the ways in which the ideosomatics of taboo structure (and partly sanction and conceal) the translation of sacred texts. His first book to draw on Damasio's somatic-marker hypothesis is "Performative Linguistics" (2003); there he draws on J. L. Austin's theory of speech acts, Jacques Derrida's theory of iterability, and Mikhail Bakhtin's theory of dialogism to argue that performativity as an activity of the speaking body is grounded in somaticity. He also draws on Daniel Simeoni's application of Pierre Bourdieu's theory of "habitus" in order to argue that his somatics of translation as developed in "The Translator's Turn" actually explains translation norms more fully than Gideon Toury in "Descriptive Translation Studies and beyond" (1995).

In 2005 Robinson began writing a series of books exploring somatic theory in different communicative contexts: modernist/formalist theories of estrangement (Robinson 2008), translation as ideological pressure (Robinson 2011), first-year writing (Robinson 2012), and the refugee experience, (de)colonization, and the intergenerational transmission of trauma (Robinson 2013).

In Robinson's articulation, somatic theory has four main planks:


In addition, he has added concepts along the way: the proprioception of the body politic as a homeostatic balancing between too much familiarity and too much strangeness (Robinson 2008); tensions between loconormativity and xenonormativity, the exosomatization of places, objects, and skin color, and paleosomaticity (Robinson 2013); ecosis and icosis (unpublished work).



</doc>
<doc id="19552" url="https://en.wikipedia.org/wiki?curid=19552" title="Media studies">
Media studies

For a history of the field, see "History of media studies".The first Media Studies M.A. program in the U.S. was introduced by John Culkin at The New School in 1975, which has since graduated more than 2,000 students. Culkin was responsible for bringing Marshall McLuhan to Fordham in 1968 and subsequently founded the Center for Understanding Media, which became the New School program.

Media is studied as a broad subject in most states in Australia, with the state of Victoria being world leaders in curriculum development . Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.

Today, almost all Australian universities teach media studies. According to the Government of Australia's "Excellence in Research for Australia" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.

In secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or "VCE") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books "Reflections of Reality") and later John Murray (who authored "The Box in the Corner", "In Focus", and "10 Lessons in Film Appreciation").

Today, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.

In Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.

Victoria also hosts the peak media teaching body known as ATOM which publishes "Metro" and "Screen Education" magazines.

In Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.

Carleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and its programs were being copied by other colleges and universities nationally and Internationally.

Today, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).

In his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that "the medium is the message", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.

McLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it the message. The electric light is over looked as a communication medium because it doesn’t have any content. It is not until the electric light is used to spell a brand name that it is recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it is given another media “content”. The content of a movie is a book, play or maybe even an opera.

McLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it is inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.

There are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.

There is no university specialized on journalism and media studies, but there are seven public universities which have a department of media stuides. Three biggest are based in Prague (Charles University), Brno (Masaryk University) and Olomouc (Palacký University). There are another nine private universities and colleges which has media studies department.

One prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books "On Television" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers "pre-interview" participants in news and public affairs programs, to ensure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.

In Germany two main branches of media theory or media studies can be identified.

The first major branch of media theory has its roots in the humanities and cultural studies, such as film studies ("Filmwissenschaft"), theater studies ("Theaterwissenschaft") and German language and literature studies ("Germanistik") as well as Comparative Literature Studies ("Komparatistik"). This branch has broadened out substantially since the 1990s. And it is on this initial basis that a culturally-based media studies (often emphasised more recently through the disciplinary title "Medienkulturwissenschaft") in Germany has primarily developed and established itself. 

This plurality of perspectives make it difficult to single out one particular site where this branch of Medienwissenschaft originated. While the Frankfurt-based theatre scholar, Hans-Theis Lehmanns term "post dramatic theater" points directly to the increased blending of co-presence and mediatized material in the German theater (and elsewhere) since the 1970s, the field of theater studies from the 1990s onwards at the Freie Universität Berlin, led in particular by Erika Fischer-Lichte, showed particular interest in the ways in which theatricality influenced notions of performativity in aesthetic events. Within the field of Film Studies, again, both Frankfurt and Berlin were dominant in the development of new perspectives on moving image media. Heide Schlüpman in Frankfurt and Gertrud Koch, first in Bochum then in Berlin, were key theorists contributing to an aesthetic theory of the cinema (Schlüpmann) as "dispositif" and the moving image as medium, particularly in the context of illusion (Koch). Many scholars who became known as media scholars in Germany originally were scholars of German, such as Friedrich Kittler, who taught at the Humboldt Universität zu Berlin, completed both his dissertation and habilitation in the context of "Germanistik". One of the early publications in this new direction is a volume edited by Helmut Kreuzer, "Literature Studies - Media Studies" ("Literaturwissenschaft – Medienwissenschaft"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.

The second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.

The term "Wissenschaft" cannot be translated straightforwardly as "studies", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.

"Medienwissenschaften" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.

Media Studies is a fast growing academic field in India, with several dedicated departments and research institutes. With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication. Anna University was the first university to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level. All the major universities in the country have mass media and journalism studies departments. Centre for the Study of Developing Societies (CSDS), Delhi has media studies as one of their major emphasis. Centre for Internet and Society, Bangaluru that does interdisciplinary research on internet and digital technologies also is worth mentioning.
Main scholars who are working on Indian media include Arvind Rajagopal, Ravi Sundaram, Robin Jeffrey, Sevanti Ninan, Shohini Ghosh, and Usha M. Rodrigues and Maya Ranganathan. The work of Nalin Mehta on the expansion of private television channels in India, Amelia Bonea's research on the history of telegraph and journalism, and Shiju Sam Varughese's work on science and mass media open new areas of research in Indian media studies.

In the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.

Communication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.

Media studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.

Media studies in New Zealand is healthy, especially due to renewed activity in the country's film industry and is taught at both secondary and tertiary education institutes. Media studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies. But what makes the case of New Zealand particularly significant in respect of Media Studies is that for more than a decade it has been a nationally mandated and very popular subject in secondary (high) schools, taught across three years in a very structured and developmental fashion, with Scholarship in Media Studies available for academically gifted students. According to the New Zealand Ministry of Education Subject Enrolment figures 229 New Zealand schools offered Media Studies as a subject in 2016, representing more than 14,000 students. 

In Pakistan, media studies programs are widely offered. University of the Punjab Lahore is the oldest department. Later on University of Karachi, Peshawar University, BZU Multaan, Islamia University Bahwalpur also started communication programs. Now, newly established universities are also offering mass communication program in which University of Gujrat emerged as a leading department. Bahria University which is established by Pakistan Navy is also offering BS in media studies.

In Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.

In the United Kingdom, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:

When Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.

James Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.

Much research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.

Mass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus. It is a very broad study as media has many platforms in the modern world. Social Media is an industry that has gotten a lot of attention in recent years. Our primary form of entertainment is no longer our TVs but we have access to a screen about worldwide events all the time.
In 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed "applied humanities": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.

Formerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.

Brooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.

The University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).

University of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title "media studies" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)

The University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.




</doc>
<doc id="104952" url="https://en.wikipedia.org/wiki?curid=104952" title="Tragicomedy">
Tragicomedy

Tragicomedy is a literary genre that blends aspects of both tragic and comic forms. Most often seen in dramatic literature, the term can variously describe by either a tragic play which contains enough comic elements to lighten the overall mood or a serious play with a happy ending.

There is no complete formal definition of tragicomedy from the classical age. It appears that the Greek philosopher Aristotle had something like the Renaissance meaning of the term (that is, a serious action with a happy ending) in mind when, in "Poetics", he discusses tragedy with a dual ending. In this respect, a number of Greek and Roman plays, for instance "Alcestis", may be called tragicomedies, though without any definite attributes outside of plot. The word itself originates with the Roman comic playwright Plautus, who coined the term somewhat facetiously in the prologue to his play "Amphitryon". The character Mercury, sensing the indecorum of the inclusion of both kings and gods alongside servants in a comedy, declares that the play had better be a "tragicomoedia":

Plautus's comment had an arguably excessive impact on Renaissance aesthetic theory, which had largely transformed Aristotle's comments on drama into a rigid theory. For "rule mongers" (the term is Giordano Bruno's), "mixed" works such as those mentioned above, more recent "romances" such as "Orlando Furioso", and even "The Odyssey" were at best puzzles; at worst, mistakes. Two figures helped to elevate tragicomedy to the status of a regular genre, by which is meant one with its own set of rigid rules. Giovanni Battista Giraldi Cinthio, in the mid-sixteenth century, both argued that the tragedy-with-comic-ending ("tragedia de lieto fin") was most appropriate to modern times and produced his own examples of such plays. Even more important was Giovanni Battista Guarini. Guarini's "Il Pastor Fido", published in 1590, provoked a fierce critical debate in which Guarini's spirited defense of generic innovation eventually carried the day. Guarini's tragicomedy offered modulated action that never drifted too far either to comedy or tragedy, mannered characters, and a pastoral setting. All three became staples of continental tragicomedy for a century and more.

In England, where practice ran ahead of theory, the situation was quite different. In the sixteenth century, "tragicomedy" meant the native sort of romantic play that violated the unities of time, place, and action, that glibly mixed high- and low-born characters, and that presented fantastic actions. These were the features Philip Sidney deplored in his complaint against the "mungrell Tragy-comedie" of the 1580s, and of which Shakespeare's Polonius offers famous testimony: "The best actors in the world, either for tragedy, comedy, history, pastoral, pastoral-comical, historical-pastoral, tragical-historical, tragical-comical-historical-pastoral, scene individuable, or poem unlimited: Seneca cannot be too heavy, nor Plautus too light. For the law of writ and the liberty, these are the only men." Some aspects of this romantic impulse remain even in the work of more sophisticated playwrights: Shakespeare's last plays, which may well be called tragicomedies, have often been called romances.

By the early Stuart period, some English playwrights had absorbed the lessons of the Guarini controversy. John Fletcher's "The Faithful Shepherdess", an adaptation of Guarini's play, was produced in 1608. In the printed edition, Fletcher offered an interesting definition of the term, worth quoting at length: "A tragi-comedie is not so called in respect of mirth and killing, but in respect it wants deaths, which is enough to make it no tragedy, yet brings some neere it, which is inough to make it no comedie." Fletcher's definition focuses primarily on events: a play's genre is determined by whether or not people die in it, and in a secondary way on how close the action comes to a death. But, as Eugene Waith showed, the tragicomedy Fletcher developed in the next decade also had unifying stylistic features: sudden and unexpected revelations, outré plots, distant locales, and a persistent focus on elaborate, artificial rhetoric.

Some of Fletcher's contemporaries, notably Philip Massinger and James Shirley, wrote successful and popular tragicomedies. Richard Brome also essayed the form, but with less success. And many of their contemporary writers, ranging from John Ford to Lodowick Carlell to Sir Aston Cockayne, made attempts in the genre.

Tragicomedy remained fairly popular up to the closing of the theaters in 1642, and Fletcher's works were popular in the Restoration as well. The old styles were cast aside as tastes changed in the eighteenth century; the "tragedy with a happy ending" eventually developed into melodrama, in which form it still flourishes.

The more subtle criticism that developed after the Renaissance stressed the thematic and formal aspects of tragicomedy, rather than plot. Gotthold Ephraim Lessing defined it as a mixture of emotions in which "seriousness stimulates laughter, and pain pleasure." Even more commonly, tragicomedy's affinity with satire and "dark" comedy have suggested a tragicomic impulse in modern theatre with Luigi Pirandello who influenced Beckett. Also it can be seen in absurdist drama. Friedrich Dürrenmatt, the Swiss dramatist, suggested that tragicomedy was the inevitable genre for the twentieth century; he describes his play "The Visit" (1956) as a tragicomedy. Tragicomedy is a common genre in post-World War II British theatre, with authors as varied as Samuel Beckett, Tom Stoppard, John Arden, Alan Ayckbourn and Harold Pinter writing in this genre. Many writers of the metamodernist and postmodernist movements have made use of tragicomedy and/or gallows humor. A notable example of a metamodernist tragicomedy is David Foster Wallace's 1996 magnum opus, "Infinite Jest".




</doc>
<doc id="18856114" url="https://en.wikipedia.org/wiki?curid=18856114" title="Integrated human studies">
Integrated human studies

Integrated human studies is an emerging educational field that equips people with knowledge and competencies across a range of disciplines to enable them to address the challenges facing human beings this century. It differs from other interdisciplinary educational initiatives in that its curriculum is purpose designed rather than simply an amalgamation of existing disciplines.

Kyoto University in Japan has offered a formal course in Integrated Human Studies since 1992 when it reorganized its College of Liberal Arts and Sciences and renamed it the Faculty of Integrated Human Studies. This was subsequently (in 2003) integrated with the Graduate School of Human and Environmental Studies to create the new Graduate School of Human and Environmental Studies.

The University of Western Australia established the Center for Integrated Human Studies in early 2008. This center brings together the sciences, social sciences, arts and humanities to focus on the nature and future of humankind. Its fundamental concern is to promote human well-being at an individual, local and global level within a sustainable environment.

Integration of disciplinary fields has arisen as a response to the “increasing specialization of [university] courses to meet the demands of technological progress, economic growth and vocational training” resulting in the development of ever narrower fields of study at tertiary level. Proponents of integrated human studies believe that a broader, interdisciplinary approach is needed to enable future decision-makers to grasp the complexities of the issues facing humankind in the 21st century and craft workable solutions.


</doc>
<doc id="40138324" url="https://en.wikipedia.org/wiki?curid=40138324" title="Humanities Indicators">
Humanities Indicators

The Humanities Indicators is a project of the American Academy of Arts and Sciences that equips researchers and policymakers, universities, foundations, museums, libraries, humanities councils and other public institutions with statistical tools for answering basic questions about primary and secondary humanities education, undergraduate and graduate education in the humanities, the humanities workforce, levels and sources of program funding, public understanding and impact of the humanities, and other areas of concern in the humanities community. 

Data from the Humanities Indicators has been widely discussed in recent conversations about a "crisis in the humanities", in light of a national decline in the number of college majors. To address questions about the workforce outcomes of humanities graduates (which are often cited as playing a role in the falling number of majors as of 2015), the Indicators issued "The State of the Humanities 2018: Graduates in the Workforce & Beyond", which examined not only their employment and earnings relative to other fields, but also graduates’ satisfaction with their work after graduation and their lives more generally. The data reveal that despite disparities in median earnings, humanities majors are quite similar to graduates from other fields with respect to their "perceived" well-being. The report was widely cited in the media as an important intervention in the discussion.



</doc>
<doc id="1495838" url="https://en.wikipedia.org/wiki?curid=1495838" title="Romance studies">
Romance studies

Romance studies is an academic discipline that covers the study of the languages, literatures, and cultures of areas that speak a Romance language. Romance studies departments usually include the study of Spanish, French, Italian, and Portuguese. Additional languages of study include Romanian and Catalan, on one hand, and culture, history, and politics on the other hand.

Because most places in Latin America speak a Romance language, Latin America is also studied in Romance studies departments. As a result, non-Romance languages in use in Latin America, such as Quechua, are sometimes also taught in Romance studies departments.

Romance studies departments differ from single- or two-language departments in that they attempt to break down the barriers in scholarship among the various languages, through interdisciplinary or comparative work. These departments differ from Romance "language" departments in that they place a heavier emphasis on connections between language and literature, among others.



</doc>
<doc id="26826220" url="https://en.wikipedia.org/wiki?curid=26826220" title="Global intellectual history">
Global intellectual history

Global intellectual history is the history of thought in the world across the span of human history, from the invention of writing to the present. For information about the methodology of intellectual history, please see the relevant article.

In recent years, historians such as C. A. Bayly have been calling for a "global intellectual history" to be written. They stress that to understand the history of ideas across time and space, it is necessary to study from a cosmopolitan or global point of view the connections and the parallels in intellectual development across the world. Yet these separate histories and their convergence in the modern period have yet to be brought together into a single historical narrative. Nonetheless, some global histories, like Bayly's own "Birth of the Modern World" or David Armitage's "The Declaration of Independence: A Global History" offer contributions to the huge and necessarily collaborative project of writing the history of thought in a comparative and especially connective way. Other examples of transnational intellectual histories include Albert Hourani's "Arabic Thought in the Liberal Age".

The origins of human intellectual history arguably began before the invention of writing, but historians are by definition only concerned with the eras in which writing was present. In the spirit of a historiographic project that is relevant to all human beings and that has yet to be completed, the sections that follow briefly review currents of thought in pre-modern and modern history of the world, and are organized by geographic area (and within each section, chronologically).

The modern intellectual history of Europe cannot be separated from various bodies of ancient thought, from the works of classical Greek and Latin authors to the writings of the fathers of the Christian Church. Such a broad survey of topics is not attempted here, however. A debatable but defensible starting point for modern European thought might instead be identified with the birth of scholasticism and humanism in the 13th and 14th centuries. Both of these intellectual currents were associated with classical revivals (in the case of scholasticism, the rediscovery of Aristotle; in the case of humanism, of Latin antiquity, especially Cicero) and with prominent founders, Aquinas and Petrarch respectively. But they were both significantly original intellectual experiences, as well as self-consciously modern, so that they make an appropriate starting point for this survey.

What follows below is a selective and far from complete listing of significant trends and individuals in the history of European thought. While movements such as the Enlightenment or Romanticism are relatively imprecise approximations, rarely taken too seriously by scholars, they are good starting points for approaching the enormous complexity of the history of Europe's intellectual heritage. It is hoped that interested readers will pursue the listed topics in greater depth by consulting the respective articles and the suggestions for further reading.

The intellectual history of western Europe and the Americas includes:

"Pre-Modern East Asia"

The intellectual history of China is connected to the birth of scholarship in ancient China, the creation of Confucianism with its extensive exegesis of the texts of Confucius, and the active part of scholars in governments. In Korea, the yangban scholar movement drove the development of Korean intellectual history from the late Goryeo to the golden age of intellectual achievement in the Joseon Dynasty.

In China, "literati" referred to the government officials who formed the ruling class in China for over two thousand years. These "scholar-bureaucrats" were a status group of educated laymen, not ordained priests. They were not a hereditary group as their position depended on their knowledge of writing and literature. After 200 B.C. the system of selection of candidates was influenced by Confucianism and established its ethic among the literati.

Confucianism (儒家, literally "scholarly tradition") is a Chinese ethical and philosophical system originally developed from the teachings of the early Chinese sage Confucius. Confucius is seen as the founder of the teachings of Confucianism, although he claimed to follow the ways of people before him. Confucianism is a complex system of moral, social, political, philosophical, and religious thought which has had tremendous influence on the culture and history of East Asia. Some people in Europe have considered it to have been the "state religion" in East Asian countries because of governmental promotion of Confucianist values and needs.

Other ancient intellectual currents in East Asia include Buddhism and Daoism.

"Modern East Asia"

The modern intellectual history of China is considered to begin with the arrival of the Jesuits in the sixteenth century. The Jesuits brought with them new astronomical and cartographic knowledge, and were responsible for new developments in Chinese science. Science in modern China has been the subject of the work of the historian Benjamin Elman.

"Pre-Modern South Asia"

Indian thought is a broad topic that includes the ancient epics of South Asia, the development of what is now called Hinduism and Hindu philosophy and the rise of Buddhism, as well as many other topics relating to the political and artistic lives of pre-modern South Asia.

Ram Sharan Sharma's work "Aspects of Political Ideas and Institutions in India" (Motilal Banarsidass is the most authoritative account of ancient Indian political ideas and institutions. It deals with the intellectual standards in ancient India in terms of political institutions.

"Pre-Modern History"

The culture of the ancient Near East and eventually of much of Africa as well was modified significantly by the arrival of Islam beginning in the seventh century CE. The history of Islam has been the work of many scholars, both Muslim and non-Muslim, and including such luminaries as Ignác Goldziher, Marshall Hodgson and in more recent times Patricia Crone. Islamic culture is not a simple and unified entity. The history of Islam, like that of other religions, is a history of different interpretations and approaches to Islam. There is no a-historical Islam outside the process of historical development.

Islamic thought includes a variety of different intellectual disciplines, including theology, the study of the Qur'an, the study of Hadith, history, grammar, rhetoric and philosophy. For more information see the Islamic Golden Age.

Classical Islamic scholars and authors include:

Persian philosophy can be traced back as far as to Old Iranian philosophical traditions and thoughts which originated in ancient Indo-Iranian roots and were considerably influenced by Zarathustra's teachings. Throughout Iranian history and due to remarkable political and social changes such as the Macedonian, Arab and Mongol invasions of Persia a wide spectrum of schools of thoughts showed a variety of views on philosophical questions extending from Old Iranian and mainly Zoroastrianism-related traditions to schools appearing in the late pre-Islamic era such as Manicheism and Mazdakism as well as various post-Islamic schools. Iranian philosophy after Arab invasion of Persia, is characterized by different interactions with the Old Iranian philosophy, the Greek philosophy and with the development of Islamic philosophy. The Illumination School and the Transcendent Philosophy are regarded as two of the main philosophical traditions of that era in Persia.

"Modern Near and Middle East"

Islam and modernity encompass the relation and compatibility between the phenomenon of modernity, its related concepts and ideas, and the religion of Islam. In order to understand the relation between Islam and modernity, one point should be made in the beginning. Similarly, modernity is a complex and multidimensional phenomenon rather than a unified and coherent phenomenon. It has historically had different schools of thoughts moving in many directions.

Intellectual movements in Iran involve the Iranian experience of modernism, through which Iranian modernity and its associated art, science, literature, poetry, and political structures have been evolving since the 19th century. Religious intellectualism in Iran develops gradually and subtly. It reached its apogee during the Persian Constitutional Revolution (1906–11). The process involved numerous philosophers, sociologists, political scientists and cultural theorists. However the associated art, cinema and poetry remained to be developed.

"Modern Africa"

Recent concepts about African culture include the African Renaissance and Afrocentrism. The African Renaissance is a concept popularized by South African President Thabo Mbeki who called upon the African people and nations to solve the many problems troubling the African continent. It reached its height in the late 1990s but continues to be a key part of the post-apartheid intellectual agenda in South Africa. The concept however extends well beyond intellectual life to politics and economic development.

With the rise of Afrocentrism, the push away from Eurocentrism has led to the focus on the contributions of African people and their model of world civilization and history. Afrocentrism aims to shift the focus from a perceived European-centered history to an African-centered history. More broadly, Afrocentrism is concerned with distinguishing the influence of European and Oriental peoples from African achievements.

Notable modern African intellectual include:


</doc>
<doc id="53132" url="https://en.wikipedia.org/wiki?curid=53132" title="Humanities">
Humanities

Humanities are academic disciplines that study aspects of human society and culture. In the Renaissance, the term contrasted with divinity and referred to what is now called classics, the main area of secular study in universities at the time. Today, the humanities are more frequently contrasted with natural, and sometimes social, sciences as well as professional training.

The humanities use methods that are primarily critical, or speculative, and have a significant historical element—as distinguished from the mainly empirical approaches of the natural sciences, yet, unlike the sciences, it has no central discipline.
The humanities include ancient and modern languages, literature, philosophy, history, human geography, law, politics, religion, and art.

Scholars in the humanities are "humanity scholars" or "humanists". The term "humanist" also describes the philosophical position of humanism, which some "antihumanist" scholars in the humanities reject. The Renaissance scholars and artists were also called humanists. Some secondary schools offer humanities classes usually consisting of literature, global studies and art.

Human disciplines like history and cultural anthropology study subject matters that the manipulative experimental method does not apply to—and instead mainly use the comparative method and comparative research.

Anthropology is the holistic "science of humans", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.

The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (άνθρωπος) is from the Greek for "human being" or "person". Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences".

The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior". Today, anthropologists use terms such as "less complex" societies, or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist", to discuss humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.

The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Archaeology is the study of human activity through the recovery and analysis of material culture. The archaeological record consists of artifacts, architecture, biofacts or ecofacts, and cultural landscapes. Archaeology can be considered both a social science and a branch of the humanities. It has various goals, which range from understanding culture history to reconstructing past lifeways to documenting and explaining changes in human societies through time.

Archaeology is thought of as a branch of anthropology in the United States, while in Europe, it is viewed as a discipline in its own right, or grouped under other related disciplines such as history.

Classics, in the Western academic tradition, refers to the studies of the cultures of classical antiquity, namely Ancient Greek and Latin and the Ancient Greek and Roman cultures. Classical studies is considered one of the cornerstones of the humanities; however, its popularity declined during the 20th century. Nevertheless, the influence of classical ideas on many humanities disciplines, such as philosophy and literature, remains strong.

History is systematically collected information about the past. When used as the name of a field of study, "history" refers to the study and interpretation of the record of humans, societies, institutions, and any topic that has changed over time.

Traditionally, the study of history has been considered a part of the humanities. In modern academia, history is occasionally classified as a social science.

While the scientific study of language is known as linguistics and is generally considered a social science, a natural science or a cognitive science, the study of languages is still central to the humanities. A good deal of twentieth-century and twenty-first-century philosophy has been devoted to the analysis of language and to the question of whether, as Wittgenstein claimed, many of our philosophical confusions derive from the vocabulary we use; literary theory has explored the rhetorical, associative, and ordering features of language; and historical linguists have studied the development of languages across time. Literature, covering a variety of uses of language including prose forms (such as the novel), poetry and drama, also lies at the heart of the modern humanities curriculum. College-level programs in a foreign language usually include study of important works of the literature in that language, as well as the language itself.

 In common parlance, law means a rule that (unlike a rule of ethics) is enforceable through institutions. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and discipline of the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on how productivity is organised and the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed, and the adjective "legal" comes from the Latin word "LEX".

 Literature is a term that does not have a universally accepted definition, but which has variably included all written work; writing that possesses literary merit; and language that foregrounds literariness, as opposed to ordinary language. Etymologically the term derives from Latin "literatura/litteratura" "writing formed with letters", although some definitions include spoken or sung texts. Literature can be classified according to whether it is fiction or non-fiction, and whether it is poetry or prose; it can be further distinguished according to major forms such as the novel, short story or drama; and works are often categorised according to historical periods, or according to their adherence to certain aesthetic features or expectations (genre).

Philosophy—etymologically, the "love of wisdom"—is generally the study of problems concerning matters such as existence, knowledge, justification, truth, justice, right and wrong, beauty, validity, mind, and language. Philosophy is distinguished from other ways of addressing these issues by its critical, generally systematic approach and its reliance on reasoned argument, rather than experiments (experimental philosophy being an exception).

Philosophy used to be a very comprehensive term, including what have subsequently become separate disciplines, such as physics. (As Immanuel Kant noted, "Ancient Greek philosophy was divided into three sciences: physics, ethics, and logic.") Today, the main fields of philosophy are logic, ethics, metaphysics, and epistemology. Still, it continues to overlap with other disciplines. The field of semantics, for example, brings philosophy into contact with linguistics.

Since the early twentieth century, philosophy in English-speaking universities has moved away from the humanities and closer to the formal sciences, becoming much more "analytic." Analytic philosophy is marked by emphasis on the use of logic and formal methods of reasoning, conceptual analysis, and the use of symbolic and/or mathematical logic, as contrasted with the Continental style of philosophy. This method of inquiry is largely indebted to the work of philosophers such as Gottlob Frege, Bertrand Russell, G.E. Moore, and Ludwig Wittgenstein.

New philosophies and religions arose in both east and west, particularly around the 6th century BC. Over time, a great variety of religions developed around the world, with Hinduism, Jainism, and Buddhism in India, and Zoroastrianism in Persia being some of the earliest major faiths. In the east, three schools of thought were to dominate Chinese thinking until the modern day. These were Taoism, Legalism, and Confucianism. The Confucian tradition, which would attain predominance, looked not to the force of law, but to the power and example of tradition for political morality. In the west, the Greek philosophical tradition, represented by the works of Plato and Aristotle, was diffused throughout Europe and the Middle East by the conquests of Alexander of Macedon in the 4th century BC.

Abrahamic religions are those religions deriving from a common ancient tradition and traced by their adherents to Abraham (circa 1900 BCE), a patriarch whose life is narrated in the Hebrew Bible/Old Testament, where he is described as a prophet (Genesis 20:7), and in the Quran, where he also appears as a prophet. This forms a large group of related largely monotheistic religions, generally held to include Judaism, Christianity, and Islam, and comprises over half of the world's religious adherents.

The performing arts differ from the visual arts in so far as the former uses the artist's own body, face, and presence as a medium, and the latter uses materials such as clay, metal, or paint, which can be molded or transformed to create some art object. Performing arts include acrobatics, busking, comedy, dance, film, magic, music, opera, juggling, marching arts, such as brass bands, and theatre.

Artists who participate in these arts in front of an audience are called performers, including actors, comedians, dancers, musicians, and singers. Performing arts are also supported by workers in related fields, such as songwriting and stagecraft. Performers often adapt their appearance, such as with costumes and stage makeup, etc. There is also a specialized form of fine art in which the artists "perform" their work live to an audience. This is called Performance art. Most performance art also involves some form of plastic art, perhaps in the creation of props. Dance was often referred to as a "plastic art" during the Modern dance era.

Musicology as an academic discipline can take a number of different paths, including historical musicology, ethnomusicology and music theory. Undergraduate music majors generally take courses in all of these areas, while graduate students focus on a particular path. In the liberal arts tradition, musicology is also used to broaden skills of non-musicians by teaching skills such as concentration and listening.

Theatre (or theater) (Greek "theatron", "θέατρον") is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound and spectacle — indeed any one or more elements of the other performing arts. In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, Chinese opera, mummers' plays, and pantomime.

Dance (from Old French "dancier", perhaps from Frankish) generally refers to human movement either used as a form of expression or presented in a social, spiritual or performance setting. Dance is also used to describe methods of non-verbal communication (see body language) between humans or animals (bee dance, mating dance), and motion in inanimate objects ("the leaves danced in the wind"). Choreography is the art of creating dances, and the person who does this is called a choreographer.

Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic, and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet.

The great traditions in art have a foundation in the art of one of the ancient civilizations, such as Ancient Japan, Greece and Rome, China, India, Greater Nepal, Mesopotamia and Mesoamerica.

Ancient Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features (e.g., Zeus' thunderbolt).

In Byzantine and Gothic art of the Middle Ages, the dominance of the church insisted on the expression of biblical and not material truths. The Renaissance saw the return to valuation of the material world, and this shift is reflected in art forms, which show the corporeality of the human body, and the three-dimensional reality of landscape.

Eastern art has generally worked in a style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade and reflection). A characteristic of this style is that the local colour is often defined by an outline (a contemporary equivalent is the cartoon). This is evident in, for example, the art of India, Tibet and Japan.

Religious Islamic art forbids iconography, and expresses religious ideas through geometry instead. The physical and rational certainties depicted by the 19th-century Enlightenment were shattered not only by new discoveries of relativity by Einstein and of unseen psychology by Freud, but also by unprecedented technological development. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art.

Drawing is a means of making a picture, using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoals, pastels, and markers. Digital tools that simulate the effects of these are also used. The main techniques used in drawing are: line drawing, hatching, crosshatching, random hatching, scribbling, stippling, and blending. A computer aided designer who excels in technical drawing is referred to as a "draftsman" or "draughtsman".

Painting taken literally is the practice of applying pigment suspended in a carrier (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas or a wall. However, when used in an artistic sense it means the use of this activity in combination with drawing, composition and other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel to the human body itself.

Colour is highly subjective, but has observable psychological effects, although these can differ from one culture to the next. Black is associated with mourning in the West, but elsewhere white may be. Some painters, theoreticians, writers and scientists, including Goethe, Kandinsky, Isaac Newton, have written their own colour theories. Moreover, the use of language is only a generalization for a colour equivalent. The word "red", for example, can cover a wide range of variations on the pure red of the spectrum. There is not a formalized register of different colours in the way that there is agreement on different notes in music, such as C or C# in music, although the Pantone system is widely used in the printing and design industry for this purpose.

Modern artists have extended the practice of painting considerably to include, for example, collage. This began with cubism and is not painting in strict sense. Some modern painters incorporate different materials such as sand, cement, straw or wood for their texture. Examples of this are the works of Jean Dubuffet or Anselm Kiefer. Modern and contemporary art has moved away from the historic value of craft in favour of concept; this has led some to say that painting, as a serious art form, is dead, although this has not deterred the majority of artists from continuing to practise it either as whole or part of their work.

The word "humanities" is derived from the Renaissance Latin expression "studia humanitatis", or "study of "humanitas"" (a classical Latin word meaning—in addition to "humanity"—"culture, refinement, education" and, specifically, an"education befitting a cultivated man"). In its usage in the early 15th century, the "studia humanitatis" was a course of studies that consisted of grammar, poetry, rhetoric, history, and moral philosophy, primarily derived from the study of Latin and Greek classics. The word "humanitas" also gave rise to the Renaissance Italian neologism "umanisti", whence "humanist", "Renaissance humanism".

In the West, the study of the humanities can be traced to ancient Greece, as the basis of a broad education for citizens. During Roman times, the concept of the seven liberal arts evolved, involving grammar, rhetoric and logic (the trivium), along with arithmetic, geometry, astronomy and music (the quadrivium). These subjects formed the bulk of medieval education, with the emphasis being on the humanities as skills or "ways of doing".

A major shift occurred with the Renaissance humanism of the fifteenth century, when the humanities began to be regarded as subjects to study rather than practice, with a corresponding shift away from traditional fields into areas such as literature and history. In the 20th century, this view was in turn challenged by the postmodernist movement, which sought to redefine the humanities in more egalitarian terms suitable for a democratic society since the Greek and Roman societies in which the humanities originated were not at all democratic. This was in keeping with the postmodernists' nuanced view of themselves as the culmination of history.

For many decades, there has been a growing public perception that a humanities education inadequately prepares graduates for employment. The common belief is that graduates from such programs face underemployment and incomes too low for a humanities education to be worth the investment.

In fact, humanities graduates find employment in a wide variety of management and professional occupations. In Britain, for example, over 11,000 humanities majors found employment in the following occupations:
Many humanities graduates finish university with no career goals in mind. Consequently, many spend the first few years after graduation deciding what to do next, resulting in lower incomes at the start of their career; meanwhile, graduates from career-oriented programs experience more rapid entry into the labour market. However, usually within five years of graduation, humanities graduates find an occupation or career path that appeals to them.

There is empirical evidence that graduates from humanities programs earn less than graduates from other university programs. However, the empirical evidence also shows that humanities graduates still earn notably higher incomes than workers with no postsecondary education, and have job satisfaction levels comparable to their peers from other fields. Humanities graduates also earn more as their careers progress; ten years after graduation, the income difference between humanities graduates and graduates from other university programs is no longer statistically significant. Humanities graduates can earn even higher incomes if they obtain advanced or professional degrees.

The Humanities Indicators, unveiled in 2009 by the American Academy of Arts and Sciences, are the first comprehensive compilation of data about the humanities in the United States, providing scholars, policymakers and the public with detailed information on humanities education from primary to higher education, the humanities workforce, humanities funding and research, and public humanities activities. Modeled after the National Science Board's Science and Engineering Indicators, the Humanities Indicators are a source of reliable benchmarks to guide analysis of the state of the humanities in the United States.

If "The STEM Crisis Is a Myth", statements about a "crisis" in the humanities are also misleading and ignore data of the sort collected by the Humanities Indicators.

The 1980 United States Rockefeller Commission on the Humanities described the humanities in its report, "The Humanities in American Life":
Through the humanities we reflect on the fundamental question: What does it mean to be human? The humanities offer clues but never a complete answer. They reveal how people have tried to make moral, spiritual, and intellectual sense of a world where irrationality, despair, loneliness, and death are as conspicuous as birth, friendship, hope, and reason.

In 1950, a little over 1 percent of 22-year-olds in the United States had earned a humanities degrees (defined as a degree in English, language, history, philosophy); in 2010, this had doubled to about 2 and a half percent. In part, this is because there was an overall rise in the number of Americans who have any kind of college degree. (In 1940, 4.6 percent had a four-year degree; in 2016, 33.4 percent had one.) As a percentage of the type of degrees awarded, however, the humanities seem to be declining. Harvard University provides one example. In 1954, 36 percent of Harvard undergraduates majored in the humanities, but in 2012, only 20 percent took that course of study. Professor Benjamin Schmidt of Northeastern University has documented that between 1990 to 2008, degrees in English, history, foreign languages, and philosophy have decreased from 8 percent to just under 5 percent of all U.S. college degrees.

The Commission on the Humanities and Social Sciences 2013 report "The Heart of the Matter" supports the notion of a broad "liberal arts education", which includes study in disciplines from the natural sciences to the arts as well as the humanities.

Many colleges provide such an education; some require it. The University of Chicago and Columbia University were among the first schools to require an extensive core curriculum in philosophy, literature, and the arts for all students. Other colleges with nationally recognized, mandatory programs in the liberal arts are Fordham University, St. John's College, Saint Anselm College and Providence College. Prominent proponents of liberal arts in the United States have included Mortimer J. Adler and E. D. Hirsch, Jr..

Researchers in the humanities have developed numerous large- and small-scale digital corpora, such as digitized collections of historical texts, along with the digital tools and methods to analyze them. Their aim is both to uncover new knowledge about corpora and to visualize research data in new and revealing ways. Much of this activity occurs in a field called the digital humanities.

Politicians in the United States currently espouse a need for increased funding of the STEM fields, science, technology, engineering, mathematics. Federal funding represents a much smaller fraction of funding for humanities than other fields such as STEM or medicine. The result was a decline of quality in both college and pre-college education in the humanities field.

Former four-term Louisiana Governor, Edwin Edwards (D), has recently acknowledged the importance of the humanities. In a video address to the academic conference, "Revolutions in Eighteenth-Century Sociability", Edwards said

The contemporary debate in the field of critical university studies centers around the declining value of the humanities. As in America, there is a perceived decline in interest within higher education policy in research that is qualitative and does not produce marketable products. This threat can be seen in a variety of forms across Europe, but much critical attention has been given to the field of research assessment in particular. For example, the UK [Research Excellence Framework] has been subject to criticism due to its assessment criteria from across the humanities, and indeed, the social sciences. In particular, the notion of "impact" has generated significant debate.

In India, there are many institutions that offer undergraduate UG or bachelor's degree/diploma and postgraduate PG or master's degree/diploma as well as doctoral PhD and postdoctoral studies and research, in this academic discipline. Manipal Academy of Higher Education - MAHE, an Institution of Eminence as recognised by MHRD of Govt of India in 2018, houses a Faculty of Liberal Arts, Humanities and Social Sciences. 

Since the late 19th century, a central justification for the humanities has been that it aids and encourages self-reflection—a self-reflection that, in turn, helps develop personal consciousness or an active sense of civic duty.

Wilhelm Dilthey and Hans-Georg Gadamer centered the humanities' attempt to distinguish itself from the natural sciences in humankind's urge to understand its own experiences. This understanding, they claimed, ties like-minded people from similar cultural backgrounds together and provides a sense of cultural continuity with the philosophical past.

Scholars in the late 20th and early 21st centuries extended that "narrative imagination" to the ability to understand the records of lived experiences outside of one's own individual social and cultural context. Through that narrative imagination, it is claimed, humanities scholars and students develop a conscience more suited to the multicultural world we live in. That conscience might take the form of a passive one that allows more effective self-reflection or extend into active empathy that facilitates the dispensation of civic duties a responsible world citizen must engage in. There is disagreement, however, on the level of influence humanities study can have on an individual and whether or not the understanding produced in humanistic enterprise can guarantee an "identifiable positive effect on people."

There are three major branches of knowledge: natural sciences, social sciences, and the humanities. Technology is the practical extension of the natural sciences, as politics is the extension of the social sciences. Similarly, the humanities have their own practical extension, sometimes called "transformative humanities" (transhumanities) or "culturonics" (Mikhail Epstein's term):
Technology, politics and culturonics are designed to transform what their respective disciplines study: nature, society, and culture. The field of transformative humanities includes various practicies and technologies, for example, language planning, the construction of new languages, like Esperanto, and invention of new artistic and literary genres and movements in the genre of manifesto, like Romanticism, Symbolism, or Surrealism. Humanistic invention in the sphere of culture, as a practice complementary to scholarship, is an important aspect of the humanities.

The divide between humanistic study and natural sciences informs arguments of meaning in humanities as well. What distinguishes the humanities from the natural sciences is not a certain subject matter, but rather the mode of approach to any question. Humanities focuses on understanding meaning, purpose, and goals and furthers the appreciation of singular historical and social phenomena—an interpretive method of finding "truth"—rather than explaining the causality of events or uncovering the truth of the natural world. Apart from its societal application, narrative imagination is an important tool in the (re)production of understood meaning in history, culture and literature.

Imagination, as part of the tool kit of artists or scholars, helps create meaning that invokes a response from an audience. Since a humanities scholar is always within the nexus of lived experiences, no "absolute" knowledge is theoretically possible; knowledge is instead a ceaseless procedure of inventing and reinventing the context a text is read in. Poststructuralism has problematized an approach to the humanistic study based on questions of meaning, intentionality, and authorship. In the wake of the death of the author proclaimed by Roland Barthes, various theoretical currents such as deconstruction and discourse analysis seek to expose the ideologies and rhetoric operative in producing both the purportedly meaningful objects and the hermeneutic subjects of humanistic study. This exposure has opened up the interpretive structures of the humanities to criticism humanities scholarship is "unscientific" and therefore unfit for inclusion in modern university curricula because of the very nature of its changing contextual meaning.

Some, like Stanley Fish, have claimed that the humanities can defend themselves best by refusing to make any claims of utility. (Fish may well be thinking primarily of literary study, rather than history and philosophy.) Any attempt to justify the humanities in terms of outside benefits such as social usefulness (say increased productivity) or in terms of ennobling effects on the individual (such as greater wisdom or diminished prejudice) is ungrounded, according to Fish, and simply places impossible demands on the relevant academic departments. Furthermore, critical thinking, while arguably a result of humanistic training, can be acquired in other contexts. And the humanities do not even provide any more the kind of social cachet (what sociologists sometimes call "cultural capital") that was helpful to succeed in Western society before the age of mass education following World War II.

Instead, scholars like Fish suggest that the humanities offer a unique kind of pleasure, a pleasure based on the common pursuit of knowledge (even if it is only disciplinary knowledge). Such pleasure contrasts with the increasing privatization of leisure and instant gratification characteristic of Western culture; it thus meets Jürgen Habermas' requirements for the disregard of social status and rational problematization of previously unquestioned areas necessary for an endeavor which takes place in the bourgeois public sphere. In this argument, then, only the academic pursuit of pleasure can provide a link between the private and the public realm in modern Western consumer society and strengthen that public sphere that, according to many theorists, is the foundation for modern democracy.

Others, like Mark Bauerlein, argue that professors in the humanities have increasingly abandoned proven methods of epistemology ("I care only about the quality of your arguments, not your conclusions.") in favor of indoctrination ("I care only about your conclusions, not the quality of your arguments."). The result is that professors and their students adhere rigidly to a limited set of viewpoints, and have little interest in, or understanding of, opposing viewpoints. Once they obtain this intellectual self-satisfaction, persistent lapses in learning, research, and evaluation are common.

Implicit in many of these arguments supporting the humanities are the makings of arguments against public support of the humanities. Joseph Carroll asserts that we live in a changing world, a world where "cultural capital" is replaced with "scientific literacy", and in which the romantic notion of a Renaissance humanities scholar is obsolete. Such arguments appeal to judgments and anxieties about the essential uselessness of the humanities, especially in an age when it is seemingly vitally important for scholars of literature, history and the arts to engage in "collaborative work with experimental scientists or even simply to make "intelligent use of the findings from empirical science."




</doc>
<doc id="4633449" url="https://en.wikipedia.org/wiki?curid=4633449" title="Literary nonsense">
Literary nonsense

Literary nonsense (or nonsense literature) is a broad categorization of literature that balances elements that make sense with some that do not, with the effect of subverting language conventions or logical reasoning. Even though the most well-known form of literary nonsense is nonsense verse, the genre is present in many forms of literature.

The effect of nonsense is often caused by an excess of meaning, rather than a lack of it. Its humor is derived from its nonsensical nature, rather than wit or the "joke" of a punchline.

Literary nonsense, as recognized since the nineteenth century, comes from a combination of two broad artistic sources. The first and older source is the oral folk tradition, including games, songs, dramas, and rhymes, such as the nursery rhyme "Hey Diddle Diddle". The literary figure Mother Goose represents common incarnations of this style of writing.

The second, newer source of literary nonsense is in the intellectual absurdities of court poets, scholars, and intellectuals of various kinds. These writers often created sophisticated nonsense forms of Latin parodies, religious travesties, and political satire, though these texts are distinguished from more pure satire and parody by their exaggerated nonsensical effects.

Today's literary nonsense comes from a combination of both sources. Though not the first to write this hybrid kind of nonsense, Edward Lear developed and popularized it in his many limericks (starting with "A Book of Nonsense", 1846) and other famous texts such as "The Owl and the Pussycat", "The Dong with a Luminous Nose," "" and "The Story of the Four Little Children Who Went Around the World". Lewis Carroll continued this trend, making literary nonsense a worldwide phenomenon with "Alice's Adventures in Wonderland" (1865) and "Through the Looking-Glass" (1871). Carroll's poem "Jabberwocky", which appears in the latter book, is often considered quintessential nonsense literature.

In literary nonsense, certain formal elements of language and logic that facilitate meaning are balanced by elements that negate meaning. These formal elements include semantics, syntax, phonetics, context, representation, and formal diction. The genre is most easily recognizable by the various techniques or devices it uses to create this balance of meaning and lack of meaning, such as faulty cause and effect, portmanteau, neologism, reversals and inversions, imprecision (including gibberish), simultaneity, picture/text incongruity, arbitrariness, infinite repetition, negativity or mirroring, and misappropriation. Nonsense tautology, reduplication, and absurd precision have also been used in the nonsense genre. For a text to be within the genre of literary nonsense, it must have an abundance of nonsense techniques woven into the fabric of the piece. If the text employs only occasional nonsense devices, then it may not be classified as literary nonsense, though there may be a nonsensical effect to certain portions of the work. Laurence Sterne's "Tristram Shandy", for instance, employs the nonsense device of imprecision by including a blank page, but this is only one nonsense device in a novel that otherwise makes sense. In Flann O'Brien's "The Third Policeman", on the other hand, many of the devices of nonsense are present throughout, and thus it could be considered a nonsense novel.

Gibberish, light verse, fantasy, and jokes and riddles are sometimes mistaken for literary nonsense, and the confusion is greater because nonsense can sometimes inhabit these (and many other) forms and genres.

Pure gibberish, as in the "hey diddle diddle" of nursery rhyme, is a device of nonsense, but it does not make a text, overall, literary nonsense. If there is not significant sense to balance out such devices, then the text dissolves into literal (as opposed to literary) nonsense.

Light verse, which is generally speaking humorous verse meant to entertain, may share humor, inconsequentiality, and playfulness, with nonsense, but it usually has a clear point or joke, and does not have the requisite tension between meaning and lack of meaning.

Nonsense is distinct from fantasy, though there are sometimes resemblances between them. While nonsense may employ the strange creatures, other worldly situations, magic, and talking animals of fantasy, these supernatural phenomena are not nonsensical if they have a discernible logic supporting their existence. The distinction lies in the coherent and unified nature of fantasy. Everything follows logic within the rules of the fantasy world; the nonsense world, on the other hand, has no comprehensive system of logic, although it may imply the existence of an inscrutable one, just beyond our grasp. The nature of magic within an imaginary world is an example of this distinction. Fantasy worlds employ the presence of magic to logically explain the impossible. In nonsense literature, magic is rare but when it does occur, its nonsensical nature only adds to the mystery rather than logically explaining anything. An example of nonsensical magic occurs in Carl Sandburg's "Rootabaga Stories", when Jason Squiff, in possession of a magical "gold buckskin whincher", has his hat, mittens, and shoes turn into popcorn because, according to the "rules" of the magic, "You have a letter Q in your name and because you have the pleasure and happiness of having a Q in your name you must have a popcorn hat, popcorn mittens and popcorn shoes".

Riddles only appear to be nonsense until the answer is found. The most famous nonsense riddle is only so because it originally had no answer. In Carroll's "Alice in Wonderland", the Mad Hatter asks Alice "Why is a raven like a writing-desk?" When Alice gives up, the Hatter replies that he does not know either, creating a nonsensical riddle. Some seemingly nonsense texts are actually riddles, such as the popular 1940s song "Mairzy Doats", which at first appears to have little discernible meaning but has a discoverable message. Jokes are not nonsense because their humor comes from their making sense, from our "getting" it, while nonsense is funny because it does "not" make sense, we do not "get" it.

While most contemporary nonsense has been written for children, the form has an extensive history in adult configurations before the nineteenth century. Figures such as John Hoskyns, Henry Peacham, John Sandford, and John Taylor lived in the early seventeenth century and were noted nonsense authors in their time. Nonsense was also an important element in the works of Flann O'Brien and Eugène Ionesco. Literary nonsense, as opposed to the folk forms of nonsense that have always existed in written history, was only first written for children in the early nineteenth century. It was popularized by Edward Lear and then later by Lewis Carroll. Today literary nonsense enjoys a shared audience of adults and children.

"Note: None of these writers are considered "exclusively" a "nonsense writer". Some of them wrote texts considered to be in the genre (as in Lear, Carroll, Gorey, Lennon, Sandburg), while others only use nonsense as an occasional device (as in Joyce, Juster). All of these writers wrote outside of the nonsense genre also."


Writers of nonsense from other languages include:

Bob Dylan wrote some lyrics that contain nonsense techniques, especially around the mid-1960s, in songs like "Bob Dylan's 115th Dream" and "Tombstone Blues".<br>

David Byrne, frontman of the art rock/new wave group Talking Heads, employed nonsensical techniques in songwriting. Byrne often combined coherent yet unrelated phrases to make up nonsensical lyrics in songs such as: "Burning Down the House", "Making Flippy Floppy" and "Girlfriend Is Better". This tendency formed the basis of the title for the Talking Heads concert movie, "Stop Making Sense". More recently, Byrne published "Arboretum" (2006), a volume of tree-like diagrams that are, "mental maps of imaginary territory". He continues, explaining the aspect of nonsense: "Irrational logic – [...]. The application of logical scientific rigor and form to basically irrational premises. To proceed, carefully and deliberately, from nonsense, with a straight face, often arriving at a new kind of sense."

Syd Barrett, one-time frontman and founder of Pink Floyd, was known for his often nonsensical songwriting influenced by Lear and Carroll that featured heavily on Pink Floyd's first album, "The Piper at the Gates of Dawn".

Glen Baxter's comic work is often nonsense, relying on the baffling interplay between word and image.

"Zippy the Pinhead", by Bill Griffith, is an American strip that mixes philosophy, including what has been called "Heideggerian disruptions" and pop culture in its nonsensical processes.


_________. "The Complete Works of Lewis Carroll". London: Nonesuch Press, 1940.
_________. "Your Disgusting Head: The Darkest, Most Offensive—and Moist—Secrets of Your Ears, Mouth and Nose, Volume 2., 2004.
_________. "Animals of the Ocean, In particular the giant squid", Volume 3, 2006
_________. "Cold Fusion", Volume 4, 2008
_________. "Amphigorey too". New York: Perigee, 1975.
_________. "Amphigorey Also". Harvest, 1983.
_________. "Amphigorey Again". Barnes & Noble, 2002.
_________. "The Writings of John Lennon: In His Own Write, A Spaniard in the Works" New York: Simon and Schuster, 1964, 1965.
_________. "Captain Slaughterboard Drops Anchor". London: Country Life Book, 1939.
_________. "Rhymes Without Reason". Eyre & Spottiswoode, 1944.
_________. "Titus Groan". London:, London: Methuen, 1946.
_________. "Wish You Were Here", Chennai: Tara Publishing, 2003.
_________. "Today is My Day", illus. Piet Grobler, Chennai: Tara Publishing, 2003.
_________. "Tirra Lirra: Rhymes Old and New", illus. Marguerite Davis. London: George G. Harrap, 1933.
_________. "More Rootabaga Stories".


_________. "Edward Lear's Limericks and the Reversals of Nonsense," Victorian Poetry, 29 (1988): 285–299.
_________. "The Limerick and the Space of Metaphor," Genre, 21 (Spring 1988): 65–91.
_________. "Society and the Self in the Limericks of Lear," The Review of English Studies, 177 (1994): 42–62.
_________. "Edward Lear's Limericks and Their Illustrations" in Explorations in the Field of Nonsense, ed. Wim Tigges (Amsterdam: Rodopi, 1987), pp. 101–116.
_________. "An Introduction to the Nonsense Literature of Edward Lear and Lewis Carroll" in Explorations in the Field of Nonsense, ed. Wim Tigges (Amsterdam: Rodopi, 1987), pp. 47–60.
_________. "Edward Lear: Eccentricity and Victorian Angst," Victorian Poetry, 16 (1978): 112–122.
_________. "A New Defense of Nonsense; or, 'Where is his phallus?' and other questions not to ask" in Children's Literature Association Quarterly, Winter 1999–2000. Volume 24, Number 4 (186–194)
_________. "An Indian Nonsense Naissance" in "The Tenth Rasa: An Anthology of Indian Nonsense", edited by Michael Heyman, with Sumanyu Satpathy and Anushka Ravishankar. New Delhi: Penguin, 2007.<br>
_________. "Nonsense", with Kevin Shortsleeve, in "Keywords for Children's Literature". eds. Philip Nel and Lissa Paul. New York: NYU Press, 2011.<br>
_________. "The Perils and Nonpereils of Literary Nonsense Translation." Words Without Borders. June 2, 2014.
_________. "Edward Lear, 1812–1888". London: Weidenfeld & Nicolson, 1985.
_________. "The Limerick: The Sonnet of Nonsense?" "Dutch Quarterly Review", 16 (1986): 220–236.
_________. ed., "Explorations in the Field of Nonsense". Amsterdam: Rodopi, 1987.



</doc>
<doc id="44025330" url="https://en.wikipedia.org/wiki?curid=44025330" title="Im schwarzen Walfisch zu Askalon">
Im schwarzen Walfisch zu Askalon

"Im schwarzen Walfisch zu Askalon" ("In Ashkelon's Black Whale") is a popular academic commercium song. It was known as a beer-drinking song in many German speaking ancient universities. Joseph Victor von Scheffel provided the lyrics under the title Altassyrisch (Old Assyrian) 1854, the melody is from 1783 or earlier.

The lyrics reflect an endorsement of the bacchanalian mayhem of student life, similar as in Gaudeamus igitur. The song describes an old Assyrian drinking binge of a man in an inn with some references to the Classics. The desks are made of marble and the large invoice is being provided in cuneiform on bricks. However the carouser has to admit that he left his money already in Nineveh. A Nubian house servant kicks him out then and the song closes with the notion, that (compare John 4:44) a prophet has no honor in his own country, if he doesn't pay cash for his consumption. Charles Godfrey Leland has translated the poems among other works of Scheffel. Each stanza begins with the naming verse "Im Schwarzen Walfisch zu Askalon", but varies the outcome. The "Im" is rather prolonged with the melody and increases the impact. Some of the stanzas:
<br>
Im schwarzen Wallfisch zu Ascalon
<br>
Da trank ein Mann drei Tag',
<br>
Bis dass er steif wie ein Besenstiel
<br>
Am Marmortische lag.
<br>

'In the Black Whale at Ascalon
<br>
A man drank day by day,
<br>
Till, stiff as any broom-handle,
<br>
Upon the floor he lay.
<br>

In the Black Whale at Ascalon
<br>
The waiters brought the bill,
<br>
In arrow-heads on six broad tiles
<br>
To him who thus did swill.
<br>

In the Black Whale at Ascalon
<br>
No prophet hath renown;
<br>
And he who there would drink in peace
<br>
Must pay the money down.
<br>
In typical manner of Scheffel, it contains an anachronistic mixture of various times and eras, parodistic notions on current science, as e.g. Historical criticism and interpretations of the Book of Jonah as a mere shipwrecking narrative. According to Scheffel, the guest didn't try to get back in the inn as „Aussi bini, aussi bleibi, wai Ascalun, ihr grobi Kaibi“ (I been out, I stay so, you rude Aschkelon calves). There are various additional verses, including political parodist ones and verses mocking different sorts of fraternities.
The song has been used as name for traditional inns and restaurants, e.g. in Heidelberg and Bad Säckingen. In Bad Säckingen the name was used on several (consecutive) inns and was namegiver for the still existing club "Walfisch Gesellschaft Säckingen" (Walfischia), honoring Scheffel.

There is one version just and only one for mathematics, called 'International'.

In ancient times, upon the door <br>
Of Plato, there was writt'n: <br>
“To each non-mathematicus <br>
The entrance is forbidd'n.<br>

The same stanza is available in further 13 languages, including Greek (Μελαίνῃ τῇ ἐν Φαλαίνᾳ - Melaínē (black) tē (the) en (in) Phalaína (whale)) and Volapük, which are sung one after the other.




</doc>
<doc id="44183472" url="https://en.wikipedia.org/wiki?curid=44183472" title="Spatial turn">
Spatial turn

Spatial turn is an intellectual movement that places emphasis on place and space in social science and the humanities. It is closely linked with quantitative studies of history, literature, cartography, and other studies of society. The movement has been influential in providing mass amounts of data for study of cultures, regions, and specific locations.

Academics such as Ernst Cassirer and Lewis Mumford helped to define a sense of "community" and "commons" in their studies, forming the first part of a "spatial turn." The turn developed more comprehensively in the later twentieth century in French academic theories, such as those of Michel Foucault. 

Technologies have also played an important role in "turns." The introduction of Geographic Information Systems (GIS) has also been instrumental in quantifying data in the humanities for study by its place.


</doc>
<doc id="22900852" url="https://en.wikipedia.org/wiki?curid=22900852" title="History of art criticism">
History of art criticism

The history of art criticism, as part of art history, is the study of objects of art in their historical development and stylistic contexts, i.e. genre, design, format, and style, which include aesthetic considerations. This includes the "major" arts of painting, sculpture, and architecture as well as the "minor" arts of ceramics, furniture, and other decorative objects.

As a term, the history of art history (also history of art) encompasses several methods of studying and assessing the visual arts; in common usage referring to works of art and architecture. Aspects of the discipline overlap. As the art historian Ernst Gombrich once observed, "the field of art history [is] much like Caesar's Gaul, divided in three parts inhabited by three different, though not necessarily hostile tribes: (i) the connoisseurs, (ii) the critics, and (iii) the academic art historians".

As a discipline, the history of art criticism is distinguished from art criticism, which is concerned with establishing a relative artistic value upon individual works with respect to others of comparable style, or sanctioning an entire style or movement from the standpoint of its history and of its major scholars. It is also distinguished from art theory or "philosophy of art", which is concerned with the fundamental nature of art. One branch of this area of study is aesthetics, which includes investigating the enigma of the sublime and determining the essence of beauty. Technically, art history is not these things, because the art historian uses historical method to answer the questions: How did the artist come to create the work?, Who were the patrons?, Who were his or her teachers?, Who was the audience?, Who were his or her disciples?, What historical forces shaped the artist's oeuvre, and How did he or she and the creation, in turn, affect the course of artistic, political, and social events? It is, however, questionable whether many questions of this kind can be answered satisfactorily without also considering basic questions about the nature of art. Unfortunately the current disciplinary gap between art history and the philosophy of art (aesthetics) often hinders this.

The history of art criticism is not only a biographical endeavor. The history of art criticism often roots its studies in the scrutiny of individual objects. It attempt to answer in historically specific ways, questions such as: What are key features of this style?, What meaning did this object convey?, How does it function visually?, Did the artist meet their goals well?, What symbols are involved?, and Does it function discursively?

The historical backbone of the discipline is a celebratory chronology of beautiful creations commissioned by public or religious bodies or wealthy individuals in western Europe. Such a "canon" remains prominent, as indicated by the selection of objects present in art history textbooks. Nonetheless, since the 20th century there has been an effort to re-define the discipline to be more inclusive of non-Western art, art made by women, and vernacular creativity.

The history of art criticism as we know it in the 21st century began in the 19th century but has precedents that date to the ancient world. Like the analysis of historical trends in politics, literature, and the sciences, the discipline benefits from the clarity and portability of the written word, but art historians also rely on formal analysis, semiotics, psychoanalysis and iconography. Advances in photographic reproduction and printing techniques after World War II increased the ability of reproductions of artworks. Such technologies have helped to advance the discipline in profound ways, as they have enabled easy comparisons of objects. The study of visual art thus described, can be a practice that involves understanding context, form, and social significance.

Art historians, in performing their assessment within the history of art criticism, employ a number of methods in their research into the ontology and history of objects.

Practitioners of art criticism often examine work in the context of its time. At best, this is done in a manner which respects its creator's motivations and imperatives; with consideration of the desires and prejudices of its patrons and sponsors; with a comparative analysis of themes and approaches of the creator's colleagues and teachers; and with consideration of iconography and symbolism. In short, this approach examines the work of art in the context of the world within which it was created.

Practitioners of art criticism also often examine work through an analysis of form; that is, the creator's use of line, shape, color, texture, and composition. This approach examines how the artist uses a two-dimensional picture plane or the three dimensions of sculptural or architectural space to create his or her art. The way these individual elements are employed results in representational or non-representational art. Is the artist imitating an object or image found in nature? If so, it is representational. The closer the art hews to perfect imitation, the more the art is "realistic". Is the artist not imitating, but instead relying on symbolism, or in an important way striving to capture nature's essence, rather than copy it directly? If so the art is non-representational—also called abstract. Realism and abstraction exist on a continuum. Impressionism is an example of a representational style that was not directly imitative, but strove to create an "impression" of nature. If the work is not representational and is an expression of the artist's feelings, longings and aspirations, or is a search for ideals of beauty and form, the work is non-representational or a work of expressionism.

An iconographical analysis is one which focuses on particular design elements of an object. Through a close reading of such elements, it is possible to trace their lineage, and with it draw conclusions regarding the origins and trajectory of these motifs. In turn, it is possible to make any number of observations regarding the social, cultural, economic, and aesthetic values of those responsible for producing the object.

Many practitioners of art criticism use critical theory to frame their inquiries into objects. Theory is most often used when dealing with more recent objects, those from the late 19th century onward. Critical theory in art history is often borrowed from literary scholars, and it involves the application of a non-artistic analytical framework to the study of art objects. Feminist, Marxist, critical race, queer, and postcolonial theories are all well established in the discipline. As in literary studies, there is an interest among scholars in nature and the environment, but the direction that this will take in the discipline has yet to be determined.

More recently, media and digital technology introduced possibilities of visual, spatial and experiential analyses. The relevant forms vary from movies, to interactive forms, including virtual environments, augmented environments, situated media, networked media, etc. The methods enabled by such techniques are in active development and promise to include qualitative approaches that can emphasize narrative, dramatic, emotional and ludic characteristics of history and art.

The earliest surviving writing on art that can be classified as art history or art criticism are the passages in Pliny the Elder's "Natural History" (c. AD 77-79), concerning the development of Greek sculpture and painting. From them it is possible to trace the ideas of Xenokrates of Sicyon (c. 280 BC), a Greek sculptor who was perhaps the first art historian. Pliny's work, while mainly an encyclopaedia of the sciences, has thus been influential from the Renaissance onwards. (Passages about techniques used by the painter Apelles c. (332-329 BC), have been especially well-known.) Similar, though independent, developments occurred in the 6th century China, where a canon of worthy artists was established by writers in the scholar-official class. These writers, being necessarily proficient in calligraphy, were artists themselves. The artists are described in the "Six Principles of Painting" formulated by Xie He.

While personal reminiscences of art and artists have long been written and read (see Lorenzo Ghiberti "Commentarii," for the best early example), it was Giorgio Vasari, the Tuscan painter, sculptor and author of the "Lives of the Painters", who wrote the first true "history" of art. He emphasized art's progression and development, which was a milestone in this field. His was a personal and a historical account, featuring biographies of individual Italian artists, many of whom were his contemporaries and personal acquaintances. The most renowned of these was Michelangelo, and Vasari's account is enlightening, though biased in places.

Vasari's ideas about art were enormously influential, and served as a model for many, including in the north of Europe Karel van Mander's "Schilder-boeck" and Joachim von Sandrart's "Teutsche Akademie". Vasari's approach held sway until the 18th century, when criticism was leveled at his biographical account of history.

Scholars such as Johann Joachim Winckelmann (1717–1768), criticised Vasari's "cult" of artistic personality, and they argued that the real emphasis in the study of art should be the views of the learned beholder and not the unique viewpoint of the charismatic artist. Winckelmann's writings thus were the beginnings of art criticism. His two most notable works that introduced the concept of art criticism were "Gedanken über die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst, published in 1755, shortly before he left for Rome (Fuseli published an English translation in 1765 under the title Reflections on the Painting and Sculpture of the Greeks), and Geschichte der Kunst des Altertums (History of Art in Antiquity), published in 1764 (this is the first occurrence of the phrase ‘history of art’ in the title of a book)". Winckelmann critiqued the artistic excesses of Baroque and Rococo forms, and was instrumental in reforming taste in favor of the more sober Neoclassicism. Jacob Burckhardt (1818–1897), one of the founders of art history, noted that Winckelmann was 'the first to distinguish between the periods of ancient art and to link the history of style with world history'. From Winckelmann until the mid-20th century, the field of art history was dominated by German-speaking academics. Winckelmann's work thus marked the entry of art history into the high-philosophical discourse of German culture.

Winckelmann was read avidly by Johann Wolfgang Goethe and Friedrich Schiller, both of whom began to write on the history of art, and his account of the Laocoön group occasioned a response by Lessing. The emergence of art as a major subject of philosophical speculation was solidified by the appearance of Immanuel Kant's "Critique of Judgment" in 1790, and was furthered by Hegel's "Lectures on Aesthetics". Hegel's philosophy served as the direct inspiration for Karl Schnaase's work. Schnaase's "Niederländische Briefe" established the theoretical foundations for art history as an autonomous discipline, and his "Geschichte der bildenden Künste", one of the first historical surveys of the history of art from antiquity to the Renaissance, facilitated the teaching of art history in German-speaking universities. Schnaase's survey was published contemporaneously with a similar work by Franz Theodor Kugler.

Heinrich Wölfflin (1864–1945), who studied under Burckhardt in Basel, is the "father" of modern art history. Wölfflin taught at the universities of Berlin, Basel, Munich, and Zurich. A number of students went on to distinguished careers in art history, including Jakob Rosenberg and Frida Schottmuller. He introduced a scientific approach to the history of art, focusing on three concepts. Firstly, he attempted to study art using psychology, particularly by applying the work of Wilhelm Wundt. He argued, among other things, that art and architecture are good if they resemble the human body. For example, houses were good if their façades looked like faces. Secondly, he introduced the idea of studying art through comparison. By comparing individual paintings to each other, he was able to make distinctions of style. His book "Renaissance and Baroque" developed this idea, and was the first to show how these stylistic periods differed from one another. In contrast to Giorgio Vasari, Wölfflin was uninterested in the biographies of artists. In fact he proposed the creation of an "art history without names." Finally, he studied art based on ideas of nationhood. He was particularly interested in whether there was an inherently "Italian" and an inherently "German" style. This last interest was most fully articulated in his monograph on the German artist Albrecht Dürer.

Contemporaneous with Wölfflin's career, a major school of art-historical thought developed at the University of Vienna. The first generation of the Vienna School was dominated by Alois Riegl and Franz Wickhoff, both students of Moritz Thausing, and was characterized by a tendency to reassess neglected or disparaged periods in the history of art. Riegl and Wickhoff both wrote extensively on the art of late antiquity, which before them had been considered as a period of decline from the classical ideal. Riegl also contributed to the revaluation of the Baroque.

The next generation of professors at Vienna included Max Dvořák, Julius von Schlosser, Hans Tietze, Karl Maria Swoboda, and Josef Strzygowski. A number of the most important twentieth-century art historians, including Ernst Gombrich, received their degrees at Vienna at this time. The term "Second Vienna School" (or "New Vienna School") usually refers to the following generation of Viennese scholars, including Hans Sedlmayr, Otto Pächt, and Guido Kaschnitz von Weinberg. These scholars began in the 1930s to return to the work of the first generation, particularly to Riegl and his concept of "Kunstwollen", and attempted to develop it into a full-blown art-historical methodology. Sedlmayr, in particular, rejected the minute study of iconography, patronage, and other approaches grounded in historical context, preferring instead to concentrate on the aesthetic qualities of a work of art. As a result, the Second Vienna School gained a reputation for unrestrained and irresponsible formalism, and was furthermore colored by Sedlmayr's overt racism and membership in the Nazi party. This latter tendency was, however, by no means shared by all members of the school; Pächt, for example, was himself Jewish, and was forced to leave Vienna in the 1930s.

Our 21st-century understanding of the symbolic content of art comes from a group of scholars who gathered in Hamburg in the 1920s. The most prominent among them were Erwin Panofsky, Aby Warburg, and Fritz Saxl. Together they developed much of the vocabulary that continues to be used in the 21st century by art historians. "Iconography"—with roots meaning "symbols from writing" refers to subject matter of art derived from written sources—especially scripture and mythology. "Iconology" is a broader term that referred to all symbolism, whether derived from a specific text or not. Today art historians sometimes use these terms interchangeably.

Panofsky, in his early work, also developed the theories of Riegl, but became eventually more preoccupied with iconography, and in particular with the transmission of themes related to classical antiquity in the Middle Ages and Renaissance. In this respect his interests coincided with those of Warburg, the son of a wealthy family who had assembled an impressive library in Hamburg devoted to the study of the classical tradition in later art and culture. Under Saxl's auspices, this library was developed into a research institute, affiliated with the University of Hamburg, where Panofsky taught.

Warburg died in 1929, and in the 1930s Saxl and Panofsky, both Jewish, were forced to leave Hamburg. Saxl settled in London, bringing Warburg's library with him and establishing the Warburg Institute. Panofsky settled in Princeton at the Institute for Advanced Study. In this respect they were part of an extraordinary influx of German art historians into the English-speaking academy in the 1930s. These scholars were largely responsible for establishing art history as a legitimate field of study in the English-speaking world, and the influence of Panofsky's methodology, in particular, determined the course of American art history for a generation.

Heinrich Wölfflin was not the only scholar to invoke psychological theories in the study of art. Psychoanalyst Sigmund Freud wrote a book on the artist Leonardo da Vinci, in which he used Leonardo's paintings to interrogate the artist's psyche and sexual orientation. Freud inferred from his analysis that Leonardo was probably homosexual.

Though the use of posthumous material to perform psychoanalysis is controversial among art historians, especially since the sexual mores of Leonardo's time and Freud's are different, it is often attempted. One of the best-known psychoanalytic scholars is Laurie Schneider Adams, who wrote a popular textbook, "Art Across Time", and a book "Art and Psychoanalysis".

An unsuspecting turn for the history of art criticism came in 1914 when Sigmund Freud published a psychoanalytical interpretation of Michelangelo’s Moses titled Der Moses des Michelangelo as one of the first psychology based analyses on a work of art. Freud first published this work shortly after reading Vasari’s "Lives". For unknown purposes, Freud originally published the article anonymously.

Carl Jung also applied psychoanalytic theory to art. C.G. Jung was a Swiss psychiatrist, an influential thinker, and founder of analytical psychology. Jung's approach to psychology emphasized understanding the psyche through exploring the worlds of dreams, art, mythology, world religion and philosophy. Much of his life's work was spent exploring Eastern and Western philosophy, alchemy, astrology, sociology, as well as literature and the arts. His most notable contributions include his concept of the psychological archetype, the collective unconscious, and his theory of synchronicity. Jung believed that many experiences perceived as coincidence were not merely due to chance but, instead, suggested the manifestation of parallel events or circumstances reflecting this governing dynamic. He argued that a collective unconscious and archetypal imagery were detectable in art. His ideas were particularly popular among American Abstract expressionists in the 1940s and 1950s. His work inspired the surrealist concept of drawing imagery from dreams and the unconscious.

Jung emphasized the importance of balance and harmony. He cautioned that modern humans rely too heavily on science and logic and would benefit from integrating spirituality and appreciation of the unconscious realm. His work not only triggered analytical work by art historians, but it became an integral part of art-making. Jackson Pollock, for example, famously created a series of drawings to accompany his psychoanalytic sessions with his Jungian psychoanalyst, Dr. Joseph Henderson. Henderson who later published the drawings in a text devoted to Pollock's sessions realized how powerful the drawings were as a therapeutic tool.

The legacy of psychoanalysis in art history has been profound, and extends beyone Freud and Jung. The prominent feminist art historian Griselda Pollock, for example, draws upon psychoanalysis both in her reading into contemporary art and in her rereading of modernist art. With Griselda Pollock's reading of French feminist psychoanalysis and in particular the writings of Julia Kristeva and Bracha L. Ettinger, as with Rosalind Krauss readings of Jacques Lacan and Jean-François Lyotard and Catherine de Zegher's curatorial rereading of art, Feminist theory written in the fields of French feminism and Psychoanalysis has strongly informed the reframing of both men and women artists in art history.

During the mid-20th century, art historians embraced social history by using critical approaches. The goal was to show how art interacts with power structures in society. One critical approach that art historians used was Marxism. Marxist art history attempted to show how art was tied to specific classes, how images contain information about the economy, and how images can make the status quo seem natural (ideology).

Perhaps the best-known Marxist was Clement Greenberg, who came to prominence during the late 1930s with his essay "Avant-Garde and Kitsch". In the essay Greenberg claimed that the avant-garde arose in order to defend aesthetic standards from the decline of taste involved in consumer society, and seeing kitsch and art as opposites. Greenberg further claimed that avant-garde and Modernist art was a means to resist the leveling of culture produced by capitalist propaganda. Greenberg appropriated the German word 'kitsch' to describe this consumerism, although its connotations have since changed to a more affirmative notion of leftover materials of capitalist culture. Greenberg later became well known for examining the formal properties of modern art.

Meyer Schapiro is one of the best-remembered Marxist art historians of the mid-20th century. Although he wrote about numerous time periods and themes in art, he is best remembered for his commentary on sculpture from the late Middle Ages and early Renaissance, at which time he saw evidence of capitalism emerging and feudalism declining.

Arnold Hauser wrote the first Marxist survey of Western Art, entitled "The Social History of Art". He attempted to show how class consciousness was reflected in major art periods. The book was controversial when published during the 1950s since it makes generalizations about entire eras, a strategy now called "vulgar Marxism".

Marxist Art History was refined in the department of Art History at UCLA with scholars such as T.J. Clark, O.K. Werckmeister, David Kunzle, Theodor W. Adorno, and Max Horkheimer. T.J. Clark was the first art historian writing from a Marxist perspective to abandon vulgar Marxism. He wrote Marxist art histories of several impressionist and realist artists, including Gustave Courbet and Édouard Manet. These books focused closely on the political and economic climates in which the art was created.

Linda Nochlin's essay "Why have there been no great women artists?" helped to ignite feminist art history during the 1970s and remains one of the most widely read essays about female artists. In it she applies a feminist critical framework to show systematic exclusion of women from art training. Nochlin argues that exclusion from practicing art as well as the canonical history of art was the consequence of cultural conditions which curtailed and restricted women from art producing fields. The few who did succeed were treated as anomalies and did not provide a model for subsequent success.Griselda Pollock is another prominent feminist art historian, whose use of psychoanalytic theory is described above. While feminist art history can focus on any time period and location, much attention has been given to the Modern era. Some of this scholarship centers on the feminist art movement, which referred specifically to the experience of women.

As opposed to iconography which seeks to identify meaning, semiotics is concerned with how meaning is created. Roland Barthes’s connoted and denoted meanings are paramount to this examination. In any particular work of art, an interpretation depends on the identification of denoted meaning—the recognition of a visual sign, and the connoted meaning—the instant cultural associations that come with recognition. The main concern of the semiotic art historian is to come up with ways to navigate and interpret connoted meaning.

Semiotic art history seeks to uncover the codified meaning or meanings in an aesthetic object by examining its connectedness to a collective consciousness. Art historians do not commonly commit to any one particular brand of semiotics but rather construct an amalgamated version which they incorporate into their collection of analytical tools. For example, Meyer Schapiro borrowed Saussure’s differential meaning in effort to read signs as they exist within a system. According to Schapiro, to understand the meaning of frontality in a specific pictorial context, it must be differentiated from, or viewed in relation to, alternate possibilities such as a profile, or a three-quarter view. Schapiro combined this method with the work of Charles Sanders Peirce whose object, sign, and interpretant provided a structure for his approach. Alex Potts demonstrates the application of Peirce’s concepts to visual representation by examining them in relation to the Mona Lisa. By seeing the Mona Lisa, for example, as something beyond its materiality is to identify it as a sign. It is then recognized as referring to an object outside of itself, a woman, or Mona Lisa. The image does not seem to denote religious meaning and can therefore be assumed to be a portrait. This interpretation leads to a chain of possible interpretations: who was the sitter in relation to Leonardo? What significance did she have to him? Or, maybe she is an icon for all of womankind. This chain of interpretation, or “unlimited semiosis” is endless; the art historian’s job is to place boundaries on possible interpretations as much as it is to reveal new possibilities.

Semiotics operates under the theory that an image can only be understood from the viewer’s perspective. The artist is supplanted by the viewer as the purveyor of meaning, even to the extent that an interpretation is still valid regardless of whether the creator had intended it. Rosalind Krauss espoused this concept in her essay “In the Name of Picasso.” She denounced the artist’s monopoly on meaning and insisted that meaning can only be derived after the work has been removed from its historical and social context. Mieke Bal argued similarly that meaning does not even exist until the image is observed by the viewer. It is only after acknowledging this that meaning can become opened up to other possibilities such as feminism or psychoanalysis.

Aspects of the subject which have come to the fore in recent decades include interest in the patronage and consumption of art, including the economics of the art market, the role of collectors, the intentions and aspirations of those commissioning works, and the reactions of contemporary and later viewers and owners. Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting.

Scientific advances have made possible much more accurate investigation of the materials and techniques used to create works, especially infra-red and x-ray photographic techniques which have allowed many underdrawings of paintings to be seen again. Proper analysis of pigments used in paint is now possible, which has upset many attributions. Dendrochronology for panel paintings and radio-carbon dating for old objects in organic materials have allowed scientific methods of dating objects to confirm or upset dates derived from stylistic analysis or documentary evidence. The development of good colour photography, now held digitally and available on the internet or by other means, has transformed the study of many types of art, especially those covering objects existing in large numbers which are widely dispersed among collections, such as illuminated manuscripts and Persian miniatures, and many types of archaeological artworks.

The field of Art History is traditionally divided into specializations or concentrations based on eras and regions, with further sub-division based on media. Thus, someone might specialize in "19th-century German architecture" or in "16th-century Tuscan sculpture." Sub-fields are often included under a specialization. For example, the Ancient Near East, Greece, Rome, and Egypt are all typically considered special concentrations of Ancient art. In some cases, these specializations may be closely allied (as Greece and Rome, for example), while in others such alliances are far less natural (Indian art versus Korean art, for example).

Non-Western art is a relative newcomer to the Art Historical canon. Recent revisions of the semantic division between art and artifact have recast objects created in non-Western cultures in more aesthetic terms. Relative to those studying Ancient Rome or the Italian Renaissance, scholars specializing in Africa, the Ancient Americas and Asia are a growing minority.

Contemporary Art History refers to research into the period from the 1960s until today reflecting the break from the assumptions of modernism brought by artists of the neo-avant-garde and a continuity in contemporary art in terms of practice based on conceptualist and post-conceptualist practices.

In the United States, the most important art history organization is the College Art Association. It organizes an annual conference and publishes the "Art Bulletin" and "Art Journal". Similar organizations exist in other parts of the world, as well as for specializations, such as architectural history and Renaissance art history. In the UK, for example, the Association of Art Historians is the premiere organization, and it publishes a journal titled "Art History".





</doc>
<doc id="48656171" url="https://en.wikipedia.org/wiki?curid=48656171" title="Disclosing New Worlds">
Disclosing New Worlds

Disclosing New Worlds: Entrepreneurship, Democratic Action, and the Cultivation of Solidarity (1997) is a book co-authored by Fernando Flores, Hubert Dreyfus and Charles Spinosa (a consultant philosopher specializing in commercial innovation). It is a philosophical proposal intended to restore or energize democracy by social constructionism via an argument style of world disclosure but which philosophy is distinct from:

Nevertheless, the authors build on these ideas and seek to reformulate the relationship between democratic rights and economic progress when persistent technological advance obscures an uncertain future for humanity threatened by multiple issues such as peak oil, global warming and environmental degradation. The authors concentrate on three practical activities:


The authors reason that human beings are at their best when engaged in imaginative and practical innovation rather than in abstract reflection, and thus challenging accepted wisdom and conventional practices within their particular environment, or as the authors claim, when they are making history. History-making, in this account, refers not to political power changes, wars or violent revolution, but to changes in the way people understand their personal qualities and deal with their particular situations.

World disclosure (German: Erschlossenheit, literally development or comprehension) is a phenomenon first described by the German philosopher Martin Heidegger in his landmark book Being and Time. As well as the authors of this work, the idea of disclosing has also been discussed by philosophers such as John Dewey, Jürgen Habermas, Nikolas Kompridis and Charles Taylor. It refers to how things become intelligible and meaningfully relevant to ordinary people.

The authors reiterate the importance of history making and identify three types of actors:

Each has to overcome resistance to change, but do so in different ways. Solving puzzles request of each a clear strategic objective, but use different tactics to overcome obstructions. By understanding and disclosing our objectives, and discovering the sort of role we may have to play according to particular situations to reach those goals, all of us can be "history makers" and make changes to our society by changing the shared narrative that binds our particular culture. The authors quote already changed collective attitudes which have recently become much less tolerant of road accidents, various forms of discrimination and repressive public education of young people.

They promote a change to the overarching consumerism as the western world's social and economic order and ideology that encourages the acquisition of goods and services in ever-increasing amounts, in favor of sustainable development within individual communities as a precursor to disclosing a new world order that supports human equality



</doc>
<doc id="49733801" url="https://en.wikipedia.org/wiki?curid=49733801" title="Women in musicology">
Women in musicology

Women in musicology describes the role of women professors, scholars and researchers in postsecondary education musicology departments at postsecondary education institutions, including universities, colleges and music conservatories. Traditionally, the vast majority of major musicologists and music historians have been men. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the "Harvard Gazette" as "one of the world's most accomplished and admired music historians".

Susan McClary (born 1946) is a musicologist associated with the "New Musicology" who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is "Feminine Endings" (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians. In the book, McClary suggests that the sonata form (used in symphonies and string quartets) may be a sexist or misogynistic procedure that constructs of gender and sexual identity. McClary's "Conventional Wisdom" (2000) argues that the traditional musicological assumption of the existence of "purely musical" elements, divorced from culture and meaning, the social and the body, is a conceit used to veil the social and political imperatives of the worldview that produces the classical canon most prized by supposedly objective musicologists.

American musicologist Marcia Citron has asked "[w]hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers.

Other notable women scholars include:


Ethnomusicologists study the many musics around the world that emphasize their cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.
Ethnomusicology – a term coined by Jaap Kunst from the Greek words ἔθνος ("ethnos", "nation") and μουσική ("mousike", "music") – is often described as the anthropology or ethnography of music. Initially, ethnomusicology was almost exclusively oriented toward non-Western music, but now includes the study of Western music from anthropological, sociological and intercultural perspectives.

Notable ethnomusicologists include:



</doc>
<doc id="2318626" url="https://en.wikipedia.org/wiki?curid=2318626" title="German studies">
German studies

German studies is the field of humanities that researches, documents, and disseminates German language and literature in both its historic and present forms. Academic departments of German studies often include classes on German culture, German history, and German politics in addition to the language and literature component. Common German names for the field are , , and . In English the terms Germanistics or Germanics are sometimes used (mostly by Germans), but the subject is more often referred to as "German studies", "German language and literature", or "German philology".

Modern German studies is usually seen as a combination of two sub-disciplines: German linguistics and Germanophone literature studies.

German linguistics is traditionally called philology in Germany, as there is something of a difference between philologists and linguists. It is roughly divided as follows:

In addition, the discipline examines German under various aspects: the way it is spoken and written, i.e., spelling; declination; vocabulary; sentence structure; texts; etc. It compares the various manifestations such as social groupings (slang, written texts, etc.) and geographical groupings (dialects, etc.).

The study German literature is divided into two parts: "Ältere Deutsche Literaturwissenschaft" deals with the period from the beginnings of German in the early Middle Ages up to post-Medieval times around AD 1750, while the modern era is covered by "Neuere Deutsche Literaturwissenschaft". The field systematically examines German literature in terms of genre, form, content, and motifs as well as looking at it historically by author and epoch. Important areas include edition philology, history of literature, and textual interpretation. The relationships of German literature to the literatures of other languages (e.g. reception and mutual influences) and historical contexts are also important areas of concentration. "The Penguin Dictionary of Literary Terms and Literary Theory: Fourth Edition" () is printed in English but contains many German-language literary terms that apply cross-culturally in the field of literary criticism; quite a few of the in terms in the book originated in German but have since been adopted by English-language critics and scholars.

At least in Germany and Austria, German studies in academia play a central role in the education of German school teachers. Their courses usually cover four fields:
Several universities offer specialized curricula for school teachers, usually called "'". In Germany, they are leading to a two step exam and certificate by the federal states of Germany cultural authorities, called the ' ("state exam").

In recent years, German has looked for links with the fields of communications, cultural studies and media studies. In addition, the sub-branch of film studies has established itself.

As an unsystematic field of interest for individual scholars, German studies can be traced back to Tacitus' "Germania". The publication and study of legal and historical source material, such as Medieval Bible translations, were all undertaken during the German Renaissance of the sixteenth century, truly initiating the field of German studies. As an independent university subject, German studies was introduced at the beginning of the nineteenth century by Georg Friedrich Benecke, the Brothers Grimm, and Karl Lachmann.



"German studies" is taught at many German universities. Some examples are:







</doc>
<doc id="8074243" url="https://en.wikipedia.org/wiki?curid=8074243" title="The Word and the World">
The Word and the World

The Word and the World Project of the Stanford University's Learning Lab developed a large lecture, Introduction to Humanities (IHUM) course adopting pedagogical strategies and technologies designed to enhance learning. The course was given in 1997 and 1998. The goal of the curriculum innovations was to transform a large lecture course into a learning community. Professors: Larry Friedlander (English), Haun Saussy (East Asian Studies), and Tim Lenior (History); teaching fellows: Carlos Seligo and Margo Denman and lab staff: Charles Kerns and George Toye worked together to develop a holistic curriculum mediated through a website center for the course.

This course was developed in response to the shortcomings of earlier large lecture courses. This type of course typically rated poorly in student evaluations and often led students to behaviors that inhibited learning: students skipped lectures and did not read assignments or prepare for meetings; students crammed for exams and waited until the last minute to write papers with a focus on grades and not learning. They were passive participants in a system that did not foster active engagement. Often there was a lack of continuity between lecture and section. Students had widely varying levels of knowledge about the texts. Faculty had very little information about the students’ knowledge as the course proceeded and the students had little feedback on their performance.

The curriculum was based on the reading of five texts, Genesis, Blade Runner, Hamlet, Descartes' Meditations, from the viewpoint of the scholar of history, literature, or philosophy. The course emphasized methods of reading and critical and interpretive approaches, rather than content.
This first year course met weekly in two one-hour lectures for all 100 students and two one-hour discussion section meetings of 15 students each. Students and instructors engaged in web activities including structured reading assignments and asynchronous discussion forums. The web site provided rich resources to supplement the texts. Students worked cross-section group projects and on panel discussions. There was no final exam for the course; a fair was held in which students exhibited their project web sites.

A site that had a calendar, announcements, faculty bios, the asynchronous discussion forums, assignments, and student projects
The course web site linked rich resources supporting each text to engage students from novices to those well-informed on the texts. It included explanatory information, annotations, cultural information, critical analyses, visual interpretations of the texts including paintings, video recordings of multiple performances, and different cuts of the films.
The web site also had a series of on-line assignments that structured the learners’ reading activities. These on-line activities drew upon the students’ previous knowledge and guided them as they approached each text. In the first year of the project in 1997 students responses were recorded in a personal on-line, shareable portfolio, but after noting little use of the portfolio, the design was changed in the second iteration of the course in 1998 so that assignments were displayed directly in the discussion Forum.

The Forum had special features to support large lecture courses such as photographs of the posting student next to his/her message, organization into sections, and notification systems to alert faculty and groups to special postings. The forum was used to discuss lectures; it was where assignments were posted (they were not sent to an instructor); and was used for communication in forming project groups.The Forum was enhanced in the second year to allow direct linkage to assignments, faculty-controlled subject organization, and threaded discussion.

The students in the course completed projects in the second half of the course that demonstrated their understanding of the texts. Projects were constructed in many media the first year but were limited to posters and web sites in the second. Faculty met often with students helping them to plan their projects. All projects from the previous year were available as models when the course was repeated.

The project employed several modes of project and technology evaluation: questionnaires, interviews, video interaction analysis, peer review, and ethnographic studies. Findings:

-The Forum provided a venue for discussion by quiet students and accommodated different learning styles.
-The Forum and assignments helped students prepare for face-to-face meetings by discussing basic issues before class. 
-It added to workload of teaching staff.
-There was a positive correlation between scores on papers and number of Forum postings.



</doc>
<doc id="30695401" url="https://en.wikipedia.org/wiki?curid=30695401" title="Variantology">
Variantology

Variantology has been conceived as an international research project with the aim of developing a critical appraisal of the established concepts of “media”. The concept of a medium is thus opened up to approaches and disciplines that up to now have remained outside the contemporary discourse on media, such as theology, various musicology, aspects of natural sciences, fine arts or classical philology. Furthermore, it is opened up to cultures of knowledge that have long been excluded from the western discourse, like the oriental and Arabic-Islamic culture. Variantology also attempts to explore how reciprocally these disciplines then become open to thinking in categories and terms of media and communication. Consequently, the network of research that constitutes the Variantology project involves scholars based in academic institutions as well as artists, musicians and authors.

To come to a different understanding of media, a central part of research is the development of a network of scientists, artists and scholars who are engaged with the "deep time relations“ between arts, sciences, and technologies. The term "deep time relations“ refers to the notion of being a plurality of traversals through the genealogy of what we call media today. The underlying theoretical center is Michel Foucault’s concept of genealogy, which he developed from Friedrich Nietzsche’s thinking about morality as a historical- and social-generated construction. Foucault's approach in this connection was to comprehend history as a constitution of knowledge, of discourses, of objectification and so on, detached from an idea of historical subjects and previously unquestioned categories of western (Eurocentric) culture and power.

An integral part of the project is the annual international workshop. The first three Variantology workshops were held at Academy of Media Arts Cologne, the fourth one at UdK in Berlin, and the 5th at Biblioteca Nazionale Vittorio Emanuele III in Naples, Italy.


</doc>
<doc id="7873885" url="https://en.wikipedia.org/wiki?curid=7873885" title="History by period">
History by period

This history by period summarizes significant eras in the history of the world, from the ancient world to the present day.

Ancient history refers to the time period in which scientists have found the earliest remains of human activity, approximately 60,000 BC. It ends with the fall of several significant empires, such as the Western Roman Empire in the Mediterranean, the Han Dynasty in China, and the Gupta Empire in India, collectively around 650 AD.

The Bronze Age is the time period in which humans around the world began to use bronze as a major metal in tools. It is generally accepted as starting around 3600 BC and ending with the advent of iron in 1000 BC.

The Iron Age is often called Antiquity or the Classical Era, but these periods more commonly refer to only one region. It begins around 1000 BC with the widespread use of iron in tools. It is often accepted to end at approximately 650 AD, with the fall of the aforementioned major civilizations.

Note that BC and BCE refer to the same time period. BCE is an abbreviation for Before Common Era, and BC for Before Christ. AD is Anno Domini, and CE is Common Era. This is done in order to standardize time periods across the world (ISO 8601).

The Postclassical Era, also referred to as the Medieval period or, for Europe, the Middle Ages, begins around 500 CE after the fall of major civilizations, covering the advent of Islam. The period ends around 1450–1500, with events like the rise of moveable-type printing in Europe, the voyages of Christopher Columbus, and the Ottoman Empire's conquest of Constantinople.


The Modern Period covers human history from the creation of a more global network (i.e. the discovery of the Americas by Europeans) to present day.

The Early Modern Period is the first third of the Modern Period and is often used with the parent categorization. It starts with the invention of the printing press, covering the voyage of Christopher Columbus in 1492 and, more generally, the establishment of a more global network. It ends in 1750 with the beginning of British industrialization.


The Age of Revolution is a less commonly used period, but appropriately covers the time between the early modern and contemporary. It begins around 1750 with European industrialization and is marked by several political revolutions. It ends around 1945, with the relative advancement of industrialization in Europe, the United States, Japan, and Russia, and the beginning of World War II.


The Contemporary Period generally covers history still in living memory, approximately 100 years behind the current year. However, for all intents and purposes, the period will be used here as spanning from the second world war in 1945 to present day, as it is considered separate from the past eras and the newest stage of world history.





</doc>
<doc id="351227" url="https://en.wikipedia.org/wiki?curid=351227" title="Transparency (behavior)">
Transparency (behavior)

Transparency, as used in science, engineering, business, the humanities and in other social contexts, is operating in such a way that it is easy for others to see what actions are performed. Transparency implies openness, communication, and accountability.

Transparency is practiced in companies, organizations, administrations, and communities. For example, a cashier making change after a point of sale transaction by offering a record of the items purchased (e.g., a receipt) as well as counting out the customer's change on the counter demonstrates one type of transparency.

The term "transparency" has a very different meaning in information security where it is used to describe security mechanisms that are intentionally in-detectable or hidden from view. Examples include hiding utilities and tools which the user does not need to know in order to do their job, like keeping the remote re-authentication operations of Challenge-Handshake Authentication Protocol hidden from the user. 

In Norway and in Sweden, tax authorities annually release the "skatteliste" or "tax list"; official records showing the annual income and overall wealth of nearly every taxpayer.

Regulations in Hong Kong require banks to list their top earners – without naming them – by pay band.

In 2009, the Spanish government for the first time released information on how much each cabinet member is worth, but data on ordinary citizens is private.

Radical transparency is a management method where nearly all decision making is carried out publicly. All draft documents, all arguments for and against a proposal, all final decisions, and the decision making process itself are made public and remain publicly archived. This approach has grown in popularity with the rise of the Internet. Two examples of organizations utilizing this style are the GNU/Linux community and Indymedia.

Corporate transparency, a form of radical transparency, is the concept of removing all barriers to —and the facilitating of— free and easy public access to corporate information and the laws, rules, social connivance and processes that facilitate and protect those individuals and corporations that freely join, develop, and improve the process.

Accountability and transparency are of high relevance for non-governmental organisations (NGOs). In view of their responsibilities to stakeholders, including donors, sponsors, programme beneficiaries, staff, states and the public, they are considered to be of even greater importance to them than to commercial undertakings. Yet these same values are often found to be lacking in NGOs.

The "International NGO Accountability Charter", linked to the Global Reporting Initiative, documents the commitment of its members international NGOs to accountability and transparency, requiring them to submit an annual report, among others. Signed in 2006 by 11 NGOs active in the area of humanitarian rights, the INGO Accountability Charter has been referred to as the “first global accountability charter for the non-profit sector”. In 1997, the One World Trust created an "NGO Charter", a code of conduct comprising commitment to accountability and transparency.

Media transparency is the concept of determining how and why information is conveyed through various means.

If the media and the public knows everything that happens in all authorities and county administrations there will be a lot of questions, protests and suggestions coming from media and the public. People who are interested in a certain issue will try to influence the decisions. Transparency creates an everyday participation in the political processes by media and the public. One tool used to increase everyday participation in political processes is freedom of information legislation and requests. Modern democracy builds on such participation of the people and media.

There are, for anybody who is interested, many ways to influence the decisions at all levels in society.

The right and the means to examine the process of decision making is known as transparency.
In politics, transparency is used as a means of holding public officials accountable and fighting corruption. When a government's meetings are open to the press and the public, its budgets may be reviewed by anyone, and its laws and decisions are open to discussion, it is seen as transparent. It is not clear however if this provides less opportunity for the authorities to abuse the system for their own interests.

When military authorities classify their plans as secret, transparency is absent. This can be seen as either positive or negative; positive because it can increase national security, negative because it can lead to corruption and, in extreme cases, a military dictatorship.

While a liberal democracy can be a plutocracy, where decisions are made behind locked doors and the people have fewer possibilities to influence politics between the elections, a participative democracy is more closely connected to the will of the people. Participative democracy, built on transparency and everyday participation, has been used officially in northern Europe for decades. In the northern European country Sweden, public access to government documents became a law as early as 1766. It has officially been adopted as an ideal to strive for by the rest of EU, leading to measures like freedom of information laws and laws for lobby transparency.

To promote transparency in politics, Hans Peter Martin, Paul van Buitenen (Europa Transparant) and Ashley Mote decided to cooperate under the name Platform for Transparency (PfT) in 2005. Similar organizations that promotes transparency are Transparency International and the Sunlight Foundation.

A recent political movement to emerge in conjunction with the demands for transparency is the Pirate Party, a label for a number of political parties across different countries who advocate freedom of information, direct democracy, network neutrality, and the free sharing of knowledge.

21st century culture affords a higher level of public transparency than ever before, and actually requires it in many cases. Modern technology and associated culture shifts have changed how government works (see WikiLeaks), what information people can find out about each other, and the ability of politicians to stay in office if they are involved in sex scandals. Due to the digital revolution, people no longer have a high level of control over what is public information, leading to a tension between the values of transparency and privacy.

Scholarly research in any academic discipline may also be labeled as (partly) transparent (or open research) if some or all relevant aspects of the research are open in the sense of open source, open access and open data, thereby facilitating social recognition and accountability of the scholars who did the research and replication by others interested in the matters addressed by it.

Some mathematicians and scientists are critical of using closed source mathematical software such as Mathematica for mathematical proofs, because these do not provide transparency, and thus are not verifiable. Open-source software such as SageMath aims to solve this problem.

In the computer software world, open source software concerns the creation of software, to which access to the underlying source code is freely available. This permits use, study, and modification without restriction.

In computer security, the debate is ongoing as to the relative merits of the full disclosure of security vulnerabilities, versus a security-by-obscurity approach.

There is a different (perhaps almost opposite) sense of transparency in human-computer interaction, whereby a system after change adheres to its previous external interface as much as possible while changing its internal behaviour. That is, a change in a system is transparent to its users if the change is unnoticeable to them.

Sports has become a global business over the last century, and here, too, initiatives ranging from mandatory drug testing to the fighting of sports-related corruption are gaining ground based on the transparent activities in other domains.

Sigmund Freud following Friedrich Nietzsche ("On Truth and Lie in a Nonmoral Sense") regularly argues that transparency is impossible because of the occluding function of the unconscious.

Among philosophical and literary works that have examined the idea of transparency are Michel Foucault's "Discipline and Punish" or David Brin's "The Transparent Society". 
The German philosopher and media theorist Byung-Chul Han in his 2012 work "Transparenzgesellschaft" sees transparency as a cultural norm created by neoliberal market forces, which he understands as the insatiable drive toward voluntary disclosure bordering on the pornographic. According to Han, the dictates of transparency enforce a totalitarian system of openness at the expense of other social values such as shame, secrecy, and trust. He was criticized for his concepts, as they would suggest corrupt politics and for referring to the anti-democratic Carl Schmitt.

Anthropologists have long explored ethnographically the relation between revealed and concealed knowledges, and have increasingly taken up the topic in relation to accountability, transparency and conspiracy theories and practices today. Todd Sanders and Harry West for example suggest not only that realms of the revealed and concealed require each other, but also that transparency in practice produces the very opacities it claims to obviate.

Clare Birchall, Christina Gaarsten, Mikkel Flyverbom, and Mark Fenster among others, write in the vein of 'Critical Transparency Studies' which attempts to challenge particular orthodoxies concerning transparency. 
Birchall, assessed in an article "[...] whether the ascendance of transparency as an ideal limits political thinking, particularly for western socialists and radicals struggling to seize opportunities for change [...]". She argues that the promotion of 'datapreneurial' activity through open data initiatives outsources and interrupts the political contract between governed and government. She is concerned that the dominant model of governmental data-driven transparency produces neoliberal subjectivities that reduce the possibility of politics as an arena of dissent between real alternatives. She suggests that the radical Left might want to work with and reinvent secrecy as an alternative to neoliberal transparency.

Researchers at University of Oxford and Warwick Business School found that transparency can also have significant unintended consequences in the field of medical care. McGivern and Fischer found 'media spectacles' and transparent regulation combined to create 'spectacular transparency' which some perverse effects on doctors' practice and increased defensive behaviour in doctors and their staff. Similarly, in a four-year organizational study, Fischer and Ferlie found that transparency in the context of a clinical risk management can act perversely to undermine ethical behavior, leading to organizational crisis and even collapse.





</doc>
<doc id="57483945" url="https://en.wikipedia.org/wiki?curid=57483945" title="Mainzed">
Mainzed

mainzed ([maɪ̯nt͡sed]; acronym for Mainz Centre for Digitality in the Humanities and Cultural Studies) is a joint initiative of six scientific institutions to promote digital methodology in the humanities and cultural sciences in Mainz, Germany.
It was founded in the context of the academic annual celebration of the Academy of Sciences and Literature Mainz on 6 November 2015.
Partners of mainzed are the Academy of Sciences and Literature Mainz (ADW), the Mainz University of Applied Sciences (HS Mainz), the Institute for Historical Regional Studies at the University of Mainz (IGL), the Johannes Gutenberg University Mainz (JGU), the Leibniz Institute of European History Mainz (IEG) and the Romano-Germanic Central Museum Mainz – Archaeological research institute (RGZM).

mainzed is based on different long-term cooperations between various institutions in Mainz. The research and development institution for digital humanities of the Academy of Sciences and Literature Mainz, "Digitale Akademie", was founded in 2009 and is connected to the Institute of Historical Regional Studies at the University of Mainz, the Leibniz Institute of European History and the universities of Mainz. Since 1997, the Romano-Germanic Central Museum Mainz and the i3mainz – Institute for Spatial Information and Surveying Technology have operated the "Competence Centre for Spatial Information Technology in the Humanities" at the Mainz University of Applied Sciences. In autumn 2013, the informal "Network DHMainz" was created with the help of the Mainz Research Alliance. The network was responsible for the preparation of the Digital Humanities Day 2014 in Mainz where first drafts were made for the continuation of the initiative.

mainzed was founded in order to accompany and practically implement the transformation of the humanities and cultural studies in the course of digitisation in Mainz.
mainzed works in research, the support of research, qualification and transfer. Furthermore, it constitutes a social research infrastructure by offering a network of scientific exchange with regard to the development of projects and research foci for scientists of all qualification levels.

Range of competences represented in the network:


mainzed developed the inter-university master’s degree program "Digital Methods in the Humanities and Cultural Studies" in terms of organization and concept. Since 2016, each winter term 24 students have been able to begin the course of studies comprising four semesters provided that they have a bachelor’s degree in the humanities, cultural studies or with a focus in computer science.
The head of this degree program and director of mainzed Kai-Christian Bruhn received the academy price of the federal state Rhineland-Palatinate on 5 December 2017 in recognition of his interdisciplinary work in teaching and research.

mainzed is initiator of many events promoting the dialogue with the public. An example of this is the fishbowl discussion about the topic "digitalität und diversität – die Geisteswissenschaften im Jahr 2026" that took place in 2016. Mainzed has organised similar annual events with national and international guest lecturers like Mercedes Bunz and Joscha Bach.

mainzed is organised into an executive board composed of the founding director Kai-Christian Bruhn as well as his deputy Klaus Pietschmann, a scientific advisory board with representatives of the partner institutions and an executive office managed by Anne Klammt.



</doc>
<doc id="26791" url="https://en.wikipedia.org/wiki?curid=26791" title="Satire">
Satire

Satire is a genre of literature, and sometimes graphic and performing arts, in which vices, follies, abuses, and shortcomings are held up to ridicule, ideally with the intent of shaming individuals, corporations, government, or society itself into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society.

A feature of satire is strong irony or sarcasm—"in satire, irony is militant"—but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to attack.

Satire is nowadays found in many artistic forms of expression, including internet memes, literature, plays, commentary, television shows, and media such as lyrics.

The word satire comes from the Latin word "satur" and the subsequent phrase "lanx satura." "Satur" meant "full" but the juxtaposition with "lanx" shifted the meaning to "miscellany or medley": the expression "lanx satura" literally means "a full dish of various kinds of fruits".

The word "satura" as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as "satire". Quintilian famously said that "satura," that is a satire in hexameter verses, was a literary genre of wholly Roman origin ("satura tota nostra est"). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use the term "satire" in the modern broader sense was Apuleius.

To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes:
The word "satire" derives from "satura", and its origin was not influenced by the Greek mythological figure of the "satyr". In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time.

Laughter is not an essential component of satire; in fact there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque.

Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think".

Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes.

Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satire confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (be it political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it's not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as confronting public discourse.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which re-establishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society.

The state of political satire in a given society reflects the tolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindedness and love for awards and decorations.

Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes".

Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean.

Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and "philosophical beliefs of ancient Rome and Greece" (Rankin). Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of "gently [ridiculing] the absurdities and follies of human beings" (Drury). 
It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society.
A Horatian satirist's goal is to heal the situation with smiles, rather than by anger. Horatian satire is a gentle reminder to take life less seriously and evokes a wry smile. A Horatian satirist makes fun of general human folly rather than engaging in specific or personal attacks. Shamekia Thomas suggests, "In a work using Horatian satire, readers often laugh at the characters in the story who are the subject of mockery as well as themselves and society for behaving in those ways." Alexander Pope has been established as an author whose satire "heals with morals what it hurts with wit" (Green). Alexander Pope—and Horatian satire—attempt to teach.

Examples of Horatian satire:

Juvenalian satire, named for the writings of the Roman satirist Juvenal (late first century – early second century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Republic and actively attacked them through his literature. "He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent" (Podzemny). Juvenal satire follows this same pattern of abrasively ridiculing societal structures. Juvenal also, unlike Horace, attacked public officials and governmental organizations through his satires, regarding their opinions as not just wrong, but evil.

Following in this tradition, Juvenalian satire addresses perceived social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by the use of irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire can often be classified as Juvenalian.A Juvenal satirist's goal is generally to provoke some sort of political or societal change because he sees his opponent or object as evil or harmful. A Juvenal satirist mocks "societal structure, power, and civilization" (Thomas) by exaggerating the words or position of his opponent in order to jeopardize their opponent's reputation and/or power. Jonathan Swift has been established as an author who "borrowed heavily from Juvenal's techniques in [his critique] of contemporary English society" (Podzemny).

Examples of Juvenalian satire:

See Menippean satire.

In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing ("sfottò"). Teasing is the reactionary side of the comic; it limits itself to a shallow parody of physical appearance. The side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criterion to tell real satire from "sfottò", saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire.

Teasing ("sfottò") is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, and benign spoofs. Teasing typically consists of an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, and/or the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. "Sfottò" directed towards a powerful individual makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image.

Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes.

Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object". The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness". The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed". Satire about death overlaps with black humor and gallows humor.

Another classification by topics is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems. Historically, comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them.

Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective.

Satire is found not only in written literary forms. In preliterate cultures it manifests itself in ritual and folk forms, as well as in trickster tales and oral poetry.

It appears also in graphic arts, music, sculpture, dance, cartoon strips, and graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts are the modern forms of ancient satiric rituals.

One of the earliest examples of what we might call satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is not only useful, but far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious.

The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements.

The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in "The Knights"). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play "Drunkenness" contains an attack on the politician Callimedon.

The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease.

The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Gaius Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Gaius Lucilius and Persius. "Satire" in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th-century-BC poet Hipponax wrote "satirae" that were so cruel that the offended hanged themselves.

In the 2nd century AD, Lucian wrote "True History", a book satirizing the clearly unrealistic travelogues/adventures written by Ctesias, Iambulus, and Homer. He states that he was surprised they expected people to believe their lies, and stating that he, like they, has no actual knowledge or experience, but shall now tell lies as if he did. He goes on to describe a far more obviously extreme and unrealistic tale, involving interplanetary exploration, war among alien life forms, and life inside a 200 mile long whale back in the terrestrial ocean, all intended to make obvious the fallacies of books like "Indica" and "The Odyssey".

Medieval Arabic poetry included the satiric genre "hija". Satire was introduced into Arabic prose literature by the Afro-Arab author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in "hija", satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an "Arabian Nights" tale called "Ali with the Large Member".

In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire.

The terms "comedy" and "satire" became synonymous after Aristotle's "Poetics" was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as "hija" (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature.

Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the "Resaleh-ye Delgosha", as well as "Akhlaq al-Ashraf" ("Ethics of the Aristocracy") and the famous humorous fable "Masnavi Mush-O-Gorbeh" (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires.

In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "unchristian" and ignored, except for the moral satire, which mocked misbehaviour in Christian terms. Examples are "Livre des Manières" by (~1178), and some of Chaucer's "Canterbury Tales". Sometimes epic poetry (epos) was mocked, and even feudal society, but there was hardly a general interest in the genre.

Direct social commentary via satire returned with a vengeance in the 16th century, when farcical texts such as the works of François Rabelais tackled more serious issues (and incurred the wrath of the crown as a result).

Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include "Till Eulenspiegel", "Reynard the Fox", Sebastian Brant's "Narrenschiff" (1494), Erasmus's "Moriae Encomium" (1509), Thomas More's "Utopia" (1516), and "Carajicomedia" (1519).

The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. Seventeenth-century English satire once again aimed at the "amendment of vices" (Dryden).

In the 1590s a new wave of verse satire broke with the publication of Hall's "Virgidemiarum", six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire—much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship.

Satire ("Kataksh" or "Vyang") has played a prominent role in Indian and Hindi literature, and is counted as one of the "ras" of literature in ancient books. With the commencement of printing of books in local language in the nineteenth century and especially after India's freedom, this grew. Many of the works of Tulsi Das, Kabir, Munshi Premchand, village ministrels, Hari katha singers, poets, Dalit singers and current day stand up Indian comedians incorporate satire, usually ridiculing authoritarians, fundamentalists and incompetent people in power. In India, it has usually been used as a means of expression and an outlet for common people to express their anger against authoritarian entities. A popular custom in Northern India of "Bura na mano Holi hai" continues, in which comedians on the stage roast local people of importance (who are usually brought in as special guests).

The Age of Enlightenment, an intellectual movement in the 17th and 18th centuries advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties—and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early-18th-century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire.

Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his "A Modest Proposal" Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book "Gulliver's Travels" he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical "Mac Flecknoe" was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical "The Rape of the Lock". Other satirical works by Pope include the "Epistle to Dr Arbuthnot".

Alexander Pope (b. May 21, 1688) was a satirist known for his Horatian satirist style and translation of the "Iliad". Famous throughout and after the long 18th century, Pope died in 1744. Pope, in his "The Rape of the Lock", is delicately chiding society in a sly but polished voice by holding up a mirror to the follies and vanities of the upper class. Pope does not actively attack the self-important pomp of the British aristocracy, but rather presents it in such a way that gives the reader a new perspective from which to easily view the actions in the story as foolish and ridiculous. A mockery of the upper class, more delicate and lyrical than brutal, Pope nonetheless is able to effectively illuminate the moral degradation of society to the public. "The Rape of the Lock" assimilates the masterful qualities of a heroic epic, such as the "Iliad", which Pope was translating at the time of writing "The Rape of the Lock". However, Pope applied these qualities satirically to a seemingly petty egotistical elitist quarrel to prove his point wryly.

Daniel Defoe pursued a more journalistic type of satire, being famous for his "The True-Born Englishman" which mocks xenophobic patriotism, and "The Shortest-Way with the Dissenters"—advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time.

The pictorial satire of William Hogarth is a precursor to the development of political cartoons in 18th-century England. The medium developed under the direction of its greatest exponent, James Gillray from London. With his satirical works calling the king (George III), prime ministers and generals (especially Napoleon) to account, Gillray's wit and keen sense of the ridiculous made him the pre-eminent cartoonist of the era.

Ebenezer Cooke (1665–1732), author of "The Sot-Weed Factor" (1708), was among the first American colonialists to write literary satire. Benjamin Franklin (1706–1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous.

Several satiric papers competed for the public's attention in the Victorian era (1837–1901) and Edwardian period, such as "Punch" (1841) and "Fun" (1861).

Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in "The Yeomen of the Guard", a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent:

Novelists such as Charles Dickens (1812-1870) often used passages of satiric writing in their treatment of social issues.

Continuing the tradition of Swiftian journalistic satire, Sidney Godolphin Osborne (1808-1889) was the most prominent writer of scathing "Letters to the Editor" of the London Times. Famous in his day, he is now all but forgotten. His maternal grandfather William Eden, 1st Baron Auckland was considered to be a possible candidate for the authorship of the Junius letters. If this were true, we can read Osborne as following in his grandfather's satiric "Letters to the Editor" path. Osborne's satire was so bitter and biting that at one point he received a public censure from Parliament's then Home Secretary Sir James Graham. Osborne wrote mostly in the Juvenalian mode over a wide range of topics mostly centered on British government's and landlords' mistreatment of poor farm workers and field laborers. He bitterly opposed the New Poor Laws and was passionate on the subject of Great Britain's botched response to the Irish Famine and its mistreatment of soldiers during the Crimean War.

Later in the nineteenth century, in the United States, Mark Twain (1835–1910) grew to become American's greatest satirist: his novel "Huckleberry Finn" (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a runaway slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. He is prepared to do good, believing it to be wrong.

Twain's younger contemporary Ambrose Bierce (1842–1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably "The Devil's Dictionary" (1906), in which the definitions mock cant, hypocrisy and received wisdom.

Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th-century literature, satire was used by English authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel "We", made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Anatoly Lunacharsky wrote ‘Satire attains its greatest significance when a newly evolving class creates an ideology considerably more advanced than that of the ruling class, but has not yet developed to the point where it can conquer it. Herein lies its truly great ability to triumph, its scorn for its adversary and its hidden fear of it. Herein lies its venom, its amazing energy of hate, and quite frequently, its grief, like a black frame around glittering images. Herein lie its contradictions, and its power.’ Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as "Main Street" (1920), "Babbitt" (1922), "Elmer Gantry" (1927; dedicated by Lewis to H. L. Menchen), and "It Can't Happen Here" (1935), and his books often explored and satirized contemporary American values. The film "The Great Dictator" (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps.
In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as "sick comedians". In the same period, Paul Krassner's magazine "The Realist" began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. This baton was also carried by the original National Lampoon magazine, edited by Doug Kenney and Henry Beard and featuring blistering satire written by Michael O'Donoghue, P.J. O'Rourke, and Tony Hendra, among others. Prominent satiric stand-up comedian George Carlin acknowledged the influence "The Realist" had in his 1970s conversion to a satiric comedian.

A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by such luminaries as Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show "Beyond the Fringe" was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program "That Was The Week That Was".

Joseph Heller's most famous work, "Catch-22" (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. Departing from traditional Hollywood farce and screwball, director and comedian Jerry Lewis used satire in his self-directed films "The Bellboy" (1960), "The Errand Boy" (1961) and "The Patsy" (1964) to comment on celebrity and the star-making machinery of Hollywood.
The film "Dr. Strangelove" (1964) starring Peter Sellers was a popular satire on the Cold War.

Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means all uses of these or other humorous devices are satiric. Refer to the careful definition of satire that heads this article.
Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as "Mock the Week" (2005) and "Have I Got News for You" (1990–ongoing). It is found on radio quiz shows such as "The News Quiz" (1977–ongoing) and "The Now Show" (1998–ongoing). One of the most watched UK television shows of the 1980s and early 1990s, the puppet show "Spitting Image" was a satire of the royal family, politics, entertainment, sport and British culture of the era. Court Flunkey from "Spitting Image" is a caricature of James Gillray, intended as a homage to the father of political cartooning. Created by DMA Design in 1997, satire features prominently in the British video game series "Grand Theft Auto".

The television program "South Park" (1997–ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing anti-Semitism, militant atheism, homophobia, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues.

Australian Chris Lilley produces comedy art in the style of mockumentaries ("", "Summer Heights High", "Angry Boys") and his work is often described as complex social satire.
Stephen Colbert's television program, "The Colbert Report" (2005–14), is instructive in the methods of contemporary American satire. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity.

The American sketch comedy television show "Saturday Night Live" is also known for its satirical impressions and parodies of prominent persons and politicians, among some of the most notable, their parodies of U.S. political figures Hillary Clinton and of Sarah Palin.

Other political satire includes various political causes in the past, including the relatively successful Polish Beer-Lovers' Party and the joke political candidates Molly the Dog and Brian Miner.

In the United Kingdom, a popular modern satirist is Sir Terry Pratchett, author of the internationally best-selling "Discworld" book series. One of the most well-known and controversial British satirists is Chris Morris, co-writer and director of "Four Lions".

In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including "CODCO", "The Royal Canadian Air Farce", "This Is That", and "This Hour Has 22 Minutes" deal directly with current news stories and political figures, while others, like "History Bites" present contemporary social satire in the context of events and figures in history. The Canadian organization "Canada News Network" provides commentary on contemporary news events that are primarily Canadian in nature. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio.

Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip "Li'l Abner" was censored in September 1947. The controversy, as reported in "Time", centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's "Pogo" was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip "Doonesbury" focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage.
Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series "The Simpsons" and "South Park" both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors.

News satire is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. "The Onion", "Canada News Network", "Private Eye"), "Not Your Homepage," radio (e.g. "On the Hour"), television (e.g. "The Day Today", "The Daily Show", "Brass Eye") and the web (e.g. Mindry.in, The Fruit Dish, Scunt News, Faking News, El Koshary Today, The Giant Napkin, Unconfirmed Sources and The "Onion"s website). Other satires are on the list of satirists and satires. Another internet-driven form of satire is to lampoon bad internet performers. An example of this is the Internet meme character Miranda Sings.

In an interview with "Wikinews", Sean Mills, President of "The Onion", said angry letters about their news parody always carried the same message. "It's whatever affects that person", said Mills. "So it's like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that's not funny to me.' Or someone else can say, 'Cancer's "hilarious", but don't talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it."

Zhou Libo, a comedian from Shanghai, is the most popular satirist in China. His humour has interested middle-class people and has sold out shows ever since his rise to fame.

Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques. Contrarily diminution is also a satirical technique.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany and Italy satire is protected by the constitution.

Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize". Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production.

In September 2017 The Juice Media received an e-mail from the Australian National Symbols Officer requesting that the use of a satirical logo, called the "Coat of Harms" based on the Australian Coat of Arms, no longer be used as they had received complaints from the members of the public. Coincidentally 5 days later a Bill was proposed to Australian parliament to amend the Criminal Code Act 1995. If successfully passed those found to be in breach of the new amendment can face 2–5 years imprisonment.

As of June 2018, the Criminal Code Amendment (Impersonating a Commonwealth Body) Bill 2017 was before the Australian Senate with the third reading moved 10 May 2018.

Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing.

Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with his persona.

Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance) and the idea that the satirist actually does support the ideas, policies, or people he is attacking. For instance, at the time of its publication, many people misunderstood Swift's purpose in "A Modest Proposal", assuming it to be a serious recommendation of economically motivated cannibalism.

Some critics of Mark Twain see "Huckleberry Finn" as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in "Huckleberry Finn"). This same misconception was suffered by the main character of the 1960s British television comedy satire "Till Death Us Do Part". The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. (The same situation occurred with Archie Bunker in American TV show "All in the Family", a character derived directly from Garnett.)

The Australian satirical television comedy show "The Chaser's War on Everything" has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes.

The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humour and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching.

Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon.

In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse.

The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself.

In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest.

In 2006 British comedian Sacha Baron Cohen released "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan", a "mockumentary" that satirized everyone, from high society to frat boys. The film was criticized by many. Although Baron Cohen is Jewish, some complained that it was antisemitic, and the government of Kazakhstan boycotted the film. The film itself had been a reaction to a longer quarrel between the government and the comedian.

In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. Apartheid South Africa also had a long history of censorship.

On December 29, 2009, Samsung sued Mike Breen, and the "Korea Times" for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009.

On April 29, 2015, the UK Independence Party (UKIP) requested Kent Police investigate the BBC, claiming that comments made about Party leader Nigel Farage by a panelist on the comedy show "Have I Got News For You" might hinder his chances of success in the general election (which would take place a week later), and claimed the BBC breached the Representation of the People Act. Kent Police rebuffed the request to open an investigation, and the BBC released a statement, "Britain has a proud tradition of satire, and everyone knows that the contributors on "Have I Got News for You" regularly make jokes at the expense of politicians of all parties."

Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are:








</doc>
<doc id="1271927" url="https://en.wikipedia.org/wiki?curid=1271927" title="Gender equality">
Gender equality

Gender equality, also known as sexual equality or equality of the sexes, is the state of equal ease of access to resources and opportunities regardless of gender, including economic participation and decision-making; and the state of valuing different behaviors, aspirations and needs equally, regardless of gender.

To avoid complication, other genders (besides women and men) will "not" be treated in this Gender equality article.

Gender equality is the goal, while gender neutrality and gender equity are practices and ways of thinking that help in achieving the goal. Gender parity, which is used to measure gender balance in a given situation, can aid in achieving gender equality but is not the goal in and of itself. Gender equality is more than equal representation, it is strongly tied to women's rights, and often requires policy changes. As of 2017, the global movement for gender equality has not incorporated the proposition of genders besides women and men, or gender identities outside of the gender binary.

UNICEF says gender equality "means that women and men, and girls and boys, enjoy the same rights, resources, opportunities and protections. It does not require that girls and boys, or women and men, be the same, or that they be treated exactly alike."

On a global scale, achieving gender equality also requires eliminating harmful practices against women and girls, including sex trafficking, femicide, wartime sexual violence, and other oppression tactics. UNFPA stated that, "despite many international agreements affirming their human rights, women are still much more likely than men to be poor and illiterate. They have less access to property ownership, credit, training and employment. They are far less likely than men to be politically active and far more likely to be victims of domestic violence."

As of 2017, gender equality is the fifth of seventeen sustainable development goals of the United Nations. Gender inequality is measured annually by the United Nations Development Programme's Human Development Reports.

Christine de Pizan, an early advocate for gender equality, states in her 1405 book "The Book of the City of Ladies" that the oppression of women is founded on irrational prejudice, pointing out numerous advances in society probably created by women.

The Shakers, an evangelical group, which practiced segregation of the sexes and strict celibacy, were early practitioners of gender equality. They branched off from a Quaker community in the north-west of England before emigrating to America in 1774. In America, the head of the Shakers' central ministry in 1788, Joseph Meacham, had a revelation that the sexes should be equal. He then brought Lucy Wright into the ministry as his female counterpart, and together they restructured the society to balance the rights of the sexes. Meacham and Wright established leadership teams where each elder, who dealt with the men's spiritual welfare, was partnered with an eldress, who did the same for women. Each deacon was partnered with a deaconess. Men had oversight of men; women had oversight of women. Women lived with women; men lived with men. In Shaker society, a woman did not have to be controlled or owned by any man. After Meacham's death in 1796, Wright became the head of the Shaker ministry until her death in 1821.

Shakers maintained the same pattern of gender-balanced leadership for more than 200 years. They also promoted equality by working together with other women's rights advocates. In 1859, Shaker Elder Frederick Evans stated their beliefs forcefully, writing that Shakers were "the first to disenthrall woman from the condition of vassalage to which all other religious systems (more or less) consign her, and to secure to her those just and equal rights with man that, by her similarity to him in organization and faculties, both God and nature would seem to demand". Evans and his counterpart, Eldress Antoinette Doolittle, joined women's rights advocates on speakers' platforms throughout the northeastern U.S. in the 1870s. A visitor to the Shakers wrote in 1875:
The Shakers were more than a radical religious sect on the fringes of American society; they put equality of the sexes into practice. It has been argued that they demonstrated that gender equality was achievable and how to achieve it.

In wider society, the movement towards gender equality began with the suffrage movement in Western cultures in the late-19th century, which sought to allow women to vote and hold elected office. This period also witnessed significant changes to women's property rights, particularly in relation to their marital status. (See for example, Married Women's Property Act 1882.)

Since World War II, the women's liberation movement and feminism have created a general movement towards recognition of women's rights. The United Nations and other international agencies have adopted several conventions which promote gender equality. These conventions have not been uniformly adopted by all countries, and include:

Such legislation and affirmative action policies have been critical to bringing changes in societal attitudes. A 2015 Pew Research Center survey of citizens in 38 countries found that majorities in 37 of those 38 countries said that gender equality is at least "somewhat important," and a global median of 65% believe it is "very important" that women have the same rights as men. Most occupations are now equally available to men and women, in many countries.

Similarly, men are increasingly working in occupations which in previous generations had been considered women's work, such as nursing, cleaning and child care. In domestic situations, the role of Parenting or child rearing is more commonly shared or not as widely considered to be an exclusively female role, so that women may be free to pursue a career after childbirth. For further information, see Shared earning/shared parenting marriage.

Another manifestation of the change in social attitudes is the non-automatic taking by a woman of her husband's surname on marriage.

A highly contentious issue relating to gender equality is the role of women in religiously orientated societies. Some Christians or Muslims believe in Complementarianism, a view that holds that men and women have different but complementing roles. This view may be in opposition to the views and goals of gender equality.

In addition, there are also non-Western countries of low religiosity where the contention surrounding gender equality remains. In China, a cultural preference for a male child has resulted in a shortfall of women in the population. The feminist movement in Japan has made many strides which resulted in Rethe Gender Equality Bureau, but Japan still remains low in gender equality compared to other industrialized nations.

The notion of gender equality, and of its degree of achievement in a certain country, is very complex because there are countries that have a history of a high level of gender equality in certain areas of life but not in other areas. Indeed, there is a need for caution when categorizing countries by the level of gender equality that they have achieved. According to Mala Htun and Laurel Weldon "gender policy is not one issue but many" and:
Not all beliefs relating to gender equality have been popularly adopted. For example, topfreedom, the right to be bare breasted in public, frequently applies only to males and has remained a marginal issue. Breastfeeding in public is now more commonly tolerated, especially in semi-private places such as restaurants.

It is the vision that men and women should be treated equally in social, economic and all other aspects of society, and to not be discriminated against on the basis of their gender. Gender equality is one of the objectives of the United Nations Universal Declaration of Human Rights. World bodies have defined gender equality in terms of human rights, especially women's rights, and economic development. The United Nation's Millennium Development Goals Report states that their goal is to "achieve gender equality and the empowerment of women".Despite economic struggles in developing countries, the United Nations is still trying to promote gender equality, as well as help create a sustainable living environment is all its nations.Their goals also include giving women who work certain full-time jobs equal pay to the men with the same job.

There has been criticism from some feminists towards the political discourse and policies employed in order to achieve the above items of "progress" in gender equality, with critics arguing that these gender equality strategies are superficial, in that they do not seek to challenge social structures of male domination, and only aim at improving the situation of women within the societal framework of subordination of women to men, and that official public policies (such as state policies or international bodies policies) are questionable, as they are applied in a patriarchal context, and are directly or indirectly controlled by agents of a system which is for the most part male. One of the criticisms of the gender equality policies, in particular, those of the European Union, is that they disproportionately focus on policies integrating women in public life, but do not seek to genuinely address the deep private sphere oppression.

A further criticism is that a focus on the situation of women in non-Western countries, while often ignoring the issues that exist in the West, is a form of imperialism and of reinforcing Western moral superiority; and a way of "othering" of domestic violence, by presenting it as something specific to outsiders - the "violent others" - and not to the allegedly progressive Western cultures. These critics point out that women in Western countries often face similar problems, such as domestic violence and rape, as in other parts of the world. They also cite the fact that women faced de jure legal discrimination until just a few decades ago; for instance, in some Western countries such as Switzerland, Greece, Spain, and France, women obtained equal rights in family law in the 1980s. Another criticism is that there is a selective public discourse with regard to different types of oppression of women, with some forms of violence such as honor killings (most common in certain geographic regions such as parts of Asia and North Africa) being frequently the object of public debate, while other forms of violence, such as the lenient punishment for crimes of passion across Latin America, do not receive the same attention in the West. It is also argued that the criticism of particular laws of many developing countries ignores the influence of colonialism on those legal systems. There has been controversy surrounding the concepts of Westernization and Europeanisation, due to their reminder of past colonialism, and also due to the fact that some Western countries, such as Switzerland, have been themselves been very slow to give women legal rights.

There has been criticism that international law, international courts, and universal gender neutral concepts of human rights are at best silent on many of the issues important to women and at worst male centered; considering the male person to be the default. Excessive gender neutrality can worsen the situation of women, because the law "assumes" women are in the same position as men, ignoring the biological fact that in the process of reproduction and pregnancy there is no 'equality', and that apart from physical differences there are socially constructed limitations which assign a socially and culturally inferior position to women - a situation which requires a specific approach to women's rights, not merely a gender neutral one. In a 1975 interview, Simone de Beauvoir talked about the negative reactions towards women's rights from the left that was supposed to be progressive and support social change, and also expressed skepticism about mainstream international organizations.

In 2010, the European Union opened the European Institute for Gender Equality (EIGE) in Vilnius, Lithuania to promote gender equality and to fight sex discrimination. In 2015 the EU published the Gender Action Plan 2016–2020.

Gender equality is part of the national curriculum in Great Britain and many other European countries. By presidential decree, the Republic of Kazakhstan created a Strategy for Gender Equality 2006–2016 to chart the subsequent decade of gender equality efforts. Personal, Social and Health Education, religious studies and Language acquisition curricula tend to address gender equality issues as a very serious topic for discussion and analysis of its effect in society.

A large and growing body of research has shown how gender inequality undermines health and development. To overcome gender inequality the United Nations Population Fund states that, "Women's empowerment and gender equality requires strategic interventions at all levels of programming and policy-making. These levels include reproductive health, economic empowerment, educational empowerment and political empowerment."

UNFPA says that "research has also demonstrated how working with men and boys as well as women and girls to promote gender equality contributes to achieving health and development outcomes."

Social constructs of gender (that is, cultural ideals of socially acceptable masculinity and femininity) often have a negative effect on health. The World Health Organization cites the example of women not being allowed to travel alone outside the home (to go to the hospital), and women being prevented by cultural norms to ask their husbands to use a condom, in cultures which simultaneously encourage male promiscuity, as social norms that harm women's health. Teenage boys suffering accidents due to social expectations of impressing their peers through risk taking, and men dying at much higher rate from lung cancer due to smoking, in cultures which link smoking to masculinity, are cited by the WHO as examples of gender norms negatively affecting men's health. The World Health Organization has also stated that there is a strong connection between gender socialization and transmission and lack of adequate management of HIV/AIDS.

Certain cultural practices, such as female genital mutilation (FGM), negatively affect women's health. Female genital mutilation is the ritual cutting or removal of some or all of the external female genitalia. It is rooted in inequality between the sexes, and constitutes a form of discrimination against women. The practice is found in Africa, Asia and the Middle East, and among immigrant communities from countries in which FGM is common. UNICEF estimated in 2016 that 200 million women have undergone the procedure.

According to the World Health Organization, gender equality can improve men's health. The study shows that traditional notions of masculinity have a big impact on men's health. Among European men, non-communicable diseases, such as cancer, cardiovascular diseases, respiratory illnesses, and diabetes, account for the vast majority of deaths of men aged 30–59 in Europe which are often linked to unhealthy diets, stress, substance abuse, and other habits, which the report connects to behaviors often stereotypically seen as masculine behaviors like heavy drinking and smoking. Traditional gender stereotypes that keep men in the role of breadwinner and systematic discrimination preventing women from equally contributing to their households and participating in the workforce can put additional stress on men, increasing their risk of health issues and men bolstered by cultural norms, tend to take more risks and engage in interpersonal violence more often than women, which could result in fatal injuries.

Violence against women is a technical term used to collectively refer to violent acts that are primarily or exclusively committed against women. This type of violence is gender-based, meaning that the acts of violence are committed against women expressly "because" they are women, or as a result of patriarchal gender constructs. Violence and mistreatment of women in marriage has come to international attention during the past decades. This includes both violence committed inside marriage (domestic violence) as well as violence related to marriage customs and traditions (such as dowry, bride price, forced marriage and child marriage).

According to some theories, violence against women is often caused by the acceptance of violence by various cultural groups as a means of conflict resolution within intimate relationships. Studies on Intimate partner violence victimization among ethnic minorities in the United Studies have consistently revealed that immigrants are a high-risk group for intimate violence.

In countries where gang murders, armed kidnappings, civil unrest, and other similar acts are rare, the vast majority of murdered women are killed by partners/ex-partners. By contrast, in countries with a high level of organized criminal activity and gang violence, murders of women are more likely to occur in a public sphere, often in a general climate of indifference and impunity. In addition, many countries do not have adequate comprehensive data collection on such murders, aggravating the problem.

In some parts of the world, various forms of violence against women are tolerated and accepted as parts of everyday life.

In most countries, it is only in more recent decades that domestic violence against women has received significant legal attention. The Istanbul Convention acknowledges the long tradition of European countries of ignoring this form of violence.

In some cultures, acts of violence against women are seen as crimes against the male 'owners' of the woman, such as husband, father or male relatives, rather the woman herself. This leads to practices where men inflict violence upon women in order to get revenge on male members of the women's family. Such practices include payback rape, a form of rape specific to certain cultures, particularly the Pacific Islands, which consists of the rape of a female, usually by a group of several males, as revenge for acts committed by members of her family, such as her father or brothers, with the rape being meant to humiliate the father or brothers, as punishment for their prior behavior towards the perpetrators.

Richard A. Posner writes that "Traditionally, rape was the offense of depriving a father or husband of a valuable asset — his wife's chastity or his daughter's virginity". Historically, rape was seen in many cultures (and is still seen today in some societies) as a crime against the honor of the family, rather than against the self-determination of the woman. As a result, victims of rape may face violence, in extreme cases even honor killings, at the hands of their family members. Catharine MacKinnon argues that in male dominated societies, sexual intercourse is imposed on women in a coercive and unequal way, creating a continuum of victimization, where women have few positive sexual experiences. Socialization within rigid gender constructs often creates an environment where sexual violence is common. One of the challenges of dealing with sexual violence is that in many societies women are perceived as being readily available for sex, and men are seen as entitled to their bodies, until and unless women object.

In 2009, United States data showed that transgender people are likely to experience a broad range of violence in the entirety of their lifetime. Violence against trans women in Puerto Rico started to make headlines after being treated as "An Invisible Problem" decades before. It was reported at the 58th Convention of the Puerto Rican Association that many transgender women face institutional, emotional, and structural obstacles. Most trans women don't have access to health care for STD prevention and are not educated on violence prevention, mental health, and social services that could benefit them.

Trans women in the US have encountered the subject of anti-trans stigma, which includes criminalization, dehumanization, and violence against those who identify as transgender. From a societal stand point, a trans person can be victim to the stigma due to lack of family support, issues with health care and social services, police brutality, discrimination in the work place, cultural marginalisation, poverty, sexual assault, assault, bullying, and mental trauma. The Human Rights Campaign tracked over 128 cases that ended in fatality against transgender people in the US from 2013-2018, of which eighty percent included a trans woman of color. In the USA, high rates of Intimate Partner violence impact trans women differently because they are facing discrimination from police and health providers, and alienation from family. In 2018, it was reported that 77 percent of transgender people who were linked to sex work and 72 percent of transgender people who were homeless, were victims of intimate partner violence.

The importance of women having the right and possibility to have control over their body, reproduction decisions, and sexuality, and the need for gender equality in order to achieve these goals are recognized as crucial by the Fourth World Conference on Women in Beijing and the UN International Conference on Population and Development Program of Action. The World Health Organization (WHO) has stated that promotion of gender equality is crucial in the fight against HIV/AIDS.

Maternal mortality is a major problem in many parts of the world. UNFPA states that countries have an obligation to protect women's right to health, but many countries do not do that. Maternal mortality is considered today not just an issue of development but also an issue of human rights.

The right to reproductive and sexual autonomy is denied to women in many parts of the world, through practices such as forced sterilization, forced/coerced sexual partnering (e.g. forced marriage, child marriage), criminalization of consensual sexual acts (such as sex outside marriage), lack of criminalization of marital rape, violence in regard to the choice of partner (honor killings as punishment for 'inappropriate' relations). The sexual health of women is often poor in societies where a woman's right to control her sexuality is not recognized.

Adolescent girls have the highest risk of sexual coercion, sexual ill health, and negative reproductive outcomes. The risks they face are higher than those of boys and men; this increased risk is partly due to gender inequity (different socialization of boys and girls, gender based violence, child marriage) and partly due to biological factors.

Family planning is the practice of freely deciding the number of children one has and the intervals between their births, particularly by means of contraception or voluntary sterilization. Abortion is the induced termination of pregnancy. Abortion laws vary significantly by country. The availability of contraception, sterilization and abortion is dependent on laws, as well as social, cultural and religious norms. Some countries have liberal laws regarding these issues, but in practice it is very difficult to access such services due to doctors, pharmacists and other social and medical workers being conscientious objectors. Family planning is particularly important from a women's rights perspective, as having very many pregnancies, especially in areas where malnutrition is present, can seriously endanger women's health. UNFA writes that "Family planning is central to gender equality and women’s empowerment, and it is a key factor in reducing poverty".

Family planning is often opposed by governments who have strong natalist policies. During the 20th century, such examples have included the aggressive natalist policies from communist Romania and communist Albania. State mandated forced marriage was also practiced by some authoritarian governments as a way to meet population targets: the Khmer Rouge regime in Cambodia systematically forced people into marriages, in order to increase the population and continue the revolution. By contrast, the one child policy of China (1979–2015) included punishments for families with more than one child and forced abortions. Some governments have sought to prevent certain ethnic or social groups from reproduction. Such policies were carried out against ethnic minorities in Europe and North America in the 20th century, and more recently in Latin America against the Indigenous population in the 1990s; in Peru, President Alberto Fujimori (in office from 1990 to 2000) has been accused of genocide and crimes against humanity as a result of a sterilization program put in place by his administration targeting indigenous people (mainly the Quechuas and the Aymaras).

Human rights organizations have expressed concern about the legal impunity of perpetrators of crimes against women, with such crimes being often ignored by authorities. This is especially the case with murders of women in Latin America. In particular, there is impunity in regard to domestic violence.

Women are often, in law or in practice, unable to access legal institutions. UN Women has said that: "Too often, justice institutions, including the police and the courts, deny women justice". Often, women are denied legal recourse because the state institutions themselves are structured and operate in ways incompatible with genuine justice for women who experience violence.

"Harmful traditional practices" refer to forms of violence which are committed in certain communities often enough to become cultural practice, and accepted for that reason. Young women are the main victims of such acts, although men can be affected. They occur in an environment where women and girls have unequal rights and opportunities.
These practices include, according to the Office of the United Nations High Commissioner for Human Rights:

Son preference refers to a cultural preference for sons over daughters, and manifests itself through practices such as sex selective abortion; female infanticide; or abandonment, neglect or abuse of girl-children.

Abuses regarding nutrition are taboos in regard to certain foods, which result in poor nutrition of women, and may endanger their health, especially if pregnant.

The caste system in India which leads to untouchability (the practice of ostracizing a group by segregating them from the mainstream society) often interacts with gender discrimination, leading to a double discrimination faced by Dalit women. In a 2014 survey, 27% of Indians admitted to practicing untouchability.

Traditional customs regarding birth sometimes endanger the mothers. Births in parts of Africa are often attended by traditional birth attendants (TBAs), who sometimes perform rituals that are dangerous to the health of the mother. In many societies, a difficult labour is believed to be a divine punishment for marital infidelity, and such women face abuse and are pressured to "confess" to the infidelity.

Tribal traditions can be harmful to males; for instance, the Satere-Mawe tribe use bullet ants as an initiation rite. Men must wear gloves with hundreds of bullet ants woven in for ten minutes: the ants' stings cause severe pain and paralysis. This experience must be completed twenty times for boys to be considered "warriors".

Other harmful traditional practices include marriage by abduction, ritualized sexual slavery (Devadasi, Trokosi), breast ironing and widow inheritance.

UNFPA and UNICEF regard the practice of female genital mutilation as "a manifestation of deeply entrenched gender inequality. It persists for many reasons. In some societies, for example, it is considered a rite of passage. In others, it is seen as a prerequisite for marriage. In some communities – whether Christian, Jewish, Muslim – the practice may even be attributed to religious beliefs." 

An estimated 125 million women and girls living today have undergone FGM in the 29 countries where data exist. Of these, about half live in Egypt and Ethiopia. It is most commonly carried out on girls between infancy and 15 years old.

Early marriage, child marriage or forced marriage is prevalent in parts of Asia and Africa. The majority of victims seeking advice are female and aged between 18 and 23. Such marriages can have harmful effects on a girl's education and development, and may expose girls to social isolation or abuse.

The 2013 UN Resolution on Child, Early and Forced Marriage calls for an end to the practice, and states that "Recognizing that child, early and forced marriage is a harmful practice that violates abuses, or impairs human rights and is linked to and perpetuates other harmful practices and human rights violations, that these violations have a disproportionately negative impact on women and girls [...]". Despite a near-universal commitment by governments to end child marriage, "one in three girls in developing countries (excluding China) will probably be married before they are 18." UNFPA states that, "over 67 million women 20–24 year old in 2010 had been married as girls. Half were in Asia, one-fifth in Africa. In the next decade 14.2 million girls under 18 will be married every year; this translates into 39,000 girls married each day. This will rise to an average of 15.1 million girls a year, starting in 2021 until 2030, if present trends continue."

Bride price (also called bridewealth or bride token) is money, property, or other form of wealth paid by a groom or his family to the parents of the bride. This custom often leads to women having reduced ability to control their fertility. For instance, in northern Ghana, the payment of bride price signifies a woman's requirement to bear children, and women using birth control face threats, violence and reprisals. The custom of bride price has been criticized as contributing to the mistreatment of women in marriage, and preventing them from leaving abusive marriages. UN Women recommended its abolition, and stated that: "Legislation should [...] State that divorce shall not be contingent upon the return of bride price but such provisions shall not be interpreted to limit women’s right to divorce; State that a perpetrator of domestic violence, including marital rape, cannot use the fact that he paid bride price as a defence to a domestic violence charge."

The custom of bride price can also curtail the free movement of women: if a wife wants to leave her husband, he may demand back the bride price that he had paid to the woman's family; and the woman's family often cannot or does not want to pay it back, making it difficult for women to move out of violent husbands' homes.

Promoting gender equality is seen as an encouragement to greater economic prosperity. Female economic activity is a common measure of gender equality in an economy.

Gender discrimination often results in women obtaining low-wage jobs and being disproportionately affected by poverty, discrimination and exploitation. A growing body of research documents what works to economically empower women, from providing access to formal financial services to training on agricultural and business management practices, though more research is needed across a variety of contexts to confirm the effectiveness of these interventions.

Gender biases also exist in product and service provision. The term "Women's Tax", also known as "Pink Tax", refers to gendered pricing in which products or services marketed to women are more expensive than similar products marketed to men. Gender-based price discrimination involves companies selling almost identical units of the same product or service at comparatively different prices, as determined by the target market. Studies have found that women pay about $1,400 a year more than men due to gendered discriminatory pricing. Although the "pink tax" of different goods and services is not uniform, overall women pay more for commodities that result in visual evidence of feminine body image.

Since the 1950s, social scientists as well as feminists have increasingly criticized gendered arrangements of work and care and the male breadwinner role. Policies are increasingly targeting men as fathers as a tool of changing gender relations.
Shared earning/shared parenting marriage, that is, a relationship where the partners collaborate at sharing their responsibilities inside and outside of the home, is often encouraged in Western countries.

Western countries with a strong emphasis on women fulfilling the role of homemakers, rather than a professional role, include parts of German speaking Europe (i.e. parts of Germany, Austria and Switzerland); as well as the Netherlands and Ireland. In the computer technology world of Silicon Valley in the United States, "New York Times" reporter Nellie Bowles has covered harassment and bias against women as well as a backlash against female equality.

A key issue towards insuring gender equality in the workplace is the respecting of maternity rights and reproductive rights of women. Different countries have different rules regarding maternity leave, paternity leave and parental leave. Another important issue is ensuring that employed women are not "de jure" or "de facto" prevented from having a child. In some countries, employers ask women to sign formal or informal documents stipulating that they will not get pregnant or face legal punishment. Women often face severe violations of their reproductive rights at the hands of their employers; and the International Labour Organization classifies forced abortion coerced by the employer as labour exploitation. Other abuses include routine virginity tests of unmarried employed women.

The degree to which women can participate (in law and in practice) in public life varies by culture and socioeconomic characteristics. Seclusion of women within the home was a common practice among the upper classes of many societies, and this still remains the case today in some societies. Before the 20th century it was also common in parts of Southern Europe, such as much of Spain.

Women's freedom of movement continues to be legally restricted in some parts of the world. This restriction is often due to marriage laws. In some countries, women must legally be accompanied by their male guardians (such as the husband or male relative) when they leave home.

The Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) states at Article 15 (4) that:
In addition to laws, women's freedom of movement is also restricted by social and religious norms. Restrictions on freedom of movement also exist due to traditional practices such as baad, swara, or vani.

In many parts of the world, girls' access to education is very restricted. In developing parts of the world women are often denied opportunities for education as girls and women face many obstacles. These include: early and forced marriages; early pregnancy; prejudice based on gender stereotypes at home, at school and in the community; violence on the way to school, or in and around schools; long distances to schools; vulnerability to the HIV epidemic; school fees, which often lead to parents sending only their sons to school; lack of gender sensitive approaches and materials in classrooms. According to OHCHR, there have been multiple attacks on schools worldwide during the period 2009–2014 with "a number of these attacks being specifically directed at girls, parents and teachers advocating for gender equality in education". The United Nations Population Fund says:

Women are underrepresented in most countries' National Parliaments. The 2011 UN General Assembly resolution on women’s political participation called for female participation in politics, and expressed concern about the fact that "women in every part of the world continue to be largely marginalized from the political sphere". Only 22 percent of parliamentarians globally are women and therefore, men continue to occupy most positions of political and legal authority. As of November 2014, women accounted for 28% of members of the single or lower houses of parliaments in the European Union member states.

In some Western countries women have only recently obtained the right to vote.

In 2015, 61.3% of Rwanda's Lower House of Parliament were women, the highest proportion anywhere in the world, but worldwide that was one of only two such bodies where women were in the majority, the other being Bolivia's Lower House of Parliament. (See also Gender equality in Rwanda).

Equal rights for women in marriage, divorce, and property/land ownership and inheritance are essential for gender equality. The Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) has called for the end of discriminatory family laws. In 2013, UN Women stated that "While at least 115 countries recognize equal land rights for women and men, effective implementation remains a major challenge".

The legal and social treatment of married women has been often discussed as a political issue from the 19th century onwards. Until the 1970s, legal subordination of married women was common across European countries, through marriage laws giving legal authority to the husband, as well as through marriage bars. In 1978, the Council of Europe passed the "Resolution (78) 37 on equality of spouses in civil law". Switzerland was one of the last countries in Europe to establish gender equality in marriage, in this country married women's rights were severely restricted until 1988, when legal reforms providing for gender equality in marriage, abolishing the legal authority of the husband, come into force (these reforms had been approved in 1985 by voters in a referendum, who narrowly voted in favor with 54.7% of voters approving). In the Netherlands, it was only in 1984 that full legal equality between husband and wife was achieved: prior to 1984 the law stipulated that the husband's opinion prevailed over the wife's regarding issues such as decisions on children's education and the domicile of the family.

In the United States, a wife's legal subordination to her husband was fully ended by the case of "Kirchberg v. Feenstra", , a United States Supreme Court case in which the Court held a Louisiana Head and Master law, which gave sole control of marital property to the husband, unconstitutional.

There have been and sometimes continue to be unequal treatment of married women in various aspects of everyday life. For example, in Australia, until 1983 a husband had to authorize an application for an Australian passport for a married woman. Other practices have included, and in many countries continue to include, a requirement for a husband's consent for an application for bank loans and credit cards by a married woman, as well as restrictions on the wife's reproductive rights, such as a requirement that the husband consents to the wife's acquiring contraception or having an abortion. In some places, although the law itself no longer requires the consent of the husband for various actions taken by the wife, the practice continues "de facto", with the authorization of the husband being asked in practice.

Although dowry is today mainly associated with South Asia, the practice has been common until the mid-20th century in parts of Southeast Europe.

Laws regulating marriage and divorce continue to discriminate against women in many countries. In Iraq husbands have a legal right to "punish" their wives, with paragraph 41 of the criminal code stating that there is no crime if an act is committed while exercising a legal right. In the 1990s and the 21st century there has been progress in many countries in Africa: for instance in Namibia the marital power of the husband was abolished in 1996 by the "Married Persons Equality Act"; in Botswana it was abolished in 2004 by the "Abolition of Marital Power Act"; and in Lesotho it was abolished in 2006 by the "Married Persons Equality Act". Violence against a wife continues to be seen as legally acceptable in some countries; for instance in 2010, the United Arab Emirates Supreme Court ruled that a man has the right to physically discipline his wife and children as long as he does not leave physical marks. The criminalization of adultery has been criticized as being a prohibition, which, in law or in practice, is used primarily against women; and incites violence against women (crimes of passion, honor killings).

Two recent movements in countries with large Kurdish populations have implemented political gender equality. One has been the Kurdish movement in southeastern Turkey led by the Democratic Regions Party (DBP) and the Peoples' Democratic Party (HDP), from 2006 or before.
The mayorships of 2 metropolitan areas and 97 towns are led jointly by a man and a woman, both called co-mayors. Party offices are also led by a man and a woman. Local councils were formed, which also had to be co-presided over by a man and a woman together. However, in November 2016 the Turkish government cracked down on the HDP, jailing ten of its members of Parliament, including the party's male and female co-leaders.

A movement in northern Syria, also Kurdish, has been led by the Democratic Union Party (PYD). In northern Syria all villages, towns and cities governed by the PYD were co-governed by a man and a woman. Local councils were formed where each sex had to have 40% representation, and minorities also had to be represented.

Gender stereotypes arise from the socially approved roles of women and men in the private or public sphere, at home or in the workplace. In the household, women are typically seen as mother figures, which usually places them into a typical classification of being "supportive" or "nurturing". Women are expected to want to take on the role of a mother and take on primary responsibility for household needs. Their male counterparts are seen as being "assertive" or "ambitious" as men are usually seen in the workplace or as the primary breadwinner for his family. Due to these views and expectations, women often face discrimination in the public sphere, such as the workplace. Women are stereotyped to be less productive at work because they are believed to focus more on family when they get married or have children.
A gender role is a set of societal norms dictating the types of behaviors which are generally considered acceptable, appropriate, or desirable for people based on their sex. Gender roles are usually centered on conceptions of femininity and masculinity, although there are exceptions and variations.

The way women are represented in the media has been criticized as perpetuating negative gender stereotypes. The exploitation of women in mass media refers to the criticisms that are levied against the use or objectification of women in the mass media, when such use or portrayal aims at increasing the appeal of media or a product, to the detriment of, or without regard to, the interests of the women portrayed, or women in general. Concerns include the fact that all forms of media have the power to shape the population's perceptions and portray images of unrealistic stereotypical perceptions by portraying women either as submissive housewives or as sex objects. The media emphasizes traditional domestic or sexual roles that normalize violence against women. The vast array of studies that have been conducted on the issue of the portrayal of women in the media have shown that women are often portrayed as irrational, fragile, not intelligent, submissive and
subservient to men. Research has shown that stereotyped images such as these have been shown to negatively impact on the mental health of many female viewers who feel bound by these roles, causing amongst other problems, self-esteem issues, depression and anxiety.

According to a study, the way women are often portrayed by the media can lead to: "Women of average or normal appearance feeling inadequate or less beautiful in comparison to the overwhelming use of extraordinarily attractive women"; "Increase in the likelihood and acceptance of sexual violence"; "Unrealistic expectations by men of how women should look or behave"; "Psychological disorders such as body dysmorphic disorder, anorexia, bulimia and so on"; "The importance of physical appearance is emphasized and reinforced early in most girls' development." Studies have found that nearly half of females ages 6–8 have stated they want to be slimmer. (Striegel-Moore & Franko, 2002)".

A few numbers on women's representation in the media:


While in many countries, the problem lies in the lack of adequate legislation, in others the principal problem is not as much the lack of a legal framework, but the fact is that most women do not know their legal rights. This is especially the case as many of the laws dealing with women's rights are of recent date. This lack of knowledge enables to abusers to lead the victims (explicitly or implicitly) to believe that their abuse is within their rights. This may apply to a wide range of abuses, ranging from domestic violence to employment discrimination. The United Nations Development Programme states that, in order to advance gender justice, "Women must know their rights and be able to access legal systems".

The 1993 UN Declaration on the Elimination of Violence Against Women states at Art. 4 (d) [...] "States should also inform women of their rights in seeking redress through such mechanisms". Enacting protective legislation against violence has little effect, if women do not know how to use it: for example a study of Bedouin women in Israel found that 60% did not know what a restraining order was; or if they don't know what acts are illegal: a report by Amnesty International showed in Hungary, in a public opinion poll of nearly 1,200 people in 2006, a total of 62% did not know that marital rape was an illegal (it was outlawed in 1997) and therefore the crime was rarely reported. Ensuring women have a minim understanding of health issues is also important: lack of access to reliable medical information and available medical procedures to which they are entitled hurts women's health.

Gender mainstreaming is described as the public policy of assessing the different implications for women and men of any planned policy action, including legislation and programmes, in all areas and levels, with the aim of achieving gender equality. The concept of gender mainstreaming was first proposed at the 1985 Third World Conference on Women in Nairobi, Kenya. The idea has been developed in the United Nations development community. Gender mainstreaming "involves ensuring that gender perspectives and attention to the goal of gender equality are central to all activities".

According to the Council of Europe definition: "Gender mainstreaming is the (re)organization, improvement, development and evaluation of policy processes, so that a gender equality perspective is incorporated in all policies at all levels and at all stages, by the actors normally involved in policy-making."

An integrated gender mainstreaming approach is "the attempt to form alliances and common platforms that bring together the power of faith and gender-equality aspirations to advance human rights." For example, "in Azerbaijan, UNFPA conducted a study on gender equality by comparing the text of the Convention on the Elimination of All Forms of Discrimination against Women with some widely recognized Islamic references and resources. The results reflect the parallels between the Convention and many tenets of Islamic scripture and practice. The study showcased specific issues, including VAW, child marriage, respect for the dignity of women, and equality in the economic and political participation of women. The study was later used to produce training materials geared towards sensitizing religious leaders."









</doc>
<doc id="57993" url="https://en.wikipedia.org/wiki?curid=57993" title="Tragedy">
Tragedy

Tragedy (from the , "tragōidia") is a form of drama based on human suffering that invokes an accompanying catharsis or pleasure in audiences. While many cultures have developed forms that provoke this paradoxical response, the term "tragedy" often refers to a specific tradition of drama that has played a unique and important role historically in the self-definition of Western civilisation. That tradition has been multiple and discontinuous, yet the term has often been used to invoke a powerful effect of cultural identity and historical continuity—"the Greeks and the Elizabethans, in one cultural form; Hellenes and Christians, in a common activity," as Raymond Williams puts it.

From its origins in the theatre of ancient Greece 2500 years ago, from which there survives only a fraction of the work of Aeschylus, Sophocles and Euripides, as well as a large number of fragments from other poets; through its singular articulations in the works of Shakespeare, Lope de Vega, Jean Racine, and Friedrich Schiller to the more recent naturalistic tragedy of Henrik Ibsen and August Strindberg; Samuel Beckett's modernist meditations on death, loss and suffering; Müller's postmodernist reworkings of the tragic canon, tragedy has remained an important site of cultural experimentation, negotiation, struggle, and change. A long line of philosophers—which includes Plato, Aristotle, Saint Augustine, Voltaire, Hume, Diderot, Hegel, Schopenhauer, Kierkegaard, Nietzsche, Freud, Benjamin, Camus, Lacan, and Deleuze—have analysed, speculated upon, and criticised the genre.

In the wake of Aristotle's "Poetics" (335 BCE), tragedy has been used to make genre distinctions, whether at the scale of poetry in general (where the tragic divides against epic and lyric) or at the scale of the drama (where tragedy is opposed to comedy). In the modern era, tragedy has also been defined against drama, melodrama, the tragicomic, and epic theatre. Drama, in the narrow sense, cuts across the traditional division between comedy and tragedy in an anti- or a-generic deterritorialisation from the mid-19th century onwards. Both Bertolt Brecht and Augusto Boal define their epic theatre projects (non-Aristotelian drama and Theatre of the Oppressed, respectively) against models of tragedy. Taxidou, however, reads epic theatre as an incorporation of tragic functions and its treatments of mourning and speculation.

The word "tragedy" appears to have been used to describe different phenomena at different times. It derives from Classical Greek , contracted from "trag(o)-aoidiā" = "goat song", which comes from "tragos" = "he-goat" and "aeidein" = "to sing" ("cf." "ode"). Scholars suspect this may be traced to a time when a goat was either the prize in a competition of choral dancing or was that around which a chorus danced prior to the animal's ritual sacrifice. In another view on the etymology, Athenaeus of Naucratis (2nd–3rd century CE) says that the original form of the word was "trygodia" from "trygos" (grape harvest) and "ode" (song), because those events were first introduced during grape harvest.

Writing in 335 BCE (long after the Golden Age of 5th-century Athenian tragedy), Aristotle provides the earliest-surviving explanation for the origin of the dramatic art form in his "Poetics", in which he argues that tragedy developed from the improvisations of the leader of choral dithyrambs (hymns sung and danced in praise of Dionysos, the god of wine and fertility):
In the same work, Aristotle attempts to provide a scholastic definition of what tragedy is:
There is some dissent to the dithyrambic origins of tragedy, mostly based on the differences between the shapes of their choruses and styles of dancing. A common descent from pre-Hellenic fertility and burial rites has been suggested. Friedrich Nietzsche discussed the origins of Greek tragedy in his early book "The Birth of Tragedy" (1872). Here, he suggests the name originates in the use of a chorus of goat-like satyrs in the original dithyrambs from which the tragic genre developed.

Scott Scullion writes: 

Athenian tragedy—the oldest surviving form of tragedy—is a type of dance-drama that formed an important part of the theatrical culture of the city-state. Having emerged sometime during the 6th century BCE, it flowered during the 5th century BCE (from the end of which it began to spread throughout the Greek world), and continued to be popular until the beginning of the Hellenistic period. No tragedies from the 6th century and only 32 of the more than a thousand that were performed in the 5th century have survived. We have complete texts extant by Aeschylus, Sophocles, and Euripides.

Athenian tragedies were performed in late March/early April at an annual state religious festival in honor of Dionysus. The presentations took the form of a contest between three playwrights, who presented their works on three successive days. Each playwright offered a tetralogy consisting of three tragedies and a concluding comic piece called a satyr play. The four plays sometimes featured linked stories. Only one complete trilogy of tragedies has survived, the "Oresteia" of Aeschylus. The Greek theatre was in the open air, on the side of a hill, and performances of a trilogy and satyr play probably lasted most of the day. Performances were apparently open to all citizens, including women, but evidence is scant. The theatre of Dionysus at Athens probably held around 12,000 people.

All of the choral parts were sung (to the accompaniment of an "aulos") and some of the actors' answers to the chorus were sung as well. The play as a whole was composed in various verse metres. All actors were male and wore masks. A Greek chorus danced as well as sang, though no one knows exactly what sorts of steps the chorus performed as it sang. Choral songs in tragedy are often divided into three sections: strophe ("turning, circling"), antistrophe ("counter-turning, counter-circling") and epode ("after-song").

Many ancient Greek tragedians employed the "ekkyklêma" as a theatrical device, which was a platform hidden behind the scene that could be rolled out to display the aftermath of some event which had happened out of sight of the audience. This event was frequently a brutal murder of some sort, an act of violence which could not be effectively portrayed visually, but an action of which the other characters must see the effects in order for it to have meaning and emotional resonance. A prime example of the use of the "ekkyklêma" is after the murder of Agamemnon in the first play of Aeschylus' "Oresteia", when the king's butchered body is wheeled out in a grand display for all to see. Variations on the "ekkyklêma" are used in tragedies and other forms to this day, as writers still find it a useful and often powerful device for showing the consequences of extreme human actions. Another such device was a crane, the mechane, which served to hoist a god or goddess on stage when they were supposed to arrive flying. This device gave origin to the phrase "deus ex machina" ("god out of a machine"), that is, the surprise intervention of an unforeseen external factor that changes the outcome of an event.

Following the expansion of the Roman Republic (509–27 BCE) into several Greek territories between 270–240 BCE, Rome encountered Greek tragedy. From the later years of the republic and by means of the Roman Empire (27 BCE-476 CE), theatre spread west across Europe, around the Mediterranean and even reached England. While Greek tragedy continued to be performed throughout the Roman period, the year 240 BCE marks the beginning of regular Roman drama. Livius Andronicus began to write Roman tragedies, thus creating some of the first important works of Roman literature. Five years later, Gnaeus Naevius also began to write tragedies (though he was more appreciated for his comedies). No complete early Roman tragedy survives, though it was highly regarded in its day; historians know of three other early tragic playwrights—Quintus Ennius, Marcus Pacuvius and Lucius Accius.

From the time of the empire, the tragedies of two playwrights survive—one is an unknown author, while the other is the Stoic philosopher Seneca. Nine of Seneca's tragedies survive, all of which are "fabula crepidata" (tragedies adapted from Greek originals); his "Phaedra", for example, was based on Euripides' "Hippolytus". Historians do not know who wrote the only extant example of the "fabula praetexta" (tragedies based on Roman subjects), "Octavia", but in former times it was mistakenly attributed to Seneca due to his appearance as a character in the tragedy.

Seneca's tragedies rework those of all three of the Athenian tragic playwrights whose work has survived. Probably meant to be recited at elite gatherings, they differ from the Greek versions in their long declamatory, narrative accounts of action, their obtrusive moralising, and their bombastic rhetoric. They dwell on detailed accounts of horrible deeds and contain long reflective soliloquies. Though the gods rarely appear in these plays, ghosts and witches abound. Senecan tragedies explore ideas of revenge, the occult, the supernatural, suicide, blood and gore. The Renaissance scholar Julius Caesar Scaliger (1484–1558), who knew both Latin and Greek, preferred Seneca to Euripides.

Classical Greek drama was largely forgotten in Western Europe from the Middle Ages to the beginning of the 16th century. Medieval theatre was dominated by mystery plays, morality plays, farces and miracle plays. In Italy, the models for tragedy in the later Middle Ages were Roman, particularly the works of Seneca, interest in which was reawakened by the Paduan Lovato de' Lovati (1241–1309). His pupil Albertino Mussato (1261–1329), also of Padua, in 1315 wrote the Latin verse tragedy "Eccerinis", which uses the story of the tyrant Ezzelino III da Romano to highlight the danger to Padua posed by Cangrande della Scala of Verona. It was the first secular tragedy written since Roman times, and may be considered the first Italian tragedy identifiable as a Renaissance work. The earliest tragedies to employ purely classical themes are the "Achilles" written before 1390 by Antonio Loschi of Vicenza (c.1365–1441) and the "Progne" of the Venetian Gregorio Correr (1409–1464) which dates from 1428–29.

In 1515 Gian Giorgio Trissino (1478–1550) of Vicenza wrote his tragedy "Sophonisba" in the vernacular that would later be called Italian. Drawn from Livy's account of Sophonisba, the Carthaginian princess who drank poison to avoid being taken by the Romans, it adheres closely to classical rules. It was soon followed by the "Oreste" and "Rosmunda" of Trissino's friend, the Florentine Giovanni di Bernardo Rucellai (1475–1525). Both were completed by early 1516 and are based on classical Greek models, "Rosmunda" on the "Hecuba" of Euripides, and "Oreste" on the "Iphigenia in Tauris" of the same author; like "Sophonisba", they are in Italian and in blank (unrhymed) hendecasyllables. Another of the first of all modern tragedies is "A Castro", by Portuguese poet and playwright António Ferreira, written around 1550 (but only published in 1587) in polymetric verse (most of it being blank hendecasyllables), dealing with the murder of Inês de Castro, one of the most dramatic episodes in Portuguese history. Although these three Italian plays are often cited, separately or together, as being the first regular tragedies in modern times, as well as the earliest substantial works to be written in blank hendecasyllables, they were apparently preceded by two other works in the vernacular: "Pamfila" or "Filostrato e Panfila" written in 1498 or 1508 by Antonio Cammelli (Antonio da Pistoia); and a "Sophonisba" by Galeotto del Carretto of 1502.

From about 1500 printed copies, in the original languages, of the works of Sophocles, Seneca, and Euripides, as well as comedic writers such as Aristophanes, Terence and Plautus, were available in Europe and the next forty years saw humanists and poets translating and adapting their tragedies. In the 1540s, the European university setting (and especially, from 1553 on, the Jesuit colleges) became host to a Neo-Latin theatre (in Latin) written by scholars. The influence of Seneca was particularly strong in its humanist tragedy. His plays, with their ghosts, lyrical passages and rhetorical oratory, brought a concentration on rhetoric and language over dramatic action to many humanist tragedies.

The most important sources for French tragic theatre in the Renaissance were the example of Seneca and the precepts of Horace and Aristotle (and contemporary commentaries by Julius Caesar Scaliger and Lodovico Castelvetro), although plots were taken from classical authors such as Plutarch, Suetonius, etc., from the Bible, from contemporary events and from short story collections (Italian, French and Spanish). The Greek tragic authors (Sophocles and Euripides) would become increasingly important as models by the middle of the 17th century. Important models were also supplied by the Spanish Golden Age playwrights Pedro Calderón de la Barca, Tirso de Molina and Lope de Vega, many of whose works were translated and adapted for the French stage.

The common forms are the:

In English, the most famous and most successful tragedies are those of William Shakespeare and his Elizabethan contemporaries. Shakespeare's tragedies include:

A contemporary of Shakespeare, Christopher Marlowe, also wrote examples of tragedy in English, notably:

John Webster (1580?–1635?), also wrote famous plays of the genre:

Contemporary with Shakespeare, an entirely different approach to facilitating the rebirth of tragedy was taken in Italy. Jacopo Peri, in the preface to his "Euridice" refers to "the ancient Greeks and Romans (who in the opinion of many sang their staged tragedies throughout in representing them on stage)." The attempts of Peri and his contemporaries to recreate ancient tragedy gave rise to the new Italian musical genre of opera. In France, tragic operatic works from the time of Lully to about that of Gluck were not called opera, but "tragédie en musique" ("tragedy in music") or some similar name; the "tragédie en musique" is regarded as a distinct musical genre. Some later operatic composers have also shared Peri's aims: Richard Wagner's concept of "Gesamtkunstwerk" ("integrated work of art"), for example, was intended as a return to the ideal of Greek tragedy in which all the arts were blended in service of the drama. Nietzsche, in his "The Birth of Tragedy" (1872) was to support Wagner in his claims to be a successor of the ancient dramatists.

For much of the 17th century, Pierre Corneille, who made his mark on the world of tragedy with plays like "Medée" (1635) and "Le Cid" (1636), was the most successful writer of French tragedies. Corneille's tragedies were strangely un-tragic (his first version of "Le Cid" was even listed as a tragicomedy), for they had happy endings. In his theoretical works on theatre, Corneille redefined both comedy and tragedy around the following suppositions:

Corneille continued to write plays through 1674 (mainly tragedies, but also something he called "heroic comedies") and many continued to be successes, although the "irregularities" of his theatrical methods were increasingly criticised (notably by François Hédelin, abbé d'Aubignac) and the success of Jean Racine from the late 1660s signalled the end of his preeminence.

Jean Racine's tragedies—inspired by Greek myths, Euripides, Sophocles and Seneca—condensed their plot into a tight set of passionate and duty-bound conflicts between a small group of noble characters, and concentrated on these characters' double-binds and the geometry of their unfulfilled desires and hatreds. Racine's poetic skill was in the representation of pathos and amorous passion (like Phèdre's love for her stepson) and his impact was such that emotional crisis would be the dominant mode of tragedy to the end of the century. Racine's two late plays ("Esther" and "Athalie") opened new doors to biblical subject matter and to the use of theatre in the education of young women. Racine also faced criticism for his irregularities: when his play, "Bérénice", was criticised for not containing any deaths, Racine disputed the conventional view of tragedy.

For more on French tragedy of the 16th and 17th centuries, see French Renaissance literature and French literature of the 17th century.

Bourgeois tragedy (German: Bürgerliches Trauerspiel) is a form that developed in 18th-century Europe. It was a fruit of the Enlightenment and the emergence of the bourgeois class and its ideals. It is characterised by the fact that its protagonists are ordinary citizens. The first true bourgeois tragedy was an English play, George Lillo's "The London Merchant; or, the History of George Barnwell", which was first performed in 1731. Usually, Gotthold Ephraim Lessing's play "Miss Sara Sampson", which was first produced in 1755, is said to be the earliest "Bürgerliches Trauerspiel" in Germany.

In modernist literature, the definition of tragedy has become less precise. The most fundamental change has been the rejection of Aristotle's dictum that true tragedy can only depict those with power and high status. Arthur Miller's essay "Tragedy and the Common Man" (1949) argues that tragedy may also depict ordinary people in domestic surroundings thus defining Domestic tragedies. British playwright Howard Barker has argued strenuously for the rebirth of tragedy in the contemporary theatre, most notably in his volume "Arguments for a Theatre". "You emerge from tragedy equipped against lies. After the musical, you're anybody's fool," he insists. 

Critics such as George Steiner have even been prepared to argue that tragedy may no longer exist in comparison with its former manifestations in classical antiquity. In "The Death of Tragedy" (1961) George Steiner outlined the characteristics of Greek tragedy and the traditions that developed from that period. In the Foreword (1980) to a new edition of his book Steiner concluded that ‘the dramas of Shakespeare are not a renascence of or a humanistic variant of the absolute tragic model. They are, rather, a rejection of this model in the light of tragi-comic and “realistic” criteria.’ In part, this feature of Shakespeare’s mind is explained by his bent of mind or imagination which was ‘so encompassing, so receptive to the plurality of diverse orders of experience.’ When compared to the drama of Greek antiquity and French classicism Shakespeare’s forms are ‘richer but hybrid'.

Though rarer in modern day there are some who continue to embrace the genre of tragedy and have created many acclaimed works in the genre. An example of such a person is the Japanese writer Gen Urobuchi who has become world renowned for his tragic works, of which examples include "Puella Magi Madoka Magica", "Fate/Zero" and "Psycho-Pass". He himself acknowledges that he's a writer of tragedy and defines his work as such. He writes tragedies that derive from a variety of the classic traditions as well as modern takes on Shakespearean to Senecan to Greek tragedy. With Fate/Zero exemplifying the Greek model of tragedy. Modern day directors and writers cited the genre as an influence on their work, notable examples include Darren Aronofsky, Michael Haneke, Asghar Farhadi, David Simon, Kurt Sutter, Derek Cianfrance, and Louis C.K.

Numerous books and plays continue to be written in the tradition of tragedy to this day examples include "Froth on the Daydream", "The Road", "The Fault in Our Stars", "Fat City", "Rabbit Hole", "Death of a Salesman", "Thirteen Reasons Why", "Requiem for a Dream", "Revolutionary Road".

Aristotle wrote in his work "Poetics" that
tragedy is characterised by seriousness and involves a great person who experiences a reversal of fortune ("Peripeteia"). Aristotle's definition can include a change of fortune from bad to good as in the "Eumenides", but he says that the change from good to bad as in "Oedipus Rex" is preferable because this induces pity and fear within the spectators. Tragedy results in a catharsis (emotional cleansing) or healing for the audience through their experience of these emotions in response to the suffering of the characters in the drama.

According to Aristotle, "the structure of the best tragedy should not be simple but complex and one that represents incidents arousing fear and pity—for that is peculiar to this form of art." This reversal of fortune must be caused by the tragic hero's "hamartia", which is often translated as either a character flaw, or as a mistake (since the original Greek etymology traces back to "hamartanein", a sporting term that refers to an archer or spear-thrower missing his target). According to Aristotle, "The misfortune is brought about not by [general] vice or depravity, but by some [particular] error or frailty." The reversal is the inevitable but unforeseen result of some action taken by the hero. It is also a misconception that this reversal can be brought about by a higher power (e.g. the law, the gods, fate, or society), but if a character’s downfall is brought about by an external cause, Aristotle describes this as a misadventure and not a tragedy.

In addition, the tragic hero may achieve some revelation or recognition (anagnorisis--"knowing again" or "knowing back" or "knowing throughout") about human fate, destiny, and the will of the gods. Aristotle terms this sort of recognition "a change from ignorance to awareness of a bond of love or hate."

In "Poetics", Aristotle gave the following definition in ancient Greek of the word "tragedy" (τραγῳδία):

"Ἔστιν οὖν τραγῳδία μίμησις πράξεως σπουδαίας καὶ τελείας μέγεθος ἐχούσης, ἡδυσμένῳ λόγῳ χωρὶς ἑκάστῳ τῶν εἰδῶν ἐν τοῖς μορίοις, δρώντων καὶ οὐ δι᾽ ἀπαγγελίας, δι᾽ ἐλέου καὶ φόβου περαίνουσα τὴν τῶν τοιούτων παθημάτων κάθαρσιν."

which means "Tragedy is an imitation of an action that is admirable, complete (composed of an introduction, a middle part and an ending), and possesses magnitude; in language made pleasurable, each of its species separated in different parts; performed by actors, not through narration; effecting through pity and fear the purification of such emotions."

Common usage of tragedy refers to any story with a sad ending, whereas to be an Aristotelian tragedy the story must fit the set of requirements as laid out by "Poetics". By this definition social drama cannot be tragic because the hero in it is a victim of circumstance and incidents that depend upon the society in which he lives and not upon the inner compulsions — psychological or religious — which determine his progress towards self-knowledge and death. Exactly what constitutes a "tragedy", however, is a frequently debated matter.

According to Aristotle, there are four species of tragedy:

1. Complex, which involves Peripety and Discovery

2. Suffering, tragedies of such nature can be seen in the Greek mythological stories of Ajaxes and Ixions

3. Character, a tragedy of moral or ethical character. Tragedies of this nature can be found in Phthiotides and Peleus

4. Spectacle, that of a horror-like theme. Examples of this nature are Phorcides and Prometheus

G.W.F. Hegel, the German philosopher most famous for his dialectical approach to epistemology and history, also applied such a methodology to his theory of tragedy. In his essay "Hegel's Theory of Tragedy," A.C. Bradley first introduced the English-speaking world to Hegel's theory, which Bradley called the "tragic collision", and contrasted against the Aristotelian notions of the "tragic hero" and his or her "hamartia" in subsequent analyses of the Aeschylus' Oresteia trilogy and of Sophocles' Antigone. Hegel himself, however, in his seminal "The Phenomenology of Spirit" argues for a more complicated theory of tragedy, with two complementary branches which, though driven by a single dialectical principle, differentiate Greek tragedy from that which follows Shakespeare. His later lectures formulate such a theory of tragedy as a conflict of ethical forces, represented by characters, in ancient Greek tragedy, but in Shakespearean tragedy the conflict is rendered as one of subject and object, of individual personality which must manifest self-destructive passions because only such passions are strong enough to defend the individual from a hostile and capricious external world:
Hegel's comments on a particular play may better elucidate his theory: "Viewed externally, Hamlet's death may be seen to have been brought about accidentally... but in Hamlet's soul, we understand that death has lurked from the beginning: the sandbank of finitude cannot suffice his sorrow and tenderness, such grief and nausea at all conditions of life... we feel he is a man whom inner disgust has almost consumed well before death comes upon him from outside."

The writer Bharata Muni, in his work on dramatic theory "A Treatise on Theatre" (Sanskrit: "Nātyaśāstra", नाट्य शास्त्र, c. 200 BCE – 200 CE), identified several "rasas" (such as pity, anger, disgust and terror) in the emotional responses of audiences for the Sanskrit drama of ancient India. The text also suggests the notion of musical modes or jatis which are the origin of the notion of the modern melodic structures known as ragas. Their role in invoking emotions are emphasised; thus compositions emphasising the notes gandhara or rishabha are said to provoke "sadness" or "pathos" ("karuna rasa") whereas rishabha evokes heroism ("vira rasa"). Jatis are elaborated in greater detail in the text "Dattilam", composed around the same time as the "Treatise".

The celebrated ancient Indian epic, "Mahabharata", can also be related to tragedy in some ways. According to Hermann Oldenberg, the original epic once carried an immense "tragic force". It was common in Sanskrit drama to adapt episodes from the "Mahabharata" into dramatic form.





</doc>
<doc id="21207536" url="https://en.wikipedia.org/wiki?curid=21207536" title="Art history">
Art history

Art history is the study of objects of art in their historical development and stylistic contexts; that is genre, design, format, and style. The study includes painting, sculpture, architecture, ceramics, furniture, and other decorative objects.

Art history is the history of different groups of people and their culture represented throughout their artwork. Art historians compare different time periods in art history. Such as a comparison to Medieval Art to Renaissance Art. This history of cultures is shown in their art work in different forms. Art can be shown by attire, architecture, religion, sports. Or more visual pieces of art such as paintings, drawings, sculptures.

As a term, art history (its product being history of art) encompasses several methods of studying the visual arts; in common usage referring to works of art and architecture. Aspects of the discipline overlap. As the art historian Ernst Gombrich once observed, "the field of art history [is] much like Caesar's Gaul, divided in three parts inhabited by three different, though not necessarily hostile tribes: (i) the connoisseurs, (ii) the critics, and (iii) the academic art historians".

As a discipline, art history is distinguished from art criticism, which is concerned with establishing a relative artistic value upon individual works with respect to others of comparable style, or sanctioning an entire style or movement; and art theory or "philosophy of art", which is concerned with the fundamental nature of art. One branch of this area of study is aesthetics, which includes investigating the enigma of the sublime and determining the essence of beauty. Technically, art history is not these things, because the art historian uses historical method to answer the questions: How did the artist come to create the work?, Who were the patrons?, Who were his or her teachers?, Who was the audience?, Who were his or her disciples?, What historical forces shaped the artist's oeuvre, and how did he or she and the creation, in turn, affect the course of artistic, political, and social events? It is, however, questionable whether many questions of this kind can be answered satisfactorily without also considering basic questions about the nature of art. The current disciplinary gap between art history and the philosophy of art (aesthetics) often hinders this inquiry.

Art history is not only a biographical endeavor. Art historians often root their studies in the scrutiny of individual objects. They thus attempt to answer in historically specific ways, questions such as: What are key features of this style?, What meaning did this object convey?, How does it function visually?, Did the artist meet their goals well?, What symbols are involved?, and Does it function discursively?

The historical backbone of the discipline is a celebratory chronology of beautiful creations commissioned by public or religious bodies or wealthy individuals in western Europe. Such a "canon" remains prominent, as indicated by the selection of objects present in art history textbooks. Nonetheless, since the 20th century there has been an effort to re-define the discipline to be more inclusive of non-Western art, art made by women, and vernacular creativity.

Art history as we know it in the 21st century began in the 19th century but has precedents that date to the ancient world. Like the analysis of historical trends in politics, literature, and the sciences, the discipline benefits from the clarity and portability of the written word, but art historians also rely on formal analysis, semiotics, psychoanalysis and iconography. Advances in photographic reproduction and printing techniques after World War II increased the ability of reproductions of artworks. Such technologies have helped to advance the discipline in profound ways, as they have enabled easy comparisons of objects. The study of visual art thus described, can be a practice that involves understanding context, form, and social significance.

Art historians employ a number of methods in their research into the ontology and history of objects.

Art historians often examine work in the context of its time. At best, this is done in a manner which respects its creator's motivations and imperatives; with consideration of the desires and prejudices of its patrons and sponsors; with a comparative analysis of themes and approaches of the creator's colleagues and teachers; and with consideration of iconography and symbolism. In short, this approach examines the work of art in the context of the world within which it was created.

Art historians also often examine work through an analysis of form; that is, the creator's use of line, shape, color, texture, and composition. This approach examines how the artist uses a two-dimensional picture plane or the three dimensions of sculptural or architectural space to create his or her art. The way these individual elements are employed results in representational or non-representational art. Is the artist imitating an object or image found in nature? If so, it is representational. The closer the art hews to perfect imitation, the more the art is "realistic". Is the artist not imitating, but instead relying on symbolism, or in an important way striving to capture nature's essence, rather than copy it directly? If so the art is non-representational—also called abstract. Realism and abstraction exist on a continuum. Impressionism is an example of a representational style that was not directly imitative, but strove to create an "impression" of nature. If the work is not representational and is an expression of the artist's feelings, longings and aspirations, or is a search for ideals of beauty and form, the work is non-representational or a work of expressionism.

An iconographical analysis is one which focuses on particular design elements of an object. Through a close reading of such elements, it is possible to trace their lineage, and with it draw conclusions regarding the origins and trajectory of these motifs. In turn, it is possible to make any number of observations regarding the social, cultural, economic, and aesthetic values of those responsible for producing the object.

Many art historians use critical theory to frame their inquiries into objects. Theory is most often used when dealing with more recent objects, those from the late 19th century onward. Critical theory in art history is often borrowed from literary scholars, and it involves the application of a non-artistic analytical framework to the study of art objects. Feminist, Marxist, critical race, queer, and postcolonial theories are all well established in the discipline. As in literary studies, there is an interest among scholars in nature and the environment, but the direction that this will take in the discipline has yet to be determined.

More recently, media and digital technology introduced possibilities of visual, spatial and experiential analyses. The relevant forms vary from movies, to interactive forms, including virtual environments, augmented environments, situated media, networked media, etc. The methods enabled by such techniques are in active development and promise to include qualitative approaches that can emphasize narrative, dramatic, emotional and ludic characteristics of history and art.

The earliest surviving writing on art that can be classified as art history are the passages in Pliny the Elder's "Natural History" (c. AD 77-79), concerning the development of Greek sculpture and painting. From them it is possible to trace the ideas of Xenokrates of Sicyon (c. 280 BC), a Greek sculptor who was perhaps the first art historian. Pliny's work, while mainly an encyclopaedia of the sciences, has thus been influential from the Renaissance onwards. (Passages about techniques used by the painter Apelles c. (332-329 BC), have been especially well-known.) Similar, though independent, developments occurred in the 6th century China, where a canon of worthy artists was established by writers in the scholar-official class. These writers, being necessarily proficient in calligraphy, were artists themselves. The artists are described in the "Six Principles of Painting" formulated by Xie He.

While personal reminiscences of art and artists have long been written and read (see Lorenzo Ghiberti "Commentarii," for the best early example), it was Giorgio Vasari, the Tuscan painter, sculptor and author of the "Lives of the Most Excellent Painters, Sculptors, and Architects", who wrote the first true "history" of art. He emphasized art's progression and development, which was a milestone in this field. His was a personal and a historical account, featuring biographies of individual Italian artists, many of whom were his contemporaries and personal acquaintances. The most renowned of these was Michelangelo, and Vasari's account is enlightening, though biased in places.

Vasari's ideas about art were enormously influential, and served as a model for many, including in the north of Europe Karel van Mander's "Schilder-boeck" and Joachim von Sandrart's "Teutsche Akademie". Vasari's approach held sway until the 18th century, when criticism was leveled at his biographical account of history.

Scholars such as Johann Joachim Winckelmann (1717–1768), criticised Vasari's "cult" of artistic personality, and they argued that the real emphasis in the study of art should be the views of the learned beholder and not the unique viewpoint of the charismatic artist. Winckelmann's writings thus were the beginnings of art criticism. His two most notable works that introduced the concept of art criticism were "Gedanken über die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst", published in 1755, shortly before he left for Rome (Fuseli published an English translation in 1765 under the title "Reflections on the Painting and Sculpture of the Greeks"), and "Geschichte der Kunst des Altertums" ("History of Art in Antiquity"), published in 1764 (this is the first occurrence of the phrase ‘history of art’ in the title of a book)". Winckelmann critiqued the artistic excesses of Baroque and Rococo forms, and was instrumental in reforming taste in favor of the more sober Neoclassicism. Jacob Burckhardt (1818–1897), one of the founders of art history, noted that Winckelmann was 'the first to distinguish between the periods of ancient art and to link the history of style with world history'. From Winckelmann until the mid-20th century, the field of art history was dominated by German-speaking academics. Winckelmann's work thus marked the entry of art history into the high-philosophical discourse of German culture.

Winckelmann was read avidly by Johann Wolfgang Goethe and Friedrich Schiller, both of whom began to write on the history of art, and his account of the Laocoön group occasioned a response by Lessing. The emergence of art as a major subject of philosophical speculation was solidified by the appearance of Immanuel Kant's "Critique of Judgment" in 1790, and was furthered by Hegel's "Lectures on Aesthetics". Hegel's philosophy served as the direct inspiration for Karl Schnaase's work. Schnaase's "Niederländische Briefe" established the theoretical foundations for art history as an autonomous discipline, and his "Geschichte der bildenden Künste", one of the first historical surveys of the history of art from antiquity to the Renaissance, facilitated the teaching of art history in German-speaking universities. Schnaase's survey was published contemporaneously with a similar work by Franz Theodor Kugler.

Heinrich Wölfflin (1864–1945), who studied under Burckhardt in Basel, is the "father" of modern art history. Wölfflin taught at the universities of Berlin, Basel, Munich, and Zurich. A number of students went on to distinguished careers in art history, including Jakob Rosenberg and Frida Schottmuller. He introduced a scientific approach to the history of art, focusing on three concepts. Firstly, he attempted to study art using psychology, particularly by applying the work of Wilhelm Wundt. He argued, among other things, that art and architecture are good if they resemble the human body. For example, houses were good if their façades looked like faces. Secondly, he introduced the idea of studying art through comparison. By comparing individual paintings to each other, he was able to make distinctions of style. His book "Renaissance and Baroque" developed this idea, and was the first to show how these stylistic periods differed from one another. In contrast to Giorgio Vasari, Wölfflin was uninterested in the biographies of artists. In fact he proposed the creation of an "art history without names." Finally, he studied art based on ideas of nationhood. He was particularly interested in whether there was an inherently "Italian" and an inherently "German" style. This last interest was most fully articulated in his monograph on the German artist Albrecht Dürer.

Contemporaneous with Wölfflin's career, a major school of art-historical thought developed at the University of Vienna. The first generation of the Vienna School was dominated by Alois Riegl and Franz Wickhoff, both students of Moritz Thausing, and was characterized by a tendency to reassess neglected or disparaged periods in the history of art. Riegl and Wickhoff both wrote extensively on the art of late antiquity, which before them had been considered as a period of decline from the classical ideal. Riegl also contributed to the revaluation of the Baroque.

The next generation of professors at Vienna included Max Dvořák, Julius von Schlosser, Hans Tietze, Karl Maria Swoboda, and Josef Strzygowski. A number of the most important twentieth-century art historians, including Ernst Gombrich, received their degrees at Vienna at this time. The term "Second Vienna School" (or "New Vienna School") usually refers to the following generation of Viennese scholars, including Hans Sedlmayr, Otto Pächt, and Guido Kaschnitz von Weinberg. These scholars began in the 1930s to return to the work of the first generation, particularly to Riegl and his concept of "Kunstwollen", and attempted to develop it into a full-blown art-historical methodology. Sedlmayr, in particular, rejected the minute study of iconography, patronage, and other approaches grounded in historical context, preferring instead to concentrate on the aesthetic qualities of a work of art. As a result, the Second Vienna School gained a reputation for unrestrained and irresponsible formalism, and was furthermore colored by Sedlmayr's overt racism and membership in the Nazi party. This latter tendency was, however, by no means shared by all members of the school; Pächt, for example, was himself Jewish, and was forced to leave Vienna in the 1930s.

Our 21st-century understanding of the symbolic content of art comes from a group of scholars who gathered in Hamburg in the 1920s. The most prominent among them were Erwin Panofsky, Aby Warburg, and Fritz Saxl. Together they developed much of the vocabulary that continues to be used in the 21st century by art historians. "Iconography"—with roots meaning "symbols from writing" refers to subject matter of art derived from written sources—especially scripture and mythology. "Iconology" is a broader term that referred to all symbolism, whether derived from a specific text or not. Today art historians sometimes use these terms interchangeably.

Panofsky, in his early work, also developed the theories of Riegl, but became eventually more preoccupied with iconography, and in particular with the transmission of themes related to classical antiquity in the Middle Ages and Renaissance. In this respect his interests coincided with those of Warburg, the son of a wealthy family who had assembled an impressive library in Hamburg devoted to the study of the classical tradition in later art and culture. Under Saxl's auspices, this library was developed into a research institute, affiliated with the University of Hamburg, where Panofsky taught.

Warburg died in 1929, and in the 1930s Saxl and Panofsky, both Jewish, were forced to leave Hamburg. Saxl settled in London, bringing Warburg's library with him and establishing the Warburg Institute. Panofsky settled in Princeton at the Institute for Advanced Study. In this respect they were part of an extraordinary influx of German art historians into the English-speaking academy in the 1930s. These scholars were largely responsible for establishing art history as a legitimate field of study in the English-speaking world, and the influence of Panofsky's methodology, in particular, determined the course of American art history for a generation.

Heinrich Wölfflin was not the only scholar to invoke psychological theories in the study of art. Psychoanalyst Sigmund Freud wrote a book on the artist Leonardo da Vinci, in which he used Leonardo's paintings to interrogate the artist's psyche and sexual orientation. Freud inferred from his analysis that Leonardo was probably homosexual.

Though the use of posthumous material to perform psychoanalysis is controversial among art historians, especially since the sexual mores of Leonardo's time and Freud's are different, it is often attempted. One of the best-known psychoanalytic scholars is Laurie Schneider Adams, who wrote a popular textbook, "Art Across Time", and a book "Art and Psychoanalysis".

An unsuspecting turn for the history of art criticism came in 1914 when Sigmund Freud published a psychoanalytical interpretation of Michelangelo's Moses titled Der Moses des Michelangelo as one of the first psychology based analyses on a work of art. Freud first published this work shortly after reading Vasari's "Lives". For unknown purposes, Freud originally published the article anonymously.

Carl Jung also applied psychoanalytic theory to art. C.G. Jung was a Swiss psychiatrist, an influential thinker, and founder of analytical psychology. Jung's approach to psychology emphasized understanding the psyche through exploring the worlds of dreams, art, mythology, world religion and philosophy. Much of his life's work was spent exploring Eastern and Western philosophy, alchemy, astrology, sociology, as well as literature and the arts. His most notable contributions include his concept of the psychological archetype, the collective unconscious, and his theory of synchronicity. Jung believed that many experiences perceived as coincidence were not merely due to chance but, instead, suggested the manifestation of parallel events or circumstances reflecting this governing dynamic. He argued that a collective unconscious and archetypal imagery were detectable in art. His ideas were particularly popular among American Abstract expressionists in the 1940s and 1950s. His work inspired the surrealist concept of drawing imagery from dreams and the unconscious.

Jung emphasized the importance of balance and harmony. He cautioned that modern humans rely too heavily on science and logic and would benefit from integrating spirituality and appreciation of the unconscious realm. His work not only triggered analytical work by art historians, but it became an integral part of art-making. Jackson Pollock, for example, famously created a series of drawings to accompany his psychoanalytic sessions with his Jungian psychoanalyst, Dr. Joseph Henderson. Henderson who later published the drawings in a text devoted to Pollock's sessions realized how powerful the drawings were as a therapeutic tool.

The legacy of psychoanalysis in art history has been profound, and extends beyond Freud and Jung. The prominent feminist art historian Griselda Pollock, for example, draws upon psychoanalysis both in her reading into contemporary art and in her rereading of modernist art. With Griselda Pollock's reading of French feminist psychoanalysis and in particular the writings of Julia Kristeva and Bracha L. Ettinger, as with Rosalind Krauss readings of Jacques Lacan and Jean-François Lyotard and Catherine de Zegher's curatorial rereading of art, Feminist theory written in the fields of French feminism and Psychoanalysis has strongly informed the reframing of both men and women artists in art history.

During the mid-20th century, art historians embraced social history by using critical approaches. The goal was to show how art interacts with power structures in society. One critical approach that art historians used was Marxism. Marxist art history attempted to show how art was tied to specific classes, how images contain information about the economy, and how images can make the status quo seem natural (ideology).

Marcel Duchamp and Dada Movement jump started the Anti-art style. Various artist did not want to create artwork that everyone was conforming to at the time. These two movements helped other artist to create pieces that were not viewed as traditional art. Some examples of styles that branched off the anti-art movement would be Neo-Dadaism, Surrealism, and Constructivism. These styles and artist did not want to surrender to traditional ways of art. This way of thinking provoked political movements such as the Russian Revolution and the communist ideals.

Artist Isaak Brodsky work of art 'Shock-worker from Dneprstroi' in 1932 shows his political involvement within art. This piece of art can be analysed to show the internal troubles Soviet Russia was experiencing at the time.
Perhaps the best-known Marxist was Clement Greenberg, who came to prominence during the late 1930s with his essay "Avant-Garde and Kitsch". In the essay Greenberg claimed that the avant-garde arose in order to defend aesthetic standards from the decline of taste involved in consumer society, and seeing kitsch and art as opposites. Greenberg further claimed that avant-garde and Modernist art was a means to resist the leveling of culture produced by capitalist propaganda. Greenberg appropriated the German word 'kitsch' to describe this consumerism, although its connotations have since changed to a more affirmative notion of leftover materials of capitalist culture. Greenberg later became well known for examining the formal properties of modern art.

Meyer Schapiro is one of the best-remembered Marxist art historians of the mid-20th century. Although he wrote about numerous time periods and themes in art, he is best remembered for his commentary on sculpture from the late Middle Ages and early Renaissance, at which time he saw evidence of capitalism emerging and feudalism declining.

Arnold Hauser wrote the first Marxist survey of Western Art, entitled "The Social History of Art". He attempted to show how class consciousness was reflected in major art periods. The book was controversial when published during the 1950s since it makes generalizations about entire eras, a strategy now called "vulgar Marxism".

Marxist Art History was refined in the department of Art History at UCLA with scholars such as T.J. Clark, O.K. Werckmeister, David Kunzle, Theodor W. Adorno, and Max Horkheimer. T.J. Clark was the first art historian writing from a Marxist perspective to abandon vulgar Marxism. He wrote Marxist art histories of several impressionist and realist artists, including Gustave Courbet and Édouard Manet. These books focused closely on the political and economic climates in which the art was created.

Linda Nochlin's essay "Why Have There Been No Great Women Artists?" helped to ignite feminist art history during the 1970s and remains one of the most widely read essays about female artists. This was then followed by a 1972 College Art Association Panel, chaired by Nochlin, entitled "Eroticism and the Image of Woman in Nineteenth-Century Art". Within a decade, scores of papers, articles, and essays sustained a growing momentum, fueled by the Second-wave feminist movement, of critical discourse surrounding women's interactions with the arts as both artists and subjects. In her pioneering essay, Nochlin applies a feminist critical framework to show systematic exclusion of women from art training, arguing that exclusion from practicing art as well as the canonical history of art was the consequence of cultural conditions which curtailed and restricted women from art producing fields. The few who did succeed were treated as anomalies and did not provide a model for subsequent success. Griselda Pollock is another prominent feminist art historian, whose use of psychoanalytic theory is described above.

While feminist art history can focus on any time period and location, much attention has been given to the Modern era. Some of this scholarship centers on the feminist art movement, which referred specifically to the experience of women. Often, feminist art history offers a critical "re-reading" of the Western art canon, such as Carol Duncan's re-interpretation of Les Demoiselles d'Avignon. Two pioneers of the field are Mary Garrard and Norma Broude. Their anthologies "Feminism and Art History: Questioning the Litany", "The Expanding Discourse: Feminism and Art History", and "Reclaiming Feminist Agency: Feminist Art History After Postmodernism" are substantial efforts to bring feminist perspectives into the discourse of art history. The pair also co-founded the Feminist Art History Conference.

As opposed to iconography which seeks to identify meaning, semiotics is concerned with how meaning is created. Roland Barthes’s connoted and denoted meanings are paramount to this examination. In any particular work of art, an interpretation depends on the identification of denoted meaning—the recognition of a visual sign, and the connoted meaning—the instant cultural associations that come with recognition. The main concern of the semiotic art historian is to come up with ways to navigate and interpret connoted meaning.

Semiotic art history seeks to uncover the codified meaning or meanings in an aesthetic object by examining its connectedness to a collective consciousness. Art historians do not commonly commit to any one particular brand of semiotics but rather construct an amalgamated version which they incorporate into their collection of analytical tools. For example, Meyer Schapiro borrowed Saussure’s differential meaning in effort to read signs as they exist within a system. According to Schapiro, to understand the meaning of frontality in a specific pictorial context, it must be differentiated from, or viewed in relation to, alternate possibilities such as a profile, or a three-quarter view. Schapiro combined this method with the work of Charles Sanders Peirce whose object, sign, and interpretant provided a structure for his approach. Alex Potts demonstrates the application of Peirce’s concepts to visual representation by examining them in relation to the "Mona Lisa". By seeing the "Mona Lisa", for example, as something beyond its materiality is to identify it as a sign. It is then recognized as referring to an object outside of itself, a woman, or "Mona Lisa". The image does not seem to denote religious meaning and can therefore be assumed to be a portrait. This interpretation leads to a chain of possible interpretations: who was the sitter in relation to Leonardo da Vinci? What significance did she have to him? Or, maybe she is an icon for all of womankind. This chain of interpretation, or “unlimited semiosis” is endless; the art historian's job is to place boundaries on possible interpretations as much as it is to reveal new possibilities.

Semiotics operates under the theory that an image can only be understood from the viewer's perspective. The artist is supplanted by the viewer as the purveyor of meaning, even to the extent that an interpretation is still valid regardless of whether the creator had intended it. Rosalind Krauss espoused this concept in her essay “In the Name of Picasso.” She denounced the artist's monopoly on meaning and insisted that meaning can only be derived after the work has been removed from its historical and social context. Mieke Bal argued similarly that meaning does not even exist until the image is observed by the viewer. It is only after acknowledging this that meaning can become opened up to other possibilities such as feminism or psychoanalysis.

Aspects of the subject which have come to the fore in recent decades include interest in the patronage and consumption of art, including the economics of the art market, the role of collectors, the intentions and aspirations of those commissioning works, and the reactions of contemporary and later viewers and owners. Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting.

Scientific advances have made possible much more accurate investigation of the materials and techniques used to create works, especially infra-red and x-ray photographic techniques which have allowed many underdrawings of paintings to be seen again. Proper analysis of pigments used in paint is now possible, which has upset many attributions. Dendrochronology for panel paintings and radio-carbon dating for old objects in organic materials have allowed scientific methods of dating objects to confirm or upset dates derived from stylistic analysis or documentary evidence. The development of good colour photography, now held digitally and available on the internet or by other means, has transformed the study of many types of art, especially those covering objects existing in large numbers which are widely dispersed among collections, such as illuminated manuscripts and Persian miniatures, and many types of archaeological artworks.

Concurrent to those technological advances, art historians have shown increasing interest in new theoretical approaches to the nature of artworks as objects. Thing theory, actor–network theory, and object-oriented ontology have played an increasing role in art historical literature.

The making of art, the academic history of art, and the history of art museums are closely intertwined with the rise of nationalism. Art created in the modern era, in fact, has often been an attempt to generate feelings of national superiority or love of one's country. Russian art is an especially good example of this, as the Russian avant-garde and later Soviet art were attempts to define that country's identity.

Most art historians working today identify their specialty as the art of a particular culture and time period, and often such cultures are also nations. For example, someone might specialize in the 19th-century German or contemporary Chinese art history. A focus on nationhood has deep roots in the discipline. Indeed, Vasari's "Lives of the Most Excellent Painters, Sculptors, and Architects" is an attempt to show the superiority of Florentine artistic culture, and Heinrich Wölfflin's writings (especially his monograph on Albrecht Dürer) attempt to distinguish Italian from German styles of art.

Many of the largest and most well-funded art museums of the world, such as the Louvre, the Victoria and Albert Museum, and the National Gallery of Art in Washington are state-owned. Most countries, indeed, have a national gallery, with an explicit mission of preserving the cultural patrimony owned by the government—regardless of what cultures created the art—and an often implicit mission to bolster that country's own cultural heritage. The National Gallery of Art thus showcases art made in the United States, but also owns objects from across the world.

The field of Art History is traditionally divided into specializations or concentrations based on eras and regions, with further sub-division based on media. Thus, someone might specialize in "19th-century German architecture" or in "16th-century Tuscan sculpture." Sub-fields are often included under a specialization. For example, the Ancient Near East, Greece, Rome, and Egypt are all typically considered special concentrations of Ancient art. In some cases, these specializations may be closely allied (as Greece and Rome, for example), while in others such alliances are far less natural (Indian art versus Korean art, for example).

Non-Western art is a relative newcomer to the Art Historical canon. Recent revisions of the semantic division between art and artifact have recast objects created in non-Western cultures in more aesthetic terms. Relative to those studying Ancient Rome or the Italian Renaissance, scholars specializing in Africa, the Ancient Americas and Asia are a growing minority.

"Contemporary art history" refers to research into the period from the 1960s until today reflecting the break from the assumptions of modernism brought by artists of the neo-avant-garde and a continuity in contemporary art in terms of practice based on conceptualist and post-conceptualist practices.

In the United States, the most important art history organization is the College Art Association. It organizes an annual conference and publishes the "Art Bulletin" and "Art Journal". Similar organizations exist in other parts of the world, as well as for specializations, such as architectural history and Renaissance art history. In the UK, for example, the Association of Art Historians is the premiere organization, and it publishes a journal titled "Art History".





</doc>
<doc id="569" url="https://en.wikipedia.org/wiki?curid=569" title="Anthropology">
Anthropology

Anthropology is the scientific study of humans and human behavior and societies in the past and present. Social anthropology and cultural anthropology study the norms and values of societies. Linguistic anthropology studies how language affects social life. Biological or physical anthropology studies the biological development of humans.

Archaeology, which studies past human cultures through investigation of physical evidence, is thought of as a branch of anthropology in the United States and Canada, while in Europe, it is viewed as a discipline in its own right or grouped under other related disciplines, such as history.

The abstract noun "anthropology" is first attested in reference to history. Its present use first appeared in Renaissance Germany in the works of Magnus Hundt and Otto Casmann. Their New Latin ' derived from the combining forms of the Greek words "ánthrōpos" (, "human") and "lógos" (, "study"). (Its adjectival form appeared in the works of Aristotle.) It began to be used in English, possibly via French ', by the early 18th century.

In 1647, the Bartholins, founders of the University of Copenhagen, defined " as follows:

Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Étienne Serres in 1839 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the National Museum of Natural History (France) by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use Ethnology, was formed in 1839. Its members were primarily anti-slavery activists. When slavery was abolished in France in 1848 the Société was abandoned.

Meanwhile, the Ethnological Society of New York, currently the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society. These anthropologists of the times were liberal, anti-slavery, and pro-human-rights activists. They maintained international connections.

Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in such diverse fields as anatomy, linguistics, and Ethnology, making feature-by-feature comparisons of their subject matters, were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwin's "On the Origin of Species" was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.

Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859. When he read Darwin, he became an immediate convert to "Transformisme", as the French called evolutionism. His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature".

Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled "Die Anthropologie der Naturvölker", 1859–1864. The title was soon translated as "The Anthropology of Primitive Peoples". The last two volumes were published posthumously.

Waitz defined anthropology as "the science of the nature of man". Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization, as well as ethnology, are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men".

Waitz was influential among the British ethnologists. In 1863 the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French "Société" were present, though not Broca. In his keynote address, printed in the first volume of its new publication, "The Anthropological Review", Hunt stressed the work of Waitz, adopting his definitions as a standard. Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist.

Similar organizations in other countries followed: The Anthropological Society of Madrid (1865), the American Anthropological Association in 1902, the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionist. One notable exception was the Berlin Society for Anthropology, Ethnology, and Prehistory (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation.

During the last three decades of the 19th century, a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898 the American Association for the Advancement of Science was able to report that 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology.

This meager statistic expanded in the 20th century to comprise anthropology departments in the majority of the world's higher educational institutions, many thousands in number. Anthropology has diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. The organization has reached global level. For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations.

Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, "social" anthropology in Great Britain and "cultural" anthropology in the US have been distinguished from other social sciences by its emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance it places on participant-observation or experiential immersion in the area of research. Cultural anthropology, in particular, has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork.

In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: "biological" or "physical" anthropology; "social", "cultural", or "sociocultural" anthropology; and archaeology; plus anthropological linguistics. These fields frequently overlap but tend to use different methodologies and techniques.

European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783). It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition.

Anthropology is a global discipline involving humanities, social sciences and natural sciences. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of "Homo sapiens", human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of "Homo sapiens" has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies.

According to Clifford Geertz,
Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades.

Sociocultural anthropology draws together the principle axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people "make sense" of the world around them, while social anthropology is the study of the "relationships" among individuals and groups. Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects the experience for self and group, contributing to a more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history. In that, it helps develop an understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.). There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree.

Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values. Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison. This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As a methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology. Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view.

The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).

Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West. Cultures in the Standard Cross-Cultural Sample (SCCS) of world societies are:

Biological anthropology and physical anthropology are synonymous terms to describe anthropological research focused on the study of humans and non-human primates in their biological, evolutionary, and demographic dimensions. It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation.

Archaeology is the study of the human past through its material remains. Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies. Archaeologists examine this material remains in order to deduce patterns of past human behavior and cultural practices. Ethnoarchaeology is a type of archaeology that studies the practices and material remain of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways.

Linguistic anthropology (not to be confused with anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.

One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts. To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' "Primitive Art", Claude Lévi-Strauss' "The Way of the Masks" (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'.

Media anthropology (also known as the anthropology of media or mass media) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media. Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media, and television have started to make their presences felt since the early 1990s.

Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire.

Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media. While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media. Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphics, paintings, and photographs are included in the focus of visual anthropology.

Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of anthropology, Bronisław Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic Anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as Political Economy focuses on production, in contrast. Economic anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective.

Political economy in anthropology is the application of the theories and methods of Historical Materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes. Sahlin's work on hunter-gatherers as the "original affluent society" did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system. More recently, these political economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world.

Applied anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy". More simply, applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to development anthropology (distinct from the more critical anthropology of development).

Anthropology of development tends to view development from a "critical" perspective. The kind of issues addressed and implications for the approach simply involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short, why does so much planned development fail?

"Kinship" can refer both to "the study of" the patterns of social relationships in one or more human cultures, or it can refer to "the patterns of social relationships" themselves. Over its history, anthropology has developed a number of related concepts and terms, such as "descent", "descent groups", "lineages", "affines", "cognates", and even "fictive kinship". Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage.

Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white European and American feminists. Historically, such 'peripheral' perspectives have sometimes been marginalized and regarded as less valid or important than knowledge from the western world. Feminist anthropologists have claimed that their research helps to correct this systematic bias in mainstream feminist theory. Feminist anthropologists are centrally concerned with the construction of gender across societies. Feminist anthropology is inclusive of birth anthropology as a specialization.

The first African-American female anthropologist and Caribbeanist is said to be Vera Mae Green who studied ethnic and family relations in the Caribbean as well as the United States, and thereby tried to improve the way black life, experiences, and culture were studied.

Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation". It is believed that William Caudell was the first to discover the field of medical anthropology. Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole. It focuses on the following six basic fields:

Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness. On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as "cultural psychiatry" and "transcultural psychiatry" or "ethnopsychiatry".

Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people.

Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group – with its own history, language, practices, and conceptual categories – shape processes of human cognition, emotion, perception, motivation, and mental health. It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes.

Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them.

Transpersonal anthropology studies the relationship between altered states of consciousness and culture. As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience. However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issues – for instance, the roles of myth, ritual, diet, and texts in evoking and interpreting extraordinary experiences.

Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes were taken up at two main levels. Firstly, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Secondly, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz' comparative work on "Negara", the Balinese state, is an early, famous example.

Legal anthropology or anthropology of law specializes in "the cross-cultural study of social ordering". Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation. More recent applications include issues such as human rights, legal pluralism, and political uprisings.

Public anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline – illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change".

Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science. Donna Haraway's 1985 "Cyborg Manifesto" could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings.

Digital anthropology is the study of the relationship between humans and digital-era technology, and extends to various areas where anthropology and technology intersect. It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture. The field is new, and thus has a variety of names with a variety of emphases. These include techno-anthropology, digital ethnography, cyberanthropology, and virtual anthropology.

Ecological anthropology is defined as the "study of cultural adaptations to environments". The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment". The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how their environment across space and time. The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, century anthropology and more. The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park.

Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names.

The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures. Modern anthropology assumes that there is complete continuity between magical thinking and religion, and that every religion is a cultural product, created by the human community that worships it.

Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition". Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes. There are two main approaches to urban anthropology: examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city.

Anthrozoology (also known as "human–animal studies") is the study of interaction between living things. It is an interdisciplinary field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology. A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions. It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.

Biocultural anthropology is the scientific exploration of the relationships between human biology and culture. Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences. After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology.

Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present.

Forensic anthropology is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition. A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable. The adjective "forensic" refers to the application of this subfield of science to a court of law.

Paleoanthropology combines the disciplines of paleontology and physical anthropology. It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints. Genetics and morphology of specimens are crucially important to this field. Markers on specimens, such as enamel fractures and dental decay on teeth, can also give insight into the behaviour and diet of past populations.

Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of anthropologists is the American Anthropological Association (AAA), which was founded in 1903. Its members are anthropologists from around the globe.

In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe. The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology.

Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.

As the field has matured it has debated and arrived at ethical principles aimed at protecting both the subjects of anthropological research as well as the researchers themselves, and professional societies have generated codes of ethics.

Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.

Some commentators have contended:

As part of their quest for scientific objectivity, present-day anthropologists typically urge cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that cultures should not be judged by another's values or viewpoints, but be examined dispassionately on their own terms. There should be no notions, in good anthropology, of one culture being better or worse than another culture.

Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation (including circumcision and subincision), and torture. Topics like racism, slavery, and human sacrifice attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields.

Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.

But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the Axis Powers (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.

Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little. Many anthropologists (students and teachers) were active in the antiwar movement. Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).

Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The "Principles of Professional Responsibility" issued by the American Anthropological Association and amended through November 1986 stated that "in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given." The current "Principles of Professional Responsibility" does not make explicit mention of ethics surrounding state interactions.

Anthropologists, along with other social scientists, are working with the US military as part of the US Army's strategy in Afghanistan. The "Christian Science Monitor" reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the "Human Terrain System" (HTS) program; in addition, HTS teams are working with the US military in Iraq. In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities released its final report concluding, in part, that, "When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of "anthropology" within DoD."

Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.

There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.

In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures). They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs.

Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past. Anthropologists and geographers share approaches to culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.

Because anthropology developed from so many different enterprises (see History of anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.

Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal "Exploring the City: Inquiries Toward an Urban Anthropology" mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.

Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.

In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like "Terrain" ("fieldwork"), and developing with the center founded by Marc Augé ("Le Centre d'anthropologie des mondes contemporains", the Anthropological Research Center of Contemporary Societies).

Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses.






</doc>
<doc id="59134370" url="https://en.wikipedia.org/wiki?curid=59134370" title="Three Rooms Press">
Three Rooms Press

Three Rooms Press is a New York City-based small press. It was founded in 1993 by Kat Georges and Peter Carlaftes with a focus on poetry, but the press now publishes mainly fiction, memoir, and art. Three Rooms Press's name was inspired by one of the themes in Harold Pinter's play The Homecoming. 
The press also manages an annual international dada art and poetry journal called "Maintenant", which was featured by the Brussels Poetry Fest in 2016 and 2017. Issues of "Maintenant" have been featured and sold in museums such as the Museum of Modern Art in New York City and the BelVUE Museum in Brussels. Three Rooms Press books are distributed by PGW / Ingram.

Three Rooms Press authors include:
William S. Burroughs, Robert Silverberg, Johanna Drucker, Julia Watts, William Least Heat-Moon and Meagan Brothers, among others.

Several of Three Rooms Press' books have won awards. "The Obama Inheritance," edited by Gary Philips, won the 2018 Anthony Award for Best Anthology, and also earned the Bronze Medal for the "Foreword Reviews "Indie Book Award for Anthologies. "Atrium", a poetry collection by Hala Alyan, won the Arab American Book Award for poetry in 2013. "Weird Girl and What's His Name", a young adult novel by Meagan Brothers, was named IndieFab's Young Adult Book of the Year in 2015, and was named one of the best teen books of 2015 by Kirkus Reviews.



</doc>
<doc id="60162699" url="https://en.wikipedia.org/wiki?curid=60162699" title="Change and continuity">
Change and continuity

Change and continuity is a classic dichotomy within the fields of history, historical sociology, and social sciences more broadly. The dichotomy is used to discuss and evaluate the extent to which a historical development or event represents a decisive change or whether a situation remains largely unchanged. The question of change and continuity is considered a classic discussion in the study of historical developments. One example could be a discussion about how much and why the Peace of Westphalia in 1648 represents an important change in European history. The dichotomy lends itself to constructing and evaluating historical periodizations. In terms of creating and peridizations (e.g. the Enlightenment or the Victorian Era), the dichotomy can be used to discuss whether when a period should start and end. Therefore, the dichotomy is also important in relation to understanding of historical chronology.

Economic historian Alexander Gerschenkron argues that continuity "appears to mean no more than absence of change, i.e. stability." German historian Reinhart Koselleck, however, has been said to challenge this dichotomy.


</doc>
<doc id="310864" url="https://en.wikipedia.org/wiki?curid=310864" title="Intertwingularity">
Intertwingularity

Intertwingularity is a term coined by Ted Nelson to express the complexity of interrelations in human knowledge.

Nelson wrote in "Computer Lib/Dream Machines" : "EVERYTHING IS DEEPLY INTERTWINGLED. In an important sense there are no "subjects" at all; there is only all knowledge, since the cross-connections among the myriad topics of this world simply cannot be divided up neatly."

He added the following comment in the revised edition : "Hierarchical and sequential structures, especially popular since Gutenberg, are usually forced and artificial. Intertwingularity is not generally acknowledged—people keep pretending they can make things hierarchical, categorizable and sequential when they can't."

Intertwingularity is related to Nelson's coining of the term hypertext, partially inspired by "As We May Think" (1945) by Vannevar Bush.

Peter Morville, an influential figure in information architecture, discusses intertwingularity in some of his books. In "Ambient Findability: What We Find Changes Who We Become" (2005), Morville uses the concept of intertwingularity to describe the experience of using hypertext on the web and starting to use computers embedded in everyday objects, termed ubiquitous computing. In 2014 he published a book called "Intertwingled: Information Changes Everything" about the intertwingularity of the universe, crediting Nelson with the word. 

David Weinberger wrote about intertwingularity in "Everything Is Miscellaneous: The Power of the New Digital Disorder" in 2008, explaining that providing unique identifiers for items helps enable intertwingularity.

The concept of intertwingularity was celebrated at the "Intertwingled: The Work and Influence of Ted Nelson" conference on April 14, 2014 at Chapman University. The organizers published a book called "Intertwingled: The Work and Influence of Ted Nelson" in 2015, with articles about Nelson's work and legacy. One of the organizers of the conference and editors of the book, Douglas Dechow, said "In the 1960s, he saw a world of networked, interlinked – intertwingled, if you will – documents where all of the world’s knowledge is able to interact and intermingle...He was the first, or among the first, people to have that idea."




</doc>
<doc id="4911388" url="https://en.wikipedia.org/wiki?curid=4911388" title="Knowledge-based theory of the firm">
Knowledge-based theory of the firm

The knowledge-based theory of the firm considers knowledge as the most strategically significant resource of a firm. Its proponents argue that because knowledge-based resources are usually difficult to imitate and socially complex, heterogeneous knowledge bases and capabilities among firms are the major determinants of sustained competitive advantage and superior corporate performance.

This knowledge is embedded and carried through multiple entities including organizational culture and identity, policies, routines, documents, systems, and employees. Originating from the strategic management literature, this perspective builds upon and extends the resource-based view of the firm (RBV) initially promoted by Penrose (1959) and later expanded by others (Wernerfelt 1984, Barney 1991, Conner 1991).

Although the resource-based view of the firm recognizes the important role of knowledge in firms that achieve a competitive advantage, proponents of the knowledge-based view argue that the resource-based perspective does not go far enough. Specifically, the RBV treats knowledge as a generic resource, rather than having special characteristics. It therefore does not distinguish between different types of knowledge-based capabilities. Information technologies can play an important role in the knowledge-based view of the firm in that information systems can be used to synthesize, enhance, and expedite large-scale intra- and inter-firm knowledge management (Alavi and Leidner 2001).

Whether or not the Knowledge-based theory of the firm actually constitutes a theory has been the subject of considerable debate. See for example, Foss (1996) and Phelan & Lewin (2000). According to one notable proponent of the knowledge-based view of the firm (KBV), "The emerging knowledge-based view of the firm is not a theory of the firm in any formal sense" (Grant, 2002, p. 135).




</doc>
<doc id="3013461" url="https://en.wikipedia.org/wiki?curid=3013461" title="Metaknowledge">
Metaknowledge

Metaknowledge or meta-knowledge is knowledge about a preselected knowledge.

For the reason of different definitions of knowledge in the subject matter literature, meta-information may or may not be included in meta-knowledge. Detailed cognitive, systemic and epistemic study of human knowledge requires a distinguishing of these concepts.

Meta-knowledge is a fundamental conceptual instrument in such research and scientific domains as, knowledge engineering, knowledge management, and others dealing with study and operations on knowledge, seen as a unified object/entities, abstracted from local conceptualizations and terminologies.
Examples of the first-level individual meta-knowledge are methods of planning, modeling, tagging, learning and every modification of a domain knowledge. 
Indeed, universal meta-knowledge frameworks have to be valid for the organization of meta-levels of individual meta-knowledge.

Metaknowledge may be automatically harvested from electronic publication archives, to reveal patterns in research, relationships between researchers and institutions and to identify contradictory results.




</doc>
<doc id="2158184" url="https://en.wikipedia.org/wiki?curid=2158184" title="Credential">
Credential

A credential is an attestation of qualification, competence, or authority issued to an individual by a third party with a relevant" or "de facto" authority or assumed competence to do so.

Examples of credentials include academic diplomas, academic degrees, certifications, security clearances, identification documents, badges, passwords, user names, keys, powers of attorney, and so on. Sometimes publications, such as scientific papers or books, may be viewed as similar to credentials by some people, especially if the publication was peer reviewed or made in a well-known journal or reputable publisher.

A person holding a credential is usually given documentation or secret knowledge ("e.g.," a password or key) as proof of the credential. Sometimes this proof (or a copy of it) is held by a third, trusted party. While in some cases a credential may be as simple as a paper membership card, in other cases, such as diplomas, it involves the presentation of letters directly from the issuer of the credential its faith in the person representing them in a negotiation or meeting.

Counterfeiting of credentials is a constant and serious problem, irrespective of the type of credential. A great deal of effort goes into finding methods to reduce or prevent counterfeiting. In general, the greater the perceived value of the credential, the greater the problem with counterfeiting and the greater the lengths to which the issuer of the credential must go to prevent fraud.

In diplomacy, credentials, also known as a letter of credence, are documents that ambassadors, diplomatic ministers, plenipotentiary, and chargés d'affaires provide to the government to which they are accredited, for the purpose, chiefly, of communicating to the latter the envoy's diplomatic rank. It also contains a request that full credence be accorded to his official statements. Until his credentials have been presented and found in proper order, an envoy receives no official recognition. 

The credentials of an ambassador or minister plenipotentiary are signed by the head of state, those of a chargé d'affaires by the foreign minister. Diplomatic credentials are granted and withdrawn at the pleasure of the issuing authority, based on widely varying criteria. A receiving government may reject a diplomat’s credentials by declining to receive them, but in practice this rarely happens.

In medicine, the process of credentialing is a detailed review of all permissions granted a medical doctor, physician assistant or nurse practitioner at every institution at which he or she has worked in the past, to determine a risk profile for them at a new institution. It vets the practitioner for both receiving practice insurance and the ability to bill to insurance for patient care. As well, it certifies legal and administrative body requirements, such as the Joint Commission. 

Medical practitioners must also have credentials in the form of licenses issued by the government of the jurisdictions in which they practice, which they obtain after suitable education, training, and/or practical experience. Most medical credentials are granted for a practice specific group. They may also be withdrawn in the event of fraud or malpractice by their holders. Typically they require continuing education validation and renewal to continue practice.

Information systems commonly use credentials to control access to information or other resources. The classic combination of a user's account number or name and a secret password is a widely used example of IT credentials. An increasing number of information systems use other forms of documentation of credentials, such as biometrics (fingerprints, voice recognition, retinal scans), X.509, public key certificates, and so on.

Operators of vehicles such as automobiles, boats, and aircraft must have credentials in the form of government-issued licenses in many jurisdictions. Often the documentation of the license consists of a simple card or certificate that the operator keeps on his person while operating the vehicle, backed up by an archival record of the license at some central location. Licenses are granted to operators after a period of successful training and/or examination.

This type of credential often requires certification of good health and may also require psychological evaluations and screening for substance abuse.

Operator licenses often expire periodically and must be renewed at intervals. Renewal may simply be a formality, or it may require a new round of examinations and training.

Credentials in cryptography establish the identity of a party to communication. Usually they take the form of machine-readable cryptographic keys and/or passwords. Cryptographic credentials may be self-issued, or issued by a trusted third party; in many cases the only criterion for issuance is unambiguous association of the credential with a specific, real individual or other entity. Cryptographic credentials are often designed to expire after a certain period, although this is not mandatory. An x.509 certificate is an example of a cryptographic credential.

In military and government organizations, and some private organizations, a system of compartmenting information exists to prevent the uncontrolled dissemination of information considered to be sensitive or confidential. Persons with a legitimate need to have access to such information are issued "security clearances," which can be tracked and verified to ensure that no unauthorized persons gain access to protected information.

Security clearances are among the most carefully guarded credentials. Often they are granted to individuals only after a lengthy investigation and only after their need to have access to protected information has been adequately justified to the issuing authority. The most elaborate security-clearance systems are found in the world's military organizations. Some credentials of this type are considered so sensitive that their holders are not even permitted to acknowledge that they have them (except to authorized parties). Documentation of security clearances usually consists of records keep at a secure facility and verifiable on demand from authorized parties.

Breaches of security involving security clearances are often punished by specific statutory law, particularly if they occur in the context of deliberate espionage, whereas most other counterfeiting and misuse of credentials is punished by law only when used with deliberate intent to defraud in specific contexts. Security clearances are regularly withdrawn when they are no longer justified, or when the person holding them is determined to be too great a security risk.

In many democratic nations, press credentials are not required at the national or federal level for any publication of any kind. However, individual corporations, and certain government or military entities require press credentials, such as a press pass, as a formal invitation to members of the press which grants them rights to photographs or videos, press conferences, or interviews. Press credentials indicate that a person has been verified as working for a known publication, and holding a press pass typically allows that person special treatment or access rights.

Some governments impose restrictions on who may work as a journalist, requiring anyone working for the press to carry government-issued credentials. Restricting press credentials can be problematic because of its limitations on freedom of the press, particularly if government leaders selectively grant, withhold, or withdraw press credentials to disallow critique of government policy. Any press coverage published under governments that restrict journalism in this way is often treated with skepticism by others, and may not be considered any more truthful or informative than propaganda.

Some trades and professions in some jurisdictions require special credentials of anyone practicing the trade or profession. These credentials may or may not be associated with specific competencies or skills. In some cases, they exist mainly to control the number of people who are allowed to exercise a trade or profession, in order to control salaries and wages.

Persons acting as merchants, freelancers, etc., may require special credentials in some jurisdictions as well. Here again, the purpose is mainly to control the number of people working in this way, and sometimes also to track them for tax-reporting or other purposes like people evaluation.

The academic and professional world makes very extensive use of credentials, such as diplomas, degrees, certificates, and certifications, in order to attest to the completion of specific training or education programs by students, to attest to their successful completion of tests and exams, and to provide independent validation of an individual's possession of the knowledge, skills, and ability necessary to practice a particular occupation competently.

Documentation of academic and professional credentials usually consists of a printed, formal document. The issuing institution often maintains a record of the credential as well. Academic credentials are normally valid for the lifetime of the person to whom they are issued. Professional certifications are normally valid for a limited number of years, based on the pace of change in the certified profession, and require periodic recertification through reexamination (to demonstrate continuing competency as occupational standards of practice evolve) or continuing professional development (to demonstrate continually enhanced competency). 

Acquisition of these credentials often leads to increased economic mobility and work opportunity, especially for low-income people.

Titles are credentials that identify a person as belonging to a specific group, such as nobility or aristocracy, or a specific command grade in the military, or in other largely symbolic ways. They may or may not be associated with specific authority, and they do not usually attest to any specific competence or skill (although they may be associated with other credentials that do). A partial list of such titles includes



Dynamics:


</doc>
<doc id="5493220" url="https://en.wikipedia.org/wiki?curid=5493220" title="Interactional expertise">
Interactional expertise

Interactional expertise is part of a more complex classification of expertise developed by Harry Collins and Robert Evans (both based at Cardiff University). In this initial formulation interactional expertise was part of a threefold classification of substantive expertise that also included ‘no expertise’ and ‘contributory expertise’, by which they meant the expertise needed to contribute fully to all aspects of a domain of activity.

The distinction between these three different types of expertise can be illustrated by imagining the experience of a social science researcher approaching a topic for the first time. It is easy to see that, whether the research project is to be about plumbing or physics, most researchers will start from a position of ‘no expertise’ in that area. As the research project proceeds and the social interactions between the researcher and the plumbers or physicists continue, the social researcher will become increasingly knowledgeable about that topic. For example, they will find that they can talk more interestingly about plumbing or physics and ask more pertinent questions about how it works. Eventually, the researcher may even get to the point where they can answer questions about plumbing or physics as though they were a plumber or physicist even though they can’t do plumbing or physics. It is this kind of expertise that Collins and Evans call interactional expertise.

The important thing to note about interactional expertise is that the only thing the social researcher can’t do that a practicing plumber or physicist can do is the practical work of actually installing central heating or conducting experiments. It is this difference – the difference between being able to talk like a plumber/physicist and actually do plumbing/physics – that is the difference between interactional expertise (what the researcher has) and contributory expertise (what the plumbers and physicists have). Of course, plumbers and physicists who can talk fluently about their work will have both kinds of expertise.

In identifying this separate and distinctive kind of linguistic expertise, the idea of interactional expertise makes a clear break with other theories of expertise, particularly those developed in Science and Technology Studies, which tend to see expertise as a social status granted by others rather than a property of the individual. As discussed in more detail below, the idea of interactional expertise also differs from more traditional phenomenological theories of expertise, in which the embodied expertise of the contributory expert is well-recognised but the distinctively linguistic expertise of the interactional expert appears to have been overlooked. In this context, it must be emphasised that interactional expertise is a tacit knowledge-laden ability and thus similar in kind to the more embodied contributory expertise. This means that, like contributory expertise, interactional expertise cannot be acquired from books alone and it cannot be encoded in computerised expert systems. It is a specialised natural language and, as such; it can only be acquired by linguistic interaction with experts. The difference between interactional and contributory expertise is that, in the case of interactional expertise, the tacit knowledge pertains to the language of the domain but not its practice. In the case of contributory expertise, tacit knowledge relating to both the language and practice must be acquired.

The concept of interactional expertise 

In standard philosophy of knowledge the key distinction is between knowledge that is embodied and knowledge that is formally and explicitly articulated. In this dichotomous formulation, knowledge exists either as codified rules and facts or as some intangible property of the body that performs the task. This distinction forms the basis of the key debate about Artificial Intelligence research in which Hubert Dreyfus, starting from Heidegger argued that because computers don’t have bodies they can’t do what humans do and will not, therefore, succeed in becoming intelligent, no matter how sophisticated and detailed the knowledge base and rules with which they are programmed (see Dreyfus 1972).

In 1990, Harry Collins developed an alternative critique of AI which, although similar to Dreyfus’s in that it suggested fundamental limits to what AI could achieve, grounded this explanation in an understanding of socialisation rather than embodiment. Collins’s argument was that because computers are asocial objects that cannot be socialised into the life of a community, then they cannot be intelligent. In this sense, Collins is taking the alternative to the 'thinking machine' first proposed by Alan Turing in 1950 (and now known as the Turing Test) in which so-called intelligence in a machine is defined as the ability to hold a conversation. In the Turing Test, the conversation is conducted via keyboards and the challenge for the AI community is to produce a computer that can give answers that are indistinguishable from those produced by a real human. Given that such interactions are by their very nature open-ended and context-dependent Collins argues that only a fully socialised intelligence will be able to respond appropriately to any of the new and potentially unknown sentences directed to it.

Although the argument was not made in these terms at the time, the concept of interactional expertise is important here. In the original critique of AI research, Collins distinguished between behaviour-specific action (capable of being encoded and reproduced by machines) and natural action (what humans do the rest of the time and which machines cannot reproduce). In a later work with Martin Kusch, this same distinction was re-cast as the distinction as mimeomorphic action (action performed in the same way each time and thus amenable to mechanical reproduction) and polimorphic action (actions that depend on context and local convention for their correct interpretation and continuation and thus not reproducible by machines, however sophisticated).

The link between these arguments, the embodiment debate and the idea of interactional expertise is the importance of natural language. If interactional expertise exists then it suggests that people who cannot perform a particular task or skill – and who therefore cannot have the embodied expertise associated with it – can still talk about that skill as if they did possess the embodied skills. Interactional expertise thus raises a key question about the “amount” of embodiment that is needed for expertise to be transferred. For proponents of the embodiment thesis, quite a lot of embodiment is needed as the expertise resides in the relative position, movement and feel of the body. From the perspective of interactional expertise much less embodiment is needed and, taken to its logical minimum, perhaps only the ability to hear and speak are needed.

The idea of interactional expertise also has many practical applications and accounts for many everyday practices and activities. The implication of interactional expertise is to legitimate commentary and opinion from individuals outside a group of contributory expertise without necessarily saying that all opinions and views are equally valid. Examples of circumstances in which some degree of interactional expertise would be important include:

Scientific papers and research are subject to peer review but, in most cases, the reviewers will be drawn from cognate or related fields. This is particularly common in research funding decisions, where the likelihood of an application being reviewed by non-specialists increases with the amount of money involved. Even in the case of smaller awards, and of peer reviewed papers, it is still often the case that reviewers will have contributory expertise in a different narrowly defined specialism to that of the author being reviewed. If interactional expertise did not exist, then justifying peer review would be difficult. If, however, reviewers can have expertise by virtue of their interaction with a range of cognate scientists, then the process of peer review seems reasonable.

Whilst there may be some skills that are more or less generic in the management of large organisations – presumably the kinds of skills that are taught on MBA schemes around the world – we can also ask if managers do better if they understand particularities of the business they are in charge of. Intuitively it seems reasonable to suggest that the manager of a newspaper should know something about how a journalist works or that a manager of a car factory should know something about how a Production-line works. Whilst this kind of thinking is formally included in many training schemes, the idea of interactional expertise allows us to ask about the kind of experience that is needed in order for managers who lack the embodied experience of writing copy or working on a production line to understand what this is like for those that do fulfil these roles. One implication of interactional expertise is that direct experience – working one’s way up through the ranks – may be less important than previously thought even though lots of interaction with those who do these tasks could still be important. Certainly in the management of large science projects, managers will work hard to acquire interactional expertise quickly.

In art, design, science, technology, medicine and public policy many activities are undertaken by interdisciplinary teams. In science and technology, these take the form of scientists and engineers from many different disciplines working together on a single project. It is this situation, in which different groups of specialists with different, mutually incompatible and sometimes incomprehensible ideas nonetheless manage to find a way of communicating with each other and working together that inspired Peter Galison to develop the original trading zones metaphor. Similar teams are often found in public health settings, where cases are decided by multi-disciplinary teams comprising social workers, psychologists, psychiatrists, lawyers and so on. In the trading zone case, these teams work by developing a new composite language, called a pidgin or creole language, which the group shares and uses to communicate. The idea of trading zones has been developed by Mike Gorman, who has identified different types of trading zones, and examined their operation in a range of settings, including nanotechnology. Interactional expertise offers an alternative to this approach. Instead of a new language emerging, some members of the group learn the language of the others and shift back and forth between the two worlds. This is more akin to translation between two cultures rather than the creation of a new, shared, culture.

Most journalists cover many different topics in their career but some focus on a specific area, becoming specialist journalists, covering particular beats like politics, medicine, science, environment, security and so on. In the case of science, but in other areas too, the job of journalists is to render the specialist expertise of some esoteric group intelligible and relevant to ordinary folk. In doing so, they interpret events and place them in a broader context. In many cases, journalists do this by presenting ‘both sides of the argument’ in order to provide a balanced story and prevent accusations of bias. This is fine in principle but difficult in practice, particularly for science, as it requires the journalist to make a judgement about how credible a scientific claim is and thus how it should be reported. For example, should it be reported in a balanced way, in which two more or less equal sides get to make contrasting claims, as a story about a fringe, maverick or otherwise highly uncertain claim being made but not widely supported, or simply ignored as nonsense and not reported at all? In the UK, the reporting of the MMR controversy arguably adopted the ‘balanced approach’ for too long, thus lending greater credibility to the claims that MMR was dangerous than they deserved according to most members of the scientific community. In the latter stages of the debate these stories were often produced by the general news journalists and not the specialist health or science journalists who, by virtue of what we might call their interactional expertise, no longer saw the claims as credible.





</doc>
<doc id="1519168" url="https://en.wikipedia.org/wiki?curid=1519168" title="Organizing Knowledge Cognitively">
Organizing Knowledge Cognitively

People sort and store knowledge in many different ways. The main storage types are: Concepts, Schemes and Scripts, and Personal Theories.

A concept is a system of grouping and categorizing our brain uses to sort and store information. Concepts change and adapt as the amount of knowledge about a particular subject changes and grows. For example, as a child we were told that dogs and cats are animals. The concept of an animal might have been something furry with four legs. As school progressed and we learned more about animals the concept changed to incorporate everything from mammals to amphibians to fish. 

Limited concepts can lead to two things: 

Theorists believe that creating a concept includes learning the distinct features and characteristics that are present in all examples of a concept. A good way to know if something is part of a concept is to identify the defining features of the concept and see if the object or event in question shares those defining features. For example, an animal must eat food, a plant must grow, and a vertebrate must have a spine. So, every example of an animal must have the defining feature or eating food, every plant must grow, and every vertebrate must have a spine to be included in the concept.

Most people have a mental prototype, or mental example of a concept. For example, when referring to the concept of "transportation" you might think of a car, bus, truck, or train, but not typically of a skateboard or a pogo stick. Once the prototype for a concept is found, compare new objects and experiences with that prototype. Objects or events similar to the prototype are readily accepted as instances of the concept. Objects and events that are different are often rejected as instances of the concept when, in fact, they are. 

Exemplars are similar to the prototype except your concept was formed by a mixture of different examples. This helps to limit undergeneralization, a common problem with using the prototype alone. For example, when developing the concept of birds, not only learn about sparrows and pigeons but penguins and ostriches. By learning from a variety of examples, the concept is more complete and less susceptible to error.

A scheme is simply an organized set of knowledge about specific items and events. Schemes give a general or common understanding of how things are. Schemes are not only a way to organize information but also influence our perception and interpretation of new things or experiences. A script is a scheme with a particular order or sequence. For example, if we hear a story about how a man left his house, got into his car, and went for a drive at night, we would assume that he had turned on his lights between getting in his car and going for a drive even though we are not told so. The general information about driving a car would be the scheme and the sequence of events in driving the car would be the script.

Ever since birth we have been forming our own personal theories about the world and everything in it. We form personal theories to explain the events and objects in our individual world, such as family and entertainment. Although these theories are based on observation and fact, they are not necessarily 100% correct or complete. Theories grow and change the same way as concepts and schemes. Personal theories are what influence defining features of concepts, thus influence whole concepts. This is an individual process; personal theories are formed without any outside help. This can often lead to misconceptions or false beliefs. The most correct personal theories are the ones based on the most correct concepts and schemes, the building blocks of theories.

Source: Educational Psychology- Developing Learners 4th Edition, Jeanne Ellis Ormrod


</doc>
<doc id="9511103" url="https://en.wikipedia.org/wiki?curid=9511103" title="Inert knowledge">
Inert knowledge

Inert knowledge is information which one can express but not use. The process of understanding by learners does not happen to that extent where the knowledge can be used for effective problem-solving in realistic situations.

The phenomenon of inert knowledge was first described in 1929 by Alfred North Whitehead:

An example for inert knowledge is vocabulary of a foreign language which is available during an exam but not in a real situation of communication.

An explanation for the problem of inert knowledge is that people often encode knowledge to a specific situation, so that later remindings occur only for highly similar situations.

In contrast so called conditionalized knowledge is knowledge about something which includes also knowledge as to the contexts in which that certain knowledge will be useful.


</doc>
<doc id="3468466" url="https://en.wikipedia.org/wiki?curid=3468466" title="Wise fool">
Wise fool

The wise fool, or the wisdom of the fool, is a form of literary paradox in which through a narrative a character recognized as a fool comes to be seen as a beholder of wisdom. A recognizable trope found in stories and artworks from antiquity to the twenty-first century, the wisdom of the fool often captures what reason fails to illuminate of a thing's meaning or significance; thus, the wise fool is often associated with the wisdom found through blind faith, reckless desire, hopeless romance, and wild abandon.

In turn, the wise fool is often opposed to learned or elite knowledge. While examples of the paradox can be found in a wide range of early world literature, from Greco-Roman works to the oral traditions of folk culture, the paradox received unprecedented attention from authors and artists during the Renaissance. More than Shakespeare for his range of clownish wise men or Cervantes for his lunatic genius Don Quijote, sixteenth century scholar Erasmus is often credited for creating the definitive wise fool and most famous paradox in western literature through his portrayal of Stultitia, the goddess of folly. Influential to all later fools, she shows the foolish ways of the wise and the wisdom of fools through delivering her own eulogy, "The Praise of Folly."

In his article, "The Wisdom of The Fool", Walter Kaiser illustrates that the varied names and words people have attributed to real fools in different societies when put altogether reveal the general characteristics of the wise fool as a literary construct: "empty-headed ("μάταιος", "inanis," fool), dull-witted ("μῶρος", "stultus," dolt, clown), feebleminded ("imbécile," dotard), and lacks understanding (ἄνοοσ, ἄφρων "in-sipiens"); that he is different from normal men (idiot); that he is either inarticulate ("Tor") or babbles incoherently ("fatuus") and is given to boisterous merrymaking ("buffone"); that he does not recognize the codes of propriety ("ineptus") and loves to mock others ("Narr"); that he acts like a child (νήπιος); and that he has a natural simplicity and innocence of heart (εὐήθης, natural, simpleton).

While society reprimands violent maniacs, destined to be locked away in jails or asylums, the harmless fool often receives kindnesses and benefits from the social elite. Seemingly guided by nothing other than natural instinct, the fool is not expected to grasp social conventions and thus is left to enjoy relative freedom, particularly in his or her freedom of speech. This unusual power dynamic is famously demonstrated through the fool in Shakespeare's "King Lear," who works in the royal court and remains the only character who Lear does not severely punish for speaking his mind about the king and his precarious situations. This ability to be reckless, honest, and free with language has greatly contributed to the wise fool's popularity in the literary imagination.

"To call a man a fool is not necessarily an insult, for the authentic life has frequently been pictured under the metaphor of the fool. In figures such as Socrates, Christ, and the Idiot of Dostoyevsky we see that foolishness and wisdom are not always what they seem to be." - Sam Keen, Apology for Wonder

The employment and occupation of the fool played a significant role in the ancient world. The Ancient Greek authors Xenophon and Athenaeus wrote of normal men hired to behave as insane fools and clowns while the Roman authors Lucian and Plautus left records of powerful Romans who housed deformed buffoons famous for their insolence and brazen madness. Plato, through the guise of Socrates, provides an early example of the wisdom of the fool in "The Republic" through the figure of an escaped prisoner in The Allegory of the Cave. The escaped prisoner, part of a group imprisoned from birth, returns to free his fellow inmates but is regarded as a madman in his attempts to convince his shackled friends of a greater world beyond the cave.

Numerous scholars have long regarded Socrates as the paramount wise fool of classical antiquity. Through what would come be to branded as Socratic irony, the philosopher was known to make fools of people who claimed to be wise by pretending to be an ignorant fool himself. His name also bears a strong association with the Socratic Paradox, "I know that I know nothing," a statement that has come to frame him in the oxymoron of the ignorant knower. In Plato's "Apology", this self admission of ignorance ultimately leads the oracle at Delphi to claim there is no man with greater wisdom than Socrates.

The wise fool manifested most commonly throughout the Middle Ages as a religious figure in stories and poetry. During the Islamic Golden Age (approx. 750 - 1280 CE), an entire literary genre formed around reports about the "intelligent insane." One book in particular, "Kitab Ugala al-majanin", by an-Naysaburi, a muslim author from the Abbasid Period, recounts the lives of numerous men and women recognized during their lifetimes as 'wise fools.' Folkloric variations of madmen, lost between wisdom and folly, also appear throughout the period's most enduring classic, "The Thousand and One Nights". Buhlil the Madman, also known as the Lunatic of Kufa and Wise Buhlil, is often credited as the prototype for the wise fool across the Middle East.

The fool for God's sake was a figure that appeared in both the Muslim and Christian world. Often wearing little to no clothes, this variant of the holy fool would forego all social customs and conventions and feign madness in order to be possessed with their creator's spirit. By the twelfth century in France, such feigning led to the Fête des Fous (Feast of Fools), a celebration in which clergy were allowed to behave as fools without inhibition or restraint. During the Crusades, Christ was recognized as a 'wise fool' figure through his childlike teachings that yet confounded the powerful and intellectual elite. Numerous other writers during this period would explore this theological paradox of the wise fool in Christ, sustaining the trope into the Renaissance.

The wise fool received tremendous popularity in the literary imagination during the Italian and English Renaissances. In Italian scholar Erasmus' "Moriae encomium," written in 1509 and first published in 1511, the author portrays Stultitia, the goddess of folly, and a wise fool herself, who asks what it means to be a fool and puts forth a brazen argument praising folly and claiming that all people are fools of one kind or another. According to scholar Walter Kaiser, Stultitia is "the foolish creation of the most learned man of his time, she is the literal embodiment of the word "oxymoron," and in her idiotic wisdom she represents the finest flowering of that fusion of Italian humanistic thought and northern piety which has been called Christian Humanism."

At the same time, Shakespeare greatly helped popularize the wise fool in the English theater through incorporating the trope in a variety of characters throughout many of his plays. While Shakespeare's early plays largely portray the wise fool in comic terms as a buffoon, the later plays characterize the fool in a much more melancholic and contemplative light. For example, in "King Lear", the Fool becomes the only one capable of speaking truth to the King and often takes on the role of revealing life's tragic nature to those around him. For Shakespeare, the trope became so well-known that when Olivia says of the clown Feste in Twelfth Night, "This fellow is wise enough to play the fool" (III.i.60), his audiences recognized it as a popular convention.

Numerous other authors rendered interpretations of the wise fool across the sixteenth and seventeenth centuries from Hans Sachs to Montaigne. The image of the wise fool is as well found in numerous Renaissance artworks by a range of artists including Breughel, Bosch, and Holbein the Younger. In Spain, Cervantes' novel Don Quixote exemplifies the world of the wise fool through both its title character and his companion, Sancho Panza.









</doc>
<doc id="10399772" url="https://en.wikipedia.org/wiki?curid=10399772" title="Agnotology">
Agnotology

Agnotology (formerly agnatology) is the study of culturally induced ignorance or doubt, particularly the publication of inaccurate or misleading scientific data. In 1995 Robert N. Proctor, a Stanford University professor specializing in the history of science and technology, and linguist Iain Boal coined the neologism
on the basis of the Neoclassical Greek word , "agnōsis", "not knowing" (cf. Attic Greek ἄγνωτος "unknown"), and , "-logia". More generally, the term also highlights the increasingly common condition where more knowledge of a subject leaves one more uncertain than before. David Dunning of Cornell University is another academic who studies the spread of ignorance. "Dunning warns that the internet is helping propagate ignorance – it is a place where everyone has a chance to be their own expert, he says, which makes them prey for powerful interests wishing to deliberately spread ignorance".

In his 1999 book "The Erotic Margin", Irvin C. Schick referred to "unknowledge" "to distinguish it from ignorance, and to denote socially constructed lack of knowledge, that is, a conscious absence of socially pertinent knowledge". As an example, he offered the labeling "terra incognita" in early maps, noting that "The reconstruction of parts of the globe as uncharted territory is ... the "production of unknowledge", the transformation of those parts into potential objects of Western political and economic attention. It is the enabling of colonialism."

There are many causes of culturally induced ignorance. These include the influence of the media, either through neglect or as a result of deliberate misrepresentation and manipulation. Corporations and governmental agencies can contribute to the subject matter studied by agnotology through secrecy and suppression of information, document destruction, and myriad forms of inherent or avoidable culturopolitical selectivity, inattention, and forgetfulness.

Proctor cites as a prime example of the deliberate production of ignorance the tobacco industry's advertising campaign to manufacture doubt about the cancerous and other health effects of tobacco use. Under the banner of science, the industry produced research about everything except tobacco hazards to exploit public uncertainty.

Agnotology also focuses on how and why diverse forms of knowledge do "not" "come to be", or are ignored or delayed. For example, knowledge about plate tectonics was censored and delayed for at least a decade because some evidence remained classified military information related to undersea warfare.

The term "agnotology" was first coined in a footnote in Proctor's 1995 book, "The Cancer Wars: How Politics Shapes What We Know and Don't Know About Cancer": "Historians and philosophers of science have tended to treat ignorance as an ever-expanding vacuum into which knowledge is sucked – or even, as Johannes Kepler once put it, as the mother who must die for science to be born. Ignorance, though, is more complex than this. It has a distinct and changing political geography that is often an excellent indicator of the politics of knowledge. We need a political agnotology to complement our political epistemologies".

Proctor was quoted using the term to describe his research "only half jokingly", as "agnotology" in a 2001 interview about his lapidary work with the colorful rock agate. He connected the two seemingly unrelated topics by noting the lack of geologic knowledge and study of agate since its first known description by Theophrastus in 300 BC, relative to the extensive research on other rocks and minerals such as diamonds, asbestos, granite, and coal, all of which have much higher commercial value. He said agate was a "victim of scientific disinterest", the same "structured apathy" he called "the social construction of ignorance".

He was later quoted as calling it "agnotology, the study of ignorance", in a 2003 "The New York Times" story on medical historians testifying as expert witnesses.

Proctor co-organized a pair of events with Londa Schiebinger, his wife, who is also a science history professor: the first was a workshop at the Pennsylvania State University in 2003 titled "Agnatology: The Cultural Production of Ignorance", and later a conference at Stanford University in 2005 titled "Agnotology: The Cultural Production of Ignorance".

In 2004, Londa Schiebinger
gave a more precise definition of agnotology in a paper on 18th-century voyages of scientific discovery and gender relations, and contrasted it with epistemology, the theory of knowledge, saying that the latter questions how we know while the former questions why we do not know: "Ignorance is often not merely the absence of knowledge but an outcome of cultural and political struggle".

Its use as a critical description of the political economy has been expanded upon by Michael Betancourt in a 2010 article titled "Immaterial Value and Scarcity in Digital Capitalism" and expanded in the book "The Critique of Digital Capitalism". His analysis is focused on the housing bubble as well as the bubble economy of the period from 1980 to 2008. Betancourt argues that this political economy should be termed "agnotologic capitalism" because the systemic production and maintenance of ignorance is a major feature that enables the economy to function as it allows the creation of a "bubble economy".

Betancourt's argument is posed in relation to the idea of affective labor. He states that The creation of systemic unknowns where any potential "fact" is always already countered by an alternative of apparently equal weight and value renders engagement with the conditions of reality – the very situations affective labor seeks to assuage – contentious and a source of confusion, reflected by the inability of participants in bubbles to be aware of the imminent collapse until after it has happened. The biopolitical paradigm of distraction, what [Juan Martin] Prada calls "life to enjoy", can only be maintained if the underlying strictures remain hidden from view. If affective labor works to reduce alienation, agnotology works to eliminate the potential for dissent.
In his view, the role of affective labor is to enable the continuation of the agnotologic effects that enable the maintenance of the capitalist status quo.

A similar word from the same Greek roots, "agnoiology", meaning "the science or study of ignorance, which determines its quality and conditions" or "the doctrine concerning those things of which we are necessarily ignorant" describes a branch of philosophy studied by James Frederick Ferrier in the 19th century.

Anthropologist Glenn Stone points out that most of the examples of agnotology (such as work promoting tobacco use) do not actually create a lack of knowledge so much as they create confusion. A more accurate term for such writing would be "ainigmology", from the root "ainigma" (as in "enigma"); in Greek this refers to riddles or to language that obscures the true meaning of a story.

The availability of such large amounts of knowledge in this information age may not necessarily be producing a knowledgeable citizenry. Instead it may be allowing many people to cherry-pick information in blogs or news that reinforces their existing beliefs.
and to be distracted from new knowledge by repetitive or base entertainments. There is conflicting evidence on how television viewing affects intelligence as well as values formation.

An emerging new scientific discipline that has connections to agnotology is cognitronics:
The field of cognitronics appears to be growing as international conferences have centered on the topic. The 2013 conference was held in Slovenia.





</doc>
<doc id="851927" url="https://en.wikipedia.org/wiki?curid=851927" title="Obscurantism">
Obscurantism

Obscurantism ( and ) is the practice of deliberately presenting information in an imprecise and recondite manner, often designed to forestall further inquiry and understanding. There are two historical and intellectual denotations of "Obscurantism": (1) the deliberate restriction of knowledge—opposition to disseminating knowledge; and, (2) deliberate obscurity—an abstruse style (as in literature and art) characterized by deliberate vagueness. 

The term "obscurantism" derives from the title of the 16th-century satire "Epistolæ Obscurorum Virorum" (1515–19, "Letters of Obscure Men"), that was based upon the intellectual dispute between the German humanist Johann Reuchlin and the monk Johannes Pfefferkorn of the Dominican Order, about whether or not all Jewish books should be burned as un-Christian heresy. Earlier, in 1509, the monk Pfefferkorn had obtained permission from Maximilian I, Holy Roman Emperor (1486–1519), to burn all copies of the Talmud (Jewish law and Jewish ethics) known to be in the Holy Roman Empire (AD 926–1806); the "Letters of Obscure Men" satirized the Dominican arguments for burning "un-Christian" works.

In the 18th century, Enlightenment philosophers applied the term "obscurantist" to any enemy of intellectual enlightenment and the liberal diffusion of knowledge. In the 19th century, in distinguishing the varieties of obscurantism found in metaphysics and theology from the "more subtle" obscurantism of the critical philosophy of Immanuel Kant, and of modern philosophical skepticism, Friedrich Nietzsche said: "The essential element in the black art of obscurantism is not that it wants to darken individual understanding, but that it wants to blacken our picture of the world, and darken our idea of existence."

In restricting knowledge to an élite ruling class of "the few", obscurantism is fundamentally anti-democratic, because its component anti-intellectualism and elitism exclude the people as intellectually unworthy of knowing the facts and truth about the government of their City-State. 

In 18th century monarchic France, the Marquis de Condorcet, as a political scientist, documented the aristocracy's obscurantism about the social problems that provoked the French Revolution (1789–99) that deposed them and their King, Louis XVI of France.

In the 19th century, the mathematician William Kingdon Clifford, an early proponent of Darwinism, devoted some writings to uprooting obscurantism in England, after hearing clerics—who privately agreed with him about evolution—publicly denounce evolution as un-Christian. Moreover, in the realm of organized religion, obscurantism is a distinct strain of thought independent of theologic allegiance. The distinction is that fundamentalism presupposes sincere religious belief, whereas obscurantism is based upon minority manipulation of the popular faith as political praxis; cf. Censorship.

In the 20th century, the American conservative political philosopher Leo Strauss, for whom philosophy and politics intertwined, and his Neo-conservative adherents adopted the notion of government by the enlightened few as political strategy. He noted that intellectuals, dating from Plato, confronted the dilemma of either an informed populace "interfering" with government, or if it were possible for good politicians to be truthful and still govern to maintain a stable society—hence the noble lie necessary in securing public acquiescence. In "The City and Man" (1964), he discusses the myths in "The Republic" that Plato proposes effective governing requires, among them, the belief that the country (land) ruled by the State belongs to it (despite some having been conquered from others), and that citizenship derives from more than the accident of birth in the City-State. Thus, in the "New Yorker" magazine article "Selective Intelligence", Seymour Hersh observes that Strauss endorsed the "noble lie" concept: the myths politicians use in maintaining a cohesive society.

Shadia Drury criticized Strauss's acceptance of dissembling and deception of the populace as "the peculiar justice of the wise", whereas Plato proposed the Noble Lie as based upon moral good. In criticizing "Natural Right and History" (1953), she said that "Strauss thinks that the superiority of the ruling philosophers is an intellectual superiority and not a moral one ... [he] is the only interpreter who gives a sinister reading to Plato, and then celebrates him."

Leo Strauss also was criticized for proposing the notion of "esoteric" meanings to ancient texts, obscure knowledge inaccessible to the "ordinary" intellect. In "Persecution and the Art of Writing" (1952), he proposes that some philosophers write esoterically to avert persecution by the political or religious authorities, and, per his knowledge of Maimonides, Al Farabi, and Plato, proposed that an esoteric writing style is proper for the philosophic text. Rather than explicitly presenting his thoughts, the philosopher's esoteric writing compels the reader to think independently of the text, and so learn. In the "Phædrus", Socrates notes that writing does not reply to questions, but invites dialogue with the reader, thereby minimizing the problems of grasping the written word. Strauss noted that one of writing's political dangers is students' too-readily accepting dangerous ideas—as in the trial of Socrates, wherein the relationship with Alcibiades was used to prosecute him.

For Leo Strauss, philosophers' texts offered the reader lucid "exoteric" (salutary) and obscure "esoteric" (true) teachings, which are concealed to the reader of ordinary intellect; emphasizing that writers often left contradictions and other errors to encourage the reader's more scrupulous (re-)reading of the text. In observing and maintaining the "exoteric – esoteric" dichotomy, Strauss was accused of obscurantism, and for writing esoterically. 

In the "Wired" magazine article, "Why the Future Doesn't Need Us" (April 2000), the computer scientist Bill Joy, then a Sun Microsystems chief scientist, in the sub-title proposed that: "Our most powerful twenty-first-century technologies—robotics, genetic engineering, and nanotech[nology]—are threatening to make humans an endangered species"; in the body, he posits that:

Joy's proposal for limiting the dissemination of "certain" knowledge, in behalf of preserving society, was quickly likened to obscurantism. A year later, the American Association for the Advancement of Science, in the "Science and Technology Policy Yearbook 2001", published the article "A Response to Bill Joy and the Doom-and-Gloom Technofuturists", wherein the computer scientists John Seely Brown and Paul Duguid countered his proposal as technological tunnel vision, and the predicted technologically derived problems as infeasible, for disregarding the influence of non-scientists upon such societal problems.

In the essay "Why I Am Not a Conservative" (1960), the economist Friedrich von Hayek said that political conservatism is ideologically unrealistic, because of the conservative person’s inability to adapt to changing human realities and refusal to offer a positive political program that benefits everyone in a society. In that context, Hayek used the term "obscurantism" differently, to denote and describe the denial of the empirical truth of scientific theory, because of the disagreeable moral consequences that might arise from acceptance of fact.

The second sense of "obscurantism" denotes making knowledge abstruse, that is, difficult to grasp. In the 19th and 20th centuries obscurantism became a polemical term for accusing an author of deliberately writing obscurely, in order to hide his or her intellectual vacuousness. Philosophers who are neither empiricists nor positivists often are considered obscurantists when describing the abstract concepts of their disciplines. For philosophic reasons, such authors might modify or reject verifiability, falsifiability, and logical non-contradiction. From that perspective, obscure (clouded, vague, abstruse) writing does not necessarily indicate that the writer has a poor grasp of the subject, because unintelligible writing sometimes is purposeful and philosophically considered.

In contemporary discussions of virtue ethics, Aristotle's "Nicomachean Ethics" ("The Ethics") stands accused of ethical obscurantism, because of the technical, philosophic language and writing style, and their purpose being the education of a cultured governing elite.

Immanuel Kant employed technical terms that were not commonly understood by the layman. Arthur Schopenhauer contended that post-Kantian philosophers such as Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Georg Wilhelm Friedrich Hegel deliberately imitated the abstruse style of writing practiced by Kant.

G. W. F. Hegel's philosophy, and the philosophies of those he influenced, especially Karl Marx, have been accused of obscurantism. Analytic and positivistic philosophers, such as A. J. Ayer, Bertrand Russell, and the critical-rationalist Karl Popper, accused Hegel and Hegelianism of being obscure. About Hegel's philosophy, Schopenhauer wrote that it is: "... a colossal piece of mystification, which will yet provide posterity with an inexhaustible theme for laughter at our times, that it is a pseudo-philosophy paralyzing all mental powers, stifling all real thinking, and, by the most outrageous misuse of language, putting in its place the hollowest, most senseless, thoughtless, and, as is confirmed by its success, most stupefying verbiage. ..."

Nevertheless, biographer Terry Pinkard notes "Hegel has refused to go away, even in analytic philosophy, itself." Hegel was aware of his obscurantism, and perceived it as part of philosophical thinking: to accept and transcend the limitations of quotidian (everyday) thought and its concepts. In the essay "Who Thinks Abstractly?", he said that it is not the philosopher who thinks abstractly, but the layman, who uses concepts as givens that are immutable, without context. It is the philosopher who thinks concretely, because he transcends the limits of quotidian concepts, in order to understand their broader context. This makes philosophical thought and language appear obscure, esoteric, and mysterious to the layman.

In his early works, Karl Marx criticized German and French philosophy, especially German Idealism, for its traditions of German irrationalism and ideologically motivated obscurantism. Later thinkers whom he influenced, such as the philosopher György Lukács and social theorist Jürgen Habermas, followed with similar arguments of their own. However, philosophers such as Karl Popper and Friedrich Hayek in turn criticized Marx and Marxist philosophy as obscurantist (however, see above for Hayek's particular interpretation of the term).

Martin Heidegger, and those influenced by him, such as Jacques Derrida and Emmanuel Levinas, have been labeled obscurantists by critics from analytic philosophy and the Frankfurt School of critical theory. Of Heidegger, Bertrand Russell wrote, "his philosophy is extremely obscure. One cannot help suspecting that language is here running riot. An interesting point in his speculations is the insistence that nothingness is something positive. As with much else in Existentialism, this is a psychological observation made to pass for logic." That is Russell's complete entry on Heidegger, and it expresses the sentiments of many 20th-century analytic philosophers concerning Heidegger.

In their obituaries, "Jacques Derrida, Abstruse Theorist, Dies at 74" (10 October 2004) and "Obituary of Jacques Derrida, French intellectual" (21 October 2004), "The New York Times" newspaper and "The Economist" magazine, described Derrida as a deliberately obscure philosopher.

In "Contingency, Irony, and Solidarity" (1989), Richard Rorty proposed that in "The Post Card: From Socrates to Freud and Beyond" (1978), Jacques Derrida purposefully used undefinable words (e.g. Différance), and used defined words in contexts so diverse that they render the words unintelligible, hence, the reader is unable to establish a context for his literary self. In that way, the philosopher Derrida escapes metaphysical accounts of his work. Since the work ostensibly contains no metaphysics, Derrida has, consequently, escaped metaphysics.

Derrida's philosophic work is especially controversial among American and British academics, as when the University of Cambridge awarded him an honorary doctorate, despite opposition from among the Cambridge philosophy faculty and analytical philosophers worldwide. In opposing the decision, philosophers including Barry Smith, W. V. O. Quine, David Armstrong, Ruth Barcan Marcus, René Thom, and twelve others, published a letter of protestation in "The Times" of London, arguing that "his works employ a written style that defies comprehension ... [thus] Academic status based on what seems to us to be little more than semi-intelligible attacks upon the values of reason, truth, and scholarship is not, we submit, sufficient grounds for the awarding of an honorary degree in a distinguished university."

In the "New York Review of Books" article "An Exchange on Deconstruction" (February 1984), John Searle comments on Deconstruction: "... anyone who reads deconstructive texts with an open mind is likely to be struck by the same phenomena that initially surprised me: the low level of philosophical argumentation, the deliberate obscurantism of the prose, the wildly exaggerated claims, and the constant striving to give the appearance of profundity, by making claims that seem paradoxical, but under analysis often turn out to be silly or trivial."

Jacques Lacan was an intellectual who defended obscurantism to a degree. To his students' complaint about the deliberate obscurity of his lectures, he replied: "The less you understand, the better you listen." In the 1973 seminar "Encore", he said that his "Écrits" ("Writings") were not to be understood, but would effect a meaning in the reader, like that induced by mystical texts. The obscurity is not in his writing style, but in the repeated allusions to Hegel, derived from Alexandre Kojève's lectures on Hegel, and similar theoretic divergences.

The Sokal Affair (1996) was a publishing hoax that the professor of physics Alan Sokal perpetrated on the editors and readers of "Social Text", an academic journal of post-modern cultural studies that was not then a peer-reviewed publication. In 1996, as an experiment testing editorial integrity (fact-checking, verification, peer review, etc.), Sokal submitted "Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity", a pseudoscientific article proposing that physical reality is a social construct, in order to learn if "Social Text" would "publish an article liberally salted with nonsense if: (a) it sounded good, and, (b) it flattered the editors' ideological preconceptions". Sokal's fake article was published in the Spring/Summer 1996 issue of "Social Text", which was dedicated to the Science Wars about the conceptual validity of scientific objectivity and the nature of scientific theory, among scientific realists and postmodern critics in American universities.

Sokal's "raison de guerre" ("war reason") for publication of a false article was that postmodernist critics questioned the objectivity of science, by criticising the scientific method and the nature of knowledge, usually in the disciplines of cultural studies, cultural anthropology, feminist studies, comparative literature, media studies, and science and technology studies. Whereas the scientific realists countered that objective scientific knowledge exists, riposting that postmodernist critics almost knew nothing of the science they criticized. In the event, editorial deference to "Academic Authority" (the Author-Professor) prompted the editors of "Social Text" not to fact-check Sokal's manuscript by submitting it to peer review by a scientist.

Later, concerning the lack of editorial integrity shown by the publication of his fake article in "Social Text" magazine, Sokal addressed the matter in the May 1996 edition of the "Lingua Franca" journal, in the article "A Physicist Experiments With Cultural Studies", in which he (Sokal) announced that his transformative hermeneutics article was a parody, submitted "to test the prevailing intellectual standards", and concluded that, as an academic publication, "Social Text" ignored the requisite intellectual rigor of verification and "felt comfortable publishing an article on quantum physics without bothering to consult anyone knowledgeable in the subject".

Moreover, as a public intellectual, Sokal said his hoax was an action protesting against the contemporary tendency towards obscurantism—abstruse, esoteric, and vague writing in the social sciences:

In short, my concern over the spread of subjectivist thinking is both intellectual and political. Intellectually, the problem with such doctrines is that they are false (when not simply meaningless). There is a real world; its properties are not merely social constructions; facts and evidence do matter. What sane person would contend otherwise? And yet, much contemporary academic theorizing consists precisely of attempts to blur these obvious truths—the utter absurdity of it all being concealed through obscure and pretentious language.

Moreover, independent of the hoax, as a pseudoscientific opus, the article "Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum Gravity" is described as an exemplar "pastiche of left-wing cant, fawning references, grandiose quotations, and outright nonsense, centered on the claim that physical reality is merely a social construct." Similarly to whataboutism, obscurantism is used by elevating the readers prejudices to a grandiose value-laden assumption, belief, principle(s) or pseudoscience that does not deconstruct opposing claims and is stalling a priori and/or asserting confusing jargon or technical speak to describe events, which may deny the real world existence of physical properties.



</doc>
<doc id="2231743" url="https://en.wikipedia.org/wiki?curid=2231743" title="Explicit knowledge">
Explicit knowledge

Explicit knowledge (also expressive knowledge) is knowledge that can be readily articulated, codified, stored and accessed. It can be easily transmitted to others. Most forms of explicit knowledge can be stored in certain media. Explicit knowledge is often seen as complementary to tacit knowledge.

The information contained in encyclopedias and textbooks are good examples of explicit knowledge. The most common forms of explicit knowledge are manuals, documents, procedures, and how-to videos. Knowledge also can be audio-visual. Engineering works and product design can be seen as other forms of explicit knowledge where human skills, motives and knowledge are externalized. 




</doc>
<doc id="10115122" url="https://en.wikipedia.org/wiki?curid=10115122" title="Community of practice">
Community of practice

A community of practice (CoP) is a group of people who share a craft or a profession. The concept was first proposed by cognitive anthropologist Jean Lave and educational theorist Etienne Wenger in their 1991 book "Situated Learning" . Wenger then significantly expanded on the concept in his 1998 book "Communities of Practice" .

A CoP can evolve naturally because of the members' common interest in a particular domain or area, or it can be created deliberately with the goal of gaining knowledge related to a specific field. It is through the process of sharing information and experiences with the group that members learn from each other, and have an opportunity to develop personally and professionally .

CoPs can exist in physical settings, for example, a lunch room at work, a field setting, a factory floor, or elsewhere in the environment, but members of CoPs do not have to be co-located. They form a "virtual community of practice" (VCoP) when they collaborate online, such as within discussion boards and newsgroups, or a "mobile community of practice" (MCoP) when members communicate with one another via mobile phones and participate in community work on the go.

Communities of practice are not new phenomena: this type of learning has existed for as long as people have been learning and sharing their experiences through storytelling. The idea is rooted in American pragmatism, especially C. S. Peirce's concept of the "community of inquiry" , but also John Dewey's principle of learning through occupation .

Since the publication of "Situated Learning: Legitimate Peripheral Participation" , communities of practice have been the focus of attention, first as a theory of learning and later as part of the field of knowledge management. See for a review of how the concept has changed over the years. offers a more critical view of the different ways in which the term communities of practice can be interpreted.

To understand how learning occurs outside the classroom while at the Institute for Research on Learning, Lave and Wenger studied how newcomers or novices to informal groups become established members of those groups . Lave and Wenger first used the term communities of practice to describe learning through practice and participation, which they named situated learning.

The structure of the community was created over time through a process of legitimate peripheral participation. Legitimation and participation together define the characteristic ways of belonging to a community whereas peripherality and participation are concerned with location and identity in the social world .

Lave and Wenger's research looked at how apprenticeships help people learn. They found that when newcomers join an established group or community, they spend some time initially observing and perhaps performing simple tasks in basic roles as they learn how the group works and how they can participate (an apprentice electrician, for example would watch and learn before actually doing any electrical work; initially taking on small simple jobs and eventually more complicated ones). Lave and Wenger described this socialization process as legitimate peripheral participation. The term "community of practice" is that group that Lave and Wenger referred to, who share a common interest and a desire to learn from and contribute to the community with their variety of experiences .

In his later work, abandoned the concept of legitimate peripheral participation and used the idea of an inherent tension in a duality instead. He identifies four dualities that exist in communities of practice, participation-reification, designed-emergent, identification-negotiability and local-global, although the participation-reification duality has been the focus of particular interest because of its links to knowledge management.

He describes the structure of a CoP as consisting of three interrelated terms: 'mutual engagement', 'joint enterprise' and 'shared repertoire' .


For Etienne Wenger, learning is central to human identity. A primary focus of Wenger’s more recent work is on learning as social participation – the individual as an active participant in the practices of social communities, and in the construction of his/her identity through these communities . In this context, a community of practice is a group of individuals participating in communal activity, and experiencing/continuously creating their shared identity through engaging in and contributing to the practices of their communities.

The structural characteristics of a community of practice are again redefined to a domain of knowledge, a notion of community and a practice.




In many organizations, communities of practice have become an integral part of the organization structure . These communities take on knowledge stewarding tasks that were formerly covered by more formal organizational structures. In some organizations there are both formal and informal communities of practice. There is a great deal of interest within organizations to encourage, support, and sponsor communities of practice in order to benefit from shared knowledge that may lead to higher productivity . Communities of practice are now viewed by many in the business setting as a means to capturing the tacit knowledge, or the know-how that is not so easily articulated.

An important aspect and function of communities of practice is increasing organization performance. identify four areas of organizational performance that can be affected by communities of practice: 

The communities Lave and Wenger studied were naturally forming as practitioners of craft and skill-based activities met to share experiences and insights .

Lave and Wenger observed situated learning within a community of practice among Yucatán midwives, Liberian tailors, navy quartermasters and meat cutters as well as insurance claims processors. . Other fields have made use of the concept of CoPs. Examples include education , sociolinguistics, material anthropology, medical education, second language acquisition , Parliamentary Budget Offices , health care and business sectors, and child mental health practice (AMBIT).

A famous example of a community of practice within an organization is that which developed around the Xerox customer service representatives who repaired the machines in the field . The Xerox reps began exchanging tips and tricks over informal meetings over breakfast or lunch and eventually Xerox saw the value of these interactions and created the Eureka project to allow these interactions to be shared across the global network of representatives. The Eureka database has been estimated to have saved the corporation $100 million.

Collaboration constellations differ in various ways. Some are under organizational control (e.g., teams, see below) others, like CoPs, are self-organized or under the control of individuals. For examples of how these and other collaboration types vary in terms of their temporal or boundary focus and the basis of their members' relationships, see .

A project team differs from a community of practice in several significant ways . 

By contrast,

In addition to the distinction between CoP and other types of organizational groupings found in the workplace, in some cases it is useful to differentiate CoP from community of interest (CoI).

Community of interest

Community of practice

Social capital is said to be a multi-dimensional concept, with both public and private facets (Bourdieu 1991). That is, social capital may provide value to both the individual and the group as a whole. Through informal connections that participants build in their community of practice, and in the process of sharing their expertise, learning from others, and participating in the group, members are said to be acquiring social capital – especially those members who demonstrate expertise and experience. While social capital and a community of practice are by no means synonymous, a community of practice may facilitate the development of social capital, enhance the structural dimension of social capital through connecting members and networks, and advance the relational dimension of social capital by maintaining trust and accountability among members.

 describe three kinds of knowledge: "knowledge as object", "knowledge embedded within individuals", and "knowledge embedded in a community". Communities of Practice have become associated with finding, sharing, transferring, and archiving knowledge, as well as making explicit "expertise", or tacit knowledge. Tacit knowledge is considered to be those valuable context-based experiences that cannot easily be captured, codified and stored , also .

Because knowledge management is seen "primarily as a problem of capturing, organizing, and retrieving information, evoking notions of databases, documents, query languages, and data mining" , the community of practice, collectively and individually, is considered a rich potential source of helpful information in the form of actual experiences; in other words, best practices.

Thus, for knowledge management, a community of practice is one source of content and context that if codified, documented and archived can be accessed for later use.

Members of communities of practice are thought to be more efficient and effective conduits of information and experiences. While organizations tend to provide manuals to meet the training needs of their employees, CoPs help foster the process of storytelling among colleagues which, in turn, helps them strengthen their skills on the job .

Studies have shown that workers spend a third of their time looking for information and are five times more likely to turn to a co-worker rather than an explicit source of information (book, manual, or database) . Time is saved by conferring with members of a CoP. Members of the community have tacit knowledge, which can be difficult to store and retrieve outside. For example, one person can share the best way to handle a situation based on his experiences, which may enable the other person to avoid mistakes and shorten the learning curve. In a CoP, members can openly discuss and brainstorm about a project, which can lead to new capabilities. The type of information that is shared and learned in a CoP is boundless . clarifies the difference between tacit knowledge, or knowing "how", and explicit knowledge, or knowing "what". Performing optimally in a job requires being able to convert theory into practice. Communities of practice help the individual bridge the gap between knowing "what" and knowing "how" .

As members of communities of practice, individuals report increased communication with people (professionals, interested parties, hobbyists), less dependence on geographic proximity, and the generation of new knowledge .

Communicating with others in a community of practice involves creating "social presence". defines social presence as "the degree of salience of another person in an interaction and the consequent salience of an interpersonal relationship" (p. 38). It is believed that social presence affects how likely an individual is of participating in a CoP (especially in online environments) . Management of a community of practice often faces many barriers that inhibit individuals from engaging in knowledge exchange. Some of the reasons for these barriers are egos and personal attacks, large overwhelming CoPs, and time constraints .

Motivation to share knowledge is critical to success in communities of practice. Studies show that members are motivated to become active participants in a CoP when they view knowledge as meant for the public good, a moral obligation and/or as a community interest . Members of a community of practice can also be motivated to participate by using methods such as tangible returns (promotion, raises or bonuses), intangible returns (reputation, self-esteem) and community interest (exchange of practice related knowledge, interaction).

Collaboration is essential to ensuring that communities of practice thrive. Research has found that certain factors can indicate a higher level of collaboration in knowledge exchange in a business network . Sveiby and Simons found that more seasoned colleagues tend to foster a more collaborative culture. Additionally they noted that a higher educational level also predicts a tendency to favor collaboration.

What makes a community of practice succeed depends on the purpose and objective of the community as well as the interests and resources of the members of that community. Wenger identified seven actions that could be taken in order to cultivate communities of practice:





</doc>
<doc id="397245" url="https://en.wikipedia.org/wiki?curid=397245" title="Numeracy">
Numeracy

Numeracy is the ability to reason and to apply simple numerical concepts. Basic numeracy skills consist of comprehending fundamental arithmetics like addition, subtraction, multiplication, and division. For example, if one can understand simple mathematical equations such as, 2 + 2 = 4, then one would be considered possessing at least basic numeric knowledge. Substantial aspects of numeracy also include number sense, operation sense, computation, measurement, geometry, probability and statistics. A numerically literate person can manage and respond to the mathematical demands of life.

By contrast, innumeracy (the lack of numeracy) can have a negative impact. Numeracy has an influence on career decisions, and risk perception towards health decisions. For example, innumeracy distorts risk perception towards health decisions and may negatively affect economic choices. "Greater numeracy has been associated with reduced susceptibility to framing effects, less influence of nonnumerical information such as mood states, and greater sensitivity to different levels of numerical risk".

Humans have evolved to mentally represent numbers in two major ways from observation (not formal math). These representations are often thought to be innate (see Numerical cognition), to be shared across human cultures, to be common to multiple species, and not to be the result of individual learning or cultural transmission. They are:

Approximate representations of numerical magnitude imply that one can relatively estimate and comprehend an amount if the number is large (see Approximate number system). For example, one experiment showed children and adults arrays of many dots. After briefly observing them, both groups could accurately estimate the approximate number of dots. However, distinguishing differences between large numbers of dots proved to be more challenging.

Precise representations of distinct individuals demonstrate that people are more accurate in estimating amounts and distinguishing differences when the numbers are relatively small (see Subitizing). For example, in one experiment, an experimenter presented an infant with two piles of crackers, one with two crackers the other with three. The experimenter then covered each pile with a cup. When allowed to choose a cup, the infant always chose the cup with more crackers because the infant could distinguish the difference.

Both systems—approximate representation of magnitude and precise representation quantity of individual items—have limited power. For example, neither allows representations of fractions or negative numbers. More complex representations require education. However, achievement in school mathematics correlates with an individual's unlearned approximate number sense.

Fundamental (or rudimentary) numeracy skills include understanding of the real number line, time, measurement, and estimation. Fundamental skills include basic skills (the ability to identify and understand numbers) and computational skills (the ability to perform simple arithmetical operations and compare numerical magnitudes).

More sophisticated numeracy skills include understanding of ratio concepts (notably fractions, proportions, percentages, and probabilities), and knowing when and how to perform multistep operations. Two categories of skills are included at the higher levels: the analytical skills (the ability to understand numerical information, such as required to interpret graphs and charts) and the statistical skills (the ability to apply higher probabilistic and statistical computation, such as conditional probabilities).

A variety of tests have been developed for assessing numeracy and health numeracy.

The first couple of years of childhood are considered to be a vital part of life for the development of numeracy and literacy. There are many components that play key roles in the development of numeracy at a young age, such as Socioeconomic Status (SES), parenting, Home Learning Environment (HLE), and age.

Children who are brought up in families with high SES tend to be more engaged in developmentally enhancing activities. These children are more likely to develop the necessary abilities to learn and to become more motivated to learn. More specifically, a mother's education level is considered to have an effect on the child's ability to achieve in numeracy. That is, mothers with a high level of education will tend to have children who succeed more in numeracy.

A number of studies have, moreover, proved that the education level of mother is strongly correlated with the average age of getting married. To be more precise, females who entered the marriage later, tend to have greater autonomy, chances for skills premium and level of education (i.e. numeracy). Hence, they were more likely to share this experience with children.

Parents are suggested to collaborate with their child in simple learning exercises, such as reading a book, painting, drawing, and playing with numbers. On a more expressive note, the act of using complex language, being more responsive towards the child, and establishing warm interactions are recommended to parents with the confirmation of positive numeracy outcomes. When discussing beneficial parenting behaviors, a feedback loop is formed because pleased parents are more willing to interact with their child, which in essence promotes better development in the child.

Along with parenting and SES, a strong home-learning environment increases the likelihood of the child being prepared for comprehending complex mathematical schooling. For example, if a child is influenced by many learning activities in the household, such as puzzles, coloring books, mazes, or books with picture riddles, then they will be more prepared to face school activities.

Age is accounted for when discussing the development of numeracy in children. Children under the age of 5 have the best opportunity to absorb basic numeracy skills. After the age of 7, achievement of basic numeracy skills become less influential. For example, a study was conducted to compare the reading and mathematic abilities between children, ages 5 and 7, each in three different mental capacity groups (underachieving, average, and overachieving). The differences in the amount of knowledge retained were greater between the three different groups at age 5, than between the groups at age 7. This reveals that the younger you are the greater chance you have to retain more information, like numeracy.

There seems to be a relationship between literacy and numeracy,
There is some evidence that humans may have an inborn sense of number. In one study for example, five-month-old infants were shown two dolls, which were then hidden with a screen. The babies saw the experimenter pull one doll from behind the screen. Without the child's knowledge, a second experimenter could remove, or add dolls, unseen behind the screen. When the screen was removed, the infants showed more surprise at an unexpected number (for example, if there were still two dolls). Some researchers have concluded that the babies were able to count, although others doubt this and claim the infants noticed surface area rather than number.

Numeracy has a huge impact on employment. In a work environment, numeracy can be a controlling factor affecting career achievements and failures. Many professions require individuals to have a well-developed sense of numeracy, for example: mathematician, physicist, accountant, actuary, Risk Analyst, financial analyst, engineer, and architect. 
Even outside these specialized areas, the lack of proper numeracy skills can reduce employment opportunities and promotions, resulting in unskilled manual careers, low-paying jobs, and even unemployment. For example, carpenters and interior designers need to be able to measure, use fractions, and handle budgets.
Another example pertaining to numeracy influencing employment was demonstrated at the Poynter Institute. The Poynter Institute has recently included numeracy as one of the skills required by competent journalists. Max Frankel, former executive editor of the "New York Times", argues that "deploying numbers skillfully is as important to communication as deploying verbs". Unfortunately, it is evident that journalists often show poor numeracy skills. In a study by the Society of Professional Journalists, 58% of job applicants interviewed by broadcast news directors lacked an adequate understanding of statistical materials.

With regards to assessing applicants for an employment position, psychometric numerical reasoning tests have been created by occupational psychologists, who are involved in the study of numeracy. These psychometric numerical reasoning tests are used to assess an applicants' ability to comprehend and apply numbers. These tests are sometimes administered with a time limit, resulting in the need for the test-taker to think quickly and concisely. Research has shown that these tests are very useful in evaluating potential applicants because they do not allow the applicants to prepare for the test, unlike interview questions. This suggests that an applicant's results are reliable and accurate.

These psychometric numerical reasoning tests first became prevalent during the 1980s, following the pioneering work of psychologists, such as P. Kline. In 1986 P. Kline's published a book entitled, "A handbook of test construction: Introduction to psychometric design", which explained that psychometric testing could provide reliable and objective results. These findings could then be used to effectively assess a candidate's abilities in numeracy. In the future, psychometric numerical reasoning tests will continue to be used in employment assessments to fairly and accurately differentiate and evaluate possible employment applicants.

Innumeracy is a neologism coined by an analogue with illiteracy. Innumeracy refers to a lack of ability to reason with numbers. The term "innumeracy" was coined by cognitive scientist Douglas Hofstadter. However, this term was popularized in 1989 by mathematician John Allen Paulos in his book entitled, "Innumeracy: Mathematical Illiteracy and its Consequences".

Developmental dyscalculia refers to a persistent and specific impairment of basic numerical-arithmetical skills learning in the context of normal intelligence.

The root cause of innumeracy varies. Innumeracy has been seen in those suffering from poor education and childhood deprivation of numeracy.
Innumeracy is apparent in children during the transition of numerical skills obtained before schooling and the new skills taught in the education departments because of their memory capacity to comprehend the material. Patterns of innumeracy have also been observed depending on age, gender, and race.
Older adults have been associated with lower numeracy skills than younger adults. Men have been identified to have higher numeracy skills than women. Some studies seem to indicate young people of African heritage tend to have lower numeracy skills. The Trends in International Mathematics and Science Study (TIMSS) in which children at fourth-grade (average 10 to 11 years) and eighth-grade (average 14 to 15 years) from 49 countries were tested on mathematical comprehension. The assessment included tests for number, algebra (also called patterns and relationships at fourth grade), measurement, geometry, and data. The latest study, in 2003 found that children from Singapore at both grade levels had the highest performance. Countries like Hong Kong SAR, Japan, and Taiwan also shared high levels of numeracy. The lowest scores were found in countries like South Africa, Ghana, and Saudi Arabia. Another finding showed a noticeable difference between boys and girls with some exceptions. For example, girls performed significantly better in Singapore, and boys performed significantly better in the United States.

There is a theory that innumeracy is more common than illiteracy when dividing cognitive abilities into two separate categories. David C. Geary, a notable cognitive developmental and evolutionary psychologist from the University of Missouri, created the terms "biological primary abilities" and "biological secondary abilities". Biological primary abilities evolve over time and are necessary for survival. Such abilities include speaking a common language or knowledge of simple mathematics. Biological secondary abilities are attained through personal experiences and cultural customs, such as reading or high level mathematics learned through schooling. Literacy and numeracy are similar in the sense that they are both important skills used in life. However, they differ in the sorts of mental demands each makes. Literacy consists of acquiring vocabulary and grammatical sophistication, which seem to be more closely related to memorization, whereas numeracy involves manipulating concepts, such as in calculus or geometry, and builds from basic numeracy skills. This could be a potential explanation of the challenge of being numerate.

Health numeracy has been defined as "the degree to which individuals have the capacity to access, process, interpret, communicate, and act on numerical, quantitative, graphical, biostatistical, and probabilistic health information needed to make effective health decisions". The concept of health numeracy is a component of the concept of health literacy. Health numeracy and health literacy can be thought of as the combination of skills needed for understanding risk and making good choices in health-related behavior.

Health numeracy requires basic numeracy but also more advanced analytical and statistical skills. For instance, health numeracy also requires the ability to understand probabilities or relative frequencies in various numerical and graphical formats, and to engage in Bayesian inference, while avoiding errors sometimes associated with Bayesian reasoning (see Base rate fallacy, Conservatism (Bayesian)). Health numeracy also requires understanding terms with definitions that are specific to the medical context. For instance, although 'survival' and 'mortality' are complementary in common usage, these terms are not complementary in medicine (see five-year survival rate). Innumeracy is also a very common problem when dealing with risk perception in health-related behavior; it is associated with patients, physicians, journalists and policymakers. Those who lack or have limited health numeracy skills run the risk of making poor health-related decisions because of an inaccurate perception of information. For example, if a patient has been diagnosed with breast cancer, being innumerate may hinder her ability to comprehend her physician's recommendations or even the severity of the health concern. One study found that people tended to overestimate their chances of survival or even to choose lower quality hospitals. Innumeracy also makes it difficult or impossible for some patients to read medical graphs correctly. Some authors have distinguished graph literacy from numeracy. Indeed, many doctors exhibit innumeracy when attempting to explain a graph or statistics to a patient. Once again, a misunderstanding between a doctor and patient due to either the doctor, patient, or both being unable to comprehend numbers effectively could result in serious health consequences.

Different presentation formats of numerical information, for instance natural frequency icon arrays, have been evaluated to assist both low numeracy and high numeracy individuals.

In the field of economic history, numeracy is often used to assess human capital at times when there was no data on schooling or other educational measures. Using a method called age-heaping, researchers like professor Baten study the development and inequalities of numeracy over time and throughout regions. For example, Baten and Hippe find a numeracy gap between regions in West / Central Europe and the rest of Europe for the period 1790 - 1880. At the same time, their data analysis reveals that these differences as well as within country inequality decreased over time. Taking a similar approach, Baten and Fourie find overall high levels of numeracy for people in the Cape Colony (late 17th to early 19th century).

In contrast to these studies comparing numeracy over countries or regions, it is also possible to analyze numeracy within countries. For example, Baten, Crayen and Voth look at the effects of war on numeracy in England and Baten and Priwitzer find a "military bias" in today's West Hungary: people opting for a military career had - on average - better numeracy indicators (1 BCE to 3CE).




</doc>
<doc id="21312310" url="https://en.wikipedia.org/wiki?curid=21312310" title="Remember versus know judgements">
Remember versus know judgements

There is evidence suggesting that different processes are involved in remembering something versus knowing whether it is familiar. It appears that "remembering" and "knowing" represent relatively different characteristics of memory as well as reflect different ways of using memory.

To remember is the conscious "recollection" of many vivid contextual details, such as "when" and "how" the information was learned. Remembering utilizes episodic memory and requires a deeper level of processing (e.g. undivided attention) than knowing. Errors in recollection may be due to source-monitoring errors that prevent an individual from remembering where exactly a piece of information was received. On the other hand, source monitoring may be very effective in aiding the retrieval of episodic memories. Remembering is a knowledge-based and conceptually-driven form of processing that can be influenced by many things. It is relevant to note that under this view both kinds of judgments are characteristics of individuals and thus any distinctions between the two are correlational, not causal, events.

To know is a feeling (unconscious) of "familiarity". It is the sensation that the item has been seen before, but not being able to pin down the reason why. Knowing simply reflects the familiarity of an item without recollection. Knowing utilizes semantic memory that requires perceptually based, data-driven processing. Knowing is the result of shallow maintenance rehearsal that can be influenced by many of the same aspects as semantic memory.

Remember and know responses are quite often differentiated by their functional correlates in specific areas in the brain. For instance, during "remember" situations it is found that there is greater EEG activity than "knowing", specifically, due to an interaction between frontal and posterior regions of the brain. It is also found that the hippocampus is differently activated during recall of "remembered" (vs. familiar) stimuli. On the other hand, items that are only "known", or seem familiar, are associated with activity in the rhinal cortex.

The remember-know paradigm began its journey in 1985 from the mind of Endel Tulving. He suggested that there are only two ways in which an individual can access their past. For instance, we can recall what we did last night by simply traveling back in time through memory and episodically imagining what we did (remember) or we can know something about our past such as a phone number, but have no specific memory of where the specific memory came from (know). Recollection is based on the episodic memory system, and familiarity is based on the semantic memory system. Tulving argued that the remember-know paradigm could be applied to all aspects of recollection.

In 1988 the application of the paradigm was refined to a set of instructions that could elicit reliable judgments from subjects that can be found using many variables. The remember-know paradigm has changed the way in which researchers can study memory tasks and has had implications on what were originally considered purely "episodic" memories, which can now be thought of as a combination of both remembering and knowing or episodic and semantic.

Remembering and knowing have been linked to dual-component theories, as well as unitary trace-strength/signal detection models.

Episodic and semantic memory give rise to two different states of consciousness, autonoetic and noetic, which influence two kinds of subjective experience, remembering and knowing, respectively. Autonoetic consciousness refers to the ability of recovering the episode in which an item originally occurred. In noetic consciousness, an item is familiar but the episode in which it was first encountered is absent and cannot be recollected. Remembering involves retrieval from episodic memory and knowing involves retrieval from semantic memory.

In his SPI model, Tulving stated that encoding into episodic and semantic memory is serial, storage is parallel, and retrieval is independent. By this model, events are first encoded in semantic memory before being encoded in episodic memory; thus, both systems may have an influence on the recognition of the event.

The original high-threshold model held that recognition is a probabilistic process. It is assumed that there is some probability that previously studied items will exceed a memory threshold. If an item exceeds the threshold then it is in a discrete memory state. If an item does not exceed the threshold then it is not remembered, but it may still be endorsed as old on the basis of a random guess. According to this model, a test item is either recognized (i.e., it falls above a threshold) or it is not (i.e., it falls below a threshold), with no degrees of recognition occurring between these extremes. Only target items can generate an above-threshold recognition response because only they appeared on the list. The lures, along with any targets that are forgotten, fall below threshold, which means that they generate no memory signal whatsoever. For these items, the participant has the option of declaring them to be new (as a conservative participant might do) or guessing that some of them are old (as a more liberal participant might do). False alarms in this model reflect memory-free guesses that are made to some of the lures. 
This simple and intuitively appealing model yields the once widely used correction for guessing formula, and it predicts a linear receiver operating characteristic (ROC). An ROC is simply a plot of the hit rate versus the false alarm rate for different levels of bias. A typical ROC is obtained by asking participants to supply confidence ratings for their recognition memory decisions. Several pairs of hit and false alarm rates can then be computed by accumulating ratings from different points on the confidence scale (beginning with the most confident responses). The high-threshold model of recognition memory predicts that a plot of the hit rate versus the false alarm rate (i.e., the ROC) will be linear it also predicts that the z-ROC will be curvilinear.

The dual-process account states that recognition decisions are based on the processes of recollection and familiarity. Recollection is a conscious, effortful process in which specific details of the context in which an item was encountered are retrieved. Familiarity is a relatively fast, automatic process in which one gets the feeling the item has been encountered before, but the context in which it was encountered is not retrieved. According to this view, remember responses reflect recollections of past experiences and know responses are associated with recognition on the basis of familiarity.

According to this theory, recognition decisions are based on the strength of a memory trace in reference to a certain decision threshold. A memory that exceeds this threshold is perceived as old, and trace that does not exceed the threshold is perceived as new. According to this theory, remember and know responses are products of different degrees of memory strength. There are two criteria on a decision axis; a point low on the axis is associated with a know decision, and a point high on the axis is associated with a remember decision. If memory strength is high, individuals make a "remember" response, and if memory strength is low, individuals make a "know" response.

Probably the strongest support for the use of signal detection theory in recognition memory came from the analysis of ROCs. An ROC is the function that relates the proportion of correct recognitions (hit rate) to the proportion of incorrect recognitions (false-alarm rate).

Signal-detection theory assumed a preeminent position in the field of recognition memory in large part because its predictions about the shape of the ROC were almost always shown to be more accurate than the predictions of the intuitively plausible high-threshold model. More specifically, the signal-detection model, which assumes that memory strength is a graded phenomenon (not a discrete, probabilistic phenomenon) predicts that the ROC will be curvilinear, and because every recognition memory ROC analyzed between 1958 and 1997 was curvilinear, the high-threshold model was abandoned in favor of signal-detection theory. Although signal-detection theory predicts a curvilinear ROC when the hit rate is plotted against the false alarm rate, it predicts a linear ROC when the hit and false alarm rates are converted to z scores (yielding a z-ROC).

“The predictive power of the signal detection modem seems to rely on know responses being related to transient feelings of familiarity without conscious recollection, rather than Tulving’s (1985) original definition of know awareness.

The dual-process signal-detection/high-threshold theory tries to reconcile dual-process theory and signal-detection theory into one main theory. This theory states that recollection is governed by a threshold process, while familiarity is not. Recollection is a high-threshold process (i.e., recollection either occurs or does not occur), whereas familiarity is a continuous variable that is governed by an equal-variance detection model. On a recognition test, item recognition is based on recollection if the target item has exceeded threshold, producing an "old" response. If the target item does not reach threshold, the individual must make an item recognition decision based on familiarity. According to this theory, an individual makes a "remember" response when recollection has occurred. A know response is made when recollection has not occurred, and the individual must decide whether they recognize the target item solely on familiarity.
Thus, in this model, the participant is thought to resort to familiarity as a backup process whenever recollection fails to occur.

In the past, it was suggested that remembering is associated with conceptual processing and knowing is associated with perceptual processing. However, recent studies have reported that there are some conceptual factors that influence knowing and some perceptual factors that influence remembering. Findings suggest that regardless of perceptual or conceptual factors, distinctiveness of processing at encoding is what affects remembering, and fluency of processing is what affects knowing. Remembering is associated with distinctiveness because it is seen as an effortful, consciously controlled process. Knowing, on the other hand, depends on fluency as it is more automatic and reflexic and requires much less effort.

Items of low-frequency are generally better recognized and receive more remember responses than high-frequency items. In a study, 96 words consisting of 48 low-frequency words and 48 high-frequency words were chosen by a psycholinguistic database. There were two alternate study lists, each consisting of 24 low-frequency words and 24 high-frequency words. Half of the subjects received one study list, while the other half of the participants received the other. The recognition test, which involved all 96 words, required participants to first acknowledge whether the target item was old or new; if the item was considered old, participants were further asked to distinguish whether the item was remembered (they could recollect the context in which it had been studied) or known (the item seemed familiar but they couldn't recollect contextual details). The results of this experiment were that low-frequency words received many more remember responses than high-frequency words. Since remember words are affected by distinctiveness, this makes sense; low-frequency words are experienced less than high-frequency words which makes them more distinctive. Also, there seemed to be no significant difference in the number of know responses made for low-frequency words and high-frequency words.

Items which are generated by a person receive more remember responses than items which are read, seen, or heard by a person. In addition, the generation of images to words enhances remember responses. In one study, all participants were asked to study a list of 24 common pairs of opposites, 12 had to be generated and 12 were read. The generated pairs required participants to generate them in the context of a given rule. The recognition test consisted of 48 words, 24 targets and 24 distractors. Participants were asked if items were old or new; if participants replied "old", they were then asked whether they remembered (could recollect contextual details of when it was studied) the pair or if they knew (recognized it but recollection was absent) the pair. Generation effects were seen in remember responses; items which were generated received more remember responses than read items. However, generation effects were not seen in know responses.

Remember responses depend on the amount of attention available during learning. Divided attention at learning has a negative impact on remember responses. A study was done which consisted of 72 target words which were divided into two study lists. Half of the participants were required to study the list in an undivided attention condition and half of the subjects studied the list in a divided attention condition. In the divided attention condition, subjects had to study the list while listening to and reporting high, low, or medium tone sequences. The recognition test consisted of participants deciding whether items were old or new; if items were deemed old, participants were then required to say whether items were remembered or known. It was found that the divided attention condition impaired the level of correct remember responses; however, the know responses seemed unaffected.

When more detailed, elaborate encoding and associations are made, more remember responses are reported than know responses. The opposite occurs with shallow, surface encoding which results in fewer remember responses.

The primacy effect is related to enhanced remembering. In a study, a free recall test was conducted on some lists of words and no test on other lists of words prior to a recognition test. They found that testing led to positive recency effects for remembered items; on the other hand, with no prior test, negative recency effects occurred for remembered items. Thus, both primary and recency effects can be seen in remember responses.

Masked recognition priming is a manipulation which is known to increase perceptual fluency. Since know responses increase with increased fluency of processing, masked recognition priming enhances know responses.

Briefly presenting a prime prior to test causes an increase in fluency of processing and an associated increase in feeling of familiarity. Short duration primes tend to enhance know responses. In contrast to briefly presented primes, primes which are presented for long durations are said to disrupt processing fluency as the prime saturates the representation of the target word. Thus, longer duration primes tend to have a negative impact on know responses.

Know responses are enhanced when stimuli modality match during study and test; therefore, shifting the modality of a stimulus has been found to negatively impact know responses.

Knowing is influenced by the fluency in which an item is processed, regardless of whether information is conceptual or perceptual. Know responses are enhanced by manipulations which increase perceptual and conceptual fluency. For example, masked repetition priming, modality match during study and test, and the use of easy word-fragments in word-fragment recall are all perceptual manipulations which increase know responses. An example of a conceptual manipulation which enhances know responses is when a prime item is semantically related to a target item. Manipulations which increase processing fluency do not seem to affect remember responses.

Normal aging tends to disrupt remember responses to a greater extent than know responses. This decrease in remember responses is associated with poor encoding and frontal lobe dysfunction. It has been found that older individuals fail to use elaborative encoding in comparison to younger individuals. In addition to poor encoding, older individuals tend to have problems with retrieving information that is highly specific because they are less effective at controlling their retrieval processes. It is difficult for older individuals to constrain retrieval processes to the context of the specific item that is to be retrieved.

When words are used as stimuli, more remember responses and fewer know responses are produced in comparison to when nonwords are used as stimuli.

Gradual presentation of stimuli causes an increase in familiarity and thus an increase in associated know responses; however, gradual presentation causes a decrease in remember responses.

The amygdala plays an important role during encoding and retrieval of emotional information. It has been found that although negative and positive items are remembered or known to the same extent, the processes involved in remembering and knowing differs with emotional valence.

Activity in the orbitofrontal and ventrolateral prefrontal cortex are associated with remembering for both positive and negative items. When it comes to remembering, it has been suggested that negative items may be remembered with more detail in comparison to positive or neutral items; support for this has been found in the temporo-occipital regions, which showed activity specific to negative items that were "remembered". A study found that in addition to being remembered in more detail, negative items also tended to be remembered for longer durations than positive items.

Activity in the mid-cingulate gyrus, the inferior parietal lobe, and the superior frontal lobe are all associated with knowing for both positive and negative items. These regions are said to be involved in the retrieval of both semantic and episodic information. It has been suggested that the encoding of items which people forget details for or items which are forgotten as a whole are associated with these regions. This forgetting has to do with retrieval-related processes being active at the same time as encoding-related processes. Thus, the process of retrieval may come at the expense of encoding vivid details of the item.

In addition, disproportionate activity along the cingulate gyrus, within the parietal lobe, and in the prefrontal cortex is associated with the encoding of "known" positive items. This increased activity may cause the trade-off between retrieval-related processes and encoding-related processes to occur more significantly for positive items. This supports the idea that when people are in a positive mood, they have a more holistic, general thought process and disregard details.

The functional account of remembering states that remember responses are determined by the context in which they're made; in general, recollection is based on the type of info that was encouraged by the deeper level processing task. Remember responses occur when retrieved information allow subjects to finish a memory test. The same item may elicit a remember response or a know response, depending on the context in which it is found.

In the expectancy heuristic, items that reach beyond a level of distinctiveness (the likelihood an item would later be recognized in a recognition test) elicit a remember response. Items that do not reach this level of distinctiveness elicit a know response. The level of distinctiveness is determined by the context in which items are studied and/or tested. In a given context, there is an expected level of distinctiveness; in contexts where subjects expect many distinct items, participants give fewer remember responses than when they expect few distinct items. Therefore, remember responses are affected by the expected strength of distinctiveness of items in a given context.

In addition, context can affect remember and know responses while not affecting recollection and familiarity processes. Remember and know responses are subjective decisions that can be affected by underlying memory processes. While changing recollection and familiarity processes can influence remember and know judgments, context can affect how items are weighted for remember-know decisions.

According to the distinctiveness-fluency model, items are seen as distinct when they exceed a level of memorability and items are seen as fluent when they do not reach this level but give a feeling of familiarity. Distinct items are usually unusual in comparison to the context in which they're found. Therefore, distinct items generally elicit a remember response, and fluent items elicit a know response.

In this study, the presence of source memory was utilized to estimate the extent to which episodic details were recalled; feelings of familiarity were accompanied by retrieval of partial contextual details, which are considered sufficient for an accurate source decision but not for a recollection response. Subjects that remembered the stimuli were able to differentiate the corresponding source correctly. The findings were consistent with the idea that "remember" responses, unlike "know" responses, are accompanied by memory for episodic detail, and that the loss of memory for episodic detail over time parallels the conversion of "remember" responses to "know" responses.

In the preceding task, participants are given a list of items to study in the primary learning phase. Subsequently, during the recognition stage, participants are asked to make a decision about whether presented test items existed in the previously studied list. If the participant responds "yes", they are asked why they recognized the specified item. From this, a conclusion was obtained based on whether the item was remembered or simply known.

Primarily, experimenters recorded eye movements while participants studied a series of photos. Individuals were then involved in a recognition task in which their eye movements were recorded for the second time. From the previous tasks, it was discovered that eye fixations, maintaining a visual gaze on a single location, were more clustered for remembering rather than knowing tasks. This suggests that remembering is associated with encoding a specific salient component of an item whereas recognition is activated by an augmented memory for this part of the stimulus.

In the above experiment, participants were presented with a list of 100 familiar words and were asked to read them aloud while simultaneously trying to remember each one. Subsequent to this, participants were asked to make a recognition decision based on the number of "yes" responses that were accompanied by some recollective experience. The results demonstrate the differing relationships between the "yes" and "no" conditions and "remember" and "know" memory performance. The outcome confirms that although familiarity and recollection may involve different processes, the remember/know exemplar does not probe them directly.

In the previous study, two different remember-know paradigms are explored. The first is the "remember-first method" in which a remember response is solicited prior to a know response for non-remembered items. Secondly, a trinary paradigm, in which a single response judges the "remember vs. know" and "new" alternatives is investigated. Participants are asked to subjectively decide whether their response within these studies is attributed to a recollection of specific details, "remembering", or familiarity "knowing". In the presently discussed experiment, "remember" and "know" responses generally depend on a single strength variable.

Remembering (recollection) accesses memory for separate contextual details (e.g. screen location and font size); i.e. involves retrieval of a particular context configuration.

This model uses the idea that remembering and knowing each represent different levels of confidence. In this sense remember/know judgments are viewed as quantitatively different judgments that vary along the same continuum. Subjects place their "know" and "remember" judgments on a continuum of strength. When people are very confident about recognizing an item, they assign it a "remember" response, and when they are less confident about a response it is labelled as a "know" response. A potential problem with this model is that it lacks explanatory power; it may be difficult to determine where the criteria should be placed on the continuum.

The remember-know paradigm has had great usage in clinical studies. Using this paradigm, researchers can look into the mechanisms of neuro-biological functions as well as social aspects of disorders and illnesses that plague humans. Recognition memory has been linked to advancements in the understanding of schizophrenia, epilepsy and even explaining simple autobiographical memory loss as we grow older.

The remember-know paradigm has been used to settle the debate over whether the hippocampus plays a critical role in familiarity of objects. Studies done with patients suffering from epilepsy suggest that the hippocampus plays a critical role in the familiarity of objects. A study was conducted using the remember-know distinction to understand this idea of familiarity and whether it is in fact the hippocampus that plays this critical role. This study found that the hippocampus is essentially a system based on familiarity. The hippocampus actually suppresses any sort of arousal response that would normally occur if the stimuli were novel. It is almost as though familiarity is a qualitative characteristic just as is colour or loudness.

The remember-know paradigm with epilepsy patients to distinguish whether a stimulus (picture of a face) was familiar. Patients that were found to have right temporal lobe epilepsy showed relatively lower face recognition response than those with left temporal lobe epilepsy due to damage of secondary sensory regions (including fusiform gyrus) in the brain's right hemisphere, which is responsible for perception and encoding (esp. face memory).

A remember-know paradigm was used to test whether patients with schizophrenia would exhibit abnormalities in conscious recollection due to a deterioration of frontal memory processes that are involved in encoding/retrieval of memories as well as executive functions linked to reality monitoring and decision making. Using the "remember-know" paradigm, participants first identify stimuli that they previously studied. If an item is identified as a known stimulus, the participants are asked to distinguish whether they can remember aspects of the original presentation of the identified item (remember response) or whether they know that the item was on the study list, but have no episodic memory of specifically learning it.

Results showed that patients with schizophrenia exhibit a deficit in conscious recollection for auditory stimuli. These findings, when considered together with remember/know data collected from the same set of patients for olfactory and visual recognition memory, support proposals that the abnormalities in conscious recollection stem from a breakdown in central processes rather than domain-specific processes. This study depended greatly on the remember-know paradigm to test for conscious recollection differences in these patients.

The remember-know paradigm has been used in studies that focus on the idea of a reminiscence bump and the age effects on autobiographical memory. Previous studies suggested old people had more "know" than "remember" and it was also found that younger individuals often excelled in the "remember" category but lacked in the "know".

A specific study used the remember-know paradigm while asking elderly people about their episodic memories from their university graduation. They were asked to determine whether their self reported memories were "remembered" or "known". It was hypothesized that reminiscence component of elderly adults' autobiographical recall would be strong for "remember" responses, but less so for "know" responses. It was also expected that recent memories would hold the opposite effect, that those individuals would be better at "know" responses than with "remember" responses.

Results showed that there was good retention after reminiscence bump and equal "remember" to "know" responses were reported. It was concluded that autobiographical memories were tied to both episodic and semantic memories. These results are important to demonstrate that aging is not accompanied by a decline in episodic memory due to a reliance on semantic memory as previously thought. The remember-know distinction was integral in achieving these results as well as understanding the ways in which autobiographical memory works and the prevalence of the reminiscence bump. The findings of Rybash are supported by other research.

The tip-of-the-tongue state is the phenomenon that occurs when people fail to recall information but still feel as if they are close to retrieving it from memory. In this sense an individual feels as if they "know" but cannot "remember" the actual information desired. It is a frustrating but common problem that typically occurs for individuals about once a week, is frequent among nouns and is typically resolved on its own. The occurrence of the tip-of-the-tongue state increases with age throughout adulthood. Such a feeling is indication that remembering will occur or is about to occur.

The knew-it-all-along effect is a variant of the hindsight bias. It is the tendency for people to misremember and think that they knew more in the past than they actually did. In such situations it is difficult for us to remember what it was like when we did not have an understanding of something. For example, one might have a hard time teaching a concept to another individual because they can not remember what it is like to not understand the material.

Hindsight bias is the phenomenon where people tend to view events as more predictable than they really are. This occurs because one's current knowledge influences the recollection of previous beliefs. In this phenomenon, what someone "knows" is affecting what they "remember". This inaccurate assessment of reality after it has occurred is also referred to as "creeping determinism". The hindsight bias has been found among a number of domains such as historical events, political elections and the outcome of sporting events. The hindsight bias is a common phenomenon that occurs regularly among individuals in everyday life and can be generated in a laboratory setting to help increase the understanding of memory and specifically memory distortions.


</doc>
<doc id="16757040" url="https://en.wikipedia.org/wiki?curid=16757040" title="Democratization of knowledge">
Democratization of knowledge

The democratization of knowledge is the acquisition and spread of knowledge amongst the common people, not just privileged elites such as clergy and academics. Libraries, in particular public libraries, and modern digital technology such as the internet play a key role, as they provide the masses with open access to information.

The printing press was one of the early steps towards the democratization of knowledge. Another small example of this during the Industrial Revolution was the creation of libraries for miners in some Scottish villages in the 18th century.

Wikipedia is rapidly turning into a real-time reference tool in which public entries can be updated by anyone at any time. This phenomenon—a product of the digital age—has greatly contributed to the democratization of knowledge in the post-modern era. At the same time, it has raised a number of valid criticisms in this regard (see Reliability of Wikipedia page). For instance, one could draw a distinction between the mere spread of information and the spread of "accurate" or "credible" information. Wikipedia may thus be a more reliable source of information in certain spheres, but not necessarily in others.

The democratization of technology has played a major facilitating role. Wikipedia co-founder, Larry Sanger, states in his article, that “Professionals are no longer needed for the bare purpose of the mass distribution of information and the shaping of opinion.” Sanger’s article confronts the existence of “common knowledge” and pits it against knowledge that everyone agrees on.

In terms of democratization of knowledge, Wikipedia has played a major role. For instance, Wikipedia has attracted 400 million viewers across the globe and has communicated with them in over 270 languages.

Google Book Search has been pointed to as an example of democratization of knowledge, but Malte Herwig in Der Spiegel raised concerns that the virtual monopoly Google has in the search market, combined with Google's hiding of the details of its search algorithms, could undermine this move towards democratization.

An article written in 2005 by the editors of "Reference & User Services Quarterly" calls the library the greatest force for the democratization of knowledge or information. It continues to say that public libraries in particular are inextricably linked with the history and evolution of the United States, but school library media centers, college and university libraries, and special libraries have all also been inﬂuential in their support for democracy. Libraries play an essential role in the democratization of knowledge and information by providing communities with the resources and tools to find information free of charge. Democratic access to knowledge has also been co-opted to mean providing information in a variety of formats, which essentially means electronic and digital formats for use by library patrons. Public libraries help further the democratization of information by guaranteeing freedom of access to information, by providing an unbiased variety of information sources and access to government services, as well as the promotion of democracy and an active citizenship. Dan Cohen, the founding executive director of the Digital Public Library of America, writes that the democratic access to knowledge is a profound idea that requires constant tending and revitalization. In 2004, a World Social Forum and International workshop was held entitled "Democratization of Information: Focus on Libraries". The focus of the forum was to bring awareness to the social, technological, and financial challenges facing libraries dealing with the democratization of information. Social challenges included globalization and the digital divide, technological challenges included information sources, and financial challenges constituted shrinking budgets and manpower. Longtime Free Library of Philadelphia director Elliot Shelkrot said that “Democracy depends on an informed population. And where can people get all the information they need? —At the Library.” 



</doc>
<doc id="750101" url="https://en.wikipedia.org/wiki?curid=750101" title="Domain knowledge">
Domain knowledge

Domain knowledge is knowledge of a specific, specialized discipline or field, in contrast to general knowledge, or domain-independent knowledge. The term is often used in reference to a more general discipline, as, for example, in describing a software engineer who has general knowledge of programming, as well as domain knowledge about the pharmaceutical industry. People who have domain knowledge, are often considered specialists or experts in the field.

In software engineering "domain knowledge" is knowledge about the environment in which the target system operates, for example, software agents. Domain knowledge usually must be learned from software users in the domain (as domain specialists/experts), rather than from software developers. It may include user workflows, data pipelines, business policies, configurations and constraints and is crucial in the development of a software application. Expert’s domain knowledge (frequently informal and ill-structured) is transformed in computer programs and active data, for example in a set of rules in knowledge bases, by knowledge engineers.

Communicating between end-users and software developers is often difficult. They must find a common language to communicate in. Developing enough shared vocabulary to communicate can often take a while.

The same knowledge can be included in different domain knowledge.
Knowledge which may be applicable across a number of domains is called "domain-independent" knowledge, for example logics and mathematics.
Operations on domain knowledge are performed by meta-knowledge.




</doc>
<doc id="8957879" url="https://en.wikipedia.org/wiki?curid=8957879" title="Body of knowledge">
Body of knowledge

A body of knowledge (BOK or BoK) is the complete set of concepts, terms and activities that make up a professional domain, as defined by the relevant learned society or professional association. It is a type of knowledge representation by any knowledge organization. Several definitions of BOK have been developed, for example:
A body of knowledge is the accepted ontology for a specific domain. A BOK is more than simply a collection of terms; a professional reading list; a library; a website or a collection of websites; a description of professional functions; or even a collection of information.

The following are examples of bodies of knowledge from professional organisations:




</doc>
<doc id="3067655" url="https://en.wikipedia.org/wiki?curid=3067655" title="Scientia potentia est">
Scientia potentia est

The phrase "scientia potentia est"" (or "scientia est potentia" or also "scientia potestas est")" is a Latin aphorism meaning "knowledge is power". It is commonly attributed to Sir Francis Bacon, although there is no known occurrence of this precise phrase in Bacon's English or Latin writings. However, the expression "ipsa scientia potestas est" ('knowledge itself is power') occurs in Bacon's "Meditationes Sacrae" (1597). The exact phrase "scientia potentia est" was written for the first time in the 1668 version of the work "Leviathan" by Thomas Hobbes, who was secretary to Bacon as a young man.

The related phrase "sapientia est potentia" is often translated as "wisdom is power".

The earliest documented occurrence of the phrase "Knowledge is power" is from Imam Ali (599-661 CE), as recorded in the tenth-century book Nahj Al-Balagha:
Knowledge is power and it can command obedience ("maʿrifatu al-ʿilmi dīnun yadānu bihi"). A man of knowledge during his lifetime can make people obey and follow him and he is praised and venerated after his death. Remember that knowledge is a ruler and wealth is its subject. (Saying 147.5)
Another account of this concept is found in the Shahnameh by the Persian poet Ferdowsi (940–1020 CE) who wrote: "Capable is he who is wise" (in Persian: توانا بود هر که دانا بود). This hemistich is translated to English as "knowledge is power" or "One who has wisdom is powerful".

A proverb in practically the same wording is found in Hebrew, in the Biblical Book of Proverbs (24:5): גֶּבֶר-חָכָם בַּעוֹז; וְאִישׁ-דַּעַת, מְאַמֶּץ-כֹּחַ. This was translated in the latin Vulgata as "vir sapiens et fortis est et vir doctus robustus et validus" and in the King James Version, the first English official edition, as "A wise man is strong, a man of knowledge increaseth strength".

The first known reference of the exact phrase appeared in the Latin edition of "Leviathan" (1668; the English version had been published in 1651). This passage from Part 1 ("De Homine"), Chapter X ("De Potentia, Dignitate et Honore") occurs in a list of various attributes of man which constitute power; in this list, "sciences" or "the sciences" are given a minor position:
In the English version this passage reads as thus:
On a later work, "De Corpore" (1655), also written in Latin, Hobbes expanded the same idea:

In Jean Hampton, "Hobbes and the social contract tradition" (1988), Hampton indicates that this quote is 'after Bacon' and in a footnote, that 'Hobbes was Bacon's secretary as a young man and had philosophical discussions with him (Aubrey 1898, 331).

The closest expression in Bacon's works is, perhaps, the expression "scientia potestas est", found in his "" (1597), which is perhaps better translated as "knowledge is His power", because the context of the sentence refers to the qualities of God and is imbedded in a discussion of heresies that deny the power of God:
"statuuntque latiores terminos scientiae Dei quam potestatis, vel potius ejus partis potestatis Dei (nam et ipsa scientia potestas est) qua scit, quam ejus qua movet et agit: ut praesciat quaedam otiose, quae non praedestinet et praeordinet".

The English translation of this section includes the following:

Interpretation of the notion of power meant by Bacon must therefore take into account his distinction between the power of knowing and the power of working and acting, the opposite of what is assumed when the maxim is taken out of context. Indeed, the quotation has become a cliche.

In another place, Bacon wrote, "Human knowledge and human power meet in one; for where the cause is not known the effect cannot be produced. Nature to be commanded must be obeyed; and that which in contemplation is as the cause is in operation as the rule."
Ralph Waldo Emerson wrote in his essay "Old Age", included in the collection "Society and Solitude" (1870):

After the 1871, unification of Germany, "Wissen ist Macht, geographisches Wissen ist Weltmacht" (Knowledge is power, geographical knowledge is world power) was often used in German geography and the public discussion to support efforts for a German colonial empire after 1880. Julius Perthes e.g., used the motto for his publishing house. However, the installation of geographical research followed popular requests and was not imposed by the government. Especially Count Bismarck was not much interested in German colonial adventures; his envoy Gustav Nachtigal started with the first protective areas, but was more interested in ethnological aspects.

After World War I, German geography tried to contribute to efforts to regain a world power. Scholars like Karl Haushofer, a former general, and his son Albrecht Haushofer (both in close contact with Rudolf Hess) got worldwide attention with their concept of geopolitics. Associations of German geographers and school teachers welcomed the Machtergreifung and hoped to get further influence in the new regime.

The postwar geography was much more cautious; concepts of political geography and projection of power had not been widespread scholarly topics till 1989 in Germany.

Geographical knowledge is however still of importance in Germany. Germans tend to mock US politicians' and celebrities' comparable lack of interest in the topic. A Sponti (Außerparlamentarische Opposition) version of the slogan is "Wissen ist Macht, nichts wissen, macht auch nichts", a pun about the previous motto along the line "Knowledge is power, but being ignorant doesn't bother anyway". Joschka Fischer and Daniel Cohn-Bendit belong to those Spontis that nevertheless held powerful positions, in Fischer's case with no more formal education than a taxi driver's licence.

The German Bundeswehr Bataillon Elektronische Kampfführung 932, an electronic warfare unit based in Frankenberg (Eder), still uses the Latin version "Scientia potentia est" as its motto.

Though its meaning varies from author to author, the phrase often implies that with knowledge or education, one's potential or abilities in life will certainly increase. Having and sharing knowledge is widely recognized as the basis for improving one's reputation and influence, thus power. This phrase may also be used as a justification for a reluctance to share information when a person believes that withholding knowledge can deliver to that person some forms of advantage. Another interpretation is that the only true power is knowledge, as everything (including any achievement) is derived from it.


Terry Brooks. "First King of Shannara" Ballantine. C. 1996




</doc>
<doc id="434103" url="https://en.wikipedia.org/wiki?curid=434103" title="Experiential knowledge">
Experiential knowledge

Experiential knowledge is knowledge gained through experience, as opposed to a priori (before experience) knowledge: it can also be contrasted both with propositional (textbook) knowledge, and with practical knowledge.

Experiential knowledge is cognate to Michael Polanyi's personal knowledge, as well as to Bertrand Russell's contrast of Knowledge by Acquaintance and by Description.

In the philosophy of mind, the phrase often refers to knowledge that can "only" be acquired through experience, such as, for example, the knowledge of what it is like to see colours, which could not be explained to someone born blind: the necessity of experiential knowledge becomes clear if one was asked to explain to a blind person a colour like blue.

The question of a posteriori knowledge might be formulated as: can Adam or Eve know what water feels like on their skin prior to touching it for the first time?

Zen emphasises the importance of the experiential element in religious experience, as opposed to what it sees as the trap of conceptualization: as D. T. Suzuki put it, "fire. Mere talking of it will not make the mouth burn".

Experiential knowledge has also been used in the philosophy of religion as an argument against God's omniscience, questioning whether God could genuinely know everything, since he (supposedly) cannot know what it is like to sin. Commenting on the distinction between experiential knowledge and propositional knowledge, analytic philosopher and theologian William Lane Craig has stated in an interview with Robert Lawrence Kuhn for the PBS series "Closer to Truth" that because experiential knowledge is appropriate to the mind which does the knowing, in order for omniscience to be a cognitive perfection God's omniscience must entail God know only and all propositional truths and have only appropriate experiential knowledge.

Writer Barry Lopez writes about experiential knowledge and how it relates back to the environment, arguing that without experiencing nature, one cannot fully "know" and understand the relationships within ecosystems.

Carl Rogers stressed the importance of experiential knowledge both for the therapist formulating his or her theories, and for the client in therapy – both things with which most counsellors would agree.

As defined by Thomasina Borkman (Emeritus Professor of Sociology, George Mason University) experiential knowledge is the cornerstone of therapy in self-help groups, as opposed to both lay (general) and professional knowledge. Sharing in such groups is the narration of significant life experiences in a process through which the knowledge derived thereof is validated by the group and transformed into a corpus that becomes their fundamental resource and product.

Neville Symington has argued that one of the central features of the narcissist is a shying away from experiential knowledge, in favour of adopting wholesale a ready-made way of living drawn from other people's experience.

Helen Vendler has characterised Seamus Heaney's art as, in one respect, recording an experiential learning curve: "we are earthworms of the earth, and all that / has gone through us is what will be our trace".


</doc>
<doc id="216180" url="https://en.wikipedia.org/wiki?curid=216180" title="Understanding">
Understanding

Understanding is a psychological process related to an abstract or physical object, such as a person, situation, or message whereby one is able to think about it and use concepts to deal adequately with that object.
Understanding is a relation between the knower and an object of understanding. Understanding implies abilities and dispositions with respect to an object of knowledge that are sufficient to support intelligent behaviour.

Understanding is often, though not always, related to learning concepts, and sometimes also the theory or theories associated with those concepts. However, a person may have a good ability to predict the behaviour of an object, animal or system—and therefore may, in some sense, understand it—without necessarily being familiar with the concepts or theories associated with that object, animal or system in their culture. They may have developed their own distinct concepts and theories, which may be equivalent, better or worse than the recognised standard concepts and theories of their culture. Thus, understanding is correlated with the ability to make inferences.


Someone who has a more sophisticated understanding, more predictively accurate understanding, and/or an understanding that allows them to make explanations that others commonly judge to be better, of something, is said to understand that thing "deeply". Conversely, someone who has a more limited understanding of a thing is said to have a "shallow" understanding. However, the depth of understanding required to usefully participate in an occupation or activity may vary greatly.

For example, consider multiplication of integers. Starting from the most shallow level of understanding, we have (at least) the following possibilities:


For the purpose of operating a cash register at McDonald's, a person does not need a very deep understanding of the multiplication involved in calculating the total price of two Big Macs. However, for the purpose of contributing to number theory research, a person would need to have a relatively deep understanding of multiplication — along with other relevant arithmetical concepts such as division and prime numbers.

It is possible for a person, or a piece of "intelligent" software, that in reality only has a shallow understanding of a topic, to appear to have a deeper understanding than they actually do, when the right questions are asked of it. The most obvious way this can happen is by memorization of correct answers to known questions, but there are other, more subtle ways that a person or computer can (intentionally or otherwise) deceive somebody about their level of understanding, too. This is particularly a risk with artificial intelligence, in which the ability of a piece of artificial intelligence software to very quickly try out millions of possibilities (attempted solutions, theories, etc.) could create a misleading impression of the real depth of its understanding. Supposed AI software could in fact come up with impressive answers to questions that were difficult for unaided humans to answer, without really understanding the concepts "at all", simply by dumbly applying rules very quickly. (However, see the Chinese room argument for a controversial philosophical extension of this argument.)

Examinations are designed to assess students' understanding (and sometimes also other things such as knowledge and writing abilities) without falling prey to these risks. They do this partly by asking multiple different questions about a topic to reduce the risk of measurement error, and partly by forbidding access to reference works and the outside world to reduce the risk of someone else's understanding being passed off as one's own. Because of the faster and more accurate computation and memorization abilities of computers, such tests would arguably often have to be modified if they were to be used to accurately assess the understanding of an artificial intelligence.

Conversely, it is even easier for a person or artificial intelligence to fake a "shallower" level of understanding than they actually have; they simply need to respond with the same kind of answers that someone with a more limited understanding, or no understanding, would respond with — such as "I don't know", or obviously wrong answers. This is relevant for judges in Turing tests; it is unlikely to be effective to simply ask the respondents to mentally calculate the answer to a very difficult arithmetical question, because the computer is likely to simply dumb itself down and pretend not to know the answer.

Gregory Chaitin, a noted computer scientist, propounds a view that comprehension is a kind of data compression. In his essay "The Limits of Reason", he argues that "understanding" something means being able to figure out a simple set of rules that explains it. For example, we understand why day and night exist because we have a simple model—the rotation of the earth—that explains a tremendous amount of data—changes in brightness, temperature, and atmospheric composition of the earth. We have compressed a large amount of information by using a simple model that predicts it. Similarly, we understand the number 0.33333... by thinking of it as one-third. The first way of representing the number requires five concepts ("0", "decimal point", "3", "infinity", "infinity of 3"); but the second way can produce all the data of the first representation, but uses only three concepts ("1", "division", "3"). Chaitin argues that comprehension is this ability to compress data.

Cognition is the process by which sensory inputs are transformed. Affect refers to the experience of feelings or emotions. Cognition and affect constitute understanding.

In Catholicism and Anglicanism, understanding is one of the Seven gifts of the Holy Spirit.




</doc>
<doc id="859096" url="https://en.wikipedia.org/wiki?curid=859096" title="Forbidden knowledge">
Forbidden knowledge

Forbidden knowledge, which is different from secret knowledge, is used to describe forbidden books or other information to which access is restricted or deprecated for political or religious reasons. Forbidden knowledge is commonly not secret, rather a society or various institutions will use repressive mechanisms to either completely prevent the publication of information they find objectionable or dangerous (censorship), or failing that, to try to reduce the public's trust in such information (propaganda). Public repression can create paradoxical situation where the proscribed information is generally common knowledge but publicly citing it is disallowed.

A rich set of examples exist through history. 
In many cases this resulted in people defending themselves by creating political jokes. Jokes throughout history have been a powerful instrument to undermine state authority and the public truth associated with it.

Some form of public repression of facts or speculation not desirable to some people or even a majority of the population seems inevitable as societies need to create some common basis of facts to create a unified identity. Critical to political and personal freedom is the level to which this repression is organized through the state or powerful private organizations. Western secular societies have reached the consensus through the late 19th and early 20th centuries that private organizations should not be allowed to engage in compulsory censorship, forcing people to obey their dictates. For example, the separation of church and state in most Western societies mostly prevents religious organizations from repressing individuals based on their personal opinions and beliefs. As well, people are generally allowed to leave employment with a company which may regulate such personal expressions for whatever reason and find employment in less restrictive circumstances.



</doc>
<doc id="6727454" url="https://en.wikipedia.org/wiki?curid=6727454" title="Growth of knowledge">
Growth of knowledge

A term coined by Karl Popper in his work "The Logic of Scientific Discovery" to denote what he regarded as the main problem of methodology and the philosophy of science, i.e. to explain and promote the further growth of scientific knowledge. To this purpose, Popper advocated his theory of falsifiability, testability and testing.



</doc>
<doc id="3467849" url="https://en.wikipedia.org/wiki?curid=3467849" title="Half-life of knowledge">
Half-life of knowledge

The half-life of knowledge or half-life of facts is the amount of time that has to elapse before half of the knowledge or facts in a particular area is superseded or shown to be untrue. These coined terms belong to the field of quantitative analysis of science known as scientometrics.

These ideas of half-life applied to different fields differ from the concept of half-life in physics in that there is no guarantee that the knowledge or facts in areas of study are declining exponentially. It is unclear that there is any way to establish what constitutes "knowledge" in a particular area, as opposed to mere opinion or theory. 

According to Ronald Bailey scientific knowledge is growing by a factor of ten every 50 years and half of what scientists may have known about a particular subject will be wrong or obsolete in 45 years.

An engineering degree went from having a half life of 35 years in ca. 1930 to about 10 years in 1960.

Donald Hebb estimated the half-life of psychology to be five years.
The concept of "half-life of knowledge" is attributed to Fritz Machlup (1962).

The similar concept of a "half-life of facts" was coined by Samuel Arbesman, a Harvard mathematician and scholar at the Kaufmann Foundation.




</doc>
<doc id="14908508" url="https://en.wikipedia.org/wiki?curid=14908508" title="Foolishness">
Foolishness

Foolishness is the unawareness or lack of social norms which causes offence, annoyance, trouble and/or injury. The things such as impulsivity and/or influences may affect a person's ability to make otherwise reasonable decisions. In this sense, it differs from stupidity, which is the lack of intelligence. An act of foolishness is called folly.

Andreas Maercker in 1995 defined foolishness as rigid, dogmatic, and inflexible thinking which makes feelings of bitterness and probable annoyance. It is considered the foundation of illusions of grandiosity like omniscience, omnipotence and inviolability.

The Book of Proverbs characterizes traits of foolishness. Foolishness and wisdom are contrasted in Paul's letter to the Corinthians. He condemns intellectual arrogance and advocates a humble attitude instead of foolishness, in which it is then possible to learn. 

Plato transvalued reason over foolishness, to him integrity of acceptance of a state itself was the beginning of wisdom, he said "He is the wisest man who knows himself to be ill-equipped for the study of wisdom".



</doc>
<doc id="5042367" url="https://en.wikipedia.org/wiki?curid=5042367" title="Common knowledge">
Common knowledge

Common knowledge is knowledge that is known by everyone or nearly everyone, usually with reference to the community in which the term is used. Common knowledge need not concern one specific subject, e.g., science or history. Rather, common knowledge can be about a broad range of subjects, such as science, literature, history, and entertainment. Often, common knowledge does not need to be cited. Common knowledge is distinct from general knowledge. The latter has been defined by differential psychologists as referring to "culturally valued knowledge communicated by a range of non-specialist media", and is considered an aspect of ability related to intelligence. Therefore, there are substantial individual differences in general knowledge as opposed to common knowledge.

In broader terms, common knowledge is used to refer to information that a reader would accept as valid, such as information that many users may know. As an example, this type of information may include the temperature in which water freezes or boils. To determine if information should be considered common knowledge, you can ask yourself who your audience is, are you able to assume they already have some familiarity with the topic, or will the information’s credibility come into question.

Many techniques have been developed in response to the question of distinguishing truth from fact in matters that have become "common knowledge". The scientific method is usually applied in cases involving phenomena associated with astronomy, mathematics, physics, and the general laws of nature. In legal settings, rules of evidence generally exclude hearsay (which may draw on "facts" someone believes to be "common knowledge").

"Conventional wisdom" is a similar term also referring to ostensibly pervasive knowledge or analysis.

Examples of common knowledge:





</doc>
<doc id="6082368" url="https://en.wikipedia.org/wiki?curid=6082368" title="Specialization of knowledge">
Specialization of knowledge

A modern development and belief that the progress of knowledge is the result of distinct and independent spheres, and that knowledge in one discipline has little connection with knowledge in another discipline. Thus, "specialists" pursue their work in isolation from one another rather than as aspects of a unity or whole.



</doc>
<doc id="9527250" url="https://en.wikipedia.org/wiki?curid=9527250" title="Knowledge organization">
Knowledge organization

Knowledge organization (KO), organization of knowledge, organization of information, or information organization is a branch of library and information science (LIS) concerned with activities such as document description, indexing, and classification performed in libraries, databases, archives, etc. It addresses the "activities carried out and tools used by people who work in places that accumulate information resources (e.g., books, maps, documents, datasets, images) for the use of humankind, both immediately and for posterity. It discusses the processes that are in place to make resources findable, whether someone is searching for a single known item or is browsing through hundreds of resources just hoping to discover something useful. Information organization supports a myriad of information-seeking scenarios." Traditional human-based approaches performed by librarians, archivists, and subject specialists are increasingly challenged by computational (big data) algorithmic techniques. KO as a field of study is concerned with the nature and quality of such knowledge organizing processes (KOP) (such as taxonomy and ontology) as well as the resulting knowledge organizing systems (KOS).

Divergent historical and theoretical approaches towards organizing knowledge are based on different views of knowledge, cognition, language, and social organization. This richness lends itself to many complementary ways to consider knowledge organization. The academic International Society for Knowledge Organization (ISKO) engages with these issues via the research journal "Knowledge Organization".

One widely used analysis of organizational principles summarizes them as Location, Alphabet, Time, Category, Hierarchy (LATCH).

Among the major figures in the history of KO are Melvil Dewey (1851–1931) and Henry Bliss (1870–1955). 

Dewey's goal was an efficient way to manage library collections; not an optimal system to support users of libraries. His system was meant to be used in many libraries as a standardized way to manage collections. The first version of this system was created in 1876. 

An important characteristic in Henry Bliss' (and many contemporary thinkers of KO) was that the sciences tend to reflect the order of Nature and that library classification should reflect the order of knowledge as uncovered by science: 
Natural order --> Scientific Classification --> Library classification (KO)

The implication is that librarians, in order to classify books, should know about scientific developments. This should also be reflected in their education: “Again from the standpoint of the higher education of librarians, the teaching of systems of classification . . . would be perhaps better conducted by including courses in the systematic encyclopedia and methodology of all the sciences, that is to say, outlines which try to summarize the most recent results in the relation to one another in which they are now studied together. . . .” (Ernest Cushing Richardson, quoted from Bliss, 1935, p. 2).

Among the other principles, which may be attributed to the traditional approach to KO are: 


Today, after more than 100 years of research and development in LIS, the “traditional” approach still has a strong position in KO and in many ways its principles still dominate.

The date of the foundation of this approach may be chosen as the publication of S. R. Ranganathan’s Colon Classification in 1933. The approach has been further developed by, in particular, the British Classification Research Group. In many ways this approach has dominated what might be termed “modern classification theory.” 

The best way to explain this approach is probably to explain its analytico-synthetic methodology. The meaning of the term “analysis” is: Breaking down each subject into its basic concepts. The meaning of the term synthesis is: Combining the relevant units and concepts to describe the subject matter of the information package in hand.

Given subjects (as they appear in, for example, book titles) are first analyzed into a few common categories, which are termed “facets”. Ranganathan proposed his PMEST formula — Personality, Matter, Energy, Space and Time:


Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the measures “recall” and “precision” as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (“UNITERM”). The Cranfield I test found, according to Ellis (1996, 3–6) the following results:

Although these results have been criticized and questioned, the IR-tradition became much more influential while library classification research lost influence. The dominant trend has been to regard only statistical averages. What has largely been neglected is to ask: Are there certain kinds of questions in relation to which other kinds of representation, for example, controlled vocabularies, may improve recall and precision?

The best way to define this approach is probably by method: Systems based upon user-oriented approaches must specify how the design of a system is made on the basis of empirical studies of users. 

User studies demonstrated very early that users prefer verbal search systems as opposed to systems based on classification notations. This is one example of a principle derived from empirical studies of users. Adherents of classification notations may, of course, still have an argument: That notations are well-defined and that users may miss important information by not considering them. 

Folksonomies is a recent kind of KO based on users' rather than on librarians' or subject specialists' indexing.

These approaches are primarily based on using bibliographical references to organize networks of papers, mainly by bibliographic coupling (introduced by Kessler 1963) or co-citation analysis ( independently suggested by Marshakova 1973 and Small 1973). In recent years it has become a popular activity to construe bibliometric maps as structures of research fields. 

Two considerations are important in considering bibliometric approaches to KO: 


Domain analysis is a sociological-epistemological standpoint. The indexing of a given document should reflect the needs of a given group of users or a given ideal purpose. In other words, any description or representation of a given document is more or less suited to the fulfillment of certain tasks. A description is never objective or neutral, and the goal is not to standardize descriptions or make one description once and for all for different target groups. 

The development of the Danish library “KVINFO” may serve as an example that explains the domain-analytic point of view. 

KVINFO was founded by the librarian and writer Nynne Koch and its history goes back to 1965. Nynne Koch was employed at the Royal Library in Copenhagen in a position without influence on book selection. She was interested in women’s’ studies and began personally to collect printed catalog cards of books in the Royal Library, which were considered relevant for women’s studies. She developed a classification system for this subject. Later she became the head of KVINFO and got a budget for buying books and journals, and still later, KVINFO became an independent library. The important theoretical point of view is that the Royal Library had an official systematic catalog of a high standard. Normally it is assumed that such a catalog is able to identify relevant books for users whatever their theoretical orientation. This example demonstrates, however, that for a specific user group (feminist scholars), an alternative way of organizing catalog cards was important. In other words: Different points of view need different systems of organization. 

DA is the only approach to KO which has seriously examined epistemological issues in the field, i.e. comparing the assumptions made in different approaches to KO and examining the questions regarding subjectivity and objectivity in KO. Subjectivity is not just about individual differences. Such differences are of minor interest because they cannot be used as guidelines for KO. What seems important are collective views shared by many users. A kind of subjectivity about many users is related to philosophical positions. In any field of knowledge different views are always at play. In arts, for example, different views of art are always present. Such views determine views on art works, writing on art works, how art works are organized in exhibitions and how writings on art are organized in libraries (see Ørom 2003). In general it can be stated that different philosophical positions on any issue have implications for relevance criteria, information needs and for criteria of organizing knowledge.




</doc>
<doc id="30384825" url="https://en.wikipedia.org/wiki?curid=30384825" title="Cognitive justice">
Cognitive justice

The concept of cognitive justice is based on the recognition of the plurality of knowledge and expresses the right of the different forms of knowledge to co-exist. 

Indian scholar Shiv Visvanathan coined the term cognitive justice in his 1997 book "A Carnival for Science: Essays on science, technology and development". Commenting on the destructive impact of hegemonic Western science on developing countries and non-Western cultures, Visvanathan calls for the recognition of alternative sciences or non-Western forms of knowledge. He argues that different knowledges are connected with different livelihoods and lifestyles and should therefore be treated equally.

Cognitive justice is a critique on the dominant paradigm of modern science and promotes the recognition of alternative paradigms or alternative sciences by facilitating and enabling dialogue between, often incommensurable, knowledges. These dialogues of knowledge are perceived as contributing to a more sustainable, equitable, and democratic world. 

The call for cognitive justice is found in a growing variety of fields, such as ethnobiology, technology and database design, and in information and communication technology for development (ICT4D).

South-African scholar and UNESCO education expert Odora Hoppers wrote about cognitive justice in the field of education. She argued that Indigenous knowledges have to be included in the dialogues of knowledge without having to fit in the structures and standards of Western knowledge. When Indigenous knowledges are treated equally, they can play their role in making a more democratic and dialogical science, which remains connected to the livelihoods and survival of all cultures. 


</doc>
<doc id="246176" url="https://en.wikipedia.org/wiki?curid=246176" title="Gettier problem">
Gettier problem

The Gettier problem, in the field of epistemology, is a landmark philosophical problem concerning our understanding of descriptive knowledge. Attributed to American philosopher Edmund Gettier, Gettier-type counterexamples (called "Gettier-cases") challenge the long-held justified true belief (JTB) account of knowledge. The JTB account holds that knowledge is equivalent to justified true belief; if all three conditions (justification, truth, and belief) are met of a given claim, then we have knowledge of that claim. In his 1963 three-page paper titled "Is Justified True Belief Knowledge?", Gettier attempts to illustrate by means of two counterexamples that there are cases where individuals can have a justified, true belief regarding a claim but still fail to know it because the reasons for the belief, while justified, turn out to be false. Thus, Gettier claims to have shown that the JTB account is inadequate; that it does not account for all of the necessary and sufficient conditions for knowledge.

The term "Gettier problem", "Gettier case", or even the adjective "Gettiered", is sometimes used to describe any case in the field of epistemology that purports to repudiate the JTB account of knowledge.

Responses to Gettier's paper have been numerous; some reject Gettier's examples, while others seek to adjust the JTB account of knowledge and blunt the force of these counterexamples. Gettier problems have even found their way into sociological experiments, where the intuitive responses from people of varying demographics to Gettier cases have been studied.

The question of what constitutes "knowledge" is as old as philosophy itself. Early instances are found in Plato's dialogues, notably "Meno" (97a–98b) and "Theaetetus". Gettier himself was not actually the first to raise the problem named after him; its existence was acknowledged by both Alexius Meinong and Bertrand Russell, the latter of which discussed the problem in his book "Human knowledge: Its scope and limits". In fact, the problem has been known since the Middle Ages, and both Indian philosopher Dharmottara and scholastic logician Peter of Mantua presented examples of it.

Russell's case, called the stopped clock case, goes as follows: Alice sees a clock that reads two o'clock and believes that the time is two o'clock. It is, in fact, two o'clock. There's a problem, however: unknown to Alice, the clock she's looking at stopped twelve hours ago. Alice thus has an accidentally true, justified belief. Russell provides an answer of his own to the problem. Edmund Gettier's formulation of the problem was important as it coincided with the rise of the sort of philosophical naturalism promoted by W. V. O. Quine and others, and was used as a justification for a shift towards externalist theories of justification. John L. Pollock and Joseph Cruz have stated that the Gettier problem has "fundamentally altered the character of contemporary epistemology" and has become "a central problem of epistemology since it poses a clear barrier to analyzing knowledge".

Alvin Plantinga rejects the historical analysis:
Despite this, Plantinga "does" accept that some philosophers before Gettier have advanced a JTB account of knowledge, specifically C. I. Lewis and A. J. Ayer.

The JTB account of knowledge is the claim that knowledge can be conceptually analyzed as justified true belief, which is to say that the "meaning" of sentences such as "Smith knows that it rained today" can be given with the following set of conditions, which are necessary and sufficient for knowledge to obtain:

The JTB account was first credited to Plato, though Plato argued against this very account of knowledge in the "Theaetetus" (210a). This account of knowledge is what Gettier subjected to criticism.

Gettier's paper used counterexamples (see also thought experiment) to argue that there are cases of beliefs that are both true and justified—therefore satisfying all three conditions for knowledge on the JTB account—but that do not appear to be genuine cases of knowledge. Therefore, Gettier argued, his counterexamples show that the JTB account of knowledge is false, and thus that a different conceptual analysis is needed to correctly track what we mean by "knowledge".

Gettier's case is based on two counterexamples to the JTB analysis. Each relies on two claims. Firstly, that justification is preserved by entailment, and secondly that this applies coherently to Smith's putative "belief". That is, that if Smith is justified in believing P, and Smith realizes that the truth of P entails the truth of Q, then Smith would "also" be justified in believing Q. Gettier calls these counterexamples "Case I" and "Case II":

In both of Gettier's actual examples (see also counterfactual conditional), the justified true belief came about, if Smith's purported claims are disputable, as the result of entailment (but see also material conditional) from justified false beliefs that "Jones will get the job" (in case I), and that "Jones owns a Ford" (in case II). This led some early responses to Gettier to conclude that the definition of knowledge could be easily adjusted, so that knowledge was justified true belief that does not depend on false premises. The interesting issue that arises is then of how to know which premises are in reality false or true when deriving a conclusion, because as in the Gettier cases, one sees that premises can be very reasonable to believe and be likely true, but unknown to the believer there are confounding factors and extra information that may have been missed while concluding something. The question that arises is therefore to what extent would one have to be able to go about attempting to "prove" all premises in the argument before solidifying a conclusion.

In a 1966 scenario known as "The sheep in the field", Roderick Chisholm asks us to imagine that someone is standing outside a field looking at something that looks like a sheep (although in fact, it is a dog disguised as a sheep). They believe there is a sheep in the field, and in fact, they are right because there is a sheep behind the hill in the middle of the field. Hence, they have a justified true belief that there is a sheep in the field. But is that belief knowledge? A similar problem which seeks to be more plausible called the "Cow in the Field" appears in Martin Cohen's book "101 Philosophy Problems", where it is supposed that a farmer checking up on his favourite cow confuses a piece of black and white paper caught up in a distant bush for his cow. However, since the animal actually is in the field, but hidden in a hollow, again, the farmer has a justified, true belief which seems nonetheless not to qualify as "knowledge".

Another scenario by Brian Skyrms is "The Pyromaniac", in which a struck match lights not for the reasons the pyromaniac imagines but because of some unknown "Q radiation".

A different perspective on the issue is given by Alvin Goldman in the "fake barns" scenario (crediting Carl Ginet with the example). In this one, a man is driving in the countryside, and sees what looks exactly like a barn. Accordingly, he thinks that he is seeing a barn. In fact, that is what he is doing. But what he does not know is that the neighborhood generally consists of many fake barns — barn facades designed to look exactly like real barns when viewed from the road, as in the case of a visit in the countryside by Catherine II of Russia, just to please her. Since if he had been looking at one of them, he would have been unable to tell the difference, his "knowledge" that he was looking at a barn would seem to be poorly founded. A similar process appears in Robert A. Heinlein's "Stranger in a Strange Land" as an example of Fair Witness behavior.

The "no false premises" (or "no false lemmas") solution which was proposed early in the discussion proved to be somewhat problematic, as more general Gettier-style problems were then constructed or contrived in which the justified true belief does not seem to be the result of a chain of reasoning from a justified false belief.

For example:

Again, it seems as though Luke does not "know" that Mark is in the room, even though it is claimed he has a justified true belief that Mark is in the room, but it is not nearly so clear that the "perceptual belief" that "Mark is in the room" was inferred from any premises at all, let alone any false ones, nor led to significant conclusions on its own; Luke did not seem to be reasoning about anything; "Mark is in the room" seems to have been part of what he "seemed to see".

To save the "no false lemmas" solution, one must logically say that Luke's inference from sensory data does not count as a justified belief unless he consciously or unconsciously considers the possibilities of deception and self-deception. A justified version of Luke's thought process, by that logic, might go like this:

The second line counts as a false premise. However, by the previous argument, this suggests we have fewer justified beliefs than we think we do.

The main idea behind Gettier's examples is that the justification for the belief is flawed or incorrect, but the belief turns out to be true by sheer luck. Thus, a general scenario can be constructed as such:

Bob believes A is true because of B. Argument B is flawed, but A turns out to be true by a different argument C. Since A is true, Bob believes A is true, and Bob has justification for B, all of the conditions (JTB) are satisfied. However, Bob had no knowledge of A.

The Gettier problem is formally a problem in first-order logic, but the introduction by Gettier of terms such as "believes" and "knows" moves the discussion into the field of epistemology. Here, the sound (true) arguments ascribed to Smith then need also to be valid (believed) and convincing (justified) if they are to issue in the real-world discussion about "justified true belief".
Responses to Gettier problems have fallen into one of three categories:


One response, therefore, is that in none of the above cases was the belief justified because it is impossible to justify anything that is not true. Conversely, the fact that a proposition turns out to be untrue is proof that it was not sufficiently justified in the first place. Under this interpretation, the JTB definition of knowledge survives. This shifts the problem to a definition of justification, rather than knowledge. Another view is that justification and non-justification are not in binary opposition. Instead, justification is a matter of degree, with an idea being more or less justified. This account of justification is supported by mainstream philosophers such as Paul Boghossian and Stephen Hicks. In common sense usage, an idea can not only be more justified or less justified, but it can also partially justified (Smith's boss told him X) and partially unjustified (Smith's boss is a liar). Gettier's cases involve propositions that were true, believed, but which had weak justification. In case 1, the premise that the testimony of Smith's boss is "strong evidence" is rejected. The case itself depends on the boss being either wrong or deceitful (Jones did not get the job) and therefore unreliable. In case 2, Smith again has accepted a questionable idea (Jones owns a Ford) with unspecified justification. Without justification, both cases do not undermine the JTB account of knowledge.

Other epistemologists accept Gettier's conclusion. Their responses to the Gettier problem, therefore, consist of trying to find alternative analyses of knowledge. They have struggled to discover and agree upon as a beginning any single notion of truth, or belief, or justifying which is wholly and obviously accepted. Truth, belief, and justifying have not yet been satisfactorily defined, so that JTB (justified true belief) may be defined satisfactorily is still problematical, on account or otherwise of Gettier's examples. Gettier, for many years a professor at University of Massachusetts Amherst later also was interested in the epistemic logic of Hintikka, a Finnish philosopher at Boston University, who published "Knowledge and Belief" in 1962.
The most common direction for this sort of response to take is what might be called a "JTB+G" analysis: that is, an analysis based on finding some "fourth" condition—a "no-Gettier-problem" condition—which, when added to the conditions of justification, truth, and belief, will yield a set of necessary and jointly sufficient conditions.

One such response is that of Alvin Goldman (1967), who suggested the addition of a "causal" condition: a subject's belief is justified, for Goldman, only if the truth of a belief has "caused" the subject to have that belief (in the appropriate way); and for a justified true belief to count as knowledge, the subject must "also" be able to "correctly reconstruct" (mentally) that causal chain. Goldman's analysis would rule out Gettier cases in that Smith's beliefs are not caused by the truths of those beliefs; it is merely "accidental" that Smith's beliefs in the Gettier cases happen to be true, or that the prediction made by Smith: " The winner of the job will have 10 coins", on the basis of his putative belief, (see also bundling) came true in this one case. This theory is challenged by the difficulty of giving a principled explanation of how an appropriate causal relationship differs from an inappropriate one (without the circular response of saying that the appropriate sort of causal relationship is the knowledge-producing one); or retreating to a position in which justified true belief is weakly defined as the consensus of learned opinion. The latter would be useful, but not as useful nor desirable as the unchanging definitions of scientific concepts such as momentum. Thus, adopting a causal response to the Gettier problem usually requires one to adopt (as Goldman gladly does) some form of reliabilism about justification. See "Goldman"s Theory of justification.

Keith Lehrer and Thomas Paxson (1969) proposed another response, by adding a "defeasibility condition" to the JTB analysis. On their account, knowledge is "undefeated justified true belief" — which is to say that a justified true belief counts as knowledge if and only if it is also the case that there is no further truth that, had the subject known it, would have defeated her present justification for the belief. (Thus, for example, Smith's justification for believing that the person who will get the job has ten coins in his pocket is his justified belief that Jones will get the job, combined with his justified belief that Jones has ten coins in his pocket. But if Smith had known the truth that Jones will "not" get the job, that would have defeated the justification for his belief.) However, many critics (such as Marshall Swain [1974]) have argued that the notion of a "defeater" fact cannot be made precise enough to rule out the Gettier cases without also ruling out "a priori" cases of knowledge .

Pragmatism was developed as a philosophical doctrine by C.S.Peirce and William James (1842–1910). In Peirce's view, the truth is nominally defined as a sign's correspondence to its object and pragmatically defined as the ideal final opinion to which sufficient investigation "would" lead sooner or later. James' epistemological model of truth was that which "works" in the way of belief, and a belief was true if in the long run it "worked" for all of us, and guided us expeditiously through our semihospitable world.
Peirce argued that metaphysics could be cleaned up by a pragmatic approach.
Consider what effects that might "conceivably" have practical bearings you "conceive" the objects of your "conception" to have. Then, your "conception" of those effects is the whole of your "conception" of the object.
From a pragmatic viewpoint of the kind often ascribed to James, defining on a particular occasion whether a particular belief can rightly be said to be both true and justified is seen as no more than an exercise in pedantry, but being able to discern whether that belief led to fruitful outcomes is a fruitful enterprise. Peirce emphasized fallibilism, considered the assertion of absolute certainty a barrier to inquiry, and in 1901 defined truth as follows: "Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth." In other words, any unqualified assertion is likely to be at least a little wrong or, if right, still right for not entirely the right reasons. Therefore one is more veracious by being Socratic, including a recognition of one's own ignorance and knowing one may be proved wrong. This is the case, even though in practical matters one sometimes must act, if one is to act at all, with decision and complete confidence.

The difficulties involved in producing a viable fourth condition have led to claims that attempting to repair the JTB account is a deficient strategy. For example, one might argue that what the Gettier problem shows is not the need for a fourth independent condition in addition to the original three, but rather that the attempt to build up an account of knowledging by conjoining a set of independent conditions was misguided from the outset. Those who have adopted this approach generally argue that epistemological terms like justification, evidence, certainty, etc. should be analyzed in terms of a primitive notion of "knowledge," rather than vice versa. Knowledge is understood as "factive," that is, as embodying a sort of epistemological "tie" between a truth and a belief. The JTB account is then criticized for trying to get and encapsulate the factivity of knowledge "on the cheap," as it were, or via a circular argument, by replacing an irreducible notion of factivity with the conjunction of some of the properties that accompany it (in particular, truth and justification). Of course, the introduction of irreducible primitives into a philosophical theory is always problematical (some would say a sign of desperation), and such anti-reductionist accounts are unlikely to please those who have other reasons to hold fast to the method behind JTB+G accounts.

Fred Dretske (1971) developed an account of knowledge which he called "conclusive reasons", revived by Robert Nozick as what he called the "subjunctive" or truth-tracking account (1981). Nozick's formulation posits that proposition p is an instance of knowledge when:


Nozick's definition is intended to preserve Goldman's intuition that Gettier cases should be ruled out by disacknowledging "accidentally" true justified beliefs, but without risking the potentially onerous consequences of building a causal requirement into the analysis. This tactic though, invites the riposte that Nozick's account merely hides the problem and does not solve it, for it leaves open the question of "why" Smith would not have had his belief if it had been false. The most promising answer seems to be that it is because Smith's belief was "caused" by the truth of what he believes; but that puts us back in the causalist camp.

Criticisms and counter examples (notably the "Grandma case") prompted a revision, which resulted in the alteration of (3) and (4) to limit themselves to the same method (i.e. vision):


Saul Kripke has pointed out that this view remains problematic and uses a counterexample called the "Fake Barn Country example", which describes a certain locality containing a number of fake barns or facades of barns. In the midst of these fake barns is one real barn, which is painted red. There is one more piece of crucial information for this example: the fake barns cannot be painted red.

Jones is driving along the highway, looks up and happens to see the real barn, and so forms the belief


Though Jones has gotten lucky, he could have just as easily been deceived and not have known it. Therefore it doesn't fulfill premise 4, for if Jones saw a fake barn he wouldn't have any idea it was a fake barn. So this is not knowledge.

An alternate example is if Jones looks up and forms the belief


According to Nozick's view this fulfills all four premises. Therefore this is knowledge, since Jones couldn't have been wrong, since the fake barns cannot be painted red. This is a troubling account however, since it seems the first statement "I see a barn" can be inferred from "I see a red barn"; however by Nozick's view the first belief is "not" knowledge and the second is knowledge.

In the first chapter of his book "Pyrronian Reflexions on Truth and Justification", Robert Fogelin gives a diagnosis that leads to a dialogical solution to Gettier's problem. The problem always arises when the given justification has nothing to do with what really makes the proposition true. Now, he notes that in such cases there is always a mismatch between the information disponible to the person who makes the knowledge-claim of some proposition p and the information disponible to the evaluator of this knowledge-claim (even if the evaluator is the same person in a later time). A Gettierian counterexample arises when the justification given by the person who makes the knowledge-claim cannot be accepted by the knowledge evaluator because it does not fit with his wider informational setting. For instance, in the case of the fake barn the evaluator knows that a superficial inspection from someone who does not know the peculiar circumstances involved isn't a justification aceptable as making the proposition p (that it is a real barn) true.
Richard Kirkham has proposed that it is best to start with a definition of knowledge so strong that giving a counterexample to it is logically impossible. Whether it can be weakened without becoming subject to a counterexample should then be checked. He concludes that there will always be a counterexample to any definition of knowledge in which the believer's evidence does not logically necessitate the belief. Since in most cases the believer's evidence does not necessitate a belief, Kirkham embraces skepticism about knowledge. He notes that a belief can still be rational even if it is not an item of knowledge. (see also: fallibilism)

One might respond to Gettier by finding a way to avoid his conclusion(s) in the first place. However, it can hardly be argued that knowledge is justified true belief if there are cases that are justified true belief without being knowledge; thus, those who want to avoid Gettier's conclusions have to find some way to defuse Gettier's counterexamples. In order to do so, within the parameters of the particular counter-example or exemplar, they must then either accept that


or, demonstrate a case in which it is possible to circumvent surrender to the exemplar by eliminating any necessity for it to be considered that JTB apply in just those areas that Gettier has rendered obscure, without thereby lessening the force of JTB to apply in those cases where it actually is crucial.
Then, though Gettier's cases "stipulate" that Smith has a certain belief and that his belief is true, it seems that in order to propose (1), one must argue that Gettier, (or, that is, the writer responsible for the particular form of words on this present occasion known as case (1), and who makes assertion's about Smith's "putative" beliefs), goes wrong because he has the wrong notion of "justification." Such an argument often depends on an externalist account on which "justification" is understood in such a way that whether or not a belief is "justified" depends not just on the internal state of the believer, but also on how that internal state is related to the outside world. Externalist accounts typically are constructed such that Smith's putative beliefs in Case I and Case II are not really justified (even though it seems to Smith that they are), because his beliefs are not lined up with the world in the right way, or that it is possible to show that it is invalid to assert that "Smith" has any significant "particular" belief at all, in terms of JTB or otherwise. Such accounts, of course, face the same burden as causalist responses to Gettier: they have to explain what sort of relationship between the world and the believer counts as a justificatory relationship.

Those who accept (2) are by far in the minority in analytic philosophy; generally those who are willing to accept it are those who have independent reasons to say that more things count as knowledge than the intuitions that led to the JTB account would acknowledge. Chief among these are epistemic minimalists such as Crispin Sartwell, who hold that all true belief, including both Gettier's cases and lucky guesses, counts as knowledge.

Some early work in the field of experimental philosophy suggested that traditional intuitions about Gettier cases might vary cross-culturally. However, subsequent studies have consistently failed to replicate these results, instead finding that participants from different cultures do share the traditional intuition. Indeed, more recent studies have actually been providing evidence for the opposite hypothesis, that people from a variety of different cultures have surprisingly similar intuitions in these cases.



</doc>
<doc id="1255319" url="https://en.wikipedia.org/wiki?curid=1255319" title="Functional illiteracy">
Functional illiteracy

Functional illiteracy is reading and writing skills that are inadequate "to manage daily living and employment tasks that require reading skills beyond a basic level". Functional illiteracy is contrasted with illiteracy in the strict sense, meaning the inability to read or write simple sentences in any language.

Foreigners who cannot read and write in the native language where they live may also be considered functionally illiterate.

Illiteracy as well as functional illiteracy were defined on the 20th session of UNESCO in 1978 as follows:

The characteristics of functional illiteracy vary from one culture to another, as some cultures require better reading and writing skills than others. A reading level that might be sufficient to make a farmer functionally literate in a rural area of a developing country might qualify as functional illiteracy in an urban area of a technologically advanced country. In languages with regular spelling, functional illiteracy is usually defined simply as reading too slow for practical use, inability to effectively use dictionaries and written manuals, etc.

In developed countries, the level of functional literacy of an individual is proportional to income level and inversely proportional to the risk of committing crime. For example, according to the National Center for Educational Statistics in the United States:

According to begintoread.com:

In the United States, according to "Business" magazine, an estimated 15 million functionally illiterate adults held jobs at the beginning of the 21st century. The American Council of Life Insurers reported that 75% of the Fortune 500 companies provide some level of remedial training for their workers. All over the U.S.A. 30 million (14% of adults) are unable to perform simple and everyday literacy activities.

The National Center for Education Statistics provides more detail. Literacy is broken down into three parameters: prose, document, and quantitative literacy. Each parameter has four levels: below basic, basic, intermediate, and proficient. For prose literacy, for example, a below basic level of literacy means that a person can look at a short piece of text to get a small piece of uncomplicated information, while a person who is below basic in quantitative literacy would be able to do simple addition. In the US, 14% of the adult population is at the "below basic" level for prose literacy; 12% are at the "below basic" level for document literacy; and 22% are at that level for quantitative literacy. Only 13% of the population is proficient in these three areas—able to compare viewpoints in two editorials; interpret a table about blood pressure, age, and physical activity; or compute and compare the cost per ounce of food items.

The UK government's Department for Education reported in 2006 that 47% of school children left school at age 16 without having achieved a basic level in functional mathematics, and 42% fail to achieve a basic level of functional English. Every year, 100,000 pupils leave school functionally illiterate in the UK.

While in Russia, where more than 99% percent of the population is technically literate, only one third of high school graduates can comprehend the content of scientific and literary texts, according to a 2015 study.

A Literacy at Work study, published by the Northeast Institute in 2001, found that business losses attributed to basic skill deficiencies run into billions of dollars a year due to low productivity, errors, and accidents attributed to functional illiteracy.

Sociological research has demonstrated that countries with lower levels of functional illiteracy among their adult populations tend to be those with the highest levels of scientific literacy among the lower stratum of young people nearing the end of their formal academic studies. This correspondence suggests that the capacity of schools to ensure students attain the functional literacy required to comprehend the basic texts and documents associated with competent citizenship contributes to a society's level of civic literacy.




</doc>
<doc id="3657390" url="https://en.wikipedia.org/wiki?curid=3657390" title="Nous">
Nous

Nous (, ), sometimes equated to intellect or intelligence, is a term from classical philosophy for the faculty of the human mind necessary for understanding what is true or real. English words such as "understanding" are sometimes used, but three commonly used philosophical terms come directly from classical languages: "νοῦς" or "νόος" (from Ancient Greek), "intellēctus" and "intellegentia" (from Latin). To describe the activity of this faculty, the word "intellection" is sometimes used in philosophical contexts, as well as the Greek words "noēsis" and "noeîn" ("νόησις", "νοεῖν"). This activity is understood in a similar way (at least in some contexts) to the modern concept of intuition.

In philosophy, common English translations include "understanding" and "mind"; or sometimes "thought" or "reason" (in the sense of that which reasons, not the activity of reasoning). It is also often described as something equivalent to perception except that it works within the mind ("the mind's eye"). It has been suggested that the basic meaning is something like "awareness". In colloquial British English, "nous" also denotes "good sense", which is close to one everyday meaning it had in Ancient Greece.
In Aristotle's influential works, the term was carefully distinguished from sense perception, imagination, and reason, although these terms are closely inter-related. The term was apparently already singled out by earlier philosophers such as Parmenides, whose works are largely lost. In post-Aristotelian discussions, the exact boundaries between perception, understanding of perception, and reasoning have not always agreed with the definitions of Aristotle, even though his terminology remains influential.

In the Aristotelian scheme, "nous" is the basic understanding or awareness that allows human beings to think rationally. For Aristotle, this was distinct from the processing of sensory perception, including the use of imagination and memory, which other animals can do. This therefore connects discussion of "nous" to discussion of how the human mind sets definitions in a consistent and communicable way, and whether people must be born with some innate potential to understand the same universal categories in the same logical ways. Deriving from this it was also sometimes argued, especially in classical and medieval philosophy, that the individual "nous" must require help of a spiritual and divine type. By this type of account, it came to be argued that the human understanding ("nous") somehow stems from this cosmic "nous", which is however not just a recipient of order, but a creator of it. Such explanations were influential in the development of medieval accounts of God, the immortality of the soul, and even the motions of the stars, in Europe, North Africa and the Middle East, amongst both eclectic philosophers and authors representing all the major faiths of their times.

In early Greek uses, Homer used "nous" to signify mental activities of both mortals and immortals, for example what they really have on their mind as opposed to what they say aloud. It was one of several words related to thought, thinking, and perceiving with the mind. In pre-Socratic philosophy, it became increasingly distinguished as a source of knowledge and reasoning opposed to mere sense perception or thinking influenced by the body such as emotion. For example, Heraclitus complained that "much learning does not teach "nous"".

Among some Greek authors, a faculty of intelligence known as a "higher mind" came to be considered as a property of the cosmos as a whole.

The work of Parmenides set the scene for Greek philosophy to come and the concept of "nous" was central to his radical proposals. He claimed that reality as the senses perceive it is not a world of truth at all, because sense perception is so unreliable, and what is perceived is so uncertain and changeable. Instead he argued for a dualism wherein "nous" and related words (the verb for thinking which describes its mental perceiving activity, "noein", and the unchanging and eternal objects of this perception, "noēta") describe a form of perception which is not physical, but intellectual only, distinct from sense perception and the objects of sense perception.

Anaxagoras, born about 500 BC, is the first person who is definitely known to have explained the concept of a "nous" (mind), which arranged all other things in the cosmos in their proper order, started them in a rotating motion, and continuing to control them to some extent, having an especially strong connection with living things. (However Aristotle reports an earlier philosopher, Hermotimus of Clazomenae, who had taken a similar position.) Amongst the pre-Socratic philosophers before Anaxagoras, other philosophers had proposed a similar ordering human-like principle causing life and the rotation of the heavens. For example, Empedocles, like Hesiod much earlier, described cosmic order and living things as caused by a cosmic version of love, and Pythagoras and Heraclitus, attributed the cosmos with "reason" ("logos").

According to Anaxagoras the cosmos is made of infinitely divisible matter, every bit of which can inherently become anything, except Mind ("nous"), which is also matter, but which can only be found separated from this general mixture, or else mixed into living things, or in other words in the Greek terminology of the time, things with a soul ("psychē"). Anaxagoras wrote:

Concerning cosmology, Anaxagoras, like some Greek philosophers already before him, believed the cosmos was revolving, and had formed into its visible order as a result of such revolving causing a separating and mixing of different types of chemical elements. "Nous", in his system, originally caused this revolving motion to start, but it does not necessarily continue to play a role once the mechanical motion has started. His description was in other words (shockingly for the time) corporeal or mechanical, with the moon made of earth, the sun and stars made of red hot metal (beliefs Socrates was later accused of holding during his trial) and "nous" itself being a physical fine type of matter which also gathered and concentrated with the development of the cosmos. This "nous" (mind) is not incorporeal; it is the thinnest of all things. The distinction between "nous" and other things nevertheless causes his scheme to sometimes be described as a peculiar kind of dualism.

Anaxagoras' concept of "nous" was distinct from later platonic and neoplatonic cosmologies in many ways, which were also influenced by Eleatic, Pythagorean and other pre-Socratic ideas, as well as the Socratics themselves.

In some schools of Hindu philosophy, a "higher mind" came to be considered a property of the cosmos as a whole that exists within all matter (known as buddhi or mahat). In Samkhya, this faculty of intellect (buddhi) serves to differentiate matter (prakrti) from pure consciousness (purusha). The lower aspect of mind that corresponds to the senses is referred to as "manas".

Xenophon, the less famous of the two students of Socrates whose written accounts of him have survived, recorded that he taught his students a kind of teleological justification of piety and respect for divine order in nature. This has been described as an "intelligent design" argument for the existence of God, in which nature has its own "nous". For example, in his "Memorabilia" 1.4.8, he describes Socrates asking a friend sceptical of religion, "Are you, then, of the opinion that intelligence ("nous") alone exists nowhere and that you by some good chance seized hold of it, while—as you think—those surpassingly large and infinitely numerous things [all the earth and water] are in such orderly condition through some senselessness?" Later in the same discussion he compares the "nous", which directs each person's body, to the good sense ("phronēsis") of the god, which is in everything, arranging things to its pleasure (1.4.17). Plato describes Socrates making the same argument in his "Philebus" 28d, using the same words "nous" and "phronēsis".

Plato used the word "nous" in many ways that were not unusual in the everyday Greek of the time, and often simply meant "good sense" or "awareness". On the other hand, in some of his Platonic dialogues it is described by key characters in a higher sense, which was apparently already common. In his "Philebus" 28c he has Socrates say that "all philosophers agree—whereby they really exalt themselves—that mind ("nous") is king of heaven and earth. Perhaps they are right." and later states that the ensuing discussion "confirms the utterances of those who declared of old that mind ("nous") always rules the universe".

In his "Cratylus", Plato gives the etymology of Athena's name, the goddess of wisdom, from "Atheonóa" (Ἀθεονόα) meaning "god's ("theos") mind ("nous")". In his "Phaedo", Plato's teacher Socrates is made to say just before dying that his discovery of Anaxagoras' concept of a cosmic "nous" as the cause of the order of things, was an important turning point for him. But he also expressed disagreement with Anaxagoras' understanding of the implications of his own doctrine, because of Anaxagoras' materialist understanding of causation. Socrates said that Anaxagoras would "give voice and air and hearing and countless other things of the sort as causes for our talking with each other, and should fail to mention the real causes, which are, that the Athenians decided that it was best to condemn me". On the other hand, Socrates seems to suggest that he also failed to develop a fully satisfactory teleological and dualistic understanding of a mind of nature, whose aims represent the Good, which all parts of nature aim at.

Concerning the "nous" that is the source of understanding of individuals, Plato is widely understood to have used ideas from Parmenides in addition to Anaxagoras. Like Parmenides, Plato argued that relying on sense perception can never lead to true knowledge, only opinion. Instead, Plato's more philosophical characters argue that "nous" must somehow perceive truth directly in the ways gods and daimons perceive. What our mind sees directly in order to really understand things must not be the constantly changing material things, but unchanging entities that exist in a different way, the so-called "forms" or "ideas". However he knew that contemporary philosophers often argued (as in modern science) that "nous" and perception are just two aspects of one physical activity, and that perception is the source of knowledge and understanding (not the other way around).

Just exactly how Plato believed that the "nous" of people lets them come to understand things in any way that improves upon sense perception and the kind of thinking which animals have, is a subject of long running discussion and debate. On the one hand, in the "Republic" Plato's Socrates, in the Analogy of the sun and Allegory of the Cave describes people as being able to perceive more clearly because of something from outside themselves, something like when the sun shines, helping eyesight. The source of this illumination for the intellect is referred to as the Form of the Good. On the other hand, in the "Meno" for example, Plato's Socrates explains the theory of "anamnesis" whereby people are born with ideas already in their soul, which they somehow remember from previous lives. Both theories were to become highly influential.

As in Xenophon, Plato's Socrates frequently describes the soul in a political way, with ruling parts, and parts that are by nature meant to be ruled. "Nous" is associated with the rational ("logistikon") part of the individual human soul, which by nature should rule. In his "Republic", in the so-called "analogy of the divided line", it has a special function within this rational part. Plato tended to treat "nous" as the only immortal part of the soul.

Concerning the cosmos, in the "Timaeus", the title character also tells a "likely story" in which "nous" is responsible for the creative work of the demiurge or maker who brought rational order to our universe. This craftsman imitated what he perceived in the world of eternal Forms. In the "Philebus" Socrates argues that "nous" in individual humans must share in a cosmic "nous", in the same way that human bodies are made up of small parts of the elements found in the rest of the universe. And this "nous" must be in the "genos" of being a cause of all particular things as particular things.

Like Plato, Aristotle saw the "nous" or intellect of an individual as somehow similar to sense perception but also distinct. Sense perception in action provides images to the "nous", via the "sensus communis" and imagination, without which thought could not occur. But other animals have "sensus communis" and imagination, whereas none of them have "nous". Aristotelians divide perception of forms into the animal-like one which perceives "species sensibilis" or "sensible forms", and "species intelligibilis" that are perceived in a different way by the "nous".

Like Plato, Aristotle linked "nous" to "logos" (reason) as uniquely human, but he also distinguished "nous" from "logos", thereby distinguishing the faculty for setting definitions from the faculty that uses them to reason with. In his "Nicomachean Ethics", Aristotle divides the soul ("psychē") into two parts, one which has reason and one which does not, but then divides the part which has reason into the reasoning ("logistikos") part itself which is lower, and the higher "knowing" ("epistēmonikos") part which contemplates general principles ("archai"). "Nous", he states, is the source of the first principles or sources ("archai") of definitions, and it develops naturally as people gain experience. This he explains after first comparing the four other truth revealing capacities of soul: technical know how ("technē"), logically deduced knowledge ("epistēmē", sometimes translated as "scientific knowledge"), practical wisdom ("phronēsis"), and lastly theoretical wisdom ("sophia"), which is defined by Aristotle as the combination of "nous" and "epistēmē". All of these others apart from "nous" are types of reason ("logos").
Aristotle's philosophical works continue many of the same Socratic themes as his teacher Plato. Amongst the new proposals he made was a way of explaining causality, and "nous" is an important part of his explanation. As mentioned above, Plato criticized Anaxagoras' materialism, or understanding that the intellect of nature only set the cosmos in motion, but is no longer seen as the cause of physical events. Aristotle explained that the changes of things can be described in terms of four causes at the same time. Two of these four causes are similar to the materialist understanding: each thing has a material which causes it to be how it is, and some other thing which set in motion or initiated some process of change. But at the same time according to Aristotle each thing is also caused by the natural forms they are tending to become, and the natural ends or aims, which somehow exist in nature as causes, even in cases where human plans and aims are not involved. These latter two causes (the "formal" and "final"), are concepts no longer used in modern science, and encompass the continuous effect of the intelligent ordering principle of nature itself. Aristotle's special description of causality is especially apparent in the natural development of living things. It leads to a method whereby Aristotle analyses causation and motion in terms of the potentialities and actualities of all things, whereby all matter possesses various possibilities or potentialities of form and end, and these possibilities become more fully real as their potential forms become actual or active reality (something they will do on their own, by nature, unless stopped because of other natural things happening). For example, a stone has in its nature the potentiality of falling to the earth and it will do so, and actualize this natural tendency, if nothing is in the way.

Aristotle analyzed thinking in the same way. For him, the possibility of understanding rests on the relationship between intellect and sense perception. Aristotle's remarks on the concept of what came to be called the "active intellect" and "passive intellect" (along with various other terms) are amongst "the most intensely studied sentences in the history of philosophy". The terms are derived from a single passage in Aristotle's "De Anima", Book III. Following is the translation of one of those passages with some key Greek words shown in square brackets.
...since in nature one thing is the material ["hulē"] for each kind ["genos"] (this is what is in potency all the particular things of that kind) but it is something else that is the causal and productive thing by which all of them are formed, as is the case with an art in relation to its material, it is necessary in the soul ["psychē"] too that these distinct aspects be present;
the one sort is intellect ["nous"] by becoming all things, the other sort by forming all things, in the way an active condition ["hexis"] like light too makes the colors that are in potency be at work as colors ["to phōs poiei ta dunamei onta chrōmata energeiai chrōmata"].
This sort of intellect [which is like light in the way it makes potential things work as what they are] is separate, as well as being without attributes and unmixed, since it is by its thinghood a being-at-work ["energeia"], for what acts is always distinguished in stature above what is acted upon, as a governing source is above the material it works on.
Knowledge ["epistēmē"], in its being-at-work, is the same as the thing it knows, and while knowledge in potency comes first in time in any one knower, in the whole of things it does not take precedence even in time.
This does not mean that at one time it thinks but at another time it does not think, but when separated it is just exactly what it is, and this alone is deathless and everlasting (though we have no memory, because this sort of intellect is not acted upon, while the sort that is acted upon is destructible), and without this nothing thinks.

The passage tries to explain "how the human intellect passes from its original state, in which it does not think, to a subsequent state, in which it does" according to his distinction between potentiality and actuality. Aristotle says that the passive intellect receives the intelligible forms of things, but that the active intellect is required to make the potential knowledge into actual knowledge, in the same way that light makes potential colours into actual colours. As Davidson remarks:
Just what Aristotle meant by potential intellect and active intellect - terms not even explicit in the "De anima" and at best implied - and just how he understood the interaction between them remains moot. Students of the history of philosophy continue to debate Aristotle's intent, particularly the question whether he considered the active intellect to be an aspect of the human soul or an entity existing independently of man.

The passage is often read together with "Metaphysics", Book XII, ch.7-10, where Aristotle makes "nous" as an actuality a central subject within a discussion of the cause of being and the cosmos. In that book, Aristotle equates active "nous", when people think and their "nous" becomes what they think about, with the "unmoved mover" of the universe, and God: "For the actuality of thought ("nous") is life, and God is that actuality; and the essential actuality of God is life most good and eternal." Alexander of Aphrodisias, for example, equated this active intellect which is God with the one explained in "De Anima", while Themistius thought they could not be simply equated. (See below.)

Like Plato before him, Aristotle believes Anaxagoras' cosmic "nous" implies and requires the cosmos to have intentions or ends: "Anaxagoras makes the Good a principle as causing motion; for Mind ("nous") moves things, but moves them for some end, and therefore there must be some other Good—unless it is as we say; for on our view the art of medicine is in a sense health."

In the philosophy of Aristotle the soul (psyche) of a body is what makes it alive, and is its actualized form; thus, every living thing, including plant life, has a soul. The mind or intellect ("nous") can be described variously as a power, faculty, part, or aspect of the human soul. It should be noted that for Aristotle soul and "nous" are not the same. He did not rule out the possibility that "nous" might survive without the rest of the soul, as in Plato, but he specifically says that this immortal "nous" does not include any memories or anything else specific to an individual's life. In his "Generation of Animals" Aristotle specifically says that while other parts of the soul come from the parents, physically, the human "nous", must come from outside, into the body, because it is divine or godly, and it has nothing in common with the "energeia" of the body. This was yet another passage which Alexander of Aphrodisias would link to those mentioned above from "De Anima" and the "Metaphysics" in order to understand Aristotle's intentions.

Until the early modern era, much of the discussion which has survived today concerning "nous" or intellect, in Europe, Africa and the Middle East, concerned how to correctly interpret Aristotle and Plato. However, at least during the classical period, materialist philosophies, more similar to modern science, such as Epicureanism, were still relatively common also. The Epicureans believed that the bodily senses themselves were not the cause of error, but the interpretations can be. The term "prolepsis" was used by Epicureans to describe the way the mind forms general concepts from sense perceptions.

To the Stoics, more like Heraclitus than Anaxagoras, order in the cosmos comes from an entity called logos, the cosmic reason. But as in Anaxagoras this cosmic reason, like human reason but higher, is connected to the reason of individual humans. The Stoics however, did not invoke incorporeal causation, but attempted to explain physics and human thinking in terms of matter and forces. As in Aristotelianism, they explained the interpretation of sense data requiring the mind to be stamped or formed with ideas, and that people have shared conceptions that help them make sense of things ("koine ennoia"). "Nous" for them is soul "somehow disposed" ("pôs echon"), the soul being somehow disposed "pneuma", which is fire or air or a mixture. As in Plato, they treated "nous" as the ruling part of the soul.

Plutarch criticized the Stoic idea of "nous" being corporeal, and agreed with Plato that the soul is more divine than the body while "nous" (mind) is more divine than the soul. The mix of soul and body produces pleasure and pain; the conjunction of mind and soul produces reason which is the cause or the source of virtue and vice. (From: “On the Face in the Moon”)

Albinus was one of the earliest authors to equate Aristotle's "nous" as prime mover of the Universe, with Plato's Form of the Good.

Alexander of Aphrodisias was a Peripatetic (Aristotelian) and his "On the Soul" (referred to as "De anima" in its traditional Latin title), explained that by his interpretation of Aristotle, potential intellect in man, that which has no nature but receives one from the active intellect, is material, and also called the "material intellect" ("nous hulikos") and it is inseparable from the body, being "only a disposition" of it. He argued strongly against the doctrine of immortality. On the other hand, he identified the active intellect ("nous poietikos"), through whose agency the potential intellect in man becomes actual, not with anything from within people, but with the divine creator itself. In the early Renaissance his doctrine of the soul's mortality was adopted by Pietro Pomponazzi against the Thomists and the Averroists. For him, the only possible human immortality is an immortality of a detached human thought, more specifically when the "nous" has as the object of its thought the active intellect itself, or another incorporeal intelligible form.

Alexander was also responsible for influencing the development of several more technical terms concerning the intellect, which became very influential amongst the great Islamic philosophers, Al-Farabi, Avicenna, and Averroes.

Themistius, another influential commentator on this matter, understood Aristotle differently, stating that the passive or material intellect does "not employ a bodily organ for its activity, is wholly unmixed with the body, impassive, and separate [from matter]". This means the human potential intellect, and not only the active intellect, is an incorporeal substance, or a disposition of incorporeal substance. For Themistius, the human soul becomes immortal "as soon as the active intellect intertwines with it at the outset of human thought".

This understanding of the intellect was also very influential for Al-Farabi, Avicenna, and Averroes, and "virtually all Islamic and Jewish philosophers". On the other hand, concerning the active intellect, like Alexander and Plotinus, he saw this as a transcendent being existing above and outside man. Differently from Alexander, he did not equate this being with the first cause of the Universe itself, but something lower. However he equated it with Plato's Idea of the Good.

Of the later Greek and Roman writers Plotinus, the initiator of neoplatonism, is particularly significant. Like Alexander of Aphrodisias and Themistius, he saw himself as a commentator explaining the doctrines of Plato and Aristotle. But in his "Enneads" he went further than those authors, often working from passages which had been presented more tentatively, possibly inspired partly by earlier authors such as the neopythagorean Numenius of Apamea. Neoplatonism provided a major inspiration to discussion concerning the intellect in late classical and medieval philosophy, theology and cosmology.

In neoplatonism there exists several levels or "hypostases" of being, including the natural and visible world as a lower part.
This was based largely upon Plotinus' reading of Plato, but also incorporated many Aristotelian concepts, including the Unmoved Mover as "energeia". They also incorporated a theory of "anamnesis", or knowledge coming from the past lives of our immortal souls, like that found in some of Plato's dialogues.

Later Platonists distinguished a hierarchy of three separate manifestations of "nous", like Numenius of Apamea had. Notable later neoplatonists include Porphyry and Proclus.

Greek philosophy had an influence on the major religions that defined the Middle Ages, and one aspect of this was the concept of "nous".

Gnosticism was a late classical movement that incorporated ideas inspired by neoplatonism and neopythagoreanism, but which was more a syncretic religious movement than an accepted philosophical movement.

In Valentinianism, Nous is the first male Aeon. Together with his conjugate female Aeon, Aletheia (truth), he emanates from the Propator Bythos ( "Forefather Depths") and his co-eternal Ennoia ( "Thought") or Sigē ( "Silence"); and these four form the primordial Tetrad. Like the other male Aeons he is sometimes regarded as androgynous, including in himself the female Aeon who is paired with him. He is the Only Begotten; and is styled the Father, the Beginning of All, inasmuch as from him are derived immediately or mediately the remaining Aeons who complete the Ogdoad (eight), thence the Decad (ten), and thence the Dodecad (twelve); in all, thirty Aeons constitute the Pleroma.

He alone is capable of knowing the Propator; but when he desired to impart like knowledge to the other Aeons, was withheld from so doing by Sigē. When Sophia ("Wisdom"), youngest Aeon of the thirty, was brought into peril by her yearning after this knowledge, Nous was foremost of the Aeons in interceding for her. From him, or through him from the Propator, Horos was sent to restore her. After her restoration, Nous, according to the providence of the Propator, produced another pair, Christ and the Holy Spirit, "in order to give fixity and steadfastness () to the Pleroma." For this Christ teaches the Aeons to be content to know that the Propator is in himself incomprehensible, and can be perceived only through the Only Begotten (Nous).

A similar conception of Nous appears in the later teaching of the Basilideans, according to which he is the first begotten of the Unbegotten Father, and himself the parent of "Logos", from whom emanate successively "Phronesis", "Sophia", and "Dunamis". But in this teaching, Nous is identified with Christ, is named Jesus, is sent to save those that believe, and returns to Him who sent him, after a Passion which is apparent only, Simon of Cyrene being substituted for him on the cross. It is probable, however, that Nous had a place in the original system of Basilides himself; for his "Ogdoad", "the great Archon of the universe, the ineffable" is apparently made up of the five members named by Irenaeus (as above), together with two whom we find in Clement of Alexandria, "Dikaiosyne" and "Eirene", added to the originating Father.

The antecedent of these systems is that of Simon, of whose six "roots" emanating from the Unbegotten Fire, "Nous" is first. The correspondence of these "roots" with the first six "Aeons" that Valentinus derives from "Bythos", is noted by Hippolytus. Simon says in his "Apophasis Megalē",

To Nous and "Epinoia" correspond Heaven and Earth, in the list given by Simon of the six material counterparts of his six emanations. The identity of this list with the six material objects alleged by Herodotus to be worshipped by the Persians, together with the supreme place given by Simon to Fire as the primordial power, leads us to look to Iran for the origin of these systems in one aspect. In another, they connect themselves with the teaching of Pythagoras and of Plato.

According to the "Gospel of Mary", Jesus himself articulates the essence of "Nous":

During the Middle Ages, philosophy itself was in many places seen as opposed to the prevailing monotheistic religions, Islam, Christianity and Judaism. The strongest philosophical tradition for some centuries was amongst Islamic philosophers, who later came to strongly influence the late medieval philosophers of western Christendom, and the Jewish diaspora in the Mediterranean area. While there were earlier Muslim philosophers such as Al Kindi, chronologically the three most influential concerning the intellect were Al Farabi, Avicenna, and finally Averroes, a westerner who lived in Spain and was highly influential in the late Middle Ages amongst Jewish and Christian philosophers.

The exact precedents of Al Farabi's influential philosophical scheme, in which "nous" (Arabic "ʿaql") plays an important role, are no longer perfectly clear because of the great loss of texts in the Middle Ages which he would have had access to. He was apparently innovative in at least some points. He was clearly influenced by the same late classical world as neoplatonism, neopythagoreanism, but exactly how is less clear. Plotinus, Themistius and Alexander of Aphrodisias are generally accepted to have been influences. However while these three all placed the active intellect "at or near the top of the hierarchy of being", Al Farabi was clear in making it the lowest ranking in a series of distinct transcendental intelligences. He is the first known person to have done this in a clear way. He was also the first philosopher known to have assumed the existence of a causal hierarchy of celestial spheres, and the incorporeal intelligences parallel to those spheres. Al Farabi also fitted an explanation of prophecy into this scheme, in two levels. According to Davidson (p. 59):The lower of the two levels, labeled specifically as "prophecy" ("nubuwwa"), is enjoyed by men who have not yet perfected their intellect, whereas the higher, which Alfarabi sometimes specifically names "revelation" ("w-ḥ-y"), comes exclusively to those who stand at the stage of acquired intellect.
This happens in the imagination (Arabic "mutakhayyila"; Greek "phantasia"), a faculty of the mind already described by Aristotle, which al Farabi described as serving the rational part of the soul (Arabic "ʿaql"; Greek "nous"). This faculty of imagination stores sense perceptions ("maḥsūsāt"), disassembles or recombines them, creates figurative or symbolic images ("muḥākāt") of them which then appear in dreams, visualizes present and predicted events in a way different from conscious deliberation ("rawiyya"). This is under the influence, according to Al Farabi, of the active intellect. Theoretical truth can only be received by this faculty in a figurative or symbolic form, because the imagination is a physical capability and can not receive theoretical information in a proper abstract form. This rarely comes in a waking state, but more often in dreams. The lower type of prophecy is the best possible for the imaginative faculty, but the higher type of prophecy requires not only a receptive imagination, but also the condition of an "acquired intellect", where the human "nous" is in "conjunction" with the active intellect in the sense of God. Such a prophet is also a philosopher. When a philosopher-prophet has the necessary leadership qualities, he becomes philosopher-king.

In terms of cosmology, according to Davidson (p. 82) "Avicenna's universe has a structure virtually identical with the structure of Alfarabi's" but there are differences in details. As in Al Farabi, there are several levels of intellect, intelligence or "nous", each of the higher ones being associated with a celestial sphere. Avicenna however details three different types of effect which each of these higher intellects has, each "thinks" both the necessary existence and the possible being of the intelligence one level higher. And each "emanates" downwards the body and soul of its own celestial sphere, and also the intellect at the next lowest level. The active intellect, as in Alfarabi, is the last in the chain. Avicenna sees active intellect as the cause not only of intelligible thought and the forms in the "sublunar" world we people live, but also the matter. (In other words, three effects.)

Concerning the workings of the human soul, Avicenna, like Al Farabi, sees the "material intellect" or potential intellect as something that is not material. He believed the soul was incorporeal, and the potential intellect was a disposition of it which was in the soul from birth. As in Al Farabi there are two further stages of potential for thinking, which are not yet actual thinking, first the mind acquires the most basic intelligible thoughts which we can not think in any other way, such as "the whole is greater than the part", then comes a second level of derivative intelligible thoughts which could be thought. Concerning the actualization of thought, Avicenna applies the term "to two different things, to actual human thought, irrespective of the intellectual progress a man has made, and to actual thought when human intellectual development is complete", as in Al Farabi.

When reasoning in the sense of deriving conclusions from syllogisms, Avicenna says people are using a physical "cogitative" faculty ("mufakkira, fikra") of the soul, which can err. The human cogitative faculty is the same as the "compositive imaginative faculty ("mutakhayyila") in reference to the animal soul". But some people can use "insight" to avoid this step and derive conclusions directly by conjoining with the active intellect.

Once a thought has been learned in a soul, the physical faculties of sense perception and imagination become unnecessary, and as a person acquires more thoughts, their soul becomes less connected to their body. For Avicenna, different from the normal Aristotelian position, all of the soul is by nature immortal. But the level of intellectual development does affect the type of afterlife that the soul can have. Only a soul which has reached the highest type of conjunction with the active intellect can form a perfect conjunction with it after the death of the body, and this is a supreme "eudaimonia". Lesser intellectual achievement means a less happy or even painful afterlife.

Concerning prophecy, Avicenna identifies a broader range of possibilities which fit into this model, which is still similar to that of Al Farabi.

Averroes came to be regarded even in Europe as "the Commentator" to "the Philosopher", Aristotle, and his study of the questions surrounding the "nous" were very influential amongst Jewish and Christian philosophers, with some aspects being quite controversial. According to Herbert Davidson, Averroes' doctrine concerning "nous" can be divided into two periods. In the first, neoplatonic emanationism, not found in the original works of Aristotle, was combined with a naturalistic explanation of the human material intellect. "It also insists on the material intellect's having an active intellect as a direct object of thought and conjoining with the active intellect, notions never expressed in the Aristotelian canon." It was this presentation which Jewish philosophers such as Moses Narboni and Gersonides understood to be Averroes'. In the later model of the universe, which was transmitted to Christian philosophers, Averroes "dismisses emanationism and explains the generation of living beings in the sublunar world naturalistically, all in the name of a more genuine Aristotelianism. Yet it abandons the earlier naturalistic conception of the human material intellect and transforms the material intellect into something wholly un-Aristotelian, a single transcendent entity serving all mankind. It nominally salvages human conjunction with the active intellect, but in words that have little content."

This position, that humankind shares one active intellect, was taken up by Parisian philosophers such as Siger of Brabant, but also widely rejected by philosophers such as Albertus Magnus, Thomas Aquinas, Ramon Lull, and Duns Scotus. Despite being widely considered heretical, the position was later defended by many more European philosophers including John of Jandun, who was the primary link bringing this doctrine from Paris to Bologna. After him this position continued to be defended and also rejected by various writers in northern Italy. In the 16th century it finally became a less common position after the renewal of an "Alexandrian" position based on that of Alexander of Aphrodisias, associated with Pietro Pomponazzi.

The Christian New Testament makes mention of the "nous" or "noos", generally translated in modern English as "mind", but also showing a link to God's will or law:

In the writings of the Christian fathers a sound or pure "nous" is considered essential to the cultivation of wisdom.

While philosophical works were not commonly read or taught in the early Middle Ages in most of Europe, the works of authors like Boethius and Augustine of Hippo formed an important exception. Both were influenced by neoplatonism, and were amongst the older works that were still known in the time of the Carolingian Renaissance, and the beginnings of Scholasticism.

In his early years Augustine was heavily influenced by Manichaeism and afterwards by the Neo-Platonism of Plotinus. After his conversion to Christianity and baptism (387), he developed his own approach to philosophy and theology, accommodating a variety of methods and different perspectives.

Augustine used neoplatonism selectively. He used both the neoplatonic "Nous", and the Platonic Form of the Good (or "The Idea of the Good") as equivalent terms for the Christian God, or at least for one particular aspect of God. For example, God, "nous", can act directly upon matter, and not only through souls, and concerning the souls through which it works upon the world experienced by humanity, some are treated as angels.

Scholasticism becomes more clearly defined much later, as the peculiar native type of philosophy in medieval catholic Europe. In this period, Aristotle became "the Philosopher", and scholastic philosophers, like their Jewish and Muslim contemporaries, studied the concept of the "intellectus" on the basis not only of Aristotle, but also late classical interpreters like Augustine and Boethius. A European tradition of new and direct interpretations of Aristotle developed which was eventually strong enough to argue with partial success against some of the interpretations of Aristotle from the Islamic world, most notably Averroes' doctrine of their being one "active intellect" for all humanity. Notable "Catholic" (as opposed to Averroist) Aristotelians included Albertus Magnus and Thomas Aquinas, the founder Thomism, which exists to this day in various forms. Concerning the "nous", Thomism agrees with those Aristotelians who insist that the intellect is immaterial and separate from any bodily organs, but as per Christian doctrine, the whole of the human soul is immortal, not only the intellect.

The human "nous" in Eastern Orthodox Christianity is the "eye of the heart or soul" or the "mind of the heart". The soul of man, is created by God in His image, man's soul is intelligent and noetic. Saint Thalassius of Syria wrote that God created beings "with a capacity to receive the Spirit and to attain knowledge of Himself; He has brought into existence the senses and sensory perception to serve such beings". Eastern Orthodox Christians hold that God did this by creating mankind with intelligence and noetic faculties.

Human reasoning is not enough: there will always remain an "irrational residue" which escapes analysis and which can not be expressed in concepts: it is this unknowable depth of things, that which constitutes their true, indefinable essence that also reflects the origin of things in God. In Eastern Christianity it is by faith or intuitive truth that this component of an objects existence is grasped. Though God through his energies draws us to him, his essence remains inaccessible. The operation of faith being the means of free will by which mankind faces the future or unknown, these noetic operations contained in the concept of insight or "noesis". Faith ("pistis") is therefore sometimes used interchangeably with "noesis" in Eastern Christianity.

Angels have intelligence and "nous", whereas men have reason, both "logos" and "dianoia", "nous" and sensory perception. This follows the idea that man is a microcosm and an expression of the whole creation or macrocosmos. The human "nous" was darkened after the Fall of Man (which was the result of the rebellion of reason against the "nous"), but after the purification (healing or correction) of the "nous" (achieved through ascetic practices like hesychasm), the human "nous" (the "eye of the heart") will see God's uncreated Light (and feel God's uncreated love and beauty, at which point the nous will start the unceasing prayer of the heart) and become illuminated, allowing the person to become an orthodox theologian.

In this belief, the soul is created in the image of God. Since God is Trinitarian, Mankind is "Nous", reason, both "logos" and "dianoia", and Spirit. The same is held true of the soul (or heart): it has "nous", word and spirit. To understand this better first an understanding of Saint Gregory Palamas's teaching that man is a representation of the trinitarian mystery should be addressed. This holds that God is not meant in the sense that the Trinity should be understood anthropomorphically, but man is to be understood in a triune way. Or, that the Trinitarian God is not to be interpreted from the point of view of individual man, but man is interpreted on the basis of the Trinitarian God. And this interpretation is revelatory not merely psychological and human. This means that it is only when a person is within the revelation, as all the saints lived, that he can grasp this understanding completely (see "theoria"). The second presupposition is that mankind has and is composed of "nous", word and spirit like the trinitarian mode of being. Man's "nous", word and spirit are not hypostases or individual existences or realities, but activities or energies of the soul - whereas in the case with God or the Persons of the Holy Trinity, each are indeed hypostases. So these three components of each individual man are 'inseparable from one another' but they do not have a personal character" when in speaking of the being or ontology that is mankind. The "nous" as the eye of the soul, which some Fathers also call the heart, is the centre of man and is where true (spiritual) knowledge is validated. This is seen as true knowledge which is "implanted in the "nous" as always co-existing with it".

The so-called "early modern" philosophers of western Europe in the 17th and 18th centuries established arguments which led to the establishment of modern science as a methodical approach to improve the welfare of humanity by learning to control nature. As such, speculation about metaphysics, which cannot be used for anything practical, and which can never be confirmed against the reality we experience, started to be deliberately avoided, especially according to the so-called "empiricist" arguments of philosophers such as Bacon, Hobbes, Locke and Hume. The Latin motto "nihil in intellectu nisi prius fuerit in sensu" (nothing in the intellect without first being in the senses) has been described as the "guiding principle of empiricism" in the "Oxford Dictionary of Philosophy". (This was in fact an old Aristotelian doctrine, which they took up, but as discussed above Aristotelians still believed that the senses on their own were not enough to explain the mind.)

These philosophers explain the intellect as something developed from experience of sensations, being interpreted by the brain in a physical way, and nothing else, which means that absolute knowledge is impossible. For Bacon, Hobbes and Locke, who wrote in both English and Latin, "intellectus" was translated as "understanding". Far from seeing it as secure way to perceive the truth about reality, Bacon, for example, actually named the "intellectus" in his "Novum Organum", and the proœmium to his "Great Instauration", as a major source of wrong conclusions, because it is biased in many ways, for example towards over-generalizing. For this reason, modern science should be methodical, in order not to be misled by the weak human intellect. He felt that lesser known Greek philosophers such as Democritus "who did not suppose a mind or reason in the frame of things", have been arrogantly dismissed because of Aristotelianism leading to a situation in his time wherein "the search of the physical causes hath been neglected, and passed in silence". The intellect or understanding was the subject of Locke's "Essay Concerning Human Understanding".

These philosophers also tended not to emphasize the distinction between reason and intellect, describing the peculiar universal or abstract definitions of human understanding as being man-made and resulting from reason itself. Hume even questioned the distinctness or peculiarity of human understanding and reason, compared to other types of associative or imaginative thinking found in some other animals. In modern science during this time, Newton is sometimes described as more empiricist compared to Leibniz.

On the other hand, into modern times some philosophers have continued to propose that the human mind has an in-born ("a priori") ability to know the truth conclusively, and these philosophers have needed to argue that the human mind has direct and intuitive ideas about nature, and this means it can not be limited entirely to what can be known from sense perception. Amongst the early modern philosophers, some such as Descartes, Spinoza, Leibniz, and Kant, tend to be distinguished from the empiricists as rationalists, and to some extent at least some of them are called idealists, and their writings on the intellect or understanding present various doubts about empiricism, and in some cases they argued for positions which appear more similar to those of medieval and classical philosophers.

The first in this series of modern rationalists, Descartes, is credited with defining a "mind-body problem" which is a major subject of discussion for university philosophy courses. According to the presentation his , the human mind and body are different in kind, and while Descartes agrees with Hobbes for example that the human body works like a clockwork mechanism, and its workings include memory and imagination, the real human is the thinking being, a soul, which is not part of that mechanism. Descartes explicitly refused to divide this soul into its traditional parts such as intellect and reason, saying that these things were indivisible aspects of the soul. Descartes was therefore a dualist, but very much in opposition to traditional Aristotelian dualism. In his he deliberately uses traditional terms and states that his active faculty of giving ideas to his thought must be corporeal, because the things perceived are clearly external to his own thinking and corporeal, while his passive faculty must be incorporeal (unless God is deliberately deceiving us, and then in this case the active faculty would be from God). This is the opposite of the traditional explanation found for example in Alexander of Aphrodisias and discussed above, for whom the passive intellect is material, while the active intellect is not. One result is that in many Aristotelian conceptions of the "nous", for example that of Thomas Aquinas, the senses are still a source of all the intellect's conceptions. However, with the strict separation of mind and body proposed by Descartes, it becomes possible to propose that there can be thought about objects never perceived with the body's senses, such as a thousand sided geometrical figure. Gassendi objected to this distinction between the imagination and the intellect in Descartes. Hobbes also objected, and according to his own philosophical approach asserted that the "triangle in the mind comes from the triangle we have seen" and "essence in so far as it is distinguished from existence is nothing else than a union of names by means of the verb is". Descartes, in his reply to this objection insisted that this traditional distinction between essence and existence is "known to all".

His contemporary Blaise Pascal, criticised him in similar words to those used by Plato's Socrates concerning Anaxagoras, discussed above, saying that "I cannot forgive Descartes; in all his philosophy, Descartes did his best to dispense with God. But Descartes could not avoid prodding God to set the world in motion with a snap of his lordly fingers; after that, he had no more use for God."

Descartes argued that when the intellect does a job of helping people interpret what they perceive, not with the help of an intellect which enters from outside, but because each human mind comes into being with innate God-given ideas, more similar then, to Plato's theory of "anamnesis", only not requiring reincarnation. Apart from such examples as the geometrical definition of a triangle, another example is the idea of God, according to the 3rd "Meditation". Error, according to the 4th "Meditation", comes about because people make judgments about things which are not in the intellect or understanding. This is possible because the human will, being free, is not limited like the human intellect.

Spinoza, though considered a Cartesian and a rationalist, rejected Cartesian dualism and idealism. In his "pantheistic" approach, explained for example in his "Ethics", God is the same as nature, the human intellect is just the same as the human will. The divine intellect of nature is quite different from human intellect, because it is finite, but Spinoza does accept that the human intellect is a part of the infinite divine intellect.

Leibniz, in comparison to the guiding principle of the empiricists described above, added some words "nihil in intellectu nisi prius fuerit in sensu", nisi intellectus ipsi ("nothing in the intellect without first being in the senses" "except the intellect itself"). Despite being at the forefront of modern science, and modernist philosophy, in his writings he still referred to the active and passive intellect, a divine intellect, and the immortality of the active intellect.

Berkeley, partly in reaction to Locke, also attempted to reintroduce an "immaterialism" into early modern philosophy (later referred to as "subjective idealism" by others). He argued that individuals can only know sensations and ideas of objects, not abstractions such as "matter", and that ideas depend on perceiving minds for their very existence. This belief later became immortalized in the dictum, "esse est percipi" ("to be is to be perceived"). As in classical and medieval philosophy, Berkeley believed understanding had to be explained by divine intervention, and that all our ideas are put in our mind by God.

Hume accepted some of Berkeley's corrections of Locke, but in answer insisted, as had Bacon and Hobbes, that absolute knowledge is not possible, and that all attempts to show how it could be possible have logical problems. Hume's writings remain highly influential on all philosophy afterwards, and are for example considered by Kant to have shaken him from an intellectual slumber.

Kant, a turning point in modern philosophy, agreed with some classical philosophers and Leibniz that the intellect itself, although it needed sensory experience for understanding to begin, needs something else in order to make sense of the incoming sense information. In his formulation the intellect ("Verstand") has "a priori" or innate principles which it has before thinking even starts. Kant represents the starting point of German idealism and a new phase of modernity, while empiricist philosophy has also continued beyond Hume to the present day.

One of the results of the early modern philosophy has been the increasing creation of specialist fields of science, in areas that were once considered part of philosophy, and infant cognitive development and perception now tend to be discussed now more within the sciences of psychology and neuroscience than in philosophy.

Modern mainstream thinking on the mind is not dualist, and sees anything innate in the mind as being a result of genetic and developmental factors which allow the mind to develop. Overall it accepts far less innate "knowledge" (or clear pre-dispositions to particular types of knowledge) than most of the classical and medieval theories derived from philosophers such as Plato, Aristotle, Plotinus and Al Farabi.

Apart from discussions about the history of philosophical discussion on this subject, contemporary philosophical discussion concerning this point has continued concerning what the ethical implications are of the different alternatives still considered likely.

Classical conceptions of nous are still discussed seriously in theology. There is also still discussion of classical nous in non-mainstream metaphysics or spiritualism, such as Noetics, promoted for example by the Institute of Noetic Sciences.



</doc>
<doc id="32116915" url="https://en.wikipedia.org/wiki?curid=32116915" title="Faith literate">
Faith literate

Faith literate describes the ability of an individual to become knowledgeable of other religions and faith other than the one a person believes in.

A faith literate individual understands the key effects of each religion/belief system by means of the values, attitudes and influence it causes in individuals, families and communities. Faith literates believe in recognizing religious and secular worldviews in practice and thoughts and take hold of what makes each religion/belief system what it is. IT involves understanding and knowing the fundamental formative attribute of all religions.

In the United Kingdom, there are institutes and consultancies being set up that offer religious understanding training for the public and private sectors. Even the government is also committed to a program of faith literacy in the public sector. This is aimed to be significantly enhance organizational multiplicity among other things. Faith literacy is also intended to facilitate a move beyond the functional levels of conversation.

Tony Blair, former British Prime Minister, also mentioned in an interview that he reads Quran and Bible every day since it is crucial to be faith literate in a globalised world like ours. In Uganda, the Bishop of Kigezi also urged the government patrons to be more ‘faith literate’. Realizing the importance of this concept, the Economic and Social Research Council in UK started a three-year research paper in faith literacy.



</doc>
<doc id="3002191" url="https://en.wikipedia.org/wiki?curid=3002191" title="Network of practice">
Network of practice

Network of practice (often abbreviated as NoP) is a concept originated by John Seely Brown and Paul Duguid. This concept, related to the work on communities of practice by Jean Lave and Etienne Wenger, refers to the overall set of various types of informal, emergent social networks that facilitate information exchange between individuals with practice-related goals. In other words, networks of practice range from communities of practice where learning occurs to electronic networks of practice (often referred to as virtual or electronic communities).

To further define the concept, first the term network implies a set of individuals who are connected through social relationships, whether they be strong or weak. Terms such as community tend to denote a stronger form of relationship, but networks refer to all networks of social relationships, be they weak or strong. Second, the term practice represents the substrate that connects individuals in their networks. The principal ideas are that practice implies the actions of individuals and groups when conducting their work, e.g., the practice of software engineers, journalists, educators, etc., and that practice involves interaction among individuals.

What distinguishes a network of practice from other networks is that the primary reason for the emergence of relationships within a network of practice is that individuals interact through information exchange in order to perform their work, asking for and sharing knowledge with each other. A network of practice can be distinguished from other networks that emerge due to other factors, such as interests in common hobbies or discussing sports while taking the same bus to work, etc. Finally, practice need not necessarily be restricted to include those within one occupation or functional discipline. Rather it may include individuals from a variety of occupations; thus, the term, practice, is more appropriate than others such as occupation.

As indicated above, networks of practice incorporate a range of informal, emergent networks, from communities of practice to electronic networks of practice. In line with Lave & Wenger's original work (1991), Brown & Duguid propose that communities of practice are a localized and specialized subset of networks of practice, typically consisting of strong ties linking individuals engaged in a shared practice who typically interact in face-to-face situations. At the opposite end of the spectrum are electronic networks of practice, which are often referred to as virtual or electronic communities and consisting of weak ties. In electronic networks of practice, individuals may never get to know one another or meet face-to-face, and they generally coordinate through means such as blogs, electronic mailing lists, or bulletin boards.

In contrast to the use of formal controls to support knowledge exchange often used in formal work groups, such as contractual obligation, organizational hierarchies, monetary incentives, or mandated rules, networks of practice promote knowledge flows along lines of practice through informal social networks. Therefore, one way to distinguish between networks of practice and work groups created through formal organizational mandate is by the nature of the "control mechanisms".

A second group of distinguishing properties refers to their "composition". Networks of practice and formal work groups vary in terms of their "size" since networks of practice may range from a few select individuals to very large, open electronic networks consisting of thousands of participants while groups are generally smaller. They also vary in terms of "who can participate". Work groups and virtual teams typically consist of members who are formally designated and assigned. In contrast, networks of practice consist of volunteers without formal restrictions placed on membership.

Finally, networks of practice and formal work groups vary in terms of "expectations about participation". In formal work groups and virtual teams, participation is jointly determined and members are expected to achieve a specific work goal. Participation in communities of practice is jointly determined, such that individuals generally approach specific others for help. In electronic networks of practice, participation is individually determined; knowledge seekers have no control over who responds to their questions or the quality of the responses. In turn, knowledge contributors have no assurances that seekers will understand the answer provided or be willing to reciprocate the favor.




</doc>
<doc id="3947034" url="https://en.wikipedia.org/wiki?curid=3947034" title="General knowledge">
General knowledge

General knowledge has been defined in differential psychology as "culturally valued knowledge communicated by a range of non-specialist media" and encompassing a wide subject range. This definition excludes highly specialized learning that can only be obtained with extensive training and information confined to a single medium. General knowledge is an important component of crystallized intelligence. It is strongly associated with general intelligence and with openness to experience.

Studies have found that people who are highly knowledgeable in a particular domain tend to be knowledgeable in many. General knowledge is thought to be supported by long-term semantic memory ability.
Differential psychology researchers define general knowledge as "culturally valued knowledge communicated by a range of non-specialist media." The scope of this definition includes all areas of knowledge available to laypersons without requiring extensive training. The definition excludes "ephemera", or information confined to a single medium, such as television sitcoms. Researchers have identified 20 domains of knowledge that meet the above criteria:


Researchers have acknowledged that other domains of general knowledge may exist. Factor analysis suggested that the 20 domains could be categorised into six factors: current affairs, fashion, family, physical health and recreation, arts, and science. All six of these factors were highly intercorrelated (i.e. people who scored high in a particular domain tended to score highly in most other domains) and were all related to a single higher-order general knowledge factor. The existence of a single general factor suggests that individual differences across a range of knowledge domains may have both common causes and specific influences; interest in a particular area and educational course content appear to be important contributors.

High scorers on tests of general knowledge tend to also score highly on intelligence tests. IQ has been found to robustly predict general knowledge scores even after accounting for differences in age, sex, and five factor model personality traits. In the Cattell-Horn-Carroll theory of intelligence, general knowledge is considered a component of crystallized intelligence. Standardized IQ tests may therefore include measures of general knowledge, such as in the information subtest of the Wechsler Adult Intelligence Scale.

General knowledge is also moderately associated with verbal ability, though only weakly or not at all with numerical and spatial ability. As with crystallized intelligence, general knowledge has been found to increase with age.

Research has found positive relationships between different domains of knowledge, suggesting that individuals who are highly knowledgeable in a particular domain usually have a good long-term memory for factual information in general. Due to the positive intercorrelations between knowledge domains, individual differences in general knowledge may reflect differences in ability to retrieve information from long-term semantic memory. A general factor of long-term semantic memory could be explained by the existence of an underlying neurophysiological process responsible for retaining information in long-term memory. Individual differences in the efficiency of such processes might explain why all domains of semantic memory appear to be intercorrelated.

People high in general knowledge tend to be highly open to new experiences and in typical intellectual engagement. The relationship between openness to experience and general knowledge remains robust even when IQ is taken into account. People high in openness may be more motivated to engage in intellectual pursuits that increase their knowledge. Relationships between general knowledge and other five factor model traits tend to be weak and inconsistent. Though one study found that extraversion and neuroticism were negatively correlated with general knowledge, others found that they were unrelated. Inconsistent results have also been found for conscientiousness.

Research has found that on average males tend to score higher than females on tests of overall general knowledge and in most domains of knowledge tested. Males also score higher than females on the information subtest of the WAIS and the WISC, with small to medium effect sizes. In a comparison between male and female university students in 19 domains of academic knowledge, males had greater knowledge in 14 domains, especially in physical science and technology, but also in humanities and civics. A general knowledge composite across all 19 tests showed a male advantage of medium effect size. A study of university students in Northern Ireland found that males scored higher in general knowledge than females, as well as in 12 of 19 specific knowledge domains. Females scored moderately higher than males in medicine and cookery. The authors of this study suggested that this male advantage most likely reflects differences in interests rather than differences in verbal or memory ability. Similar results were found in a study of German high school students.
Male advantages in general knowledge are not attributable to differences between males and females in reasoning ability (i.e. fluid intelligence), socio-economic status, or exposure to school course content. Although males appear to have greater general knowledge, there is some evidence that females tend to have an advantage in "autobiographical" knowledge, or memory of personal experiences. While general knowledge is supported by semantic memory, autobiographical knowledge is supported by episodic memory, which is not tested in intelligence tests and tends to be difficult to measure because of the uniquely personal nature of such memories.

A number of studies have assessed whether performance on a general knowledge test can predict achievement in particular areas, namely in academics, proofreading, and creativity.

General knowledge has been found to predict exam results in a study of British schoolchildren. The study examined cognitive ability and personality predictors of exam performance and found that general knowledge was positively correlated with GCSE English, mathematics, and overall exam results. General knowledge test scores predicted exam results, even after controlling for IQ, five factor model personality traits, and learning styles.

General knowledge has been found to robustly predict proofreading skills in university students. A study found that proofreading had a larger correlation with general knowledge than with general intelligence, verbal reasoning, or openness to experience. In a multiple regression analysis using general knowledge, general intelligence, verbal reasoning, five factor personality traits, and learning styles as predictors, only general knowledge was a significant predictor.

General knowledge has been found to have weak associations with measures of creativity. In a study examining contributions of personality and intelligence to creativity, general knowledge was positively correlated with tests of divergent thinking, but was unrelated to a biographical measure of creative achievement, self-rated creativity, or a composite measure of creativity. The relationship between general knowledge and divergent thinking became non-significant when controlling for fluid intelligence.

Many game shows use general knowledge questions. Game shows such as "Who Wants to Be a Millionaire?" and "Fifteen to One" centre their questions on general knowledge, while others shows focus questions more on specific subjects. Some shows ask questions both on specific subjects and on general knowledge, including "Eggheads" and ""Mastermind"". In "Mastermind", contestants choose their own "specialist subject" before answering general knowledge questions, whereas in "Eggheads" the subjects are chosen at random.


</doc>
<doc id="1409006" url="https://en.wikipedia.org/wiki?curid=1409006" title="Common knowledge (logic)">
Common knowledge (logic)

Common knowledge is a special kind of knowledge for a group of agents. There is "common knowledge" of "p" in a group of agents "G" when all the agents in "G" know "p", they all know that they know "p", they all know that they all know that they know "p", and so on "ad infinitum".

The concept was first introduced in the philosophical literature by David Kellogg Lewis in his study "Convention" (1969). The sociologist Morris Friedell defined common knowledge in a 1969 paper. It was first given a mathematical formulation in a set-theoretical framework by Robert Aumann (1976). Computer scientists grew an interest in the subject of epistemic logic in general – and of common knowledge in particular – starting in the 1980s. There are numerous puzzles based upon the concept which have been extensively investigated by mathematicians such as John Conway.

The philosopher Stephen Schiffer, in his book "Meaning", independently developed a notion he called "mutual knowledge" which functions quite similarly to Lewis's "common knowledge".

The idea of common knowledge is often introduced by some variant of the following puzzle:

On an island, there are "k" people who have blue eyes, and the rest of the people have green eyes. At the start of the puzzle, no one on the island ever knows their own eye color. By rule, if a person on the island ever discovers they have blue eyes, that person must leave the island at dawn; anyone not making such a discovery always sleeps until after dawn. On the island, each person knows every other person's eye color, there are no reflective surfaces, and there is no communication of eye color.

At some point, an outsider comes to the island, calls together all the people on the island, and makes the following public announcement: "At least one of you has blue eyes". The outsider, furthermore, is known by all to be truthful, and all know that all know this, and so on: it is common knowledge that he is truthful, and thus it becomes common knowledge that there is at least one islander who has blue eyes. The problem: assuming all persons on the island are completely logical and that this too is common knowledge, what is the eventual outcome?

The answer is that, on the "k"th dawn after the announcement, all the blue-eyed people will leave the island.

The solution can be seen with an inductive argument. If "k" = 1 (that is, there is exactly one blue-eyed person), the person will recognize that they alone have blue eyes (by seeing only green eyes in the others) and leave at the first dawn. If "k" = 2, no one will leave at the first dawn. The two blue-eyed people, seeing only one person with blue eyes, "and" that no one left on the 1st dawn (and thus that "k" > 1), will leave on the second dawn. Inductively, it can be reasoned that no one will leave at the first "k" − 1 dawns if and only if there are at least "k" blue-eyed people. Those with blue eyes, seeing "k" − 1 blue-eyed people among the others and knowing there must be at least "k", will reason that they must have blue eyes and leave.

What's most interesting about this scenario is that, for "k" > 1, the outsider is only telling the island citizens what they already know: that there are blue-eyed people among them. However, before this fact is announced, the fact is not "common knowledge".

For "k" = 2, it is merely "first-order" knowledge. Each blue-eyed person knows that there is someone with blue eyes, but each blue eyed person does "not" know that the other blue-eyed person has this same knowledge.

For "k" = 3, it is "second order" knowledge. Each blue-eyed person knows that a second blue-eyed person knows that a third person has blue eyes, but no one knows that there is a "third" blue-eyed person with that knowledge, until the outsider makes his statement.

In general: For "k" > 1, it is "("k" − 1)th order" knowledge. Each blue-eyed person knows that a second blue-eyed person knows that a third blue-eyed person knows that... (repeat for a total of "k" − 1 levels) a "k"th person has blue eyes, but no one knows that there is a ""k"th" blue-eyed person with that knowledge, until the outsider makes his statement. The notion of "common knowledge" therefore has a palpable effect. Knowing that everyone knows does make a difference. When the outsider's public announcement (a fact already known to all) becomes common knowledge, the blue-eyed people on this island eventually deduce their status, and leave.

Common knowledge can be given a logical definition in multi-modal logic systems in which the modal operators are interpreted epistemically. At the propositional level, such systems are extensions of propositional logic. The extension consists of the introduction of a group "G" of "agents", and of "n" modal operators "K" (with "i" = 1, ..., "n") with the intended meaning that "agent "i" knows." Thus "K formula_1" (where formula_1 is a formula of the calculus) is read "agent "i" knows formula_1." We can define an operator "E" with the intended meaning of "everyone in group "G" knows" by defining it with the axiom

By abbreviating the expression formula_5 with formula_6 and defining formula_7, we could then define common knowledge with the axiom

There is however a complication. The languages of epistemic logic are usually "finitary", whereas the axiom above defines common knowledge as an infinite conjunction of formulas, hence not a well-formed formula of the language. To overcome this difficulty, a "fixed-point" definition of common knowledge can be given. Intuitively, common knowledge is thought of as the fixed point of the "equation" formula_9. In this way, it is possible to find a formula formula_10 implying formula_11 from which, in the limit, we can infer common knowledge of formula_1.

This "syntactic" characterization is given semantic content through so-called "Kripke structures". A Kripke structure is given by (i) a set of states (or possible worlds) "S", (ii) "n" "accessibility relations" formula_13, defined on formula_14, intuitively representing what states agent "i" considers possible from any given state, and (iii) a valuation function formula_15 assigning a truth value, in each state, to each primitive proposition in the language. The semantics for the knowledge operator is given by stipulating that formula_16 is true at state "s" iff formula_1 is true at "all" states "t" such that formula_18. The semantics for the common knowledge operator, then, is given by taking, for each group of agents "G", the reflexive and transitive closure of the formula_19, for all agents "i" in "G", call such a relation formula_20, and stipulating that formula_21 is true at state "s" iff formula_1 is true at "all" states "t" such that formula_23.

Alternatively (yet equivalently) common knowledge can be formalized using set theory (this was the path taken by the Nobel laureate Robert Aumann in his seminal 1976 paper). We will start with a set of states "S". We can then define an event "E" as a subset of the set of states "S". For each agent "i", define a partition on "S", "P". This partition represents the state of knowledge of an agent in a state. In state "s", agent "i" knows that one of the states in "P"("s") obtains, but not which one. (Here "P"("s") denotes the unique element of "P" containing "s". Note that this model excludes cases in which agents know things that are not true.)

We can now define a knowledge function "K" in the following way:

That is, "K"("e") is the set of states where the agent will know that event "e" obtains. It is a subset of "e".

Similar to the modal logic formulation above, we can define an operator for the idea that "everyone knows "e"".

As with the modal operator, we will iterate the "E" function, formula_26 and formula_27. Using this we can then define a common knowledge function,

The equivalence with the syntactic approach sketched above can easily be seen: consider an Aumann structure as the one just defined. We can define a correspondent Kripke structure by taking (i) the same space "S", (ii) accessibility relations formula_19 that define the equivalence classes corresponding to the partitions formula_30, and (iii) a valuation function such that it yields value "true" to the primitive proposition "p" in all and only the states "s" such that formula_31, where formula_32 is the event of the Aumann structure corresponding to the primitive proposition "p". It is not difficult to see that the common knowledge accessibility function formula_20 defined in the previous section corresponds to the finest common coarsening of the partitions formula_30 for all formula_35, which is the finitary characterization of common knowledge also given by Aumann in the 1976 article.

Common knowledge was used by David Lewis in his pioneering game-theoretical account of convention. In this sense, common knowledge is a concept still central for linguists and philosophers of language (see Clark 1996) maintaining a Lewisian, conventionalist account of language.

Robert Aumann introduced a set theoretical formulation of common knowledge (theoretically equivalent to the one given above) and proved the so-called agreement theorem through which: if two agents have common prior probability over a certain event, and the posterior probabilities are common knowledge, then such posterior probabilities are equal. A result based on the agreement theorem and proven by Milgrom shows that, given certain conditions on market efficiency and information, speculative trade is impossible.

The concept of common knowledge is central in game theory. For several years it has been thought that the assumption of common knowledge of rationality for the players in the game was fundamental. It turns out (Aumann and Brandenburger 1995) that, in 2-player games, common knowledge of rationality is not needed as an epistemic condition for Nash equilibrium strategies.

Computer scientists use languages incorporating epistemic logics (and common knowledge) to reason about distributed systems. Such systems can be based on logics more complicated than simple propositional epistemic logic, see Wooldridge "Reasoning about Artificial Agents", 2000 (in which he uses a first-order logic incorporating epistemic and temporal operators) or van der Hoek et al. "Alternating Time Epistemic Logic".

In his 2007 book, "The Stuff of Thought: Language as a Window into Human Nature," Steven Pinker uses the notion of common knowledge to analyze the kind of indirect speech involved in innuendoes.






</doc>
<doc id="10969154" url="https://en.wikipedia.org/wiki?curid=10969154" title="Bildung">
Bildung

Bildung (, ""education, formation, etc."") refers to the German tradition of self-cultivation (as related to the German for: creation, image, shape), wherein philosophy and education are linked in a manner that refers to a process of both personal and cultural maturation. This maturation is described as a harmonization of the individual's mind and heart and in a unification of selfhood and identity within the broader society, as evidenced with the literary tradition of "Bildungsroman".

In this sense, the process of harmonization of mind, heart, selfhood and identity is achieved through personal transformation, which presents a challenge to the individual's accepted beliefs. In Hegel's writings, the challenge of personal growth often involves an agonizing alienation from one's "natural consciousness" that leads to a reunification and development of the self. Similarly, although social unity requires well-formed institutions, it also requires a diversity of individuals with the freedom (in the positive sense of the term) to develop a wide-variety of talents and abilities and this requires personal agency. However, rather than an end state, both individual and social unification is a process that is driven by unrelenting negations.

In this sense, education involves the shaping of the human being with regard to his/her own humanity as well as his/her innate intellectual skills. So, the term refers to a process of becoming that can be related to a process of becoming within Existentialism.

The term "Bildung" also corresponds to the Humboldtian model of higher education from the work of Prussian philosopher and educational administrator Wilhelm von Humboldt (1767–1835). Thus, in this context, the concept of education becomes a lifelong process of human development, rather than mere training in gaining certain external knowledge or skills. Such training in skills is known by the German words "Erziehung", and "Ausbildung". "Bildung" in contrast is seen as a process wherein an individual's spiritual and cultural sensibilities as well as life, personal and social skills are in process of continual expansion and growth. Bildung is seen as a way to become more free due to higher self-reflection. Von Humboldt wrote with respect to "Bildung" in 1793/1794: "Education ["Bildung"], truth and virtue" must be disseminated to such an extent that the "concept of mankind" takes on a great and dignified form in each individual (GS, I, p. 284). However, this shall be achieved personally by each individual, who must "absorb the great mass of material offered to him by the world around him and by his inner existence, using all the possibilities of his receptiveness; he must then reshape that material with all the energies of his own activity and appropriate it to himself so as to create an interaction between his own personality and nature in a most general, active and harmonious form".Most explicitly in Hegel's writings, the "Bildung" tradition rejects the pre-Kantian metaphysics of being for a post-Kantian metaphysics of experience that rejects universal narratives. Much of Hegel's writings were about the nature of education (both "Bildung" and "Erziehung"), reflecting his own role as a teacher and administrator in German secondary schools, and in his more general writings.
A more contemporary view was developed by Tony Waters: "Bildung", I discovered in my 2 years in Germany, is an organizing cultural principle for German higher education that trumps both careerism and disciplinary silos. It is generally translated as "education", but in fact it means more—dictionary definitions often refer to "self-cultivation", "philosophy", "personal and cultural maturation" and even "existentialism". Bildung is the cry of the land of poets and thinkers against the demands of credentialism, professionalism, careerism and the financial temptations dangled to graduating students.In this way, fulfillment is achieved through practical activity that promotes the development of one's own individual talents and abilities which in turn lead to the development of one's society. In this way, Bildung does not simply accept the socio-political status quo, but rather it includes the ability to engage in a critique of one's society, and to ultimately challenge the society to actualize its own highest ideals.




</doc>
<doc id="33788375" url="https://en.wikipedia.org/wiki?curid=33788375" title="Mutual knowledge (logic)">
Mutual knowledge (logic)

Mutual knowledge is a fundamental concept about information in game theory, (epistemic) logic, and epistemology. An event is mutual knowledge if all agents know that the event occurred. However, mutual knowledge by itself implies nothing about what agents know about other agents' knowledge: i.e. it is possible that an event is mutual knowledge but that each agent is unaware that the other agents know it has occurred. Common knowledge is a related but stronger notion; any event that is common knowledge is also mutual knowledge.

The philosopher Stephen Schiffer, in his book "Meaning", developed a notion he called "mutual knowledge" which functions quite similarly to David K. Lewis's "common knowledge".



</doc>
<doc id="34023953" url="https://en.wikipedia.org/wiki?curid=34023953" title="Perspicacity">
Perspicacity

Perspicacity (also called perspicaciousness) is a penetrating discernment (from the Latin perspicācitās, meaning throughsightedness, discrimination)—a clarity of vision or intellect which provides a deep understanding and insight.

In 17th-century Europe, René Descartes devised systematic rules for clear thinking in his work "Regulæ ad directionem ingenii" (Rules for the direction of natural intelligence). In Descartes' scheme, intelligence consisted of two faculties: perspicacity, which provided an understanding or intuition of distinct detail; and sagacity, which enabled reasoning about the details in order to make deductions. Rule 9 was "De Perspicacitate Intuitionis" (On the Perspicacity of Intuition). He summarised the rule as 

In his study of the elements of wisdom, the modern psychometrician Robert Sternberg identified perspicacity as one of its six components or dimensions; the other five being reasoning, sagacity, learning, judgement and the expeditious use of information. In his analysis, perspicacity was described as 

In an article dated October 7, 1966, the journal "Science" discussed NASA scientist-astronaut program recruitment efforts:
Being perspicacious about other people, rather than having false illusions, is a sign of good mental health. The quality is needed in psychotherapists who engage in person-to-person dialogue and counselling of the mentally ill.

The artist René Magritte illustrated the quality in his 1936 painting "Perspicacity". The picture shows an artist at work who studies his subject intently: it is an egg. But the painting which he is creating is not of an egg; it is an adult bird in flight.



</doc>
<doc id="221284" url="https://en.wikipedia.org/wiki?curid=221284" title="Dispersed knowledge">
Dispersed knowledge

Dispersed knowledge in economics is the notion that no single agent has information as to all of the factors which influence prices and production throughout the system.

Each agent in a market for assets, goods, or services possesses incomplete knowledge as to most of the factors which affect prices in that market. For example, no agent has full information as to other agents' budgets, preferences, resources or technologies, not to mention their plans for the future and numerous other factors which affect prices in those markets.

Market prices are the result of price discovery, in which each agent participating in the market makes use of its current knowledge and plans to decide on the prices and quantities at which it chooses to transact. The resulting prices and quantities of transactions may be said to reflect the current state of knowledge of the agents currently in the market, even though no single agent commands information as to the entire set of such knowledge.

Some economists believe that market transactions provide the basis for a society to benefit from the knowledge that is dispersed among its constituent agents. For example, in his "Principles of Political Economy", John Stuart Mill states that one of the justifications for a laissez faire government policy is his belief that self-interested individuals throughout the economy, acting independently, can make better use of dispersed knowledge than could the best possible government agency.
Friedrich Hayek claimed that "dispersed knowledge is "essentially" dispersed, and cannot possibly be gathered together and conveyed to an authority charged with the task of deliberately creating order".



Dispersed knowledge will give rise to uncertainty which will lead to different kinds of results.
Richard LeFauve highlights the advantages of organizational structure in companies:

"Before if we had a tough decision to make, we would have two or three different perspectives with strong support of all three. In a traditional organization the bossman decides after he’s heard all three alternatives. At Saturn we take time to work it out, and what generally happens is that you end up with a fourth answer which none of the portions had in the first place. but one that all three portions of the organization fully support (AutoWeeR, Oct. 8, 1990. p. 20)."

Companies are supposed to think highly of the dispersed knowledge and make adjustments to meet demands.
Tsoukas stated:

"A firm’s knowledge is distributed, not only in a computational sense . . . or in Hayek’s (1945, p. 521) sense that the factual knowledge of the particular circumstances of time and place cannot be surveyed as a whole. But, more radically, a firm’s knowledge is distributed in the sense that it is inherently indeterminate: nobody knows in advance what that knowledge is or need be. Firms are faced with radical uncertainty: they do not, they cannot, know what they need to know."

There are several strategies targeting at the problems caused by dispersed knowledge.

First of all, replacing knowledge by getting access to knowledge can be one of the strategies.

What's more, the capability to complete incomplete knowledge can deal with knowledge gaps created by the dispersed knowledge.

In addition, making a design of institutions with reasonable coordination mechanisms can be regarded as the third strategy.

Besides, resolving organization units into smaller ones should be taken into consideration.

Last but not least, providing more data to decision maker will be helpful for making a correct decision.



</doc>
<doc id="34215300" url="https://en.wikipedia.org/wiki?curid=34215300" title="Success trap">
Success trap

The success trap refers to business organizations that focus on the exploitation of their (historically successful) current business activities and as such neglect the need to explore new territory and enhance their long-term viability.

The success trap arises when a firm overemphasizes exploitation investments, even if explorative investments are required for successful adaptation. Exploitation draws on processes that serve to incrementally improve existing knowledge, while exploration involves the pursuit and acquisition of new knowledge. Firms and other organizations that have been performing well over an extended period of time are exposed to strong path dependence in exploitative activities, at the cost of explorative activities with which they have little experience. For example, in the 1990s Polaroid’s management failed to respond to the transition from analogue to digital photography, although the rise of digital technology had been evident since the 1980s. Other well-known examples of companies that got caught in the success trap include Kodak, Rubbermaid and Caterpillar.

A key condition giving rise to a firm getting caught in the success trap is the company culture, having been created based on the understanding of what makes success, the culture then solidifies. When the environment changes there is an initial dismissing of the significance of the change and the (over time) subsequent failure to adjust the strategy of the firm. Thus, top managers do not ‘see’ the upcoming exogenous change, because their thinking and policies tend to constrain exploration and experimentation within the firm and inhibit the ability to bring about strategic change. A broader perspective arises from how exploration activities are suppressed in publicly owned companies as a result of the interplay between the CEO and other top executives, the Board of Directors, the pressure for short-term (improvements in) results arising from the capital market, and the substantial delay between the investment in exploration efforts and the return on these efforts.

The success trap can be best avoided early on, for example, by closely monitoring how other (e.g. leading) firms maintain a balance between exploitation and exploration activities, as well as by continually collecting information about changing customer needs, newly emerging technologies and other changes in the market and competitive environment. Drawing on this type of information, the executive board and board of directors together need to develop and sustain a shared long-term vision and strategy regarding the investments in exploitation and exploration activities. Once a publicly owned corporation has been suppressing exploration over an extended period of time, it tends to be almost impossible to get out of the success trap without major interventions - such as a hostile takeover by another corporation or an exit from the stock exchange.

Firms that fall into the success trap suffer long term consequences. They grow their revenues at a lower pace than other companies and also create less shareholder value than more exploratory companies. These patterns can be observed for S&P 500 companies in the USA in the aggregate and also within industries.



</doc>
<doc id="10044813" url="https://en.wikipedia.org/wiki?curid=10044813" title="Noogony">
Noogony

Noogony is a general term for any theory of knowledge that attempts to explain the origin of concepts in the human mind by considering sense or "a posteriori" data as solely relevant.

The word was used, famously, by Kant in his "Critique of Pure Reason" to refer to what he understood to be Locke's account of the origin of concepts. While Kant himself maintained that some concepts, e.g. cause and effect, did not "arise" from experience, he took Locke to be suggesting that "all" concepts came from experience.

Historically, Kant presents a caricature of Locke's position, not a completely accurate account of Locke's epistemology. Locke's actual theory of knowledge was more subtle than Kant seems to render it in his "Critique". As Guyer/Wood note in their edition of the "Critique":Presumably Kant here has in mind Locke's claim that sensation and reflection are the two sources of all our ideas, and is understanding Locke's reflection to be reflection on sensation only. This would be a misunderstanding of Locke, since Locke says that we get simple ideas from reflection on the "operations of our own Mind," a doctrine which is actually a precursor to Kant's view that the laws of our own intuition and thinking furnish the forms of knowledge to be added to the empirical contents furnished by sensation, although of course Locke did not go very far in developing this doctrine; in particular, he did not see that mathematics and logic could be used as sources of information about the operations of the mind.



</doc>
<doc id="15201" url="https://en.wikipedia.org/wiki?curid=15201" title="Interdisciplinarity">
Interdisciplinarity

Interdisciplinarity or interdisciplinary studies involves the combining of two or more academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics etc. It is about creating something by thinking across boundaries. It is related to an "interdiscipline" or an "interdisciplinary field," which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term "interdisciplinary" is sometimes confined to academic settings.

The term "interdisciplinary" is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. "Interdisciplinary" may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.

The adjective "interdisciplinary" is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.

Although “interdisciplinary” and “interdisciplinarity” are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that "the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge", while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. The building of Roman roads required men who understood surveying, material science, logistics and several other disciplines. Any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics, and even sinology.

Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science, technology and society programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies.

At another level, interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization and isolation in information silos. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.

Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differing of perspectives and methods. For example, a discipline that places more emphasis on quantitative rigor may produce practitioners who are more scientific in their traininf than others; in turn, colleagues in "softer" disciplines may associate quantitative approaches with difficulty grasp the broader dimensions of a problem and lower rigor in theoretical and qualitative argumentation. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). On the other hand, and from the disciplinary perspective, much interdisciplinary work may be seen as "soft", lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work. For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure.

Interdisciplinary programs may fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds.

Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines, many of them named for their parent disciplines, include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as "interdisciplines". On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into "professional", "organizational", and "cultural" obstacles.

An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008). As of September 1, 2014, the Center for the Study of Interdisciplinarity has ceased to exist. This is the result of administrative decisions at the University of North Texas.

An interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge.

In contrast, studies of interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'.

Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions.

While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works—and does not—in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account.

Since 1998, there has been an ascendancy in the value of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem solving in the 21st century. This has been echoed by federal funding agencies, particularly the National Institutes of Health under the direction of Elias Zerhouni, who has advocated that grant proposals be framed more as interdisciplinary collaborative projects than single researcher, single discipline ones.

At the same time, many thriving longstanding bachelor's in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.

There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on "specialized segments of attention" (adopting one particular perspective), to the idea of "instant sensory awareness of the whole", an attention to the "total field", a "sense of the whole pattern, of form and function as a unity", an "integral idea of structure and configuration". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.

An article in the "Social Science Journal" attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinarity:"To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study."In turn, interdisciplinary "richness" of any two instances of knowledge, research, or education can be ranked by weighing four variables: number of disciplines involved, the "distance" between them, the novelty of any particular combination, and their extent of integration.

Interdisciplinary knowledge and research are important because:

"The modern mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole [...]. The Olympic games were designed to test the arete of the whole man, not a merely specialized skill [...]. The great event was the pentathlon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modern times: the Greeks would have regarded it as a monstrosity.""Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line.""It is the custom among those who are called "practical" men to condemn any man capable of a wide survey as a visionary: no man is thought worthy of a voice in politics unless he ignores or does not know nine tenths of the most important relevant facts."





</doc>
<doc id="21758835" url="https://en.wikipedia.org/wiki?curid=21758835" title="Institutional memory">
Institutional memory

Institutional memory is a collective set of facts, concepts, experiences and knowledge held by a group of people.

Institutional memory has been defined as "the stored knowledge within the organization."

Institutional memory requires the ongoing transmission of these memories between members of this group. Elements of institutional memory may be found in corporations, professional groups, government bodies, religious groups, academic collaborations, and by extension in entire cultures. There are different ideas about how institutional memory is transferred, whether it is between people or through written sources.

Institutional memory may be encouraged to preserve an ideology or way of work in such a group. Conversely, institutional memory may be ingrained to the point that it becomes hard to challenge if something is found to contradict that which was previously thought to have been correct. Institutional memory may have influence on organizational identity, choice of individuals, and actions of the individuals interacting with the institution.

Institutional knowledge is gained by organizations translating historical data into useful knowledge and wisdom. Memory depends upon the preservation of data and also the analytical skills necessary for its effective use within the organization.

Religion is one of the significant institutional forces acting on the collective memory attributed to humanity. Alternatively, the evolution of ideas in Marxist theory, is that the mechanism whereby knowledge and wisdom are passed down through the generations is subject to economic determinism. In all instances, social systems, cultures, and organizations have an interest in controlling and using institutional memories.

Organizational structure determines the training requirements and expectations of behaviour associated with various roles. This is part of the implicit institutional knowledge. Progress to higher echelons requires assimilation of this, and when outsiders enter at a high level, effectiveness tends to deteriorate if this morale is unjustly ignored.



</doc>
<doc id="628450" url="https://en.wikipedia.org/wiki?curid=628450" title="Activity theory">
Activity theory

Activity theory (AT; ) is an umbrella term for a line of eclectic social sciences theories and research with its roots in the Soviet psychological activity theory pioneered by Lev Vygotsky , Alexei Leont'ev and Sergei Rubinstein. These scholars sought to understand human activities as systemic and socially situated phenomena and to go beyond paradigms of reflexology (the teaching of Vladimir Bekhterev and his followers) and classical conditioning (the teaching of Ivan Pavlov and his school), psychoanalysis and behaviorism. It became one of the major psychological approaches in the former USSR, being widely used in both theoretical and applied psychology, and in education, professional training, ergonomics, social psychology and work psychology.

Activity theory is more of a descriptive meta-theory or framework than a predictive theory. It considers an entire work/activity system (including teams, organizations, etc.) beyond just one actor or user. It accounts for environment, history of the person, culture, role of the artifact, motivations, and complexity of real-life activity. One of the strengths of AT is that it bridges the gap between the individual subject and the social reality—it studies both through the mediating activity. The unit of analysis in AT is the concept of object-oriented, collective and culturally mediated human activity, or "activity system". This system includes the object (or objective), subject, mediating artifacts (signs and tools), rules, community and division of labor. The motive for the activity in AT is created through the tensions and contradictions within the elements of the system. According to ethnographer Bonnie Nardi, a leading theorist in AT, activity theory "focuses on practice, which obviates the need to distinguish 'applied' from 'pure' science—understanding everyday practice in the real world is the very objective of scientific practice. ... The object of activity theory is to understand the unity of consciousness and activity." Sometimes called "Cultural-Historical Activity Theory", this approach is particularly useful for studying a group that exists "largely in virtual form, its communications mediated largely through electronic and printed texts."

AT is particularly useful as a lens in qualitative research methodologies (e.g., ethnography, case study). AT provides a method of understanding and analyzing a phenomenon, finding patterns and making inferences across interactions, describing phenomena and presenting phenomena through a built-in language and rhetoric. A particular activity is a goal-directed or purposeful interaction of a subject with an object through the use of tools. These tools are exteriorized forms of mental processes manifested in constructs, whether physical or psychological. AT recognizes the internalization and externalization of cognitive processes involved in the use of tools, as well as the transformation or development that results from the interaction.

The origins of activity theory can be traced to several sources, which have subsequently given rise to various complementary and intertwined strands of development. This account will focus on three of the most important of these strands. The first is associated with the Moscow Institute of Psychology and in particular the "troika" of young Russian researchers, Vygotsky, Leont'ev and Luria. Vygotsky founded cultural-historical psychology, a field that became the basis for modern AT; Leont'ev, one of the principal founders of activity theory, both developed and reacted against Vygotsky's work. Leont'ev's formulation of general activity theory is currently the most influential in post-Soviet developments in AT, which have largely been in social-scientific, organizational, and writing-studies rather than psychological research.

The second major line of development within activity theory involves Russian scientists, such as P. K. Anokhin and Nikolai Bernstein, more directly concerned with the neurophysiological basis of activity; its foundation is associated with the Soviet philosopher of psychology Sergei Rubinstein. This work was subsequently developed by researchers such as Pushkin, Zinchenko & Gordeeva, Ponomarenko, Zarakovsky and others, and is currently most well-known through the work on systemic-structural activity theory being carried out by G. Z. Bedny and his associates.

Finally, in the Western world, discussions and use of AT are primarily framed within the Scandinavian activity theory strand, developed by Yrjö Engeström.

After Vygotsky's early death, Leont'ev became the leader of the research group nowadays known as the Kharkov School of Psychology and extended Vygotsky's research framework in significantly new ways. Leont'ev first examined the psychology of animals, looking at the different degrees to which animals can be said to have mental processes. He concluded that Pavlov's reflexionism was not a sufficient explanation of animal behaviour and that animals have an active relation to reality, which he called "activity". In particular, the behaviour of higher primates such as chimpanzees could only be explained by the ape's formation of multi-phase plans using tools.

Leont'ev then progressed to humans and pointed out that people engage in "actions" that do not in themselves satisfy a need, but contribute towards the eventual satisfaction of a need. Often, these actions only make sense in a social context of a shared work activity. This led him to a distinction between "activities", which satisfy a need, and the "actions" that constitute the activities. Leont'ev also argued that the activity in which a person is involved is reflected in their mental activity, that is (as he puts it) material reality is "presented" to consciousness, but only in its vital meaning or significance.

Activity theory also influenced the development of organizational-activity game as developed by Georgy Shchedrovitsky.

AT remained virtually unknown outside the Soviet Union until the mid-1980s, when it was picked up by Scandinavian researchers. The first international conference on activity theory was not held until 1986. The earliest non-Soviet paper cited by Nardi is a 1987 paper by Yrjö Engeström: "Learning by expanding". This resulted in a reformulation of AT. Kuutti notes that the term "activity theory" "can be used in two senses: referring to the original Soviet tradition or referring to the international, multi-voiced community applying the original ideas and developing them further."

The Scandinavian AT school of thought seeks to integrate and develop concepts from Vygotsky's Cultural-historical psychology and Leont'ev's activity theory with Western intellectual developments such as Cognitive Science, American Pragmatism, Constructivism, and Actor-Network Theory. It is known as Scandinavian activity theory. Work in the systems-structural theory of activity is also being carried on by researchers in the US and UK.

Some of the changes are a systematisation of Leont'ev's work. Although Leont'ev's exposition is clear and well structured, it is not as well-structured as the formulation by Yrjö Engeström. Kaptelinin remarks that Engeström "proposed a scheme of activity different from that by Leont'ev; it contains three interacting entities—the individual, the object and the community—instead of the two components—the individual and the object—in Leont'ev's original scheme."

Some changes were introduced, apparently by importing notions from human–computer interaction theory. For instance, the notion of "rules", which is not found in Leont'ev, was introduced. Also, the notion of collective subject was introduced in the 1970s and 1980s (Leont'ev refers to "joint labour activity", but only has individuals, not groups, as activity subjects).

The goal of activity theory is understanding the mental capabilities of a single individual. However, it rejects the "isolated" individuals as insufficient unit of analysis, analyzing the cultural and technical aspects of human actions.

Activity theory is most often used to describe actions in a socio-technical system through six related elements (Bryant et al. as defined by Leonti'ev 1981 and redefined in Engeström 1987) of a conceptual system expanded by more nuanced theories:

Activity theory helps explain how social artifacts and social organization mediate social action (Bryant et al.).

The application of activity theory to information systems derives from the work of Bonnie Nardi and Kari Kuutti. Kuutti's work is addressed below. Nardi's approach is, briefly, as follows: Nardi (p. 6) described activity theory as "...a powerful and clarifying descriptive tool rather than a strongly predictive theory. The object of activity theory is to understand the unity of consciousness and activity...Activity theorists argue that consciousness is not a set of discrete disembodied cognitive acts (decision making, classification, remembering), and certainly it is not the brain; rather, consciousness is located in everyday practice: you are what you do." Nardi (p. 5) also argued that "activity theory proposes a strong notion of "mediation"—all human experience is shaped by the tools and sign systems we use." Nardi (p. 6) explained that "a basic tenet of activity theory is that a notion of consciousness is central to a depiction of activity. Vygotsky described consciousness as a phenomenon that unifies attention, intention, memory, reasoning, and speech..." and (p. 7) "Activity theory, with its emphasis on the importance of motive and consciousness—which belongs only to humans—sees people and things as fundamentally different. People are not reduced to 'nodes' or 'agents' in a system; 'information processing' is not seen as something to be modelled in the same way for people and machines."

In a later work, Nardi et al. in comparing activity theory with cognitive science, argue that "activity theory is above all a social theory of consciousness" and therefore "... activity theory wants to define consciousness, that is, all the mental functioning including
remembering, deciding, classifying, generalising, abstracting and so forth, as a product of our social interactions with other people and of our use of tools." For Activity Theorists "consciousness" seems to refer to any mental functioning, whereas most other approaches to psychology distinguish conscious from unconscious functions.

Over the last 15 years the use and exploration of activity theory in information systems has grown. One stream of research has focused on technology mediated change and the implementation of technologies and how they disrupt, change and improve organisational work activity. In these studies, activity systems are used to understand emergent contradictions in the work activity, which are temporarily resolved using information systems (tools) and/or arising from the introduction of information systems. Information science studies use a similar approach to activity theory in order to understand information behaviour "in context".
In the field of ICT and development (a field of study within information systems) the use of activity theory has also been used to inform development of IT systems and to frame the study of ICT in development settings.

In addition, Etengoff & Daiute have conducted recent work exploring how social media interfaces can be productively used to mediate conflicts. Their work has illustrated this perspective with analyses of online interactions between gay men and their religious family members and Sunni-Muslim emerging adults' efforts to maintain a positive ethnic identity via online religious forums in post 9/11 contexts.

The rise of the personal computer challenged the focus in traditional systems developments on mainframe systems for automation of existing work routines. It furthermore brought forth a need to focus on how to work on materials and objects through the computer. In the search of theoretical and methodical perspectives suited to deal with issues of flexibility and more advanced mediation between the human being, material and outcomes through the interface, it seemed promising to turn to the still rather young HCI research tradition that had emerged primarily in the US (for further discussion see Bannon & Bødker, 1991).

Specifically the cognitive science-based theories lacked means of addressing a number of issues that came out of the empirical projects (see Bannon & Bødker, 1991): 1. Many of the early advanced user interfaces assumed that the users were the designers themselves, and accordingly built on an assumption of a generic user, without concern for qualifications, work environment, division of work, etc. 2.In particular the role of the artifact as it stands between the user and her materials, objects and outcomes was ill understood. 3. In validating findings and designs there was a heavy focus on novice users whereas everyday use by experienced users and concerns for the development of expertise were hardly addressed. 4.Detailed task analysis and the idealized models created through task analysis failed to capture the complexity and contingency of real-life action. 5.From the point of view of complex work settings, it was striking how most HCI focused on one user – one computer in contrast to the ever-ongoing cooperation and coordination of real work situations (this problem later lead to the development of CSCW). 6.Users were mainly seen as objects of study.

Because of these shortcomings, it was necessary to move outside cognitive science-based HCI to find or develop the necessary theoretical platform. European psychology had taken different paths than had American with much inspiration from dialectical materialism (Hydén 1981, Engeström, 1987). Philosophers such as Heidegger and Wittgenstein came to play an important role, primarily through discussions of the limitations of AI (Winograd & Flores 1986, Dreyfus & Dreyfus 1986). Suchman (1987) with a similar focus introduced ethnomethodology into the discussions, and Ehn (1988) based his treatise of design of computer artifacts on Marx, Heidegger and Wittgenstein.
The development of the activity theoretical angle was primarily carried out by Bødker (1991, 1996) and by Kuutti (Bannon & Kuutti, 1993, Kuutti, 1991, 1996), both with strong inspiration from Scandinavian activity theory groups in psychology. Bannon (1990, 1991) and Grudin (1990a and b) made significant contributions to the furthering of the approach by making it available to the HCI audience. The work of Kaptelinin (1996) has been important to connect to the earlier development of activity theory in Russia. Nardi produced the, hitherto, most applicable collection of activity theoretical HCI literature (Nardi, 1996).

At the end of the 1990s, a group of Russian and American activity theorists working in the systems-cybernetic tradition of Bernshtein and Anokhin began to publish English-language articles and books dealing with topics in human factors and ergonomics and, latterly, human–computer interaction. Under the rubric of systemic-structural activity theory (SSAT), this work represents a modern synthesis within activity theory which brings together the cultural-historical and systems-structural strands of the tradition (as well as other work within Soviet psychology such as the Psychology of Set) with findings and methods from Western human factors/ergonomics and cognitive psychology.

The development of SSAT has been specifically oriented toward the analysis and design of the basic elements of human work activity: tasks, tools, methods, objects and results, and the skills, experience and abilities of involved subjects. SSAT has developed techniques for both the qualitative and quantitative description of work activity. Its design-oriented analyses specifically focus on the interrelationship between the structure and self-regulation of work activity and the configuration of its material components.

This section presents a short introduction to activity theory, and some brief comments on human creativity in activity theory and the implications of activity theory for tacit knowledge and learning.

Activity theory begins with the notion of activity. An activity is seen as a system of human "doing" whereby a subject works on an object in order to obtain a desired outcome. In order to do this, the subject employs tools, which may be external (e.g. an axe, a computer) or internal (e.g. a plan). As an illustration, an activity might be the operation of an automated call centre. As we shall see later, many subjects may be involved in the activity and each subject may have one or more motives (e.g. improved supply management, career advancement or gaining control over a vital organisational power source). A simple example of an activity within a call centre might be a telephone operator (subject) who is modifying a customer's billing record (object) so that the billing data is correct (outcome) using a graphical front end to a database (tool).

Kuutti formulates activity theory in terms of the structure of an activity. "An activity is a form of doing directed to an object, and activities are distinguished from each other according
to their objects. Transforming the object into an outcome motivates the existence of an activity. An object can be a material thing, but it can also be less tangible."

Kuutti then adds a third term, the tool, which 'mediates' between the activity and the object. "The tool is at the same time both enabling and limiting: it empowers the subject in the transformation process with the historically collected experience and skill 'crystallised' to it, but it also restricts the interaction to be from the perspective of that particular tool or instrument; other potential features of an object remain invisible to the subject...".

As Verenikina remarks, tools are "social objects with certain modes of operation developed socially in the course of labour and are only possible because they correspond to the objectives of a practical action."

An activity is modelled as a three-level hierarchy. Kuutti schematises processes in activity theory as a three-level system.

Verenikina paraphrases Leont'ev as explaining that "the non-coincidence of action and operations... appears in actions with tools, that is, material objects which are crystallised operations, not actions nor goals. If a person is confronted with a specific goal of, say, dismantling a machine, then they must make use of a variety of operations; it makes no difference how the individual operations were learned because the formulation of the operation proceeds differently to the formulation of the goal that initiated the action."

The levels of activity are also characterised by their purposes: "Activities are oriented to motives, that is, the objects that are impelling by themselves. Each motive is an object, material or ideal, that satisfies a need. Actions are the processes functionally subordinated to activities; they are directed at specific conscious goals... Actions are realised through operations that are determined by the actual conditions of activity."

Engeström developed an extended model of an activity, which adds another component, community ("those who share the same object"), and then adds rules to mediate between subject and community, and the division of labour to mediate between object and community.

Kuutti asserts that "These three classes should be understood broadly. A tool can be anything used in the transformation process, including both material tools and tools for thinking. Rules cover both explicit and implicit norms, conventions, and social relations within a community. Division of labour refers to the explicit and implicit organisation of the community as related to the transformation process of the object into the outcome."

Activity theory therefore includes the notion that an activity is carried out within a social context, or specifically in a community. The way in which the activity fits into the context is thus established by two resulting concepts:

Activity theory provides a number of useful concepts that can be used to address the lack of expression for 'soft' factors which are inadequately represented by most process modelling frameworks. One such concept is the internal plane of action. Activity theory recognises that each activity takes place in two planes: the external plane and the internal plane. The external plane represents the objective components of the action while the internal plane represents the subjective components of the action. Kaptelinin defines the "internal plane of actions" as "[...] a concept developed in activity theory that refers to the human ability to perform manipulations with an internal representation of external objects before starting actions with these objects in reality."

The concepts of motives, goals and conditions discussed above also contribute to the modelling of soft factors. One principle of activity theory is that many activities have multiple motivation ('polymotivation'). For instance, a programmer in writing a program may address goals aligned towards multiple motives such as increasing his or her annual bonus, obtaining relevant career experience and contributing to organisational objectives.

Activity theory further argues that subjects are grouped into communities, with rules mediating between subject and community and a division of labour mediating between object and community. A subject may be part of several communities and a community, itself, may be part of other communities.

Human creativity plays an important role in activity theory, that "human beings... are essentially creative beings" in "the creative, non-predictable character". Tikhomirov also analyses the importance of "creative activity", contrasting it to "routine" activity, and notes the important shift brought about by computerisation in the balance towards creative activity.

Karl Marx, a sociological theorist, argued that humans are unique compared to other species in that humans create everything they need to survive. According to Marx, this is described as species-being. Marx believed we find our true identity in what we produce in our personal labor.

Activity theory has an interesting approach to the difficult problems of "learning" and, in particular, "tacit knowledge". Learning has been a favourite subject of management theorists, but it has often been presented in an abstract way separated from the work processes to which the learning should apply. Activity theory provides a potential corrective to this tendency. For instance, Engeström's review of Nonaka's work on "knowledge creation" suggests enhancements based on activity theory, in particular suggesting that the organisational learning process includes preliminary stages of goal and problem formation not found in Nonaka. Lompscher, rather than seeing learning as "transmission", sees the formation of learning goals and the
student's understanding of which things they need to acquire as the key to the formation of the learning activity.

Of particular importance to the study of learning in organisations is the problem of "tacit knowledge", which according to Nonaka, "is highly personal and hard to formalise, making it difficult to communicate to others or to share with others." Leont'ev's concept of operation provides an important insight into this problem. In addition, the key idea of "internalisation" was originally introduced by Vygotsky as "the internal reconstruction of an external operation." Internalisation has subsequently become a key term of the theory of tacit knowledge and has been defined as "a process of embodying explicit knowledge into tacit knowledge." Internalisation has been described by Engeström as the "key psychological mechanism" discovered by Vygotsky and is further discussed by Verenikina.






</doc>
<doc id="3461883" url="https://en.wikipedia.org/wiki?curid=3461883" title="Know-it-all">
Know-it-all

A know-it-all or know-all is a person who constantly presents their input as though they were professionally trained, schooled or have firsthand insight into subjects when it is evident this is not the case. A know-it-all will quickly reject opinions, suggestions, thoughts and commentary from others as incorrect, nonsensical and disruptive. 








</doc>
<doc id="2738697" url="https://en.wikipedia.org/wiki?curid=2738697" title="Autoepistemic logic">
Autoepistemic logic

The autoepistemic logic is a formal logic for the representation and reasoning of knowledge about knowledge. While propositional logic can only express facts, autoepistemic logic can express knowledge and lack of knowledge about facts.

The stable model semantics, which is used to give a semantics to logic programming with negation as failure, can be seen as a simplified form of autoepistemic logic.

The syntax of autoepistemic logic extends that of propositional logic by a modal operator formula_1 indicating knowledge: if formula_2 is a formula, formula_3 indicates that formula_2 is known. As a result, formula_5 indicates that formula_6 is known and formula_7 indicates that formula_2 is not known.

This syntax is used for allowing reasoning based on knowledge of facts. For example, formula_9 means that formula_2 is assumed false if it is not known to be true. This is a form of negation as failure.

The semantics of autoepistemic logic is based on the "expansions" of a theory, which have a role similar to models in propositional logic. While a propositional model specifies which axioms are true or false, an expansion specifies which formulae formula_3 are true and which ones are false. In particular, the expansions of an autoepistemic formula formula_12 makes this distinction for every subformula formula_3 contained in formula_12. This distinction allows formula_12 to be treated as a propositional formula, as all its subformulae containing formula_1 are either true or false. In particular, checking whether formula_12 entails formula_2 in this condition can be done using the rules of the propositional calculus. In order for an initial assumption to be an expansion, it must be that a subformula formula_2 is entailed if and only if formula_3 has been initially assumed true.

In terms of possible world semantics, an expansion of formula_12 consists of an S5 model of formula_12 in which the possible worlds consist only of worlds where formula_12 is true. [The possible worlds need not contain all such consistent worlds; this corresponds to the fact that modal propositions are assigned truth values before checking derivability of the ordinary propositions.] Thus, autoepistemic logic extends S5; the extension is proper, since formula_24 and formula_25 are tautologies of autoepistemic logic, but not of S5.

For example, in the formula formula_26, there is only a single “boxed subformula”, which is formula_27. Therefore, there are only two candidate expansions, assuming it true or false, respectively. The check for them being actual expansions is as follows.

formula_27 is false : with this assumption, formula_12 becomes tautological, as formula_30 is equivalent to formula_31, and formula_32 is assumed true; therefore, formula_33 is not entailed. This result confirms the assumption implicit in formula_27 being false, that is, that formula_33 is not currently known. Therefore, the assumption that formula_27 is false is an expansion.

formula_27 is true : together with this assumption, formula_12 entails formula_33; therefore, the initial assumption that is implicit in formula_27 being true, i.e., that formula_33 is known to be true, is satisfied. As a result, this is another expansion.

The formula formula_12 has therefore two expansions, one in which formula_33 is not known and one in which formula_33 is known. The second one has been regarded as unintuitive, as the initial assumption that formula_27 is true is the only reason why formula_33 is true, which confirms the assumption. In other words, this is a self-supporting assumption. A logic allowing such a self-support of beliefs is called "not strongly grounded" to differentiate them from "strongly grounded" logics, in which self-support is not possible. Strongly grounded variants of autoepistemic logic exist.

In uncertain inference, the known/unknown duality of truth values is replaced by a degree of certainty of a fact or deduction; certainty may vary from 0 (completely uncertain/unknown) to 1 (certain/known). In probabilistic logic networks, truth values are also given a probabilistic interpretation ("i.e." truth values may be uncertain, and, even if almost certain, they may still be "probably" true (or false).)




</doc>
<doc id="1106205" url="https://en.wikipedia.org/wiki?curid=1106205" title="Cognitive closure (philosophy)">
Cognitive closure (philosophy)

In philosophy of science and philosophy of mind, cognitive closure is the proposition that human minds are constitutionally incapable of solving certain perennial philosophical problems. Owen Flanagan calls this position anti-constructive naturalism or the "new mysterianism" and the primary advocate of the hypothesis, Colin McGinn, calls it transcendental naturalism acknowledging the possibility that solutions may be an intelligent non-human of some kind. According to McGinn, such philosophical questions include the mind-body problem, identity of the self, foundations of meaning, free will, and knowledge, both "a priori" and empirical.

For Friedrich Hayek, "The whole idea of the mind explaining itself is a logical contradiction"... and "takes this incompleteness—the constitutional inability of mind to explain itself—to be a generalized case of Gödel's incompleteness theorem... Hayek is "not" a naturalistic agnostic, that is, the view that science "currently" cannot offer an explanation of the mind-body relationship, but in principle it could."

Noam Chomsky argues that the cognitive capabilities of all organisms are limited by biology and that certain problems may be beyond our understanding:
As argued in Kant's "Critique of Pure Reason", human thinking is unavoidably structured by categories of the understanding:
These are ideas to which there is no escape, thus they pose a limit to thinking. What can be known through the categories is called phenomena and what is outside the categories is called noumena, the unthinkable "things in themselves".

In his (famous) essay "What Is It Like to Be a Bat?" Thomas Nagel mentions the possibility of cognitive closure to the subjective character of experience and the (deep) implications that it has for materialist reductionist science. Owen Flanagan noted in his 1991 book "Science of the Mind" that some modern thinkers have suggested that consciousness will never be completely explained. Flanagan called them "the new mysterians" after the rock group Question Mark and the Mysterians. According to McGinn, the solution to the mind-body problem cannot be grasped, despite the fact that the solution is "written in our genes".

Emergent materialism is a similar but different claim that humans are not smart enough to determine "the relationship between mind and matter."

While the nature of consciousness is complex, according to some philosophers, that does not imply closure, thus, McGinn's argument is flawed.



</doc>
<doc id="33456414" url="https://en.wikipedia.org/wiki?curid=33456414" title="A Causal Theory of Knowing">
A Causal Theory of Knowing

"A Causal Theory of Knowing" is a philosophical essay written by Alvin Goldman in 1967, published in "The Journal of Philosophy". It is based on existing theories of knowledge in the realm of epistemology, the study of philosophy through the scope of knowledge. The essay attempts to define knowledge by connecting facts, beliefs and knowledge through underlying and connective series called causal chains. It provides a causal theory of knowledge.

A causal chain is repeatedly described as a sequence of events for which one event in a chain causes the next. According to Goldman, these chains can only exist with the presence of an accepted fact, a belief in the fact, and a cause for the subject to believe the fact. The essay also explores the ideas of perception and memory through the use of the causal chains and the concept of knowledge.

The essay is regarded as an improvement and rebuttal of Edmund Gettier's "Is Justified True Belief Knowledge?", which is one of many attempts to explain the necessary conditions for knowledge to develop. Goldman implements the causal connection to reiterate his own theory of knowledge. Knowledge exists, says Goldman, if and only if the belief is justified by a reaction to the accepted fact.

Goldman's theory later counters that of Michael Clark, stating that his own theory including figures and diagrams is more appropriate than Clark's. "A Causal Theory of Knowing" uses figures which make explicit references to causal beliefs. Clark's model does not utilize these arrows, and Goldman states that the lack of these arrows deems Clark's model deficient.

Alvin Goldman, currently a professor of philosophy at Rutgers University, wrote "A Causal Theory of Knowing" when he was in his late twenties. Goldman received his Ph.D. from Princeton University, and has taught at numerous universities.

Goldman's research deals mainly with epistemology and other cognitive sciences. "A Causal Theory of Knowing" was Goldman's first published paper explaining his own views of epistemology. Currently, Goldman has written more than ten essays focusing on knowledge and cognitive science.

The essay starts with a definition of Gettier's theory, followed by multiple reiterations of the idea of causal connections, figures to explain knowledge through a visual perspective, and references to perception and memory through causal chains.

The essay tends to focus on examples in which knowledge or other sensations do not exist, rather than proving a certain fact to be known. Goldman also states on multiple occasions that he does not wish to explain the causal process in detail, instead pointing out counterexamples. At numerous times in the essay, he also points out that he does not intend to give definitive answers to each of the propositions mentioned.

Goldman also refocuses the idea of perception, or knowledge through sensation (specifically sight) using his own theory of knowing. The concept of causal perception indicates that one observes something only if the object itself causes the sensation of sight to be accepted as known. Thus, the object's existence must be factual and one must believe its existence. While all knowledge comes from facts, inferred knowledge emerges when physical object facts cause sense data which can be perceived as senses. The sense data can also be used to make conclusions, known as inferred knowledge, about certain physical object facts.

From "A Causal Theory of Knowing", Goldman constructs the idea that memory is also a causal process. Memory is explained as being an extension of knowledge into the future, and remembering is the act of recalling a fact that has already been known. Further, the theory states that if knowledge is forgotten at one time, it cannot be considered a memory in the future. According to Goldman, if a fact is known at Time 1 but forgotten at Time 2, and then at Time 3 that the fact is perceived again but not known, at Time 3 the original fact is not a memory because there is no causal connection between the fact and the memory.



</doc>
<doc id="639511" url="https://en.wikipedia.org/wiki?curid=639511" title="Episteme">
Episteme

"Episteme" is a philosophical term derived from the Ancient Greek word ἐπιστήμη "epistēmē", which can refer to knowledge, science or understanding, and which comes from the verb , meaning "to know, to understand, or to be acquainted with". 

Plato contrasts episteme with "doxa": common belief or opinion. Episteme is also distinguished from "techne": a craft or applied practice. The word "epistemology" is derived from episteme.

The French philosopher Michel Foucault used the term "épistémè" in a specialized sense in his work "The Order of Things" to mean the historical, but non-temporal, "a priori" which grounds knowledge and its discourses and thus represents the condition of their possibility within a particular epoch. 

In subsequent writings, he made it clear that several "épistémè" may co-exist and interact at the same time, being parts of various power-knowledge systems. But he did not discard the concept:
Yet in Foucault's "The Order of Things" he describes "épistémè" as:

However, if in any given culture and at any given moment, there is always only one episteme that defines the conditions of possibility of all knowledge, whether expressed in a theory or silently invested in a practice. (Foucault, 168)

Foucault's use of "épistémè" has been asserted as being similar to Thomas Kuhn's notion of a "paradigm", as for example by Jean Piaget. However, there are decisive differences. 

Whereas Kuhn's "paradigm" is an all-encompassing collection of beliefs and assumptions that result in the organization of scientific worldviews and practices, Foucault's "episteme" is not merely confined to science but to a wider range of discourse (all of science itself would fall under the "episteme" of the epoch). 

Kuhn's paradigm shifts are a consequence of a series of conscious decisions made by scientists to pursue a neglected set of questions. Foucault's "episteme" is something like the 'epistemological unconscious' of an era; the resultant configuration of knowledge of a particular "episteme" is, to Foucault, based on a set of primordial, fundamental assumptions that are so basic to the "episteme" that they're experientially "invisible" to the constituents (such as people, organizations, or systems) operating within the "episteme."

Moreover, Kuhn's concept corresponds to what Foucault calls "theme" or theory of a science, but Foucault analyzed how "opposing" theories and themes could "co-exist" within a science. Kuhn doesn't search for the conditions of possibility of opposing discourses within a science, but simply for the invariant dominant paradigm governing scientific research (supposing that "one" paradigm always "is" pervading, except under paradigmatic transition). 

Foucault attempts to demonstrate the constitutive limits of discourse, and in particular, the rules enabling their productivity; however, Foucault maintained that though ideology may infiltrate and form science, it need not do so: it must be demonstrated how ideology actually forms the science in question; contradictions and lack of objectivity is not an indicator of ideology. "Truth is a thing of this world: it is produced only by virtue of multiple forms of constraint. And it induces regular effects of power. Each society has its regime of truth, its "general politics” of truth: that is, the types of discourse which it accepts and makes function as true; the mechanisms and instances which enable one to distinguish true and false statements, the means by which each is sanctioned; the techniques and procedures accorded value in the acquisition of truth; the status of those who are charged with saying what counts as true." Kuhn's and Foucault's notions are possibly influenced by the French philosopher of science Gaston Bachelard's notion of an "epistemological rupture", as indeed was Althusser.

In 1997, Judith Butler used the concept of episteme in her book "", examining the use of speech-act theory for political purposes.




</doc>
<doc id="582687" url="https://en.wikipedia.org/wiki?curid=582687" title="Jnana">
Jnana

In Indian philosophy and religion, jñāna (Sanskrit: ज्ञान, pronounced gya:nə, IPA: [ɡʲa:nə] or [ɟɲɑːnə]) (Pali: "ñāṇa") or gyan/gian (Hindi: "jñān") is "knowledge".

The idea of jnana centers on a cognitive event which is recognized when experienced. It is knowledge inseparable from the total experience of reality, especially a total or divine reality (Brahman). 

The root jñā- is cognate to English "know", as well as to the Greek γνώ- (as in γνῶσις "gnosis") and Russian "знание". Its antonym is "ajñāna" "ignorance".

In Tibetan Buddhism, it refers to pure awareness that is free of conceptual encumbrances, and is contrasted with vijnana, which is a moment of 'divided knowing'. Entrance to, and progression through the ten stages of Jnana/Bhimis, will lead one to complete enlightenment and nirvana.

In the Vipassanā tradition of Buddhism there are the following ñanas according to Mahasi Sayadaw. As a person meditates these ñanas or "knowledges" will be experienced in order. The experience of each may be brief or may last for years and the subjective intensity of each is variable. Each ñana could also be considered a jhāna although many are not stable and the mind has no way to remain embedded in the experience. Experiencing all the ñanas will lead to the first of the Four stages of enlightenment then the cycle will start over at a subtler level.

Sahu explains:
Jnana yoga (Yoga of Knowledge) is one of the three main paths (margas), which are supposed to lead towards moksha (liberation) from material miseries. The other two main paths are Karma yoga and Bhakti Yoga. Rāja yoga (classical yoga) which includes several yogas, is also said to lead to moksha. It is said that each path is meant for a different temperament of personality.

According to the Jain texts like Tattvārthsūtra and Sarvārthasiddhi, knowledge is of five kinds:

"Gyan" or "Gian" refers to spiritual knowledge. It is mentioned throughout the Guru Granth Sahib.




</doc>
<doc id="1628055" url="https://en.wikipedia.org/wiki?curid=1628055" title="Know thyself">
Know thyself

The saying "know thyself" was inscribed on temple entrances in Kemet (Ancient Egypt) such as in the temples of Luxor . It is nowadays commonly attributed to Ancient Greece (Greek: , transliterated: '; also ' with the ε contracted), as one of the Delphic maxims and was inscribed in the pronaos (forecourt) of the Temple of Apollo at Delphi according to the Greek writer Pausanias (10.24.1). The phrase was later expounded upon by the philosopher Socrates who taught that: 

In Latin the phrase, "know thyself," is given as "nosce te ipsum" or "temet nosce".

The maxim, or aphorism, "know thyself" has had a variety of meanings attributed to it in literature.

The Greek aphorism has been attributed to at least the following ancient Greek sages:

Diogenes Laërtius attributes it to Thales ("Lives" I.40), but also notes that Antisthenes in his "Successions of Philosophers" attributes it to Phemonoe, a mythical Greek poet, though admitting that it was appropriated by Chilon. In a discussion of moderation and self-awareness, the Roman poet Juvenal quotes the phrase in Greek and states that the precept descended "e caelo" (from heaven) ("" 11.27). The 10th-century Byzantine encyclopedia the "Suda" recognized Chilon and Thales as the sources of the maxim "Know Thyself."

The authenticity of all such attributions is doubtful; according to Parke and Wormell (1956), "The actual authorship of the three maxims set up on the Delphian temple may be left uncertain. Most likely they were popular proverbs, which tended later to be attributed to particular sages."

Listed chronologically:
The ancient Greek playwright Aeschylus uses the maxim "know thyself" in his play "Prometheus Bound." The play about a mythological sequence, thereby places the maxim within the context of Greek mythology. In this play, the demi-god Prometheus first rails at the Olympian gods, and against what he believes to be the injustice of his having been bound to a cliffside by Zeus, king of the Olympian gods. The demi-god Oceanus comes to Prometheus to reason with him, and cautions him that he should "know thyself". In this context, Oceanus is telling Prometheus that he should know better than to speak ill of the one who decides his fate and accordingly, perhaps he should better know his place in the "great order of things."

One of Socrates's students, the historian Xenophon, described some of the instances of Socrates's use of the Delphic maxim 'Know Thyself' in his history titled: "Memorabilia." In this writing, Xenophon portrayed his teacher's use of the maxim as an organizing theme for Socrates's lengthy dialogue with Euthydemus.

Plato, another student of Socrates, employs the maxim 'Know Thyself' extensively by having the character of Socrates use it to motivate his dialogues. Benjamin Jowett's index to his translation of the "Dialogues of Plato" lists six dialogues which discuss or explore the Delphic maxim: 'know thyself.' These dialogues (and the Stephanus numbers indexing the pages where these discussions begin) are "Charmides" (164D), "Protagoras" (343B), "Phaedrus" (229E), "Philebus" (48C), "Laws" (II.923A), "Alcibiades I" (124A, 129A, 132C).

In Plato's "Charmides", Critias refers to the maxim consistently with the view expressed in the "Suda", with Critias saying, "for they imagined that 'Know Thyself!' was a piece of advice which the god gave and not his salutation of the worshippers at their first coming in." In modern words Critias gives his opinion that 'Know Thyself!' was an admonition to those entering the sacred temple to remember or know their place and Critias says, " 'know thyself!' and 'be temperate!' are the same. In the balance of the "Charmides", Plato has Socrates lead a longer inquiry as to how we may gain knowledge of ourselves.

In Plato's "Phaedrus", Socrates uses the maxim 'know thyself' as his explanation to Phaedrus to explain why he has no time for the attempts to rationally explain mythology or other far flung topics. Socrates says, "But I have no leisure for them at all; and the reason, my friend, is this: I am not yet able, as the Delphic inscription has it, to know myself; so it seems to me ridiculous, when I do not yet know that, to investigate irrelevant things."

In Plato's "Protagoras", Socrates lauds the authors of pithy and concise sayings delivered precisely at the right moment and says that Lacedaemon, or Sparta, educates its people to that end. Socrates lists the Seven Sages as Thales, Pittacus, Bias, Solon, Cleobulus, Myson, and Chilon, who he says are gifted in that Lacedaemonian art of concise words "twisted together, like a bowstring, where a slight effort gives great force." Socrates says examples of them are, "the far-famed inscriptions, which are in all men's mouths,--'Know thyself,' and 'Nothing too much'.". Having lauded the maxims, Socrates then spends a great deal of time getting to the bottom of what one of them means, the saying of Pittacus, 'Hard is it to be good.' The irony here is that although the sayings of Delphi bear 'great force,' it is not clear how to live life in accordance with their meanings. Although, the concise and broad nature of the sayings suggests the active partaking in the usage and personal discovery of each maxim; as if the intended nature of the saying lay not in the words but the self-reflection and self-referencing of the person thereof.
In Plato's "Philebus" dialogue, Socrates refers back to the same usage of 'know thyself' from "Phaedrus" to build an example of the ridiculous for Protarchus. Socrates says, as he did in Phaedrus, that people make themselves appear ridiculous when they are trying to know obscure things before they know themselves. Plato also alluded to the fact that understanding 'thyself,' would have a greater yielded factor of understanding the nature of a human being. Syllogistically, understanding oneself would enable thyself to have an understanding of others as a result.

The "Suda", a 10th-century encyclopedia of Greek knowledge, says: "the proverb is applied to those whose boasts exceed what they are", and that "know thyself" is a warning to pay no attention to the opinion of the multitude.

One work by the Medieval philosopher Peter Abelard is entitled "Scito te ipsum" (“know yourself”) or "Ethica".

From 1539 onwards the phrase "nosce te ipsum" and its Latin variants were often used in the anonymous texts written for anatomical fugitive sheets printed in Venice as well as for later anatomical atlases printed throughout Europe. The 1530s fugitive sheets are the first instances in which the phrase was applied to knowledge of the human body attained through dissection.

In 1651 Thomas Hobbes used the term "nosce teipsum" which he translated as 'read thyself' in his famous work, "The Leviathan". He was responding to a popular philosophy at the time that you can learn more by studying others than you can from reading books. He asserts that one learns more by studying oneself: particularly the feelings that influence our thoughts and motivate our actions. As Hobbes states, "but to teach us that for the similitude of the thoughts and passions of one man, to the thoughts and passions of another, whosoever looketh into himself and considereth what he doth when he does think, opine, reason, hope, fear, etc., and upon what grounds; he shall thereby read and know what are the thoughts and passions of all other men upon the like occasions."

In 1734 Alexander Pope wrote a poem entitled "An Essay on Man, Epistle II", which begins "Know then thyself, presume not God to scan, The proper study of mankind is Man."

In 1735 Carl Linnaeus published the first edition of "Systema Naturae" in which he described humans ("Homo") with the simple phrase "Nosce te ipsum".

In 1750 Benjamin Franklin, in his "Poor Richard's Almanack", observed the great difficulty of knowing one's self, with: "There are three Things extremely hard, Steel, a Diamond, and to know one's self." 

In 1754 Jean-Jacques Rousseau lauded the "inscription of the Temple at Delphi" in his "Discourse on the Origin of Inequality".

In 1831, Ralph Waldo Emerson wrote a poem entitled "Γνώθι Σεαυτόν", or Gnothi Seauton ('Know Thyself'), on the theme of 'God in thee.' The poem was an anthem to Emerson's belief that to 'know thyself' meant knowing the God which Emerson felt existed within each person.

In 1832 Samuel T. Coleridge wrote a poem entitled "Self Knowledge" in which the text centers on the Delphic maxim 'Know Thyself' beginning, 'Gnôthi seauton!--and is this the prime And heaven-sprung adage of the olden time!--' and ending with 'Ignore thyself, and strive to know thy God!' Coleridge's text references JUVENAL, xi. 27.

In 1902 Hugo von Hofmannsthal has his 16th-century alter ego in his letter to Francis Bacon mention a book he intended to call "Nosce te ipsum".

In 1904 Sarah Ida Shaw and Elanor Dorcas Pond founded the Delta Delta Delta fraternity. Part of their motto is Self-Knowledge, Self-Reverence, and Self-Discipline.

The Wachowskis used one of the Latin versions ("temet nosce") of this aphorism as inscription over the Oracle's door in their movies "The Matrix" (1999) and "The Matrix Revolutions" (2003). The transgender character Nomi in the Netflix show Sense8, again directed by The Wachowskis, has a tattoo on her arm with the Greek version of this phrase.

"Know Thyself" is the motto of Hamilton College, of Lyceum International School (Nugegoda, Sri Lanka) and of İpek University (Ankara, Turkey).

"Know Yourself" was the departing message of Miz Cracker on the tenth season of RuPaul's Drag Race.




</doc>
<doc id="13886123" url="https://en.wikipedia.org/wiki?curid=13886123" title="Noology">
Noology

Noology derives from the ancient Greek words νοῦς, "nous" or "mind" and λόγος, "logos". Noology thus outlines a systematic study and organization of thought, knowledge and the mind.

In the "Critique of Pure Reason", Immanuel Kant uses "noology" synonymously with rationalism, distinguishing it from empiricism: 

The Spanish philosopher Xavier Zubiri developed his own notion of noology.

The term is also used to describe the science of intellectual phenomena. It is the study of images of thought, their emergence, their genealogy, and their creation.




</doc>
<doc id="38724482" url="https://en.wikipedia.org/wiki?curid=38724482" title="Meta-Functional Expertise">
Meta-Functional Expertise

Meta-functional expertise is the breadth of one’s strategically important knowledge. This is different from the traditional conceptualization of expertise, which is generally considered to be a great depth of knowledge in a defined area. Thus, experts are people who are distinguished as knowing a lot about a particular subject.

Meta-functional experts, on the other hand, are considered be somewhat knowledgeable in many different areas but not necessarily an expert in any single domain. Someone high on meta-functional expertise is similar to a generalist in that they have a wide array of knowledge. However, where generalists know many different things meta-functional experts have enough depth of knowledge in each area to be considered knowledgeable by other members of their team at work.

Individuals high on meta-functional expertise are: 

Groups with more meta-functional experts on them perform better because they:


</doc>
<doc id="39380347" url="https://en.wikipedia.org/wiki?curid=39380347" title="Encyclopedic knowledge">
Encyclopedic knowledge

To have encyclopedic knowledge is to have "vast and complete" knowledge about a large number of diverse subjects. A person having such knowledge is called a human encyclopedia or a walking encyclopedia. The concept of encyclopedic knowledge was once attributed to exceptionally well-read or knowledgeable persons such as Plato, Aristotle, Hildegard von Bingen, Leonardo da Vinci, Immanuel Kant, or G. W. F. Hegel. Tom Rockmore described Hegel, for example, as a polymath and "a modern Aristotle, perhaps the last person to know everything of value that was known during his lifetime." Such persons are generally described as such based on their deep cognitive grasp of multiple and diverse fields of inquiry—an intellectually exceptional subset of philosophers who might also be differentiated from the multi-talented, the genius, or the "Renaissance man."

It is no longer considered realistic, or feasible, for any one person to be truthfully described as having encyclopedic knowledge. The concept has been subsumed into the discourses on the production of knowledge and artificial intelligence. Instead, we are now preoccupied with knowledge bases distributed as software or web services.

The idea of encyclopedic knowledge has made many appearances in popular culture, being especially widespread in detective fiction. In 1887, Sir Arthur Conan Doyle introduced his fictional master sleuth, Sherlock Holmes, who applied his keen deductive acumen and prodigious range of knowledge to solve his cases. "Encyclopedia Brown" is a series of books by Donald J. Sobol featuring the adventures of boy detective Leroy Brown, nicknamed "Encyclopedia" for his intelligence and range of knowledge that was first published in 1963.

One of the most celebrated is the fictional "Hitchhiker's Guide to the Galaxy" by the late Douglas Adams which began its evolution through numerous mediums as a British radio program in 1978. In 2004, NPR contributor A.J. Jacobs published "The Know-It-All", about his experience reading the entire "Encyclopædia Britannica" from start to finish.

While deep encyclopedic knowledge across numerous fields of inquiry by a single person is no longer feasible, encyclopedic knowledge within a field of inquiry or topic has great historical precedent and is still often ascribed to individuals. For example, it has been said of Raphael Lemkin that "his knowledge of the logic behind the Nazi war machine was encyclopedic."

In 1900, Alexander Graham Bell, who set out to read the entire "Encyclopædia Britannica" himself, served as the second president of the National Geographic Society and declared the Society should cover "the world and all that is in it." While this goal sounds all-encompassing, it is in fact a statement towards comprehensive geographic knowledge, meaning the scope of the National Geographic Society's enterprise should attempt to be terrestrially unbounded.

In an era of specialization, be it academic or functional or epistemological, obtaining "domain-specific" encyclopedic knowledge as an expert is typically celebrated and often rewarded by institutions in modern society. (This appreciation for having extensive niche knowledge, however, should not be confused with the historical experimentation and debate surrounding the "division of labor" which has been argued to limit the knowledge of workers compelled to perform repetitive tasks for the sake of an overall increase in economic productivity.)

Edward Said, in his seminal postcolonial work, "Orientalism", examines the encyclopedic endeavor in great detail, revealing it to be an historically . Orientalists' "unremitting ambition was to master "all" of a world, not some easily delimited part of it such as an author or a collection of texts."

Tim Chambers, an early Wikipedian who proposed the name "Wikipedia", has a page of historical interest in the Wikimedia archives entitled "The Value of Encyclopedic Knowledge" in which he describes a new model for growing encyclopedic knowledge "powered by "numerous" scholars around the world." The idea of encyclopedic knowledge being re-constellated as a community of knowledge is central to the theory of Connectivism as established by George Siemens and Stephen Downes.



</doc>
<doc id="18456" url="https://en.wikipedia.org/wiki?curid=18456" title="Literacy">
Literacy

Dictionaries traditionally define literacy as the ability to read and write. In the modern world, this is one way of interpreting literacy. One more broad interpretation sees literacy as knowledge and competence in a specific area. The concept of literacy has evolved in meaning. The modern term's meaning has been expanded to include the ability to use language, numbers, images, computers, and other basic means to understand, communicate, gain useful knowledge, solve mathematical problems and use the dominant symbol systems of a culture. The concept of literacy is expanding across OECD countries to include skills to access knowledge through technology and ability to assess complex contexts. A person who travels and resides in a foreign country but is unable to read or write in the language of the host country would be regarded by the locals as illiterate.

The key to literacy is reading development, a progression of skills which begins with the ability to understand spoken words and decode written words, and which culminates in the deep understanding of text. Reading development involves a range of complex language-underpinnings including awareness of speech sounds (phonology), spelling patterns (orthography), word meaning (semantics), grammar (syntax) and patterns of word formation (morphology), all of which provide a necessary platform for reading fluency and comprehension.

Once these skills are acquired, a reader can attain full language literacy, which includes the abilities to apply to printed material critical analysis, inference and synthesis; to write with accuracy and coherence; and to use information and insights from text as the basis for informed decisions and creative thought. The inability to do so is called "illiteracy" or "analphabetism".

Experts at a United Nations Educational, Scientific and Cultural Organization (UNESCO) meeting have proposed defining literacy as the "ability to identify, understand, interpret, create, communicate and compute, using printed and written materials associated with varying contexts". The experts note: "Literacy involves a continuum of learning in enabling individuals to achieve their goals, to develop their knowledge and potential, and to participate fully in their community and wider society".

Literacy emerged with the development of numeracy and computational devices as early as 8000 BCE. Script developed independently at least five times in human history Mesopotamia, Egypt, the Indus civilization, lowland Mesoamerica, and China.
The earliest forms of written communication originated in Serbia (Vinča culture), followed by Sumer, located in southern Mesopotamia about 3500-3000 BCE. During this era, literacy was "a largely functional matter, propelled by the need to manage the new quantities of information and the new type of governance created by trade and large scale production". Writing systems in Mesopotamia first emerged from a recording system in which people used impressed token markings to manage trade and agricultural production. The token system served as a precursor to early cuneiform writing once people began recording information on clay tablets. Proto-cuneiform texts exhibit not only numerical signs, but also ideograms depicting objects being counted.

Egyptian hieroglyphs emerged from 3300-3100 BCE and depicted royal iconography that emphasized power amongst other elites. The Egyptian hieroglyphic writing system was the first notation system to have phonetic values.

Writing in lowland Mesoamerica was first put into practice by the Olmec and Zapotec civilizations in 900-400 BCE. These civilizations used glyphic writing and bar-and-dot numerical notation systems for purposes related to royal iconography and calendar systems.

The earliest written notations in China date back to the Shang Dynasty in 1200 BCE. These systematic notations were found inscribed on bones and recorded sacrifices made, tributes received, and animals hunted, which were activities of the elite. These oracle-bone inscriptions were the early ancestors of modern Chinese script and contained logosyllabic script and numerals.

Indus script is largely pictorial and has not been deciphered yet. It may or may not include abstract signs. It is thought that they wrote from right to left and that the script is thought to be logographic. Because it has not been deciphered, linguists disagree on whether it is a complete and independent writing system; however, it is genuinely thought to be an independent writing system that emerged in the Harappa culture.

These examples indicate that early acts of literacy were closely tied to power and chiefly used for management practices, and probably less than 1% of the population was literate, as it was confined to a very small ruling elite.

According to social anthropologist Jack Goody, there are two interpretations that regard the origin of the alphabet. Many classical scholars, such as historian Ignace Gelb, credit the Ancient Greeks for creating the first alphabetic system (c. 750 BCE) that used distinctive signs for consonants and vowels. But Goody contests, "The importance of Greek culture of the subsequent history of Western Europe has led to an over-emphasis, by classicists and others, on the addition of specific vowel signs to the set of consonantal ones that had been developed earlier in Western Asia".

Thus, many scholars argue that the ancient Semitic-speaking peoples of northern Canaan (modern-day Syria) invented the consonantal alphabet as early as 1500 BCE. Much of this theory's development is credited to English archeologist Flinders Petrie, who, in 1905, came across a series of Canaanite inscriptions located in the turquoise mines of Serabit el-Khadem. Ten years later, English Egyptologist Alan Gardiner reasoned that these letters contain an alphabet, as well as references to the Canaanite goddess Asherah. In 1948, William F. Albright deciphered the text using additional evidence that had been discovered subsequent to Goody's findings. This included a series of inscriptions from Ugarit, discovered in 1929 by French archaeologist Claude F. A. Schaeffer. Some of these inscriptions were mythological texts (written in an early Canaanite dialect) that consisted of a 32-letter cuneiform consonantal alphabet.

Another significant discovery was made in 1953 when three arrowheads were uncovered, each containing identical Canaanite inscriptions from twelfth century BCE. According to Frank Moore Cross, these inscriptions consisted of alphabetic signs that originated during the transitional development from pictographic script to a linear alphabet. Moreover, he asserts, "These inscriptions also provided clues to extend the decipherment of earlier and later alphabetic texts".

The consonantal system of the Canaanite script inspired alphabetical developments in subsequent systems. During the Late Bronze Age, successor alphabets appeared throughout the Mediterranean region and were employed for Phoenician, Hebrew and Aramaic.

According to Goody, these cuneiform scripts may have influenced the development of the Greek alphabet several centuries later. Historically, the Greeks contended that their writing system was modeled after the Phoenicians. However, many Semitic scholars now believe that Ancient Greek is more consistent with an early form Canaanite that was used c. 1100 BCE. While the earliest Greek inscriptions are dated c. eighth century BCE, epigraphical comparisons to Proto-Canaanite suggest that the Greeks may have adopted the consonantal alphabet as early as 1100 BCE, and later "added in five characters to represent vowels".

Phoenician, which is considered to contain the first "linear alphabet", rapidly spread to the Mediterranean port cities in northern Canaan. Some archeologists believe that Phoenician scripture had some influence on the developments of the Hebrew and Aramaic alphabets based on the fact that these languages evolved during the same time period, share similar features, and are commonly categorized into the same language group.

When the Israelites migrated to Canaan between 1200 and 1001 BCE, they also adopted a variation of the Canaanite alphabet. Baruch ben Neriah, Jeremiah's scribe, used this alphabet to create the later scripts of the Old Testament. The early Hebrew alphabet was prominent in the Mediterranean region until Chaldean Babylonian rulers exiled the Jews to Babylon in the sixth century BCE. It was then that the new script ("Square Hebrew") emerged and the older one rapidly died out.

The Aramaic alphabet also emerged sometime between 1200 and 1000 BCE. As the Bronze Age collapsed, the Aramaeans moved into Canaan and Phoenician territories and adopted their scripts. Although early evidence of this writing is scarce, archeologists have uncovered a wide range of later Aramaic texts, written as early as the seventh century BCE. Due to its longevity and prevalence in the region, Achaemenid rulers would come to adopt it as a "diplomatic language". The modern Aramaic alphabet rapidly spread east to the Kingdom of Nabataea, then to Sinai and the Arabian Peninsula, eventually making its way to Africa. Aramaean merchants carried older variations of Aramaic as far as India, where it later influenced the development of the Brahmi script. It also led to the developments of Arabic and Pahlavi (an Iranian adaptation), "as well as for a range of alphabets used by early Turkish and Mongol tribes in Siberia, Mongolia and Turkestan". Literacy at this period spread with the merchant classes and may have grown to number 15-20% of the total population.

The Aramaic language declined with the spread of Islam, which was accompanied by the spread of Arabic.

Until recently it was thought that the majority of people were illiterate in ancient times. However, recent work challenges this perception. Anthony DiRenzo asserts that Roman society was "a civilization based on the book and the register", and "no one, either free or slave, could afford to be illiterate". Similarly Dupont points out, "The written word was all around them, in both public and private life: laws, calendars, regulations at shrines, and funeral epitaphs were engraved in stone or bronze. The Republic amassed huge archives of reports on every aspect of public life". The imperial civilian administration produced masses of documentation used in judicial, fiscal and administrative matters as did the municipalities. The army kept extensive records relating to supply and duty rosters and submitted reports. Merchants, shippers, and landowners (and their personal staffs) especially of the larger enterprises must have been literate.

In the late fourth century the Desert Father Pachomius would expect literacy of a candidate for admission to his monasteries:
they shall give him twenty Psalms or two of the Apostles' epistles or some other part of Scripture. And if he is illiterate he shall go at the first, third and sixth hours to someone who can teach and has been appointed for him. He shall stand before him and learn very studiously and with all gratitude. The fundamentals of a syllable, the verbs and nouns shall all be written for him and even if he does not want to he shall be compelled to read.

In the course of the 4th and 5th century the Churches made efforts to ensure a better clergy in particular among the bishops who were expected to have a classical education, which was the hallmark of a socially acceptable person in higher society (and possession of which allayed the fears of the pagan elite that their cultural inheritance would be destroyed). Even after the remnants of the Western Roman Empire fell in the 470s literacy continued to be a distinguishing mark of the elite as communications skills were still important in political and Church life (bishops were largely drawn from the senatorial class) in a new cultural synthesis that made "Christianity the Roman religion,". However, these skills were less in needed than previously in the absence of the large imperial administrative apparatus whose middle and top echelons the elite had dominated as if by right. Even so, in pre-modern times it is unlikely that literacy was found in more than about 30-40% of the population. The highest percentage of literacy during the Dark Ages was among the clergy and monks who supplied much of the staff needed to administer the states of western Europe.

Post-Antiquity illiteracy was made much worse by the lack of a suitable writing medium. When the Western Roman Empire collapsed, the import of papyrus to Europe ceased. Since papyrus perishes easily and does not last well in the wetter European climate, parchment was used, which was expensive and accessible only by the Church and the wealthy. Paper was introduced into Europe in Spain in the 11th century. Its use spread north slowly over the next four centuries. Literacy saw a resurgence as a result, and by the 15th century paper had largely replaced parchment except for luxury manuscripts.

The Reformation stressed the importance of literacy and being able to read the Bible. The Protestant countries were the first to attain full literacy; Scandinavian countries were fully literate in the early 17th century. The Church demanded literacy as the pre-requisite for marriage in Sweden, further propagating full literacy.

Literacy data published by UNESCO displays that since 1950, the adult literacy rate at the world level has increased by 5 percentage points every decade on average, from 55.7 per cent in 1950 to 86.2 per cent in 2015. However, for four decades, the population growth was so rapid that the number of illiterate adults kept increasing, rising from 700 million in 1950 to 878 million in 1990. Since then, the number has fallen markedly to 745 million in 2015, although it remains higher than in 1950 despite decades of universal education policies, literacy interventions and the spread of print material and information and communications technology (ICT). However, these trends have been far from uniform across regions.

Available global data indicates significant variations in literacy rates between world regions. North America, Europe, West Asia, and Central Asia have achieved almost full adult literacy (individuals at or over the age of 15) for both men and women. Most countries in East Asia and the Pacific, as well as Latin America and the Caribbean, are above a 90% literacy rate for adults. Illiteracy persists to a greater extent in other regions: 2013 UNESCO Institute for Statistics (UIS) data indicates adult literacy rates of only, 67.55% in South Asia and North Africa, 59.76% in Sub-Saharan Africa.

In much of the world, high youth literacy rates suggest that illiteracy will become less and less common as younger generations with higher educational attainment levels replace older ones. However, in sub-Saharan Africa and South Asia, where the vast majority of the world's illiterate youth live, lower school enrollment implies that illiteracy will persist to a greater degree. According to 2013 UIS data, the youth literacy rate (individuals ages 15 to 24) is 84.03% in South Asia and North Africa, and 70.06% in Sub-Saharan Africa.

That being said, literacy has rapidly spread in several regions in the last twenty-five years (see image).

On a worldwide scale, illiteracy disproportionately impacts women. According to 2015 UIS data collected by the UNESCO Institute for Statistics, about two-thirds (63%) of the world's illiterate adults are women. This disparity was even starker in previous decades: from 1970 to 2000, the global gender gap in literacy would decrease by roughly 50%. In recent years, however, this progress has stagnated, with the remaining gender gap holding almost constant over the last two decades. In general, the gender gap in literacy is not as pronounced as the regional gap; that is, differences between countries in overall literacy are often larger than gender differences within countries. However, the gap between men and women would narrow from 1990 onwards, after the increase of male adult literacy rates at 80 per cent (see image).

Sub-Saharan Africa, the region with the lowest overall literacy rates, also features the widest gender gap: just 52% of adult females are literate, and 68% among adult men. Similar gender disparity persists in two other regions, North Africa (86% adult male literacy, 70% adult female literacy) and South Asia (77% adult male literacy, 58% adult female literacy).

The 1990 World Conference on Education for All, held in Jomtien, Thailand, would bring attention to the literacy gender gap and prompt many developing countries to prioritize women's literacy. In the past decade, global development agendas would increasingly address the issue of female literacy. For example, UN Secretary-General Ban Ki-moon would center his 2010 International Literacy Day speech around the theme "Empowering Women Through Literacy Empowers Us All," emphasizing the broad societal progress that higher female literacy rates could promote.

In many contexts, female illiteracy co-exists with other aspects of gender inequality. Martha Nussbaum, for example, make illiterate women more vulnerable to becoming trapped in an abusive marriage, given that illiteracy limits their employment opportunities and worsens their intra-household bargaining position. Moreover, Nussbaum links literacy to the potential for women to effectively communicate and collaborate with one another in order "to participate in a larger movement for political change."

Social barriers prevent expanding literacy skills among women and girls. Making literacy classes available can be ineffective when it conflicts with the use of the valuable limited time of women and girls. School age girls, in many contexts, face stronger expectations than their male counterparts to perform household work and care after younger siblings. Generational dynamics can also perpetuate these disparities: illiterate parents may not readily appreciate the value of literacy for their daughters, particularly in traditional, rural societies with expectations that girls will remain at home.

A 2015 World Bank and the International Center for Research on Women review of academic literature would conclude that child marriage, which predominantly impacts girls, tends to reduce literacy levels. A 2008 analysis of the issue in Bangladesh found that for every additional year of delay in a girl's marriage, her likelihood of literacy would increase by 5.6 percent. Similarly, a 2014 study found that in sub-Saharan Africa, marrying early would significantly decrease a girl's probability of literacy, holding other variables constant. A 2015 review of the child marriage literature therefore would recommend marriage postponement as part of a strategy to increase educational attainment levels, including female literacy in particular.

While women and girls comprise the majority of the global illiterate population, in many developed countries a literacy gender gap exists in the opposite direction. Data from the Programme for International Student Assessment (PISA) has consistently indicated the literacy underachievement of boys within member countries of the Organisation for Economic Co-operation and Development (OECD). In view of such findings, many education specialists have recommended changes in classroom practices to better accommodate boys' learning styles, and to remove any gender stereotypes that may create a perception of reading and writing as feminine activities.

Many policy analysts consider literacy rates as a crucial measure of the value of a region's human capital. For example, literate people can be more easily trained than illiterate people, and generally have a higher socioeconomic status; thus they enjoy better health and employment prospects. The international community has come to consider literacy as a key facilitator and goal of development. In regard to the Sustainable Development Goals adopted by the UN in 2015, the UNESCO Institute for Lifelong Learning has declared the "central role of literacy in responding to sustainable development challenges such as health, social equality, economic empowerment and environmental sustainability." A majority of prisoners have been found to be illiterate: In Edinburgh prison, winner of the 2010 Libraries Change Lives Award, "the library has become the cornerstone of the prison's literacy strategy" and thus recidivism and reoffending can be reduced, and incarcerated persons can work toward attaining higher socioconomic status once released.

Illiterate people are generally less knowledgeable about hygiene and nutritional practices, an unawareness which can exacerbate a wide range of health issues. Within developing countries in particular, literacy rates also have implications for child mortality; in these contexts, children of literate mothers are 50% more likely to live past age 5 than children of illiterate mothers. Public health research has thus increasingly concerned itself with the potential for literacy skills to allow women to more successfully access health care systems, and thereby facilitate gains in child health.

For example, a 2014 descriptive research survey project correlates literacy levels with the socioeconomic status of women in Oyo State, Nigeria. The study claims that developing literacy in this area will bring "economic empowerment and will encourage rural women to practice hygiene, which will in turn lead to the reduction of birth and death rates."

Literacy can increase job opportunities and access to higher education. In 2009, the National Adult Literacy agency (NALA) in Ireland commissioned a cost benefit analysis of adult literacy training. This concluded that there were economic gains for the individuals, the companies they worked for, and the Exchequer, as well as the economy and the country as a whole—for example, increased GDP. Korotayev and coauthors have revealed a rather significant correlation between the level of literacy in the early 19th century and successful modernization and economic breakthroughs in the late 20th century, as "literate people could be characterized by a greater innovative-activity level, which provides opportunities for modernization, development, and economic growth".

While informal learning within the home can play an important role in literacy development, gains in childhood literacy often occur in primary school settings. Continuing the global expansion of public education is thus a frequent focus of literacy advocates. These kinds of broad improvements in education often require centralized efforts undertaken by national governments; alternatively, local literacy projects implemented by NGOs can play an important role, particularly in rural contexts.

Funding for both youth and adult literacy programs often comes from large international development organizations. USAID, for example, steered donors like the Bill and Melinda Gates Foundation and the Global Partnership for Education toward the issue of childhood literacy by developing the Early Grade Reading Assessment. Advocacy groups like the National Institute of Adult Continuing Education have frequently called upon international organizations such as UNESCO, the International Labour Organization, the World Health Organization, and the World Bank to prioritize support for adult women's literacy. Efforts to increase adult literacy often encompass other development priorities as well; for example, initiatives in Ethiopia, Morocco, and India have combined adult literacy programs with vocational skills trainings in order to encourage enrollment and address the complex needs of women and other marginalized groups who lack economic opportunity.

In 2013, the UNESCO Institute for Lifelong Learning published a set of case studies on programs that successfully improved female literacy rates. The report features countries from a variety of regions and of differing income levels, reflecting the general global consensus on "the need to empower women through the acquisition of literacy skills." Part of the impetus for UNESCO's focus on literacy is a broader effort to respond to globalization and "the shift towards knowledge-based societies" that it has produced. While globalization presents emerging challenges, it also provides new opportunities: many education and development specialists are hopeful that new information and communications technologies (ICTs) will have the potential to expand literacy learning opportunities for children and adults, even those in countries that have historically struggled to improve literacy rates through more conventional means.

The Human Development Index, produced by the United Nations Development Programme (UNDP), uses education as one of its three indicators; originally, adult literacy represented two-thirds of this education index weight. In 2010, however, the UNDP replaced the adult literacy measure with mean years of schooling. A 2011 UNDP research paper framed this change as a way to "ensure current relevance," arguing that gains in global literacy already achieved between 1970 and 2010 meant that literacy would be "unlikely to be as informative of the future." Other scholars, however, have since warned against overlooking the importance of literacy as an indicator and a goal for development, particularly for marginalized groups such as women and rural populations.

Unlike medieval times, when reading and writing skills were restricted to a few elites and the clergy, these literacy skills are now expected from every member of a society. Literacy is a human right essential for lifelong learning and social change. As supported by the 1996 Report of the International Commission on Education for the Twenty-First Century, and the 1997 Hamburg Declaration: ‘Literacy, broadly conceived as the basic knowledge and skills needed by all in a rapidly changing world, is a fundamental human right. (...) There are millions, the majority of whom are women, who lack opportunities to learn or who have insufficient skills to be able to assert this right. The challenge is to enable them to do so. This will often imply the creation of preconditions for learning through awareness raising and empowerment. Literacy is also a catalyst for participation in social, cultural, political and economic activities, and for learning throughout life’. 

The public library has long been a force promoting literacy in many countries. In the U.S. context, the American Library Association promotes literacy through the work of the Office for Literacy and Outreach Services. This committee's charge includes ensuring equitable access to information and advocating for adult new and non-readers. The Public Library Association recognizes the importance of early childhood in the role of literacy development and created, in collaboration with the Association for Library Service to Children, Every Child Ready to Read @your library in order to inform and support parents and caregivers in their efforts to raise children who become literate adults. The release of the National Assessment of Adult Literacy (NAAL) report in 2005 revealed that approximately 14% of U.S. adults function at the lowest level of literacy; 29% of adults function at the basic functional literacy level and cannot help their children with homework beyond the first few grades. The lack of reading skills hinders adults from reaching their full potential. They might have difficulty getting and maintaining a job, providing for their families, or even reading a story to their children. For adults, the library might be the only source of a literacy program.

Dia! Which stand for Diversity in Action and is also known as "El Dia de los Ninos/El dia de los libros (Children's Day/Book Day)" is a program which celebrates the importance of reading to children from all cultural and linguistic backgrounds. Dia! is celebrated every year on 30 April in schools, libraries, and homes and this website provides tools and programs to encourage reading in children. Parents, caregivers, and educators can even start a book club.

This community literacy program was initiated in 1992 by the Orange County Public Library in California. The mission of READ/Orange County is to "create a more literate community by providing diversified services of the highest quality to all who seek them." Potential tutors train during an extensive 23-hour tutor training workshop in which they learn the philosophy, techniques and tools they will need to work with adult learns. After the training, the tutors invest at least 50 hours a year to tutoring their student.The organization builds on people's experience as well as education rather than trying to make up for what has not been learned. The program seeks to equip students with skills to continue learning in the future. The guiding philosophy is that an adult who learns to read creates a ripple effect in the community. The person becomes an example to children and grandchildren and can better serve the community.

Located in Boulder, Colorado, the program recognized the difficulty that students had in obtaining child care while attending tutoring sessions, and joined with the University of Colorado to provide reading buddies to the children of students. Reading Buddies matches children of adult literacy students with college students who meet with them once a week throughout the semester for an hour and a half. The college students receive course credit to try to enhance the quality and reliability of their time. Each Reading Buddies session focuses primarily on the college student reading aloud with the child. The goal is to help the child gain interest in books and feel comfortable reading aloud. Time is also spent on word games, writing letters, or searching for books in the library. Throughout the semester the pair work on writing and illustrating a book together. The college student's grade is partly dependent on the completion of the book. Although Reading Buddies began primarily as an answer to the lack of child care for literacy students, it has evolved into another aspect of the program. Participating children show marked improvement in their reading and writing skills throughout the semester.

Approximately 120,000 adults in Hillsborough County are illiterate or read below the fourth-grade level; According to 2003 Census statistics, 15 percent of Hillsborough County residents age 16 and older lacked basic prose literacy skills. Since 1986, the Hillsborough Literacy Council is "committed to improving literacy by empowering adults through education". Sponsored by the statewide Florida Literacy Coalition and affiliated with Tampa-Hillsborough Public Library System, HLC strives to improve the literacy ability of adults in Hillsborough County, Florida. Using library space, the HLC provides tutoring for English for speakers of other languages (ESOL) in small groups or one-on-one tutoring. Through one-on-one tutoring, the organization works to help adult students reach at least the fifth-grade level in reading. The organization also provides volunteer-run conversation groups for English practice.

Traditionally, literacy is the ability to use written language actively and passively; one definition of literacy is the ability to "read, write, spell, listen, and speak". Since the 1980s, some have argued that literacy is ideological, which means that literacy always exists in a context, in tandem with the values associated with that context. Prior work viewed literacy as existing autonomously.

Some have argued that the definition of literacy should be expanded. For example, in the United States, the National Council of Teachers of English and the International Reading Association have added "visually representing" to the traditional list of competencies. Similarly, in Scotland, literacy has been defined as: "The ability to read, write and use numeracy, to handle information, to express ideas and opinions, to make decisions and solve problems, as family members, workers, citizens and lifelong learners". It is argued that literacy includes the cultural, political, and historical contexts of the community in which communication takes place.

A basic literacy standard in many places is the ability to read the newspaper. Increasingly, communication in commerce and in general requires the ability to use computers and other digital technologies. Since the 1990s, when the Internet came into wide use in the United States, some have asserted that the definition of literacy should include the ability to use tools such as web browsers, word processing programs, and text messages. Similar expanded skill sets have been called computer literacy, information literacy, and technological literacy. Some scholars propose the idea multiliteracies which includes Functional Literacy, Critical Literacy, and Rhetorical Literacy.

"Arts literacy" programs exist in some places in the United States. Visual literacy also includes the ability to understand visual forms of communication such as body language, pictures, maps, and video. (An inability to "read" body language is one of the features of autism.) Evolving definitions of literacy often include all the symbol systems relevant to a particular community.

Other genres under study by academia include critical literacy, media literacy, ecological literacy and health literacy With the increasing emphasis on evidence-based decision making, and the use of statistical graphics and information, statistical literacy is becoming a very important aspect of literacy in general. The International Statistical Literacy Project is dedicated to the promotion of statistical literacy among all members of society.

Given that a large part of the benefits of literacy can be obtained by having access to a literate person in the household, some recent literature in economics, starting with the work of Kaushik Basu and James Foster, distinguishes between a "proximate illiterate" and an "isolated illiterate". The former refers to an illiterate person who lives in a household with literates and the latter to an illiterate who lives in a household of all illiterates. What is of concern is that many people in poor nations are not just illiterates but isolated illiterates.

Teaching English literacy in the United States is dominated by a focus on a set of discrete decoding skills. From this perspective, literacy—or, rather, reading—comprises a number of subskills that can be taught to students. These skill sets include phonological awareness, phonics (decoding), fluency, comprehension, and vocabulary. Mastering each of these subskills is necessary for students to become proficient readers.

From this same perspective, readers of alphabetic languages must understand the alphabetic principle to master basic reading skills. For this purpose a writing system is "alphabetic" if it uses symbols to represent individual language sounds, though the degree of correspondence between letters and sounds varies between alphabetic languages. Syllabic writing systems (such as Japanese kana) use a symbol to represent a single syllable, and logographic writing systems (such as Chinese) use a symbol to represent a morpheme.

There are any number of approaches to teaching literacy; each is shaped by its informing assumptions about what literacy is and how it is best learned by students. Phonics instruction, for example, focuses on reading at the level of the word. It teaches readers to observe and interpret the letters or groups of letters that make up words. A common method of teaching phonics is synthetic phonics, in which a novice reader pronounces each individual sound and "blends" them to pronounce the whole word. Another approach is embedded phonics instruction, used more often in whole language reading instruction, in which novice readers learn about the individual letters in words on a just-in-time, just-in-place basis that is tailored to meet each student's reading and writing learning needs. That is, teachers provide phonics instruction opportunistically, within the context of stories or student writing that feature many instances of a particular letter or group of letters. Embedded instruction combines letter-sound knowledge with the use of meaningful context to read new and difficult words. Techniques such as directed listening and thinking activities can be used to aid children in learning how to read and reading comprehension.

In a 2012 proposal, it has been claimed that reading can be acquired naturally if print is constantly available at an early age in the same manner as spoken language. If an appropriate form of written text is made available before formal schooling begins, reading should be learned inductively, emerge naturally, and with no significant negative consequences. This proposal challenges the commonly held belief that written language requires formal instruction and schooling. Its success would change current views of literacy and schooling. Using developments in behavioral science and technology, an interactive system (Technology Assisted Reading Acquisition, TARA) would enable young pre-literate children to accurately perceive and learn properties of written language by simple exposure to the written form.

In Australia a number of State governments have introduced Reading Challenges to improve literacy. The Premier's Reading Challenge in South Australia, launched by Premier Mike Rann has one of the highest participation rates in the world for reading challenges. It has been embraced by more than 95% of public, private and religious schools.

Programs have been implemented in regions that have an ongoing conflict or in a post-conflict stage. The Norwegian Refugee Council Pack program has been used in 13 post-conflict countries since 2003. The program organizers believe that daily routines and other wise predictable activities help the transition from war to peace. Learners can select one area in vocational training for a year-long period. They complete required courses in agriculture, life skills, literacy and numeracy. Results have shown that active participation and management of the members of the program are important to the success of the program. These programs share the use of integrated basic education, e.g. literacy, numeracy, scientific knowledge, local history and culture, native and mainstream language skills, and apprenticeships.

Although there is considerable awareness that language deficiencies (lacking proficiency) are disadvantageous to immigrants settling in a new country, there appears to be a lack of pedagogical approaches that address the instruction of literacy to migrant English language learners (ELLs). Harvard scholar Catherine Snow (2001) called for a gap to be addresses: "The TESOL field needs a concerted research effort to inform literacy instruction for such children ... to determine when to start literacy instruction and how to adapt it to the LS reader's needs". The scenario becomes more complex when there is no choice in such decisions as in the case of the current migration trends with citizens from the Middle East and Africa being relocated to English majority nations due to various political or social reasons. Recent developments to address the gap in teaching literacy to second or foreign language learners has been ongoing and promising results have been shown by Pearson and Pellerine (2010) which integrates Teaching for Understanding, a curricular framework from the Harvard Graduate School of Education. A series of pilot projects had been carried out in the Middle East and Africa (see Patil, 2016). In this work significant interest from the learners perspective have been noticed through the integration of visual arts as springboards for literacy oriented instruction. In one case migrant women had been provided with cameras and a walking tour of their local village was provided to the instructor as the women photographed their tour focusing on places and activities that would later be used for writings about their daily life. In essence a narrative of life. Other primers for writing activities include: painting, sketching, and other craft projects (e.g. gluing activities).

A series of pilot studies were carried out to investigate alternatives to instructing literacy to migrant ELLs, starting from simple trials aiming to test the teaching of photography to participants with no prior photography background, to isolating painting and sketching activities that could later be integrated into a larger pedagogical initiative. In efforts to develop alternative approaches for literacy instruction utilising visual arts, work was carried out with Afghan labourers, Bangladeshi tailors, Emirati media students, internal Ethiopian migrants (both labourers and university students), and a street child.
It should be pointed out that in such challenging contexts sometimes the teaching of literacy may have unforeseen barriers. The "EL Gazette" reported that in the trials carried out in Ethiopia, for example, it was found that all ten of the participants had problems with vision. In order to overcome this, or to avoid such challenges, preliminary health checks can help inform pre-teaching in order to better assist in the teaching/learning of literacy.

In a visual arts approach to literacy instruction a benefit can be the inclusion of both a traditional literacy approach (reading and writing) while at the same time addressing 21st Century digital literacy instruction through the inclusion of digital cameras and posting images onto the web. Many scholars feel that the inclusion of digital literacy is necessary to include under the traditional umbrella of literacy instruction specifically when engaging second language learners. (Also see: Digital literacy.) 

Other ways in which visual arts have been integrated into literacy instruction for migrant populations include integrating aspects of visual art with the blending of core curricular goals.

A more pressing challenge in education is the instruction of literacy to Migrant English Language Learners (MELLs), a term coined by Pellerine. It is not just limited to English. “Due to the growing share of immigrants in many Western societies, there has been increasing concern for the degree to which immigrants acquire language that is spoken in the destination country” (Tubergen 2006). Remembering that teaching literacy to a native in their L1 can be challenging, and the challenge becomes more cognitively demanding when in a second language (L2), the task can become considerably more difficult when confronted with a migrant who has made a sudden change (migrated) and requires the second language upon arrival in the country of destination. In many instances a migrant will not have the opportunity, for many obvious reasons, to start school again at grade one and acquire the language naturally. In these situations alternative interventions need to take place.

In working with illiterate people (and individuals with low-proficiency in an L2) following the composition of some artifact like in taking a photo, sketching an event, or painting an image, a stage of orality has been seen as an effective way to understand the intention of the learner.

In the accompanying image from left to right a) an image taken during a phototour of the participant's village. This image is of the individual at her shop, and this is one of her products that she sells, dung for cooking fuel. The image helps the interlocutor understand the realities of the participants daily life and most importantly it allows the participant the opportunity to select what they feel is important to them. b) This is an image of a student explaining and elaborating the series of milestones in her life to a group. In this image the student had a very basic ability and with some help was able to write brief captions under the images. While she speaks a recording of her story takes place to understand her story and to help develop it in the L2. The third image is of a painting that had been used with a composite in Photoshop. With further training participants can learn how to blend images they would like to therefore introducing elements of digital literacies, beneficial in many spheres of life in the 21st century.

In the following image (see right) you can see two samples 1) One in Ethiopia from stencil to more developed composition based on a village tour, photography, and paintings. 2) In the Middle East at a tailor's shop focusing English for Specific Purposes (ESP) and in this example the writing has evolved from photography, sketching, and in situ exposure for the instructor (much like the village tour in sample one).

From the work based in Ethiopia, participants were asked to rate preference of activity, on a scale of 1-10. The survey prompt was: On a scale of 1 - 10 how would you rate photography as an activity that helped you get inspiration for your writing activities (think of enjoyment and usefulness). The following activities were rated, in order of preference - activities used as primers for writing:


More research would need to be conducted to confirm such trends.

In bringing work together from students in culminating projects, authorship programs have been successful in bringing student work together in book format. Such artifacts can be used to both document learning, but more importantly reinforce language and content goals.

The culmination of such writings, into books can evoke both intrinsic and extrinsic motivation. Form feedback by students involved in such initiatives the responses have indicated that the healthy pressures of collective and collaborative work was beneficial.

Teaching people to read and write, in a traditional sense of the meaning (literacy) is a very complex task in a native language. To do this in a second language becomes increasingly more complex, and in the case of migrants relocating to another country there can be legal and policy driven boundaries that prohibit the naturalization and acquisition of citizen ship based on language proficiency. In Canada for example despite a debate, language tests are required years after settling into Canada. Similar exists globally, see:, and for example.

The "EL Gazette" reviewed Pellerine's work with migrant English language learners and commented: "Handing English language learners a sponge and some paint and asking them to ‘paint what comes’ might not appear like a promising teaching method for a foreign language. But Canadian EL instructor and photographer Steve Pellerine has found that the technique, along with others based around the visual arts, has helped some of his most challenging groups to learn". Visual arts have been viewed as an effective way to approach literacy instruction - the art being primers for subsequent literacy tasks within a scaffolded curricular design, such at Teaching for Understanding (TfU) or Understanding by Design (UbD).

Nearly one in ten young adult women have poor reading and writing skills in the UK in the 21st century. This seriously damages their employment prospects and many are trapped in poverty. Lack of reading skill is a social stigma and women tend to hide their difficulty rather than seeking help. Girls on average do better than boys at English in school. A quarter of British adults would struggle to read a bus timetable.

Literacy is first documented in the area of modern England on 24 September 54 BCE, on which day Julius Caesar and Quintus Cicero wrote to Cicero "from the nearest shores of Britain". Literacy was widespread under Roman rule, but became very rare, limited almost entirely to churchmen, after the fall of the Western Roman Empire. In 12th and 13th century England, the ability to recite a particular passage from the Bible in Latin entitled a common law defendant to the so-called benefit of clergy: i.e. trial before an ecclesiastical court, where sentences were more lenient, instead of a secular one, where hanging was a likely sentence. Thus literate lay defendants often claimed benefit of clergy, while an illiterate person who had memorized the psalm used as the literacy test, Psalm 51 ("O God, have mercy upon me..."), could also claim benefit of clergy. Despite lacking a system of free and compulsory primary schooling, England reached near universal literacy in the 19th century as a result of shared, informal learning provided by family members, fellow workers, and/or benevolent employers. Even with near universal literacy rates, the gap between male and female literacy rates persisted until the early 20th century. Many women in the West during the 19th century were able to read, but unable to write.

Formal higher education in the arts and sciences in Wales, from the Middle Ages to the 18th century, was the preserve of the wealthy and the clergy. As in England, Welsh history and archaeological finds dating back to the Bronze Age reveal not only reading and writing, but also alchemy, botany, advanced maths and science. Following the Roman occupation and the conquest by the English, education in Wales was at a very low ebb in the early modern period; in particular, formal education was only available in English while the majority of the population spoke only Welsh. The first modern grammar schools were established in Welsh towns such as Ruthin, Brecon, and Cowbridge. One of the first modern national education methods to use the native Welsh language was started by Griffith Jones in 1731. Jones was the rector of Llanddowror from 1716 and remained there for the rest of his life. He organized and introduced a Welsh medium circulating school system, which was attractive and effective for Welsh speakers, while also teaching them English, which gave them access to broader educational sources. The circulating schools may have taught half the country's population to read. Literacy rates in Wales by the mid-18th century were one of the highest.
The ability to read did not necessarily imply the ability to write. The 1686 church law ("kyrkolagen") of the Kingdom of Sweden (which at the time included all of modern Sweden, Finland, Latvia and Estonia) enforced literacy on the people, and by 1800 the ability to read was close to 100%. This was directly dependent on the need to read religious texts in the Lutheran faith in Sweden and Finland. As a result, literacy in these countries was inclined towards reading, specifically. But as late as the 19th century, many Swedes, especially women, could not write. The exception to this rule were the men and women of Iceland who achieved widespread literacy without formal schooling, libraries, or printed books via informal tuition by religious leaders and peasant teachers. That said, the situation in England was far worse than in Scandinavia, France, and Prussia: as late as 1841, 33% of all Englishmen and 44% of Englishwomen signed marriage certificates with their mark as they were unable to write (government-financed public education was not available in England until 1870 and, even then, on a limited basis).

Historian Ernest Gellner argues that Continental European countries were far more successful in implementing educational reform precisely because their governments were more willing to invest in the population as a whole. Government oversight allowed countries to standardize curriculum and secure funding through legislation thus enabling educational programs to have a broader reach.

Although the present-day concepts of literacy have much to do with the 15th-century invention of the movable type printing press, it was not until the Industrial Revolution of the mid-19th century that paper and books became affordable to all classes of industrialized society. Until then, only a small percentage of the population were literate as only wealthy individuals and institutions could afford the materials. Even , the cost of paper and books is a barrier to universal literacy in some less-industrialized nations.

On the other hand, historian Harvey Graff argues that the introduction of mass schooling was in part an effort to control the type of literacy that the working class had access to. According to Graff, literacy learning was increasing outside of formal settings (such as schools) and this uncontrolled, potentially critical reading could lead to increased radicalization of the populace. In his view, mass schooling was meant to temper and control literacy, not spread it. Graff also points out, using the example of Sweden, that mass literacy can be achieved without formal schooling or instruction in writing.

Research on the literacy rates of Canadians in the colonial days rested largely on examinations of the proportion of signatures to marks on parish acts (birth, baptismal, and marriage registrations). Although some researchers have concluded that signature counts drawn from marriage registers in nineteenth century France corresponded closely with literacy tests given to military conscripts, others regard this methodology as a "relatively unimaginative treatment of the complex practices and events that might be described as literacy" (Curtis, 2007, p. 1-2). But censuses (dating back to 1666) and official records of New France offer few clues of their own on the population's levels of literacy, therefore leaving few options in terms of materials from which to draw literary rate estimates.

In his research of literacy rates of males and females in New France, Trudel found that in 1663, of 1,224 persons in New France who were of marriageable age, 59% of grooms and 46% of brides wrote their name; however, of the 3,000-plus colony inhabitants, less than 40% were native born. Signature rates were therefore likely more reflective of rates of literacy among French immigrants. Magnuson's (1985) research revealed a trend: signature rates for the period of 1680–1699 were 42% for males, 30% for females; in 1657-1715, they were 45% for males and 43% for females; in 1745-1754, they were higher for females than for males. He believed that this upward trend in rates of females’ ability to sign documents was largely attributed to the larger number of female religious orders, and to the proportionately more active role of women in health and education, while the roles of male religious orders were largely to serve as parish priests, missionaries, military chaplains and explorers. 1752 marked the date that Canada's first newspaper—the "Halifax Gazette"—began publication.

The end of the Seven Years' War in 1763 allowed two Philadelphia printers to come to Québec City and to begin printing a bilingual "Quebec Gazette" in 1764, while in 1785 Fleury Mesplet started publication of the "Montreal Gazette", which is now the oldest continuing newspaper in the country.

In the 19th century, everything about print changed, and literature in its many forms became much more available. But educating the Canadian population in reading and writing was nevertheless a huge challenge. Concerned about the strong French Canadian presence in the colony, the British authorities repeatedly tried to help establish schools that were outside the control of religious authorities, but these efforts were largely undermined by the Catholic Church and later the Anglican clergy.

From the early 1820s in Lower Canada, classical college curriculum, which was monopolized by the Church, was also subject to growing liberal and lay criticism, arguing it was fit first and foremost to produce priests, when Lower Canadians needed to be able to compete effectively with foreign industry and commerce and with the immigrants who were monopolizing trade (Curtis, 1985). Liberal and lay attempts to promote parish schools generated a reaction from the Catholic and later the Anglican clergy in which the dangers of popular literacy figured centrally. Both churches shared an opposition to any educational plan that encouraged lay reading of the Bible, and spokesmen for both warned of the evil and demoralizing tendencies of unregulated reading in general. Granted the power to organize parish schooling through the Vestry School Act of 1824, the Catholic clergy did nothing effective.

Despite this, the invention of the printing press had laid the foundation for the modern era and universal social literacy, and so it is that with time, "technologically, literacy had passed from the hands of an elite to the populace at large. Historical factors and sociopolitical conditions, however, have determined the extent to which universal social literacy has come to pass".

In 1871 only about half of French Canadian men in Canada self-reported that they were literate, whereas 90 percent of other Canadian men said they could read and write, but information from the Canadian Families Project sample of the 1901 Census of Canada indicated that literacy rates for French Canadians and other Canadians increased, as measured by the ability of men between the ages of 16 and 65 to answer literacy questions. Compulsory attendance in schools was legislated in the late 19th century in all provinces but Quebec, but by then, a change in parental attitudes towards educating the new generation meant that many children were already attending regularly. Unlike the emphasis of school promoters on character formation, the shaping of values, the inculcation of political and social attitudes, and proper behaviour, many parents supported schooling because they wanted their children to learn to read, write, and do arithmetic. Efforts were made to exert power and religious, moral, economic/professional, and social/cultural influence over children who were learning to read by dictating the contents of their school readers accordingly. But educators broke from these spheres of influence and also taught literature from a more child-centred perspective: for the pleasure of it.

Educational change in Québec began as a result of a major commission of inquiry at the start of what came to be called the "Quiet Revolution" in the early 1960s. In response to the resulting recommendations, the Québec government revamped the school system in an attempt to enhance the francophone population's general educational level and to produce a better-qualified labour force. Catholic Church leadership was rejected in favour of government administration and vastly increased budgets were given to school boards across the province.

With time, and with continuing inquiry into the literacy achievement levels of Canadians, the definition of literacy moved from a dichotomous one (either a person could, or couldn't write his or her name, or was literate or illiterate), to ones that considered its multidimensionality, along with the qualitative and quantitative aspects of literacy. In the 1970s, organizations like the Canadian Association for Adult Education (CAAE) believed that one had to complete the 8th grade to achieve functional literacy. Examination of 1976 census data, for example, found that 4,376,655, or 28.4% of Canadians 15 years of age and over reported a level of schooling of less than grade 9 and were thus deemed not functionally literate. But in 1991, UNESCO formally acknowledged Canada's findings that assessment of educational attainment as proxy measure of literacy was not as reliable as was direct assessment. This dissatisfaction manifested itself in the development of actual proficiency tests that measure reading literacy more directly.

Canada conducted its first literacy survey in 1987 which discovered that there were more than five million functionally illiterate adults in Canada, or 24 per cent of the adult population. Statistics Canada then conducted three national and international literacy surveys of the adult population — the first one in 1989 commissioned by the Human Resources and Skills Development Canada (HRSDC) department.

This first survey was called the "Literacy Skills Used in Daily Activities" (LSUDA) survey, and was modeled on the 1985 U.S. survey of young adults (YALS). It represented a first attempt in Canada to produce skill measures deemed comparable across languages. Literacy, for the first time, was measured on a continuum of skills. The survey found that 16% of Canadians had literacy skills too limited to deal with most of the printed material encountered in daily life whereas 22% were considered "narrow" readers.

In 1994-95, Canada participated in the first multi-country, multi-language assessment of adult literacy, the International Adult Literacy Survey (IALS). A stratified multi-stage probability sample design was used to select the sample from the Census Frame. The sample was designed to yield separate samples for the two Canadian official languages, English and French, and participants were measured on the dimensions of prose literacy, document literacy and quantitative literacy. The survey found that 42.2%, 43% and 42.2% of Canadians between the ages of 16 and 65 scored at the lowest two levels of Prose Literacy, Document Literacy and Quantitative Literacy, respectively. The survey presented many important correlations, among which was a strong plausible link between literacy and a country's economic potential.

In 2003, Canada participated in the Adult Literacy and Life Skills Survey (ALL). This survey contained identical measures for assessing the prose and document literacy proficiencies, allowing for comparisons between survey results on these two measures and found that 41.9% and 42.6% of Canadians between the ages of 16 and 65 scored at the lowest two levels of Prose Literacy and document literacy respectively. Further, Canadians’ mean scores also improved on both the prose and the document literacy scales. Energy production:36%, transportation: 24%, homes and businesses: 12%, industry: 11%, agriculture: 10%, and waste: 7%.

The OECD's Programme for the International Assessment of Adult Competencies (PIAAC) is expected to produce new comparative skill profiles in late 2013.

In the last 40 years, the rate of illiteracy in Mexico has been steadily decreasing. In the 1960s, because the majority of the residents of the federal capital were illiterate, the planners of the Mexico City Metro designed a system of unique icons to identify each station in the system in addition to its formal name. However, The INEGI's census data of 1970 showed a national average illiteracy rate of 25.8%; the last census data puts the national average at 6.9%. Mexico still has a gender educational bias. The illiteracy rate for women in the last census was 8.1% compared with 5.6% for men. Rates differ across regions and states. Chiapas, Guerrero and Oaxaca, the states with the highest poverty rate, had greater than 15% illiteracy in 2010(17.8%, 16.7% and 16.3 respectively). In contrast, the illiteracy rates in the Federal District (D.F. / Mexico City) and in some northern states like Nuevo León, Baja California, and Coahuila were below 3% in the 2010 census (2.1%, 2.2%, 2.6% and 2.6% respectively).

Before the 20th century white illiteracy was not uncommon and many of the slave states made it illegal to teach slaves to read. By 1900 the situation had improved somewhat, but 44% of black people remained illiterate. There were significant improvements for African American and other races in the early 20th century as the descendants of former slaves, who had had no educational opportunities, grew up in the post Civil War period and often had some chance to obtain a basic education. The gap in illiteracy between white and black adults continued to narrow through the 20th century, and in 1979 the rates were about the same.

Full prose proficiency, as measured by the ability to process complex and challenging material such as would be encountered in everyday life, is achieved by about 13% of the general, 17% of the white, and 2% of the African American population. However 86% of the general population had basic or higher prose proficiency as of 2003, with a decrease distributed across all groups in the full proficiency group vs. 1992 of more than 10% consistent with trends, observed results in the SAT reading score to the present (2015).

Before colonization, oral storytelling and communication composed most if not all Native American literacy. Native people communicated and retained their histories verbally—it was not until the beginning of American Indian boarding schools that reading and writing forms of literacy were forced onto Native Americans. While literacy rates of English increased, forced assimilation exposed Native children to physical and sexual abuse, unsanitary living conditions, and even death. Many students ran away in an attempt to hold on to their cultural identity and literary traditions that were relevant to their community. While these formalized forms of literacy prepared Native youth to exist in the changing society, they destroyed all traces of their cultural literacy. Native children would return to their families unable to communicate with them due to the loss of their indigenous language. In the 20th and 21st century, there is still a struggle to learn and maintain cultural language. But education initiatives and programs have increased overall—according to the 2010 census, 86 percent of the overall population of Native Americans and Alaska Natives have high school diplomas, and 28 percent have a bachelor's degree or higher.

In 1964 in Brazil, Paulo Freire was arrested and exiled for teaching peasants to read. Since democracy returned to Brazil, however, there has been a steady increase in the percentage of literate people. Educators with the Axé project within the city of Salvador, Bahía attempt to improve literacy rates among urban youth, especially youth living on the streets, through the use of music and dances of the local culture. They are encouraged to continue their education and become professionals.

The literacy rates in Africa vary significantly between countries. The highest registered literacy rate in the region is in Equatorial Guinea and Libya (both 94.2%), while the lowest literacy rate is in South Sudan (27%). Poorer youth in sub-Saharan Africa have fewer educational opportunities to become literate compared with wealthier families. They often must leave school because of being needed at home to farm or care for siblings.

In sub-Saharan Africa, the rate of literacy has not improved enough to compensate for the effects of demographic growth. As a result, the number of illiterate adults has risen by 27% over the last 20 years, reaching 169 million in 2010. Thus, out of the 775 million illiterate adults in the world in 2010, more than one fifth were in sub- Saharan Africa – in other words, 20% of the adult population. The countries with the lowest levels of literacy in the world are also concentrated in this region. These include Niger (28.7%), Burkina Faso (28.7%), Mali (33.4%), Chad (35.4%) and Ethiopia (39%), where adult literacy rates are well below 50%. There are, however, certain exceptions, like Equatorial Guinea, with a literacy rate of 94%.

The literacy rate of Algeria is around 70%: education is compulsory and free in Algeria up to age of 17.

Botswana has among the highest literacy rates in the developing world with around 85% of its population being literate.

Burkina Faso has a very low literacy rate of 28.7%. The government defines literacy as anyone at least 15 years of age and up who can read and write. To improve the literacy rate, the government has received at least 80 volunteer teachers. A severe lack of primary school teachers causes problems for any attempt to improve the literacy rate and school enrollment.

Egypt has a relatively high literacy rate. The adult literacy rate in 2010 was estimated at 72%.
Education is compulsory from ages 6 to 15 and free for all children to attend. 93% of children enter primary school today, compared with 87% in 1994.

Djibouti has an estimated literacy rate of 70%.

According to the Ministry of Information of Eritrea, the nation has an estimated literacy rate of 80%.

The Ethiopians are among the first literate people in the world, having written, read, and created manuscripts in their ancient language of Ge'ez (Amharic) since the second century CE. All boys learned to read the Psalms around the age of 7. National literacy campaign introduced in 1978 increased literacy rates to between 37% (unofficial) and 63% (official) by 1984.

Guinea has a literacy rate of 41%. The Guinea government defines literacy as anyone who can read or write who is at least 15 years old. Guinea was the first to use the Literacy, Conflict Resolution, and Peacebuilding project. This project was developed to increase agriculture production, develop key skills, resolve conflict, improve literacy, and numeracy skills. The LCRP worked within refugee camps near the border of Sierra Leone, however this project only lasted from 1999 to 2001. There are several other international projects working within the country that have similar goals.

The literacy rate in Kenya among people below 20 years of age is over 70%, as the first 8 years of primary school are provided tuition-free by the government. In January 2008, the government began offering a restricted program of free secondary education. Literacy is much higher among the young than the old population, with the total being about 53% for the country. Most of this literacy, however, is elementary—not secondary or advanced.

Mali has one of the lowest literacy rates in the world, at 33.4%, with males having a 43.1% literacy rate and females having a 24.6% literacy rate. The government defines literacy as anyone who is at least 15 and over who can read or write. The government of Mali and international organizations in recent years has taken steps to improve the literacy rate. The government recognized the slow progress in literacy rates and began created ministries for basic education and literacy for their national languages in 2007. To also improve literacy the government planned to increase its education budget by 3%, when this was purposed it was at 35% in 2007. The lack of literate adults causes the programs to be slowed. The programs need qualified female trainers is a major problem because most men refuse to send female family members to be trained under male teachers.

Free education in Mauritius didn't proceed beyond the primary level until 1976, so many women now in their 50s or older left school at age 12. The younger generation (below 50) are however extremely well educated with very high educational expectations placed upon pupils. Education is today free from pre-primary to tertiary (only admission fees remain at University level). Most professional people have at least a bachelor's degree. Mauritian students consistently rank top in the world each year for the Cambridge International O Level, International A and AS level examinations. Most Mauritian children, even at primary level, attend tuition after school and at weekends to cope with the highly competitive public school system where admission to prestigious public colleges (secondary) and most sought after university courses depend on merit based academic performance.

The adult literacy rate was estimated at 89.8% in 2011. Male literacy was 92.3% and Female literacy 87.3%.

Niger has an extremely low literacy rate at 28.7%. However, the gender gap between males and females is a major problem for the country, men have a literacy rate of 42.9% and women a literacy rate of 15.1%. The Nigerien government defines literacy as anyone who can read or write over the age of 15. The Niass Tijaniyya, a predominant group of the Sufi brotherhoods, has started anti-poverty, empowerment, and literacy campaigns. The women in Kiota had not attempted to improve their education, or economic standing. Saida Oumul Khadiri Niass, known as Maman, through talking to men and women throughout the community changed the community's beliefs on appropriate behavior for women because the community recognized she was married to a leader of the Niass Tijaniyya. Maman's efforts has allowed women in Kiota to own small businesses, sell in the market place, attend literacy classes, and organize small associations that can give micro loans. Maman personally teaches children in and around Kiota, with special attention to girls. Maman has her students require instructor permission to allow the girls' parents to marry their daughters early. This increases the amount of education these girls receive, as well as delaying marriage, pregnancy, and having children.

Senegal has a literacy rate of 49.7%; the government defines literacy as anyone who is at least 15 years of age and over who can read and write. However, many students do not attend school long enough to be considered literate. The government did not begin actively attempting to improve the literacy rate until 1971 when it gave the responsibility to Department for Vocational Training at the Secretariat for Youth and Sports. This department and subsequent following departments had no clear policy on literacy until the Department of Literacy and Basic Education was formed in 1986. The government of Senegal relies heavily on funding from the World Bank to fund its school system.

There is no reliable data on the nationwide literacy rate in Somalia. A 2013 FSNAU survey indicates considerable differences per region, with the autonomous northeastern Puntland region having the highest registered literacy rate at 72%.

The Sierra Leone government defines literacy as anyone over the age of 15 who can read and write in English, Mende, Temne, or Arabic. Official statics put the literacy rate at 43.3%. Sierra Leone was the second country to use the Literacy, Conflict Resolution and Peacebuilding project. However, due to fighting near the city where the project was centered causing the project to be delayed until an arms amnesty was in place.

Uganda has a literacy rate of 72.2%.

Zimbabwe has a high literacy rate of 86.5% (2016 est.).

 Afghanistan has one of the lowest literacy rates in the world at 28.1% with males having a literacy rate of 43.1% and females with a literacy rate of 12.6%. The Afghan government considers someone literate if they are 15 years of age or older, and if they can read and write. To improve the literacy rate U.S. military trainers have been teaching Afghan Army recruits how to read before teaching to fire a weapon. U.S. commanders in the region estimate that as many as 65% of recruits may be illiterate.

The PRC conducts standardized testing to assess proficiency in Standard Chinese, known as "putonghua," but it is primarily for foreigners or those needing to demonstrate professional proficiency in the Beijing dialect. Literacy in languages like Chinese can be assessed by reading comprehension tests, just as in other languages, but historically has often been graded on the number of Chinese characters introduced during the speaker's schooling, with a few thousand considered the minimum for practical literacy. Social science surveys in China have repeatedly found that just more than half the population of China is conversant in spoken putonghua.

Literacy is defined by the Registrar General and Census Commissioner of India, as "[the ability of] a person aged 7 years and above [to]... both write and read with understanding in any language." According to the 2011 census, 74.04 percent.

Laos has the lowest level of adult literacy in all of Southeast Asia other than East Timor.

Obstacles to literacy vary by country and culture as writing systems, quality of education, availability of written material, competition from other sources (television, video games, cell phones, and family work obligations), and culture all influence literacy levels. In Laos, which has a phonetic alphabet, reading is relatively easy to learn—especially compared to English, where spelling and pronunciation rules are filled with exceptions, and Chinese, with thousands of symbols to be memorized. But a lack of books and other written materials has hindered functional literacy in Laos, where many children and adults read so haltingly that the skill is hardly beneficial.

A literacy project in Laos addresses this by using what it calls "books that make literacy fun!" The project, Big Brother Mouse, publishes colorful, easy-to-read books, then delivers them by holding book parties at rural schools. Some of the books are modeled on successful western books by authors such as Dr. Seuss; the most popular, however, are traditional Lao fairy tales. Two popular collections of folktales were written by Siphone Vouthisakdee, who comes from a village where only five children finished primary school.

Big Brother Mouse has also created village reading rooms, and published books for adult readers about subjects such as Buddhism, health, and baby care.

In Pakistan, the National Commission for Human Development (NCHD) aims to bring literacy to adults, especially women.
ISLAMABAD - UNESCO Islamabad Director Kozue Kay Nagata has said, "Illiteracy in Pakistan has fallen over two decades, thanks to the government and people of Pakistan for their efforts working toward meeting the Millennium Development Goals". "Today, 70 percent of Pakistani youths can read and write. In 20 years, illiterate population has been reduced significantly", she said while speaking at a function held in connection with International Literacy Day.

However, she also emphasised on the need to do more to improve literacy in the country and said, "The proportion of population in Pakistan lacking basic reading and writing is too high. This is a serious obstacle for individual fulfillment, to the development of societies, and to mutual understanding between peoples." Referring to the recent national survey carried out by the Ministry of Education, Trainings and Standards in Higher Education with support of UNESCO, UNICEF, and provincial and areas departments of education, Nagata pointed out that, in Pakistan, although primary school survival rate is 70 percent, gender gap still exists with only 68 percent of girls’ survival rate compared to 71 percent for boys. Specifically in the case of Punjab, she said, primary school survival rate today is better with 76 percent, but not without a gender gap of 8 percent points with 72 percent girls’ survival rate compared to 80 percent for boys. She also pointed out that average per student spending in primary level (age 5-9) was better in Punjab: Rs 6,998, compared to the national average. In Balochistan, although almost the same amount (Rs 6,985) as in Punjab is spent per child, the primary school survival rate is only 53 percent. Girls’ survival rate is slightly better with 54 percent than that of boys which is 52 percent. Literate Pakistan Foundation, a non-profit organization, which was established in 2003, is a case study, which brings to light the solutions for removing this menace from its roots. It works to improve rate of literacy in Pakistan.

The data of the survey shows that in Khyber Pakhtunkhwa, primary school survival rate is 67 percent which is lower than the national average of 70 percent. Furthermore, gender gap also exists with only 65 percent of girls’ survival rate compared to that of boys which is 68 percent. Per-student education expenditure in primary level (age 5-9) in Khyber Pakhtunkhwa is Rs 8,638. In Sindh, primary school survival rate is 63percent, with a gender gap of only 67 percent of girls’ survival rate compared to 60 percent for boys. Per student education expenditure in primary level (age 5-9) in Sindh is Rs 5,019. Nagata made reference to the survey report and mentioned that the most common reason in Pakistan for children (both boys and girls) of age 10 to 18 years leaving school before completing primary grade is "the child not willing to go to school", which may be related to quality and learning outcome. She said, however, and sadly, for the girls living in rural communities the second highest reason for dropout is "parents did not allow" which might be related to prejudice and cultural norm against girls.

Early Filipinos devised and used their own system of writings from 300 BC, which derived from the Brahmic family of scripts of Ancient India. Baybayin became the most widespread of these derived scripts by the 11th century.

Early chroniclers, who came during the first Spanish expeditions to the islands, noted the proficiency of some of the natives, especially the chieftain and local kings, in Sanskrit, Old Javanese, Old Malay, and several other languages.

During the Spanish colonization of the islands, reading materials were destroyed to a far much less extent compared to the Spanish colonization of the Americas. Education and literacy was introduced only to the Peninsulares and remained a privilege until the Americans came.

The Americans introduced the public schools system to the country which drove literacy rates up. English became the lingua franca in the Philippines. It was only during a brief period in the Japanese occupation of the Philippines that the Japanese were able to teach their language in the Philippines and teach the children their written language.

After World War II, the Philippines had the highest literacy rates in Asia. It nearly achieved universal literacy once again in the 1980s and 1990s. Ever since then, the literacy rate has plummeted only to start regaining a few percentage years back.

The DepEd, CHED, and other academic institutions encourage children to improve literacy skills and knowledge. The government has a program of literacy teaching starting in kindergarten. New reforms are being brought in shifting to a K-12 system which will teach children their regional languages before English, as opposed to the ten-year basic education program which teaches English and Filipino, the country's two official languages, from Grade 1.

With a literacy rate of 92.5%, Sri Lanka has one of the most literate populations amongst developing nations. Its youth literacy rate stands at 98%, computer literacy rate at 35%, and primary school enrollment rate at over 99%. An education system which dictates 9 years of compulsory schooling for every child is in place. The free education system established in 1945, is a result of the initiative of C. W. W. Kannangara and A. Ratnayake. It is one of the few countries in the world that provide universal free education from primary to tertiary stage.

Approximately 56% of Australians aged 15 to 74 achieve Level 3 literacy or above Australian Bureau of Statistics 2011-2012 and 83% of five-year-olds are on track to develop good language and cognitive skills Australian Early Development Census 2012 summary report. In 2012-2013, Australia had 1515 public library service points, lending almost 174 million items to 10 million members of Australian public library services, at an average per capita cost of just under AU$45 Australian Public Library Statistics 2012-2013.





</doc>
<doc id="46821647" url="https://en.wikipedia.org/wiki?curid=46821647" title="Internet science">
Internet science

Internet science is an interdisciplinary science, which looks at all aspects of the co-evolution in the Internet networks and society and studies it. It works in the intersection of and in the gaps among a wide range of disciplines that have had to respond to the impact of the Internet on their 'home turf' and/or offer specific conceptual or methodological contributions. These include many natural sciences (e.g. complexity science, computer science, engineering, life sciences, mathematics, physics, psychology, statistics, systems and evolutionary biology), social sciences (e.g. anthropology, economics, philosophy, sociology, and political science), humanities (e.g. art, history, linguistics, literature and history) and some existing interdisciplines that cross traditional Faculty boundaries (e.g. technology, medicine, law). Professor Noshir Contractor and others have located it at the intersection of computational social science, network science, network engineering and Web science. By understanding the role of society in shaping Internet networks and being shaped by them Internet science aims to take care of the Internet in a way similar to that in which Web science aims to take care of the Web. The lingua franca in this interdisciplinary area include Internet standards and associated implementation, social processes, Internet infrastructure and policy.

There are a lot of disciplines, which support 'Internet science' using different analysis tools, designs and languages. In order to have a productive and effective dialogue between disciplines, network will have to know the incentives to make cooperation opportunities. There are three main elements of Internet science: Multidisciplinary convergence, Observability and Constructive experimentation.

The European Commission funded a Network of Excellence on Internet Science (project acronym EINS) over the period December 2011-May 2015 under the FP7 funding programme. The Network in May 2015 had 48 member universities and research organisations and 180 individual affiliate researchers. Two major international Internet science conferences were held in April 2013 and May 2015 together with an unconference at the University of Bologna in May 2014 and official workshops at international academic conferences such as Human Behavior and the Evolution of Society and international inter-governmental and multistakeholder conferences such as the 2013 United Nations Internet Governance Forum.

Significant areas of current Internet science research include:

Net neutrality is the rule where Internet service providers should treat all the traffic on their networks equally. This means that companies should not slow down access or block any website content on the Web. In the United States, high-speed Internet service providers (ISPs), including AT&T, Comcast, Time Warner and Verizon, have sought support for a two-tiered Internet service model.

In 2014, President Obama announced a new plan to preserve "net neutrality" and to prevent Internet service providers from blocking or slowing websites or creating different tiers of speed. He said-, "No service should be stuck in a ‘slow lane’ because it does not pay a fee," he wrote in a statement. "That kind of gatekeeping would undermine the level playing field essential to the Internet’s growth."

Internet privacy (online privacy) is an opportunity of individuals to regulate the flow of information and have access to data, which is generated during a browsing session. Moreover, internet privacy may include some risks, like phishing, pharming, spyware and malware.

The fact is that Google has signed two contracts with wind developers to power its data center in Finland with 100% renewable energy. Facebook's decision was to build a data center in Iowa and has helped to drive the local energy provider in order to scrap plans to build a nuclear power plant, and instead build a $2bn(£1.23bn) wind farm, which has led to the biggest single order of wind turbines on record.

Infrastructure provides a large range of vital services, for example an ability to move goods, people and information. Also it provides a range of vital services such as the ability to move goods, people, and information safely. It is necessary to support social life and nation's economic because it is essential that such infrastructural services continue in the face of various hazards. It concerns of the idea that infrastructural services gas, electricity, water, transport, banking are highly interconnected and mutually dependent in various complex ways, being linked both physically and through important ICT systems, in order to breakdowns quickly escalate into whole infrastructure failure.

An initial listing of datasets, analytic tools and e-Infrastructures is available in a dedicated Internet science evidence base.There is ongoing activity on the development of Internet Science curricula, initially on a postgraduate level.

1934: The first person who imagined a 'Radiated Library' in 1934 was Paul Otlet.

1965: Two different computers started to communicate at MIT Lincoln Lab by using a packet-switching technology.

1968: Beranek and Newman have discovered an effectiveness and final version of the Interface Message Processor (IMP) specifications.

1969: The nodes were installed by UCLA’s Network Measurement Centre, Stanford Research Institute (SRI), University of California-Santa Barbara and University of Utah.

1972: Ray Tomlinson has introduced a network email. The Internetworking Working Group (INWG) forms to address, which afterwards needs to be established for standard protocols.

1973: The term 'Internet' was born. Also a global networking becomes a reality as the University College of London (England) and Royal Radar Establishment (Norway), which connects to ARPANET.

1974: The first Internet Service Provider (ISP) was born with the introduction of a commercial version of ARPANET. This is also known as a 'Telenet'.

1974: Vinton Cerf and Bob Kahn have published "A Protocol for Packet Network Interconnection," which details the design of TCP.

1976: Queen Elizabeth II sends her first e-mail.

1979: USENET forms to host news and discussion groups.

1981: The National Science Foundation (NSF) provided a grant in order to demonstrate the Computer Science Network (CSNET) and afterwards to provide networking services to university computer scientists.

1982: Transmission Control Protocol (TCP) and Internet Protocol (IP) arise the protocol for ARPANET.

1983: The Domain Name System (DNS) established the familiar .edu, .gov, .com, .mil, .org, .net, and .int system in order to name websites.

1984: William Gibson was the first person who used the term "cyberspace."

1985: Symbolics.com, the website for Symbolics Computer Corp. in Massachusetts, was the first registered domain.

1986: The National Science Foundation’s NSFNET goes online to connected supercomputer centers at 56,000 bits per second — the speed of a typical dial-up computer modem.

1987: The number of hosts on the Internet exceeds 20,000. Cisco ships its first router.

1989: World.std.com becomes the first commercial provider of dial-up access to the Internet.

1990: Tim Berners-Lee develops HyperText Markup Language (HTML). This technology continues to have a large impact on ways how humans view and navigate the Internet in present days.

1991: CERN introduces the World Wide Web to the public.

1992: The first audio and video were distributed over the Internet. The phrase "surfing the Internet" was very popular.

1993: The number of websites reached 600 and the White House and United Nations go online.

1994: Netscape Communications was born. Microsoft created a Web browser for Windows 95.

1995: Compuserve, America Online and Prodigy began to provide Internet access.

1996: The browser war, primarily between the two major players Microsoft and Netscape, heated up.

1997: PC makers removed or hid Microsoft’s Internet software on new versions of Windows 95.

1998: The Google search engine was born and changed the way users engage with the Internet.

1999: The Netscape has been bought by AOL.

2000: The dot-com bubble bursted.

2001: A federal judge shouted down Napster.

2003. The SQL Slammer worm has spread worldwide in just 10 minutes.

2004: Facebook went online and the era of social networking began.

2005: YouTube.com has been launched.

2006: AOL changed its business model and offered the most services for free and relyied on advertising in order to generate revenue.

2009: 40th anniversary of the Internet.

2010: 400 million active users have been reached in Facebook.

2011: Twitter and Facebook played a large role in the Middle East revolts.


</doc>
<doc id="605511" url="https://en.wikipedia.org/wiki?curid=605511" title="Foreknowledge">
Foreknowledge

Foreknowledge is the concept of knowledge regarding future events.

Types of foreknowledge include:


</doc>
<doc id="1941913" url="https://en.wikipedia.org/wiki?curid=1941913" title="Self-knowledge (psychology)">
Self-knowledge (psychology)

Self-knowledge is a term used in psychology to describe the information that an individual draws upon when finding an answer to the question "What am I like?".

While seeking to develop the answer to this question, self-knowledge requires ongoing self-awareness and self-consciousness (which is not to be confused with consciousness). Young infants and chimpanzees display some of the traits of self-awareness and agency/contingency, yet they are not considered as also having self-consciousness. At some greater level of cognition, however, a self-conscious component emerges in addition to an increased self-awareness component, and then it becomes possible to ask "What am I like?", and to answer with self-knowledge.

Self-knowledge is a component of the self or, more accurately, the self-concept. It is the knowledge of oneself and one's properties and the "desire" to seek such knowledge that guide the development of the self-concept. Self-knowledge informs us of our mental representations of ourselves, which contain attributes that we uniquely pair with ourselves, and theories on whether these attributes are stable or dynamic.

The self-concept is thought to have three primary aspects:

The affective and executive selves are also known as the "felt" and "active" selves respectively, as they refer to the emotional and behavioral components of the self-concept.
Self-knowledge is linked to the cognitive self in that its motives guide our search to gain greater clarity and assurance that our own self-concept is an accurate representation of our "true self"; for this reason the cognitive self is also referred to as the "known self". The cognitive self is made up of everything we know (or "think we know" about ourselves). This implies physiological properties such as hair color, race, and height etc.; and psychological properties like beliefs, values, and dislikes to name but a few.

Self knowledge also helps you to know who you really are. Knowing yourself is a must in life. You must focus on yourself rather than focusing on others. Have knowledge on your self more.

Self-knowledge and its structure affect how events we experience are encoded, how they are selectively retrieved/recalled, and what conclusions we draw from how we interpret the memory. The analytical interpretation of our own memory can also be called "meta memory", and is an important factor of "meta cognition".

The connection between our memory and our self-knowledge has been recognized for many years by leading minds in both philosophy and psychology, yet the precise specification of the relation remains a point of controversy.


Self-theories have traditionally failed to distinguish between different source that inform self-knowledge, these are "episodic memory" and "semantic memory". Both episodic and semantic memory are facets of "declarative memory", which contains memory of facts. Declarative memory is the explicit counterpart to "procedural memory", which is implicit in that it applies to skills we have learnt; they are not "facts" that can be "stated".

Episodic memory is the autobiographical memory that individuals possess which contains events, emotions, and knowledge associated with a given context.

Semantic memory does not refer to concept-based knowledge stored about a specific experience like episodic memory. Instead it includes the memory of meanings, understandings, general knowledge about the world, and factual information etc. This makes semantic knowledge independent of context and personal information. Semantic memory enables an individual to know information, including information about their selves, without having to consciously recall the experiences that taught them such knowledge.

People are able to maintain a sense of self that is supported by semantic knowledge of personal facts in the absence of direct access to the memories that describe the episodes on which the knowledge is based.
This evidence for the dissociation between episodic and semantic self-knowledge has made several things clear:

People have goals that lead them to seek, notice, and interpret information about themselves. These goals begin the quest for self-knowledge.
There are three primary motives that lead us in the search for self-knowledge:

Self-enhancement refers to the fact that people seem motivated to experience positive emotional states and to avoid experiencing negative emotional states. People are motivated to feel good about themselves in order to maximize their feelings of self-worth, thus enhancing their self-esteem.
The emphasis on "feelings" differs slightly from how other theories have previously defined self-enhancement needs, for example the "Contingencies of Self-Worth Model".
Other theorists have taken the term to mean that people are motivated to think about themselves in highly favorable terms, rather than feel they are "good".
In many situations and cultures, feelings of self-worth are promoted by thinking of oneself as highly capable or "better" than one's peers. However, in some situations and cultures, feelings of self-worth are promoted by thinking of oneself as "average" or even "worse" than others. In both cases, thoughts about the self still serve to enhance feelings of self-worth.
The universal need is not a need to think about oneself in any specific way, rather a need to maximize one's feelings of self-worth. This is the meaning of the self enhancement motive with respect to self-knowledge.

In Western societies, feelings of self-worth "are" in fact promoted by thinking of oneself in favorable terms.

See "Self-verification theory" section.

Accuracy needs influence the way in which people search for self-knowledge. People frequently wish to know the truth about themselves without regard as to whether they learn something positive or negative.
There are three considerations which underlie this need:
Accurate self-knowledge can also be instrumental in maximizing feelings of self-worth. Success is one of the number of things that make people feel good about themselves, and knowing what we are like can make successes more likely, so self-knowledge can again be adaptive. This is because self-enhancement needs can be met by knowing that one "can not" do something particularly well, thus protecting the person from pursuing a dead-end dream that is likely to end in failure.

Many theorists believe that we have a motive to protect the self-concept (and thus our self-knowledge) from change. This motive to have consistency leads people to look for and welcome information that is consistent with what they believe to be true about themselves; likewise, they will avoid and reject information which presents inconsistencies with their beliefs. This phenomenon is also known as self-verification theory.
Not everyone has been shown to pursue a self-consistency motive; but it has played an important role in various other influential theories, such as cognitive dissonance theory.

This theory was put forward by William Swann of the University of Texas at Austin in 1983 to put a name to the aforementioned phenomena. The theory states that once a person develops an idea about what they are like, they will strive to verify the accompanying self-views.
Two considerations are thought to drive the search for self-verifying feedback:
These factors of self-verification theory create controversy when persons suffering from low-self-esteem are taken into consideration. People who hold negative self-views about themselves "selectively seek negative feedback" in order to verify their self-views. This is in stark contrast to self-enhancement motives that suggest people are driven by the desire to feel good about themselves.

There are three sources of information available to an individual through which to search for knowledge about the self:

The physical world is generally a highly visible, and quite easily measurable source of information about one's self. Information one may be able to obtain from the physical world may include:


The comparative nature of self-views means that people rely heavily on the social world when seeking information about their selves. Two particular processes are important:

People compare attributes with others and draw inferences about what they themselves are like. However, the conclusions a person ultimately draws depend on whom in particular they compare themselves with. The need for accurate self-knowledge was originally thought to guide the social comparison process, and researchers assumed that comparing with others who are similar to us in the "important" ways is more informative.

People are also known to compare themselves with people who are slightly better off than they themselves are (known as an "upward comparison"); and with people who are slightly worse off or disadvantaged (known as a "downward comparison").
There is also substantial evidence that the need for "accurate" self-knowledge is neither the only, nor most important factor that guides the social comparison process, the need to feel good about ourselves affects the social comparison process.

Reflected appraisals occur when a person observes how others respond to them. The process was first explained by the sociologist Charles H. Cooley in 1902 as part of his discussion of the "looking-glass self", which describes how we see ourselves reflected in other peoples' eyes. He argued that a person's feelings towards themselves are socially determined via a three-step process:

"A self-idea of this sort seems to have three principled elements: the imagination of our appearance to the other person; the imagination of his judgment of that appearance; and some sort of self-feeling, such as pride or mortification. The comparison with a looking-glass hardly suggests the second element, the imagined judgment which is quite essential. The thing that moves us to pride or shame is not the mere mechanical reflection of ourselves, but an imputed sentiment, the imagined effect of this reflection upon another's mind." (Cooley, 1902, p. 153)

In simplified terms, Cooley's three stages are:
Note that this model is of a phenomenological nature.

In 1963, John W. Kinch adapted Cooley's model to explain how a person's "thoughts" about themselves develop rather than their "feelings".

Kinch's three stages were:
This model is also of a phenomenological approach.

Research has only revealed limited support for the models and various arguments raise their heads:

The sequence of reflected appraisals may accurately characterize patterns in early childhood due to the large amount of feedback infants receive from their parents, yet it appears to be less relevant later in life. This is because people are not passive, as the model assumes. People "actively" and "selectively" process information from the social world. Once a person's ideas about themselves take shape, these also influence the manner in which new information is gathered and interpreted, and thus the cycle continues.

The psychological world describes our "inner world". There are three processes that influence how people acquire knowledge about themselves:

Introspection involves looking inwards and directly consulting our attitudes, feelings and thoughts for meaning.
Consulting one's own thoughts and feelings can sometimes result in meaningful self-knowledge. The accuracy of introspection, however, has been called into question since the 1970s. Generally, introspection relies on people's explanatory theories of the self and their world, the accuracy of which is not necessarily related to the form of self-knowledge that they are attempting to assess. 

Comparing sources of introspection. People believe that spontaneous forms of thought provide more meaningful self-insight than more deliberate forms of thinking. Morewedge, Giblin, and Norton (2014) found that the more spontaneous a kind of thought, the more spontaneous a particular thought, and the more spontaneous thought a particular thought was perceived to be, the more insight into the self it was attributed. In addition, the more meaning the thought was attributed, the more the particular thought influenced their judgment and decision making. People asked to let their mind wander until they randomly thought of a person to whom they were attracted to, for example, reported that the person they identified provided them with more self-insight than people asked to simply think of a person to whom they were attracted to. Moreover, the greater self-insight attributed to the person identified by the (former) random thought process than by the latter deliberate thought process led those people in the random condition to report feeling more attracted to the person they identified.

Whether introspection always fosters self-insight is not entirely clear. Thinking too much about why we feel the way we do about something can sometimes confuse us and undermine true self-knowledge. Participants in an introspection condition are less accurate when predicting their own future behavior than controls and are less satisfied with their choices and decisions. In addition, it is important to notice that introspection allows the exploration of the conscious mind only, and does not take into account the unconscious motives and processes, as found and formulated by Freud. 

Wilson's work is based on the assumption that people are not always aware of "why" they feel the way they do. Bem's self-perception theory makes a similar assumption.
The theory is concerned with how people "explain" their behavior. It argues that people don't always "know" why they do what they do. When this occurs, they infer the causes of their behavior by analyzing their behavior in the context in which it occurred. Outside observers of the behavior would reach a similar conclusion as the individual performing it. The individuals then draw logical conclusions about why they behaved as they did.

"Individuals come to "know" their own attitudes, emotions, and other internal states partially by inferring them from observations of their own overt behavior and/or the circumstances in which this behavior occurs. Thus, to the extent that internal cues are weak, ambiguous, or uninterpretable, the individual is functionally in the same position as an outside observer, an observer who must necessarily rely upon those same external cues to infer the individual's inner states." (Bem, 1972, p.2)

The theory has been applied to a wide range of phenomena. Under particular conditions, people have been shown to infer their attitudes, emotions, and motives, in the same manner described by the theory.

Similar to introspection, but with an important difference: with introspection we "directly examine" our attitudes, feelings and motives. With self-perception processes we "indirectly infer" our attitudes, feelings, and motives by "analyzing our behavior".

Causal attributions are an important source of self-knowledge, especially when people make attributions for positive and negative events. The key elements in self-perception theory are explanations people give for their actions, these explanations are known as causal attributions.

Causal attributions provide answers to "Why?" questions by attributing a person's behavior (including our own) to a cause.

People also gain self-knowledge by making attributions for "other people's" behavior; for example "If nobody wants to spend time with me it must be because I'm boring".

Individuals think of themselves in many different ways, yet only some of these ideas are active at any one given time. The idea that is specifically active at a given time is known as the Current Self-Representation. Other theorists have referred to the same thing in several different ways:
The current self-representation influences information processing, emotion, and behavior and is influenced by both "personal" and "situational" factors.

Self-concept, or how people "usually" think of themselves is the most important personal factor that influences current self-representation. This is especially true for attributes that are important and self-defining.

Self-concept is also known as the self-schema, made of innumerable smaller self-schemas that are "chronically accessible".

Self-esteem affects the way people feel about themselves. People with high self-esteem are more likely to be thinking of themselves in positive terms at a given time than people suffering low self-esteem.

Mood state influences the accessibility of positive and negative self-views.

When we are happy we tend to think more about our positive qualities and attributes, whereas when we are sad our negative qualities and attributes become more accessible.

This link is particularly strong for people suffering low self-esteem.

People can deliberately activate particular self-views. We select appropriate images of ourselves depending on what role we wish to play in a given situation.

One particular goal that influences activation of self-views is the desire to feel good.

How a person thinks of themselves depends largely on the social role they are playing. Social roles influence our personal identities.

People tend to think of themselves in ways that distinguish them from their social surroundings.
Distinctiveness also influences the salience of group identities.

The size of the group affects the salience of group-identities. Minority groups are more distinctive, so group identity should be more salient among minority group members than majority group members.

Group status interacts with group size to affect the salience of social identities.

The social environment has an influence on the way people evaluate themselves as a result of social-comparison processes.

People regard themselves as at the opposite end of the spectrum of a given trait to the people in their company. However, this effect has come under criticism as to whether it is a primary effect, as it seems to share space with the assimilation effect, which states that people evaluate themselves more positively when they are in the company of others who are exemplary on some dimension.

Imagining how one appears to others has an effect on how one thinks about oneself.

Recent events can cue particular views of the self, either as a direct result of failure, or via mood.
Memory for prior events influence how people think about themselves.







</doc>
<doc id="49870344" url="https://en.wikipedia.org/wiki?curid=49870344" title="Creative computing">
Creative computing

Creative computing covers the interdisciplinary area at the cross-over of the creative arts and computing. Issues of creativity include knowledge discovery, for example. 

The "International Journal of Creative Computing" describes creative computing as follows:

Creative computing is interdisciplinary in nature and topics relating to it include applications, development method, evaluation, modeling, philosophy, principles, support environment, and theory.

The term "creative computing" is used both in the United Kingdom and the United States (e.g., at Harvard University and MIT).

A number of university degree programmes in Creative Computing exist, for example at:


The "International Journal of Creative Computing" is a quarterly peer-reviewed scientific journal published by Inderscience Publishers, covering creativity in computing and the other way around. The editor-in-chief is Hongji Yang (Bath Spa University).

The journal was established in 2013 and is abstracted and indexed in CSA, ProQuest, and DBLP databases. As of 2019, the journal appears to be defunct.


</doc>
<doc id="9106784" url="https://en.wikipedia.org/wiki?curid=9106784" title="Open knowledge">
Open knowledge

Open knowledge is knowledge that one is free to use, reuse, and redistribute without legal, social or technological restriction. Open knowledge is a set of principles and methodologies related to the production and distribution of how knowledge works in an "open" manner. Knowledge is interpreted broadly to include data, content and general information.

The concept is related to open source and the "Open Knowledge Definition" is directly derived from the Open Source Definition. Open knowledge can be seen as being a superset of open data, open content and libre open access with the aim of highlighting the commonalities between these different groups.

Similarly to other 'open' concepts such as open data and open content, though the term is rather new, the concept is old. For example, one of the earliest printed texts of which we have record is a copy of the Buddhist Diamond sutra produced in China around 868 AD, and in it can be found the dedication: "for universal free distribution".

In the early twentieth century a debate about intellectual property rights developed within the German Social Democratic Party. A key contributor was Karl Kautsky who in 1902 devoted a section of a pamphlet to "Intellectual Production" which he distinguished from material production:

This view was based on an analysis according to which Karl Marx's Law of value only affected material production, not intellectual production.

With the development of the public Internet from the early 1990s, it became far easier to copy and share information across the world. The phrase 'information wants to be free' became a rallying cry for people who wanted to create an internet without the commercial barriers that they felt inhibited creative expression in traditional material production.

Wikipedia was founded in 2001 with the ethos of providing information which could be edited and modified to improve its quality. The success of Wikipedia became instrumental in making open knowledge something that millions of people interacted with and contributed to.



</doc>
<doc id="7030491" url="https://en.wikipedia.org/wiki?curid=7030491" title="Pantomath">
Pantomath

A pantomath is a person who wants to know and knows everything. The word itself is not to be found in common online English dictionaries, the "OED", dictionaries of obscure words, or dictionaries of neologisms.

Logic dictates that there are no literal nonfictional pantomaths, but the word pantomath seems to have been used to imply a polymath in a superlative sense, a "ne plus ultra" ("nothing more beyond") as it were, one who satisfies requirements even stricter than those to be applied to the polymath. In theory, a pantomath is not to be confused with a polymath in its less strict sense, much less with the related but very different terms philomath and know-it-all.

A pantomath ("pantomathēs", παντομαθής, meaning "having learnt all", from the Greek roots παντ- "all", "every" and the root μαθ-, meaning "learning", "understanding") is a person whose astonishingly wide interests and knowledge span the entire range of the arts and sciences.

Pantomath is typically used to convey the sense that a great individual has achieved a pinnacle of learning, that an "automath" has taken autodidacticism to an endpoint. As an example, the obscure and rare term seems to have been applied to those with an astonishingly wide knowledge and interests by these two authors from different eras: G. M. Young has been called a pantomath, as has Rupert Hart-Davis.

According to a critical view, Goethe's monumental breadth of knowledge and accomplishments, together with his serene, supernal wisdom, a wisdom which has been described as aloof, even inhuman, made him worthy of the denomination Olympian.



</doc>
<doc id="22781" url="https://en.wikipedia.org/wiki?curid=22781" title="Omniscience">
Omniscience

Omniscience () is the capacity to know everything. In monotheistic religions, such as Sikhism and the Abrahamic religions, this is an attribute of God. In Jainism, omniscience is an attribute that any individual can eventually attain. In Buddhism, there are differing beliefs about omniscience among different schools.

In Islam, Allah is attributed with absolute omniscience. He knows the past, the present and the future. It is compulsory for a Muslim to believe that Allah is indeed omniscient as stated in one of the six articles of faith which is :

It is however, believed that human can only change their predestination (wealth, health, deed etc.) and not divine decree (date of birth, date of death, family etc.), thus allowing free wills.

The topic of omniscience has been much debated in various Indian traditions, but no more so than by the Buddhists. After Dharmakirti's excursions into the subject of what constitutes a valid cognition, Śāntarakṣita and his student Kamalaśīla thoroughly investigated the subject in the Tattvasamgraha and its commentary the Panjika. The arguments in the text can be broadly grouped into four sections:

Some modern Christian theologians argue that God's omniscience is inherent rather than total, and that God chooses to limit his omniscience in order to preserve the freewill and dignity of his creatures. John Calvin, among other theologians of the 16th century, comfortable with the definition of God as being omniscient in the total sense, in order for worthy beings' abilities to choose freely, embraced the doctrine of predestination.

In Jainism, omniscience is considered the highest type of perception. In the words of a Jain scholar,
"The perfect manifestation of the innate nature of the self, arising on the complete annihilation of the obstructive veils, is called omniscience."

Jainism views infinite knowledge as an inherent capability of every soul. "Arihanta" is the word used by Jains to refer to those human beings who have conquered all inner passions (like attachment, greed, pride, anger) and possess "Kevala Jnana" (infinite knowledge). They are said to be of two kinds:

Whether omniscience, particularly regarding the choices that a human will make, is compatible with free will has been debated by theologians and philosophers. The argument that divine foreknowledge is not compatible with free will is known as theological fatalism. It is argued that if humans are free to choose between alternatives, God could not know what this choice will be.

A question arises: if an omniscient entity knows everything, even about its own decisions in the future, does it therefore forbid any free will to that entity? William Lane Craig states that the question subdivides into two:

However, this kind of argument fails to recognize its use of the modal fallacy. It is possible to show that the first premise of arguments like these is fallacious. 

Some philosophers, such as Patrick Grim, Linda Zagzebski, Stephan Torre and William Mander have discussed the issue of whether the apparent exclusively first-person nature of conscious experience is compatible with God's omniscience. There is a strong sense in which conscious experience is private, meaning that no outside observer can gain knowledge of what it is like to be me "as me". If a subject cannot know what it is like to be another subject in an objective manner, the question is whether that limitation applies to God as well. If it does, then God cannot be said to be omniscient since there is then a form of knowledge that God lacks access to.

The philosopher Patrick Grim most notably raised this issue. Linda Zagzebski tried to avoid it by introducing the notion of "perfect empathy", a proposed relation that God can have to subjects that would allow God to have perfect knowledge of their conscious experience. William Mander argued that God can only have such knowledge if our experiences are part of God's broader experience. Stephan Torre claimed that God can have such knowledge if self-knowledge involves the ascription of properties, either to oneself or to others. Patrick Grim saw this line of reasoning as a motivation for accepting atheism.





</doc>
<doc id="49304088" url="https://en.wikipedia.org/wiki?curid=49304088" title="Book desert">
Book desert

A book desert is a geographic area (country, state, county, city, neighborhood, home) where printed books and other reading material are allegedly hard to obtain, particularly without access to an automobile or other form of transportation. Some researchers have defined book deserts by linking them to poverty and low income, while others use a combination of factors that include census data, income, ethnicity, geography, language, and the number of books in a home.

Initiatives that increase the availability of books by such measures as bookmobiles and librarians on bicycles have been offered as possible solutions to book deserts, as have Little Free Libraries and offering children's literature available online, free of charge.

In the past, researchers have studied how the absence or scarcity of books impact how a child's early literacy and language skills develop. The term "book desert" came into regular use in the mid-2010s and the social enterprise Unite for Literacy is credited as having coined the term. Unite for Literacy created an operational definition of a book desert when they published the U.S. Book Desert Map: A geographic area (country, state, county, census tract) where it is predicted that a majority of homes have less than 100 printed books. In March 2014, James LaRue, director of the American Library Association's Office for Intellectual Freedom and the Freedom to Read Foundation, used the term in an issue of "American Libraries", where he described the term as applying to houses with 25 or fewer books in them and discussed ways to lessen or eradicate the problem.

In July 2016, professors Susan B. Neuman and Naomi Moland published a study in "Urban Education", where they examined how the lack of printed reading material among low-income and poverty stricken neighborhoods impacts early childhood development and used the term to describe areas and homes with little access to written materials. This study built upon other research Neuman had conducted in 2001 on the same topic and the researchers found few stores in Detroit, Los Angeles, and Washington, D.C.; the focus areas of their research had print resources for children ages 0 through 18. Of those stores, many were dollar stores. "The Atlantic" reported that in 2015 Neuman and JetBlue Airways held an experiment to foster literacy by providing book vending machines in a low-income Washington D.C. neighborhood. Over 20,000 books were given away, prompting Neuman to conclude that the neighborhood's parents did care about their children's education but lacked "the resources to enable their children to be successful."

Multiple factors are credited as contributing to the formation of book deserts, the most frequently highlighted of which tends to be poverty and low income. Other factors tend to include language and geography, as some areas lack access to bookstores or public or community school libraries that would provide books. Book store closures due to bankruptcy or other financial difficulties are also occasionally cited as a contributing factor, when the closure leaves the area without a bookseller.

Unite for Literacy has developed a book desert map of the United States powered by Esri's ArcGIS platform, which provides a visual presentation of the lack of books in the nation, states, counties and census tracts. To create the map, Unite for Literacy performed a statistical analyses of data from the National Assessment of Educational Progress and the American Community Survey. Data used in the map includes the number of books in 4th graders' homes, average community income, ethnic diversity, geographic location and home language. Unite for Literacy unveiled the map during the Clinton Global Initiative America (CGI America) meeting held in Denver, Colorado, in June 2014.



</doc>
<doc id="306326" url="https://en.wikipedia.org/wiki?curid=306326" title="Ignorance">
Ignorance

Ignorance is a lack of knowledge. The word ignorant is an adjective that describes a person in the state of being unaware, and can describe individuals who deliberately ignore or disregard important information or facts, or individuals who are unaware of important information or facts. Ignorance can appear in three different types: factual ignorance (absence of knowledge of some fact), objectual ignorance (unacquaintance with some object), and technical ignorance (absence of knowledge of how to do something).

People do not experience instant gratification and therefore they do not invest time and effort in learning and developing. Eradicating ignorance completely from an individual’s life is an impossible task but reducing the gap can truly benefit the individual in a long term.

Also, making the decision to remain ignorant by committing to an ideology despite scientific proof is a dangerous mindset that can inhibit an individual from discovering the truth and therefore developing as an individual.

Ignorance can stifle learning, especially if the ignorant person believes they are not ignorant. A person who falsely believes they are knowledgeable, does not try to clarify their beliefs, but rather relies on the ignorant position. They may also reject valid but contrary information, neither realizing its importance nor understanding it. This concept is elucidated in Justin Kruger and David Dunning's work, "Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments", in stupidy in otherwise known as the Dunning–Kruger effect.

Another reason why people are not able to obtain knowledge is inaccessibility to education. In some parts of the world, educational infrastructure is insufficient and therefore people are not able to seek for knowledge. The phenomenon can be observed widely in underdeveloped countries where many children either cannot afford to go to school or live too far from a school. On the other hand, in developed countries most children have access to a K12 education but once they graduate high school they encounter obstacles to pursue a higher education due to high costs. This lack of education can contribute to individuals not being able to practice critical thinking and seek for knowledge for themselves.

Remaining in a state of ignorance can lead to serious economic downfalls, relationship crises, legal issues, and more. It is important for human survival to be knowledgeable on different topics. For example, one must be aware of ways to prevent certain diseases, avoid certain poisonous foods, avoid war, etc..

Individuals who are financially literate can manage finances wisely and therefore create a more stable life for themselves and their families. People who understand finance and economics can vote better on local and national economic regulations that benefit the whole society.

Another important aspect is ignorance in the legal field. People must be aware of laws so they can live free and in harmony with others. Ignorance of the law is not accepted as a legal defense, so it is the responsibility of the individual to know the law. Willingly ignoring the law can lead to arrest or isolated living.

Ignorance can have a negative effects on individuals and societies, but can also benefit them by creating within them the desire to know more. For example, ignorance within science opens the opportunity to seek knowledge and make discoveries by asking new questions. This though can only take place if the individual possesses a curious mind.

Studies suggest that adults with an adequate education who perform enriching and challenging jobs are happier, and more in control of their environment. The confidence that adults obtain through the sense of control that education provides allows those adults to go for more leadership positions and seek for power throughout their lives.

The writer, Thomas Pynchon, articulated the following about the scope and structure of one's ignorance: "Ignorance is not just a blank space on a person's mental map. It has contours and coherence, and for all I know rules of operation as well. So as a corollary to [the advice of] writing about what we know, maybe we should add getting familiar with our ignorance, and the possibilities therein for writing a good story", and ignorance of the observation of the reality of the human condition.





</doc>
<doc id="33570327" url="https://en.wikipedia.org/wiki?curid=33570327" title="History of knowledge">
History of knowledge

The history of knowledge is the field covering the accumulated and known human knowledge created or discovered during the history of the world and its historic forms, focus, accumulation, bearers, impacts, mediations, distribution, applications, societal contexts, conditions and methods of production. It is related to, separate from the history of science, the history of scholarship and the history of philosophy. The scope of the history of knowledge encompass all the discovered and created fields of human derived knowledge such as logic, philosophy, mathematics, science, sociology, psychology, data mining etc.


Burke, Peter. "Writing the Social History of Knowledge." Theory, Culture & Society, December 21, 2010.

History of Knowledge (blog), http://historyofknowledge.net

Lässig, Simone. "The History of Knowledge and the Expansion of the Historical Research Agenda." "Bulletin of the German Historical Institute" (Fall 2016): 29–58.


</doc>
<doc id="9892" url="https://en.wikipedia.org/wiki?curid=9892" title="Expert">
Expert

An expert is someone who has a prolonged or intense experience through practice and education in a particular field. Informally, an expert is someone widely recognized as a reliable source of technique or skill whose faculty for judging or deciding rightly, justly, or wisely is accorded authority and status by peers or the public in a specific well-distinguished domain. An expert, more generally, is a person with extensive knowledge or ability based on research, experience, or occupation and in a particular area of study. Experts are called in for advice on their respective subject, but they do not always agree on the particulars of a field of study. An expert can be believed, by virtue of credential, training, education, profession, publication or experience, to have special knowledge of a subject beyond that of the average person, sufficient that others may officially (and legally) rely upon the individual's opinion. Historically, an expert was referred to as a sage (Sophos). The individual was usually a profound thinker distinguished for wisdom and sound judgment.

In specific fields, the definition of expert is well established by consensus and therefore it is not always necessary for individuals to have a professional or academic qualification for them to be accepted as an expert. In this respect, a shepherd with 50 years of experience tending flocks would be widely recognized as having complete expertise in the use and training of sheep dogs and the care of sheep. Another example from computer science is that an expert system may be taught by a human and thereafter considered an expert, often outperforming human beings at particular tasks. In law, an expert witness must be recognized by argument and authority.

Research in this area attempts to understand the relation between expert knowledge, skills and personal characteristics and exceptional performance. Some researchers have investigated the cognitive structures and processes of experts. The fundamental aim of this research is to describe what it is that experts know and how they use their knowledge to achieve performance that most people assume requires extreme or extraordinary ability. Studies have investigated the factors that enable experts to be fast and accurate.

Expertise characteristics, skills and knowledge of a person (that is, expert) or of a system, which distinguish experts from novices and less experienced people. In many domains there are objective measures of performance capable of distinguishing experts from novices: expert chess players will almost always win games against recreational chess players; expert medical specialists are more likely to diagnose a disease correctly; etc.

The word expertise is used to refer also to Expert Determination, where an expert is invited to decide a disputed issue. The decision may be binding or advisory, according to the agreement between the parties in dispute.

There are two academic approaches to the understanding and study of expertise. The first understands expertise as an emergent property of communities of practice. In this view expertise is socially constructed; tools for thinking and scripts for action are jointly constructed within social groups enabling that group jointly to define and acquire expertise in some domain.

In the second view expertise is a characteristic of individuals and is a consequence of the human capacity for extensive adaptation to physical and social environments. Many accounts of the development of expertise emphasize that it comes about through long periods of deliberate practice. In many domains of expertise estimates of 10 years' experience deliberate practice are common. Recent research on expertise emphasizes the nurture side of the nature and nurture argument. Some factors not fitting the nature-nurture dichotomy are biological but not genetic, such as starting age, handedness, and season of birth.

In the field of education there is a potential "expert blind spot" (see also Dunning–Kruger effect) in newly practicing educators who are experts in their content area. This is based on the "expert blind spot hypothesis" researched by Mitchell Nathan and Andrew Petrosino (2003: 906). Newly practicing educators with advanced subject-area expertise of an educational content area tend to use the formalities and analysis methods of their particular area of expertise as a major guiding factor of student instruction and knowledge development, rather than being guided by student learning and developmental needs that are prevalent among novice learners.

The blind spot metaphor refers to the physiological blind spot in human vision in which perceptions of surroundings and circumstances are strongly impacted by their expectations. Beginning practicing educators tend to overlook the importance of novice levels of prior knowledge and other factors involved in adjusting and adapting pedagogy for learner understanding. This expert blind spot is in part due to an assumption that novices’ cognitive schemata are less elaborate, interconnected, and accessible than experts’ and that their pedagogical reasoning skills are less well developed (Borko & Livingston, 1989: 474). Essential knowledge of subject matter for practicing educators consists of overlapping knowledge domains: subject matter knowledge and pedagogical content matter (Borko, Eisenhart, Brown, Underhill, Jones, & Agard, 1992: 195). Pedagogical content matter consists of an understanding of how to represent certain concepts in ways appropriate to the learner contexts, including abilities and interests. The expert blind spot is a pedagogical phenomenon that is typically overcome through educators’ experience with instructing learners over time.
In line with the socially constructed view of expertise, expertise can also be understood as a form of power; that is, experts have the ability to influence others as a result of their defined social status. By a similar token, a fear of experts can arise from fear of an intellectual elite's power. In earlier periods of history, simply being able to read made one part of an intellectual elite. The introduction of the printing press in Europe during the fifteenth century and the diffusion of printed matter contributed to higher literacy rates and wider access to the once-rarefied knowledge of academia. The subsequent spread of education and learning changed society, and initiated an era of widespread education whose elite would now instead be those who produced the written content itself for consumption, in education and all other spheres.

Plato's "Noble Lie", concerns expertise. Plato did not believe most people were clever enough to look after their own and society's best interest, so the few clever people of the world needed to lead the rest of the flock. Therefore, the idea was born that only the elite should know the truth in its complete form and the rulers, Plato said, must tell the people of the city "the noble lie" to keep them passive and content, without the risk of upheaval and unrest.

In contemporary society, doctors and scientists, for example, are considered to be experts in that they hold a body of dominant knowledge that is, on the whole, inaccessible to the layman. However, this inaccessibility and perhaps even mystery that surrounds expertise does not cause the layman to disregard the opinion of the experts on account of the unknown. Instead, the complete opposite occurs whereby members of the public believe in and highly value the opinion of medical professionals or of scientific discoveries , despite not understanding it.

A number of computational models have been developed in cognitive science to explain the development from novice to expert. In particular, Herbert A. Simon and Kevin Gilmartin proposed a model of learning in chess called MAPP (Memory-Aided Pattern Recognizer). Based on simulations, they estimated that about 50,000 chunks (units of memory) are necessary to become an expert, and hence the many years needed to reach this level. More recently, the CHREST model (Chunk Hierarchy and REtrieval STructures) has simulated in detail a number of phenomena in chess expertise (eye movements, performance in a variety of memory tasks, development from novice to expert) and in other domains.

An important feature of expert performance seems to be the way in which experts are able to rapidly retrieve complex configurations of information from long-term memory. They recognize situations because they have meaning. It is perhaps this central concern with meaning and how it attaches to situations which provides an important link between the individual and social approaches to the development of expertise. Work on "Skilled Memory and Expertise" by Anders Ericsson and James J. Staszewski confronts the paradox of expertise and claims that people not only acquire content knowledge as they practice cognitive skills, they also develop mechanisms that enable them to use a large and familiar knowledge base efficiently.

Work on expert systems (computer software designed to provide an answer to a problem, or clarify uncertainties where normally one or more human experts would need to be consulted) typically is grounded on the premise that expertise is based on acquired repertoires of rules and frameworks for decision making which can be elicited as the basis for computer supported judgment and decision-making. However, there is increasing evidence that expertise does not work in this fashion. Rather, experts recognize situations based on experience of many prior situations. They are in consequence able to make rapid decisions in complex and dynamic situations.

In a critique of the expert systems literature suggest:

The role of long term memory in the skilled memory effect was first articulated by Chase and Simon in their classic studies of chess expertise. They asserted that organized patterns of information stored in long term memory (chunks) mediated experts' rapid encoding and superior retention. Their study revealed that all subjects retrieved about the same number of chunks, but the size of the chunks varied with subjects' prior experience. Experts' chunks contained more individual pieces than those of novices. This research did not investigate how experts find, distinguish, and retrieve the right chunks from the vast number they hold without a lengthy search of long term memory.

Skilled memory enables experts to rapidly encode, store, and retrieve information within the domain of their expertise and thereby circumvent the capacity limitations that typically constrain novice performance. For example, it explains experts' ability to recall large amounts of material displayed for only brief study intervals, provided that the material comes from their domain of expertise. When unfamiliar material (not from their domain of expertise) is presented to experts, their recall is no better than that of novices.

The first principle of skilled memory, the "meaningful encoding principle," states that experts exploit prior knowledge to durably encode information needed to perform a familiar task successfully. Experts form more elaborate and accessible memory representations than novices. The elaborate semantic memory network creates meaningful memory codes that create multiple potential cues and avenues for retrieval.

The second principle, the "retrieval structure principle" states that experts develop memory mechanisms called retrieval structures to facilitate the retrieval of information stored in long term memory. These mechanisms operate in a fashion consistent with the meaningful encoding principle to provide cues that can later be regenerated to retrieve the stored information efficiently without a lengthy search.

The third principle, the "speed up principle" states that long term memory encoding and retrieval operations speed up with practice, so that their speed and accuracy approach the speed and accuracy of short term memory storage and retrieval.

Examples of skilled memory research described within the Ericsson and Stasewski study include:

Much of the research regarding expertise involves the studies of how experts and novices differ in solving problems (Chi, M. T. H., Glasser R., & Rees, E.,1982). Mathematics (Sweller, J., Mawer, R. F., & Ward, M. R., 1983) and physics (Chi, Feltovich, & Glaser, 1981) are common domains for these studies.

One of the most cited works in this area, Chi et al. (1981), examines how experts (PhD students in physics) and novices (undergraduate students that completed one semester of mechanics) categorize and represent physics problems. They found that novices sort problems into categories based upon surface features (e.g., keywords in the problem statement or visual configurations of the objects depicted). Experts, however, categorize problems based upon their deep structures (i.e., the main physics principle used to solve the problem).

Their findings also suggest that while the schemas of both novices and experts are activated by the same features of a problem statement, the experts’ schemas contain more procedural knowledge which aid in determining which principle to apply, and novices’ schemas contain mostly declarative knowledge which do not aid in determining methods for solution.

Relative to a specific field, an expert has:

Marie-Line Germain (Germain, 2006) developed a psychometric measure of perception of employee expertise called the Generalized Expertise Measure (GEM). She defined a behavioral dimension in experts, in addition to the dimensions suggested by Swanson and Holton (2001). Her 16-item scale contains objective expertise items and subjective expertise items. Objective items were named Evidence-Based items. Subjective items (the remaining 11 items from the measure below) were named Self-Enhancement items because of their behavioral component.

(Condensed from Germain, 2006).


Scholars in rhetoric have also turned their attention to the concept of the expert. Considered an appeal to ethos or "the personal character of the speaker", established expertise allows a speaker to make statements regarding special topics of which the audience may be ignorant. In other words, the expert enjoys the deference of the audience’s judgment and can appeal to authority where a non-expert cannot.

In The Rhetoric of Expertise, E. Johanna Hartelius defines two basic modes of expertise: autonomous and attributed expertise. While an autonomous expert can "possess expert knowledge without recognition from other people," attributed expertise is "a performance that may or may not indicate genuine knowledge." With these two categories, Hartelius isolates the rhetorical problems faced by experts: just as someone with autonomous expertise may not possess the skill to persuade people to hold their points of view, someone with merely attributed expertise may be persuasive but lack the actual knowledge pertaining to a given subject. The problem faced by audiences follows from the problem facing experts: when faced with competing claims of expertise, what resources do non-experts have to evaluate claims put before them?

Hartelius and other scholars have also noted the challenges that projects such as Wikipedia pose to how experts have traditionally constructed their authority. In "Wikipedia and the Emergence of Dialogic Expertise", she highlights Wikipedia as an example of the "dialogic expertise" made possible by collaborative digital spaces. Predicated upon the notion that "truth emerges from dialogue", Wikipedia challenges traditional expertise both because anyone can edit it and because no single person, regardless of their credentials, can end a discussion by fiat. In other words, the community, rather than single individuals, direct the course of discussion. The production of knowledge, then, as a process of dialogue and argumentation, becomes an inherently rhetorical activity.

Hartelius calls attention to two competing norm systems of expertise: “network norms of dialogic collaboration” and “deferential norms of socially sanctioned professionalism”; Wikipedia being evidence of the first. Drawing on a Bakhtinian framework, Hartelius posits that Wikipedia is an example of an epistemic network that is driven by the view that individuals’ ideas clash with one another so as to generate expertise collaboratively. Hartelius compares Wikipedia’s methodology of open-ended discussions of topics to that of Bakhtin’s theory of speech communication, where genuine dialogue is considered a live event, which is continuously open to new additions and participants. Hartelius acknowledges that knowledge, experience, training, skill, and qualification are important dimensions of expertise but posits that the concept is more complex than sociologists and psychologists suggest. Arguing that expertise is rhetorical, then, Hartelius explains that expertise: “(...) is not simply about one person’s skills being different from another’s. It is also fundamentally contingent on a struggle for ownership and legitimacy.”. Effective communication is an inherent element in expertise in the same style as knowledge is. Rather than leaving each other out, substance and communicative style are complementary. Hartelius further suggests that Wikipedia’s dialogic construction of expertise illustrates both the instrumental and the constitutive dimensions of rhetoric; instrumentally as it challenges traditional encyclopedias and constitutively as a function of its knowledge production. Going over the historical development of the encyclopedic project, Hartelius argues that changes in traditional encyclopedias have led to changes in traditional expertise. Wikipedia’s use of hyperlinks to connect one topic to another depends on, and develops, electronic interactivity meaning that Wikipedia’s way of knowing is dialogic. Dialogic expertise then, emerges from multiple interactions between utterances within the discourse community. The ongoing dialogue between contributors on Wikipedia not only results in the emergence of truth; it also explicates the topics one can be an expert of. As Hartelius explains: “The very act of presenting information about topics that are not included in traditional encyclopedias is a construction of new expertise.”. While Wikipedia insists that contributors must only publish preexisting knowledge, the dynamics behind dialogic expertise creates new information nonetheless. Knowledge production is created as a function of dialogue. According to Hartelius, dialogic expertise has emerged on Wikipedia not only because of its interactive structure but also because of the site’s hortative discourse which is not found in traditional encyclopedias. By Wikipedia’s hortative discourse, Hartelius means various encouragements to edit certain topics and instructions on how to do so that appear on the site. One further reason to the emergence of dialogic expertise on Wikipedia is the site’s , which function as a techne; explicating Wikipedia’s expert methodology.

Building on Hartelius, Damien Pfister developed the concept of "networked expertise". Noting that Wikipedia employs a "many to many" rather than a "one to one" model of communication, he notes how expertise likewise shifts to become a quality of a group rather than an individual. With the information traditionally associated with individual experts now stored within a text produced by a collective, knowing about something is less important than knowing how to find something. As he puts it, "With the internet, the historical power of subject matter expertise is eroded: the archival nature of the Web means that what and how to information is readily available." The rhetorical authority previously afforded to subject matter expertise, then, is given to those with the procedural knowledge of how to find information called for by a situation.

An expert differs from the specialist in that a specialist has to "be able to solve" a problem and an expert has to "know its solution". The opposite of an expert is generally known as a layperson, while someone who occupies a middle grade of understanding is generally known as a technician and often employed to assist experts. A person may well be an expert in one field and a layperson in many other fields. The concepts of experts and expertise are debated within the field of epistemology under the general heading of expert knowledge. In contrast, the opposite of a specialist would be a generalist or polymath.

The term is widely used informally, with people being described as 'experts' in order to bolster the relative value of their opinion, when no objective criteria for their expertise is available. The term crank is likewise used to disparage opinions. Academic elitism arises when experts become convinced that only their opinion is useful, sometimes on matters beyond their personal expertise.

In contrast to an expert, a novice (known colloquially as a newbie or 'greenhorn') is any person that is new to any science or field of study or activity or social cause and who is undergoing training in order to meet normal requirements of being regarded a mature and equal participant.

"Expert" is also being mistakenly interchanged with the term "authority" in new media. An expert can be an authority if through relationships to people and technology, that expert is allowed to control access to his expertise. However, a person who merely wields authority is not by right an expert. In new media, users are being misled by the term "authority". Many sites and search engines such as Google and Technorati use the term "authority" to denote the link value and traffic to a particular topic. However, this authority only measures populist information. It in no way assures that the author of that site or blog is an expert.

Some characteristics of the development of an expert have been found to include

Mark Twain defined an expert as "an ordinary fellow from another town". Will Rogers described an expert as "A man fifty miles from home with a briefcase." Danish scientist and Nobel laureate Niels Bohr defined an expert as "A person that has made every possible mistake within his or her field."
Malcolm Gladwell describes expertise as a matter of practicing the correct way for a total of around 10,000 hours.

Scholar, Know-how, Skill, Competence, Excellence, Technical government, Insider, Tutor expertise in adult education
Anti-intellectualism, Denialism, The Death of Expertise, Gibson's law
Dreyfus model of skill acquisition, Dunning–Kruger effect, Pygmalion effect, Rational skepticism




</doc>
<doc id="13233189" url="https://en.wikipedia.org/wiki?curid=13233189" title="Wisdom of the crowd">
Wisdom of the crowd

The wisdom of the crowd is the collective opinion of a group of individuals rather than that of a single expert.

A large group's aggregated answers to questions involving quantity estimation, general world knowledge, and spatial reasoning has generally been found to be as good as, but often superior to, the answer given by any of the individuals within the group. An explanation for this phenomenon is that there is idiosyncratic noise associated with each individual judgment, and taking the average over a large number of responses will go some way toward canceling the effect of this noise. This process, while not new to the Information Age, has been pushed into the mainstream spotlight by social information sites such as Wikipedia, Yahoo! Answers, Quora, Stack Exchange and other web resources that rely on collective human knowledge.

Trial by jury can be understood as at least partly relying on wisdom of the crowd, compared to bench trial which relies on one or a few experts. In politics, sometimes sortition is held as an example of what wisdom of the crowd would look like. Decision-making would happen by a diverse group instead of by a fairly homogenous political group or party.

Research within cognitive science has sought to model the relationship between wisdom of the crowd effects and individual cognition.

In the context of wisdom of the crowd, the term "crowd" takes on a broad meaning. One definition characterizes a crowd as a group of people amassed by an open call for participation. While crowds are often leveraged in online applications, they can also be utilized in offline contexts. In some cases, members of a crowd may be offered monetary incentives for participation. Certain applications of "wisdom of the crowd", such as jury duty in the United States, mandate crowd participation.

Aristotle is credited as the first person to write about the "wisdom of the crowd" in his work titled "Politics". According to Aristotle, "it is possible that the many, though not individually good men, yet when they come together may be better, not individually but collectively, than those who are so, just as public dinners to which many contribute are better than those supplied at one man's cost".

The classic wisdom-of-the-crowds finding involves point estimation of a continuous quantity. At a 1906 country fair in Plymouth, 800 people participated in a contest to estimate the weight of a slaughtered and dressed ox. Statistician Francis Galton observed that the median guess, 1207 pounds, was accurate within 1% of the true weight of 1198 pounds. This has contributed to the insight in cognitive science that a crowd's individual judgments can be modeled as a probability distribution of responses with the median centered near the true value of the quantity to be estimated.

In recent years, the "wisdom of the crowd" phenomenon has been leveraged in business strategy and advertising spaces. Firms such as Napkin Labs aggregate consumer feedback and brand impressions for clients. Meanwhile, companies such as Trada invoke crowds to design advertisements based on clients' requirements.

Non-human examples are prevalent. For example, the Golden Shiner is a fish that prefers shady areas. The single Shiner has a very difficult time finding shady regions in a body of water whereas a large group is much more efficient at finding the shade.

Wisdom-of-the-crowds research routinely attributes the superiority of crowd averages over individual judgments to the elimination of individual noise, an explanation that assumes independence of the individual judgments from each other. Thus the crowd tends to make its best decisions if it is made up of diverse opinions and ideologies.

Averaging can eliminate random errors that affect each person's answer in a different way, but not systematic errors that affect the opinions of the entire crowd in the same away. So for instance, a wisdom-of-the-crowd technique would not be expected to compensate for cognitive biases.

Scott E. Page introduced the diversity prediction theorem: "The squared error of the collective prediction equals the average squared error minus the predictive diversity". Therefore, when the diversity in a group is large, the error of the crowd is small.

Miller and Stevyers reduced the independence of individual responses in a wisdom-of-the-crowds experiment by allowing limited communication between participants. Participants were asked to answer ordering questions for general knowledge questions such as the order of U.S. presidents. For half of the questions, each participant started with the ordering submitted by another participant (and alerted to this fact), and for the other half, they started with a random ordering, and in both cases were asked to rearrange them (if necessary) to the correct order. Answers where participants started with another participant's ranking were on average more accurate than those from the random starting condition. Miller and Steyvers conclude that different item-level knowledge among participants is responsible for this phenomenon, and that participants integrated and augmented previous participants' knowledge with their own knowledge.

Crowds tend to work best when there is a correct answer to the question being posed, such as a question about geography or mathematics. When there is not a precise answer crowds can come to arbitrary conclusions.

The wisdom of the crowd effect is easily undermined. Social influence can cause the average of the crowd answers to be wildly inaccurate, while the geometric mean and the median are far more robust.

Experiments run by the Swiss Federal Institute of Technology found that when a group of people were asked to answer a question together they would attempt to come to a consensus which would frequently cause the accuracy of the answer to decrease. i.e. what is the length of a border between two countries? One suggestion to counter this effect is to ensure that the group contains a population with diverse backgrounds.

The insight that crowd responses to an estimation task can be modeled as a sample from a probability distribution invites comparisons with individual cognition. In particular, it is possible that individual cognition is probabilistic in the sense that individual estimates are drawn from an "internal probability distribution." If this is the case, then two or more estimates of the same quantity from the same person should average to a value closer to ground truth than either of the individual judgments, since the effect of statistical noise within each of these judgments is reduced. This of course rests on the assumption that the noise associated with each judgment is (at least somewhat) statistically independent. Another caveat is that individual probability judgments are often biased toward extreme values (e.g., 0 or 1). Thus any beneficial effect of multiple judgments from the same person is likely to be limited to samples from an unbiased distribution.

Vul and Pashler (2008) asked participants for point estimates of continuous quantities associated with general world knowledge, such as "What percentage of the world's airports are in the United States?" Without being alerted to the procedure in advance, half of the participants were immediately asked to make a second, different guess in response to the same question, and the other half were asked to do this three weeks later. The average of a participant's two guesses was more accurate than either individual guess. Furthermore, the averages of guesses made in the three-week delay condition were more accurate than guesses made in immediate succession. One explanation of this effect is that guesses in the immediate condition were less independent of each other (an anchoring effect) and were thus subject to (some of) the same kind of noise. In general, these results suggest that individual cognition may indeed be subject to an internal probability distribution characterized by stochastic noise, rather than consistently producing the best answer based on all the knowledge a person has. These results were mostly confirmed in a high-powered pre-registered replication. The only result that was not fully replicated was that a delay in the second guess generates a better estimate.

Hourihan and Benjamin (2010) tested the hypothesis that the estimate improvements observed by Vul and Pashler in the delayed responding condition were the result of increased independence of the estimates. To do this Hourihan and Benjamin capitalized on variations in memory span among their participants. In support they found that averaging repeated estimates of those with lower memory spans showed greater estimate improvements than the averaging the repeated estimates of those with larger memory spans.

Rauhut and Lorenz (2011) expanded on this research by again asking participants to make estimates of continuous quantities related to real world knowledge – however, in this case participants were informed that they would make five consecutive estimates. This approach allowed the researchers to determine, firstly, the number of times one needs to ask oneself in order to match the accuracy of asking others and then, the rate at which estimates made by oneself improve estimates compared to asking others. The authors concluded that asking oneself an infinite number of times does not surpass the accuracy of asking just one other individual. Overall, they found little support for a so-called “mental distribution” from which individuals draw their estimates; in fact, they found that in some cases asking oneself multiple times actually reduces accuracy. Ultimately, they argue that the results of Vul and Pashler (2008) overestimate the wisdom of the “crowd within” – as their results show that asking oneself more than three times actually reduces accuracy to levels below that reported by Vul and Pashler (who only asked participants to make two estimates).

Müller-Trede (2011) attempted to investigate the types of questions in which utilizing the “crowd within” is most effective. He found that while accuracy gains were smaller than would be expected from averaging ones’ estimates with another individual, repeated judgments lead to increases in accuracy for both year estimation questions (e.g., when was the thermometer invented?) and questions about estimated percentages (e.g., what percentage of internet users connect from China?). General numerical questions (e.g., what is the speed of sound, in kilometers per hour?), however, did not show improvement with repeated judgments, while averaging individual judgments with those of a random other did improve accuracy. This, Müller-Trede argues, is the result of the bounds implied by year and percentage questions.

Van Dolder and Van den Assem (2018) studied the "crowd within" using a large database from three estimation competitions organised by Holland Casino. For each of these competitions, they find that within-person aggregation indeed improves accuracy of estimates. Furthermore, they also confirm that this method works better if there is a time delay between subsequent judgments. However, even when there is considerable delay between estimates, the benefit pales against that of between-person aggregation: the average of a large number of judgements from the same person is barely better than the average of two judgements from different people.

Herzog and Hertwig (2009) attempted to improve on the “wisdom of many in one mind” (i.e., the “crowd within”) by asking participants to use dialectical bootstrapping. Dialectical bootstrapping involves the use of dialectic (reasoned discussion that takes place between two or more parties with opposing views, in an attempt to determine the best answer) and bootstrapping (advancing oneself without the assistance of external forces). They posited that people should be able to make greater improvements on their original estimates by basing the second estimate on antithetical information. Therefore, these second estimates, based on different assumptions and knowledge than that used to generate the first estimate would also have a different error (both systematic and random) than the first estimate – increasing the accuracy of the average judgment. From an analytical perspective dialectical bootstrapping should increase accuracy so long as the dialectical estimate is not too far off and the errors of the first and dialectical estimates are different. To test this, Herzog and Hertwig asked participants to make a series of date estimations regarding historical events (e.g., when electricity was discovered), without knowledge that they would be asked to provide a second estimate. Next, half of the participants were simply asked to make a second estimate. The other half were asked to use a consider-the-opposite strategy to make dialectical estimates (using their initial estimates as a reference point). Specifically, participants were asked to imagine that their initial estimate was off, consider what information may have been wrong, what this alternative information would suggest, if that would have made their estimate an overestimate or an underestimate, and finally, based on this perspective what their new estimate would be. Results of this study revealed that while dialectical bootstrapping did not outperform the wisdom of the crowd (averaging each participants’ first estimate with that of a random other participant), it did render better estimates than simply asking individuals to make two estimates.

Hirt and Markman (1995) found that participants need not be limited to a consider-the-opposite strategy in order to improve judgments. Researchers asked participants to consider-an-alternative – operationalized as any plausible alternative (rather than simply focusing on the “opposite” alternative) – finding that simply considering an alternative improved judgments.

Not all studies have shown support for the “crowd within” improving judgments. Ariely and colleagues asked participants to provide responses based on their answers to true-false items and their confidence in those answers. They found that while averaging judgment estimates between individuals significantly improved estimates, averaging repeated judgment estimates made by the same individuals did not significantly improve estimates.

Although classic wisdom-of-the-crowds findings center on point estimates of single continuous quantities, the phenomenon also scales up to higher-dimensional problems that do not lend themselves to aggregation methods such as taking the mean. More complex models have been developed for these purposes. A few examples of higher-dimensional problems that exhibit wisdom-of-the-crowds effects include:

In further exploring the ways to improve the results a new technique called the "surprisingly popular" was developed by scientists at MIT's Sloan Neuroeconomics Lab in collaboration with Princeton University. For a given question, people are asked to give two responses: What they think the right answer is, and what they think popular opinion will be. The averaged difference between the two indicates the correct answer. It was found that the "surprisingly popular" algorithm reduces errors by 21.3 percent in comparison to simple majority votes, and by 24.2 percent in comparison to basic confidence-weighted votes where people express how confident they are of their answers and 22.2 percent compared to advanced confidence-weighted votes, where one only uses the answers with the highest average.



</doc>
<doc id="3009184" url="https://en.wikipedia.org/wiki?curid=3009184" title="Discernment">
Discernment

Discernment is the ability to obtain sharp perceptions or to judge well (or the activity of so doing). In the case of judgment, discernment can be psychological or moral in nature. Within judgment, discernment 
involves going past the mere perception of something and making nuanced judgments about its properties or qualities. Considered as a virtue, a discerning individual is considered to possess wisdom, and be of good judgement; especially so with regard to subject matter often overlooked by others.

In Christianity, the word may have several meanings. Discernment can describe the process of determining God's desire in a situation or for one's life or identifying the true nature of a thing, such as discerning whether a thing is good, evil, or may even transcend the limiting notion of duality. In large part, it describes the interior search for an answer to the question of one's vocation, namely, determining whether or not God is calling one to the married life, single life, consecrated life, ordained ministry or any other calling.

Discernment of Spirits is a term used in both Roman Catholic and Charismatic (Pentacostal) Christian theology to indicate judging various spiritual agents for their moral influence.

The process of individual discernment has required tasks in order for good discernment. These tasks include taking time in making the decision, using both the head and heart, and assessing important values. Time is necessary to make a good choice, decisions made in a hurry are often not the best decisions. When time is available to assess the situation it improves the discernment process. When time allots the tentative decision can be revisited days later to make sure that the individual is satisfied with their choice after the discernment process. Making decisions with the head means to first reflect on the situation and emphasize the rational aspect of the decision making process. In order to make a decision that is ours it also requires the heart in which the individual makes based on feelings as well as rationality. Values in the discernment process are weighing options that decide what is most important to the individual. Every individuals value system is different which effects each individual discernment process. Combining values, using both the head and heart and taking sufficient time when making decision are the main steps for a successful discernment process.

Group discernment is a separate branch of discernment. In group discernment each individual must first undergo their own discernment process. The individual must keep in mind what is best for the group as a whole as well as the individual when making a decision. The same principles of values, using the head and heart, as well as giving the decision making process ample time all still apply in group discernment. Group discernment is different because it requires multiple people to have a unanimous decision in order to move forward forward. Group discernment requires discussion and persuasion between individuals to arrive at a decision.

Christian spiritual discernment can be separated from other types of discernment because every decision is to be made in accordance with God's will. The fundamental definition for Christian discernment is a decision making process in which an individual makes a discovery that can lead to future action. In the process of Christian spiritual discernment God guides the individual to help them arrive at the best decision. The way to arrive at the best decision in Christian spiritual discernment is to seek out internal external signs of God's action and then apply them to the decision at hand. Christian Discernment also has an emphasis on Jesus, and making decisions that align with those of Jesus within the New Testament. The focus on God and Jesus when making decisions is what separates Christian discernment from secular discernment. Ignatius of Loyola is often regarded as the master of the discernment of spirits. Ignatian discernment comes from Ignatius of Loyola (1491-1556) when he created his own unique way of Catholic discernment. Ignatian discernment uses a series of Spiritual Exercises for discerning life choices and focuses on noticing God in all aspects of life. The Spiritual Exercises are designed to help people who are facing a major life decision. There are seven steps of discernment to be followed that include identifying the issue, taking time to pray about the choice, making a wholehearted decision, discussing the choice with a mentor and then finally trusting the decision made. While Ignatius was a religious man his discernment process can be applied to anyone independent of religion.


</doc>
<doc id="43249770" url="https://en.wikipedia.org/wiki?curid=43249770" title="Knowledge acquisition">
Knowledge acquisition

Knowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. 

Expert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. 

As expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process.

One approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.

A more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics. 


</doc>
<doc id="2572627" url="https://en.wikipedia.org/wiki?curid=2572627" title="Knowledge ark">
Knowledge ark

A knowledge ark (also known as a doomsday ark or doomsday vault) is a collection of knowledge preserved in such a way that future generations would have access to said knowledge if current means of access were lost. 

Scenarios where availability to information (such as the Internet) would be lost could be described as Existential Risks or Extinction Level Events. A knowledge ark could take the form of a traditional Library or a modern computer database. It could also include images only (such as photographs of important information, or diagrams of critical processes). 

A knowledge ark would have to be resistant to the effects of natural or man-made disasters to be viable. Such an ark should include, but would not be limited to, information or material relevant to the survival and prosperity of human civilization.

Current examples include the Svalbard Global Seed Vault, a seedbank which is intended to preserve a wide variety of plant seeds (such as important crops) in case of their extinction.

A Lunar ark has been proposed which would store and transmit valuable information to receiver stations on Earth. The success of this would also depend on the availability of compatible receiver equipment on Earth, and adequate knowledge of that equipment's operation.

Other types of knowledge arks might include genetic material. With the potential for widespread personal DNA sequencing becoming a reality, an individual might agree to store their genetic code in a digital or analog storage format which would enable later retrieval of that code. If a species was sequenced before extinction, its genome would remain available for study even in the case of extinction.

The Phoenix mars lander (landed on surface of Mars in 2008) included the Visions of Mars DVD, a library on Mars. 



</doc>
<doc id="7983699" url="https://en.wikipedia.org/wiki?curid=7983699" title="Knowledge divide">
Knowledge divide

The knowledge divide is the gap in the standards of living between those who can find, create, manage, process, and disseminate information or knowledge, and those who are impaired in this process. According to a 2005 UNESCO World Report, the rise in the 21st century of a global information society has resulted in the emergence of knowledge as a valuable resource, increasingly determining who has access to power and profit. The rapid dissemination of information on a potentially global scale as a result of new information media and the globally uneven ability to assimilate knowledge and information has resulted in potentially expanding gaps in knowledge between individuals and nations.

In the 21st century, the emergence of the knowledge society becomes pervasive. The transformations of world's economy and of each society have a fast pace. Together with information and communication technologies (ICT), these new paradigms have the power to reshape the global economy. In order to keep pace with innovations, to come up with new ideas, people need to produce and manage knowledge. This is why knowledge has become essential for all societies.

According to UNESCO and the World Bank, knowledge gaps between nations may occur due to the varying degrees by which individual nations incorporate the following elements:


The information and ICT systems that support knowledge are very important. This is why digitization is viewed as closely related to knowledge. Scientists generally agree that there is a digital divide, recently different reports also showed the existence of knowledge divide.

The creation and effective use of knowledge are increasingly related to the development of an ICT infrastructure. Without ICT, it is impossible to have an infrastructure able to process the huge flow of information required in an advanced economy. In particular, without adequate technical support, it is difficult to develop and use e-learning and electronic documents to overcome time and space constraints.

The digital divide is, however, but one important part of the larger knowledge divide. As UNESCO states, "closing the digital divide will not suffice to close the knowledge divide, for access to useful, relevant knowledge is more than simply a matter of infrastructure—it depends on training, cognitive skills and regulatory frameworks geared towards access to contents."

In the book Digital Dead End, Virginia Eubanks criticizes the way that the digital divide is generally thought of as a division between haves and have-nots, where the solution is distribution. This over-simplistic depiction obscures the fact that often social and structural inequality is at the root of the divide. According to a study done by Eubanks with women of the YWCA, the women of the community "insisted that have-nots possess many different kinds of crucial information and skills." In other words, it is not simply knowledge of the technology itself that is the issue but the structural system based on perpetuating the status quo in which the haves "hoard" knowledge.

First, it was noticed that a great difference exists between the North and the South (rich countries vs. poor countries). The development of knowledge depends on spreading Internet and computer technology and also on the development of education in these countries. If a country has attained a higher literacy level then this will result in a higher level of knowledge.
Indeed, UNESCO's report details many social issues in knowledge divide related to globalization. There was noticed a knowledge divide with respect to




</doc>
