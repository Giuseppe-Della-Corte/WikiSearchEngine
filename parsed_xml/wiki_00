<doc id="10772350" url="https://en.wikipedia.org/wiki?curid=10772350" title="History">
History

History (from Greek , "historia", meaning 'inquiry; knowledge acquired by investigation') is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.

History can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing "perspective" on the problems of the present.

Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the "disinterested investigation" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the "father of history", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals, was known to be compiled from as early as 722 BC although only 2nd-century BC texts have survived.

Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.

The word "history" comes from the Ancient Greek ἱστορία ("historía"), meaning 'inquiry', 'knowledge from inquiry', or 'judge'. It was in that sense that Aristotle used the word in his "History of Animals." The ancestor word is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either 'judge' or 'witness', or similar). The Greek word was borrowed into Classical Latin as "historia", meaning "investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative". "History" was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as "stær" ('history, narrative, story'), but this word fell out of use in the late Old English period. Meanwhile, as Latin became Old French (and Anglo-Norman), "historia" developed into forms such as "istorie", "estoire", and "historie", with new developments in the meaning: "account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)".

It was from Anglo-Norman that "history" was borrowed into Middle English, and this time the loan stuck. It appears in the 13th-century "Ancrene Wisse", but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's "Confessio Amantis" of the 1390s (VI.1383): "I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire". In Middle English, the meaning of "history" was "story" in general. The restriction to the meaning "the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs" arose in the mid-15th century. With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about "Natural History". For him, "historia" was "the knowledge of objects determined by space and time", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).

In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both 'history' and 'story'. "Historian" in the sense of a "researcher of history" is attested from 1531. In all European languages, the substantive "history" is still used to mean both "what happened with men", and "the scholarly study of the happened", the latter sense sometimes distinguished with a capital letter, or the word "historiography". The adjective "historical" is attested from 1661, and "historic" from 1669.

Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, "All history is contemporary history". History is facilitated by the formation of a "true discourse of past" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse.

All events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the "true past").

The study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences. It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification. In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.

Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before.

Archaeology is a discipline that is especially helpful in dealing with buried sites and objects, which, once unearthed, contribute to the study of history. But archaeology rarely stands alone. It uses narrative sources to complement its discoveries. However, archaeology is constituted by a range of methodologies and approaches which are independent from history; that is to say, archaeology does not "fill the gaps" within textual sources. Indeed, "historical archaeology" is a specific branch of archaeology, often contrasting its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA; has sought to understand the contradiction between textual documents and the material record, demonstrating the possession of slaves and the inequalities of wealth apparent via the study of the total historical environment, despite the ideology of "liberty" inherent in written documents at this time.

There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant overlaps are often present, as in "The International Women's Movement in an Age of Transition, 1830–1975." It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.

The history of the world is the memory of the past experience of "Homo sapiens sapiens" around the world, as that experience has been preserved, largely in written records. By "prehistory", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:

This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.

Historiography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, "medieval historiography during the 1960s" means "Works of medieval history written during the 1960s"). Thirdly, it may refer to why history is produced: the Philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.

Philosophy of history is a branch of philosophy concerning the eventual significance, if any, of human history. Furthermore, it speculates as to a possible teleological end to its development—that is, it asks if there is a design, purpose, directive principle, or finality in the processes of human history. Philosophy of history should not be confused with historiography, which is the study of history as an academic discipline, and thus concerns its methods and practices, and its development as a discipline over time. Nor should philosophy of history be confused with the history of philosophy, which is the study of the development of philosophical ideas through time.

The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.

Herodotus of Halicarnassus (484 BC – ca.425 BC) has generally been acclaimed as the "father of history". However, his contemporary Thucydides (c. 460 BC – c. 400 BC) is credited with having first approached history with a well-developed historical method in his work the "History of the Peloponnesian War". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention. In his historical method, Thucydides emphasized chronology, a neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.

There were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the "Records of the Grand Historian" ("Shiji"). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his "Shiji" as the official format for historical texts, as well as for biographical literature.

Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.

In the preface to his book, the "Muqaddimah" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized "idle superstition and uncritical acceptance of historical data." As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the "father of historiography" or the "father of the philosophy of history".

In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods: 

By the "rich ore" Spencer meant scientific theory of history. Meanwhile Henry Thomas Buckle expressed a dream of history becoming one day science: 

Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to “what really happened” and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures “are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them.”

In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from the pen of a professional Historian. Whatever other theories of history we have, they were written by experts from other fields (for example, Marxian theory of history). More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.

In sincere opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. "histoire des mentalités"). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was "Alltagsgeschichte" (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.

Marxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book "In Defence of History", Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, "The Killing of History".

The Marxist theory of historical materialism theorises that society is fundamentally determined by the "material conditions" at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe. Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.

Historical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow "organising ideas and classificatory generalisations" to be used by historians. The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.

The field of history generally leaves prehistory to the archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of "chapters" so that periods in history could unfold not only in a relative chronology but also narrative chronology. This narrative content could be in the form of functional-economic interpretation. There are periodisation, however, that do not have this narrative aspect, relying largely on relative chronology and, thus, devoid of any specific meaning.

Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework with one account explaining that "cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena.

Particular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book "Histoire de France" (1833), "without geographical basis, the people, the makers of history, seem to be walking on air." Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.


Military history concerns warfare, strategies, battles, weapons, and the psychology of combat. The "new military history" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.

The history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include "Church History", "The Catholic Historical Review", and "History of Religions". Topics range widely from political and cultural and artistic dimensions, to theology and liturgy. This subject studies religions from all regions and areas of the world where humans have lived.

"Social history", sometimes called the "new social history", is the field that includes history of ordinary people and their strategies and institutions for coping with life. In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%. In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).
The "old" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible." While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in."

The chief subfields of social history include:
Smaller specialties include:

Cultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.
Cultural history includes the study of art in society as well is the study of images and human visual production (iconography).

Diplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars. More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of "political history" is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, "diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies." She adds that after 1945, the trend reversed, allowing social history to replace it.

Although economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments. Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history; Business history is most often taught in business schools.

Environmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.

World history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States, Japan and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.

It has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.

The World History Association publishes the "Journal of World History" every quarter since 1990. The H-World discussion list serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.

A people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.

Intellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.

Gender history is a sub-field of History and Gender studies, which looks at the past from the perspective of gender. It is in many ways, an outgrowth of women's history. Despite its relatively short life, Gender History (and its forerunner Women's History) has had a rather significant effect on the general study of history. Since the 1960s, when the initially small field first achieved a measure of acceptance, it has gone through a number of different phases, each with its own challenges and outcomes. Although some of the changes to the study of history have been quite obvious, such as increased numbers of books on famous women or simply the admission of greater numbers of women into the historical profession, other influences are more subtle.

Public history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.

Professional and amateur historians discover, collect, organize, and present information about past events.They discover this information through archaeological evidence, written primary sources from the past and other various means such as place names. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.

Since the 20th century, Western historians have disavowed the aspiration to provide the "judgement of history." The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory.

Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.
Closely related to deceptive historical revisionism, works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.

A major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.

In the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.

From the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.

At the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.

The teaching of history in French schools was influenced by the "Nouvelle histoire" as disseminated after the 1960s by "Cahiers pédagogiques and Enseignement" and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote "active methods" which would give pupils "the immense happiness of discovery." Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.

In several countries history textbooks are tools to foster nationalism and patriotism, and give students the official line about national enemies.

In many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained. It was standard policy in communist countries to present only a rigid Marxist historiography.

In the United States, especially the southern part history about slavery and the American Civil War are controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as "workers" instead of slaves in a textbook.

Academic historians have often fought against the politicization of the textbooks, sometimes with success.

In 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an "almost pacifistic and deliberately unpatriotic undertone" and reflects "principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace." The result is that "German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness."







</doc>
<doc id="11325429" url="https://en.wikipedia.org/wiki?curid=11325429" title="Historical figure">
Historical figure

A historical figure is a famous person in history, such as Catherine the Great, Abraham Lincoln, George Washington, or Napoleon.

The significance of such figures in human progress has been debated. Some think they play a crucial role, while others say they have little impact on the broad currents of thought and social change. The concept is generally used in the sense that the person really existed in the past, as opposed to being legendary. However, the legends that can grow up around historical figures may be hard to distinguish from fact. Sources are often incomplete and may be inaccurate, particularly those from early periods of history. Without a body of personal documents, the more subtle aspects of personality of a historical figure can only be deduced. With historical figures who were also religious figures attempts to separate fact from belief may be controversial.

In education, presenting information as if it were being told by a historical figure may give it greater impact. Since classical times, students have been asked to put themselves in the place of a historical figure as a way of bringing history to life. Historical figures are often represented in fiction, where fact and fancy are combined. In earlier traditions, before the rise of a critical historical tradition, authors took less care to be as accurate when describing what they knew of historical figures and their actions, interpolating imaginary elements intended to serve a moral purpose to events: such is the Monk of St. Gall's anecdotal account of Charlemagne, "De Carolo Magno". More recently there has been a tendency once again for authors to freely depart from the "facts" when they conflict with their creative goals.

The significance of historical figures has long been the subject of debate by philosophers. Hegel (1770–1831) considered that "world-historical figures" played a pivotal role in human progress, but felt that they were bound to emerge when change was needed. Thomas Carlyle (1795–1881) saw the study of figures such as Muhammad, William Shakespeare and Oliver Cromwell as key to understanding history. Herbert Spencer (1820–1903), an early believer in evolution and in the universality of natural law, felt that historical individuals were of little importance.

The German philosopher Hegel defined the concept of the world-historical figure, who embodied the ruthless advance of Immanuel Kant's World Spirit, often overthrowing outdated structures and ideas. To him, Napoleon was such a figure.
Hegel proposed that a world-historical figure essentially posed a challenge, or thesis, and this would generate an antithesis, or opposing force. Eventually a synthesis would resolve the conflict.
Hegel viewed Julius Caesar as a world historical figure, who appeared at a stage when Rome had grown to the point it could no longer continue as a republican city state but had to become an empire. Caesar failed in his bid to make himself an emperor, and was assassinated, but the empire came into existence soon afterward, and Caesar's name has become synonymous with "emperor" in forms such as "kaiser" or "czar".

Søren Kierkegaard, in his early essay "The Concept of Irony", generally agrees with Hegel's views, such as his characterization of Socrates as a world-historical figure who acted as a destructive force on Greek received views of morality.
In Hegel's view, Socrates broke down social harmony by questioning the meaning of concepts like "justice" and "virtue".
Eventually, the Athenians quite rightly *(Name source) condemned Socrates to death. But they could not stop the evolution of thought that Socrates had begun, which would lead to the concept of individual conscience.
Hegel said of world-historical figures,
However, Hegel, Thomas Carlyle and others noted that the great historical figures were just representative men, expressions of the material forces of history. Essentially they have little choice about what they do. This is in conflict with the views of George Bancroft or Ralph Waldo Emerson, who praised self-reliance and individualism, and in conflict with Karl Marx and Friedrich Engels, who also felt that individuals can determine their destiny.
Engels found that Hegel's system contained an "internal and incurable contradiction", resting as it does on both dialectical relativism and idealistic absolutism.

The Scottish philosopher and evolutionist Herbert Spencer, who was highly influential in the latter half of the nineteenth century, felt that historical figures were relatively unimportant.
He wrote to a friend, "I ignore utterly the personal element in history, and, indeed, show little respect for history altogether as it is ordinarily conceived."
He wrote, "The births, deaths, and marriages of kings, and other like historic trivialities, are committed to memory, not because of any direct benefits that can possibly result from knowing them: but because society considers them parts of a good education."
In his essay "What Knowledge Is of Most Worth?" he wrote:
Taken to an extreme, one may consider that what Hegel calls the "world spirit" and T. S. Eliot calls "those vast impersonal forces" hold us in their grip. What happens is predetermined. 
Both Hegel and Marx advocated historical inevitability in contrast to the doctrine of contingency, allowing for alternative outcomes, that was advocated by Friedrich Nietzsche, Michel Foucault and others.
However, Marx argued against the use of the "historical inevitability" argument when used to explain the destruction of early communes in Russia.
As an orthodox Marxist, Vladimir Lenin accepted the laws of history that Marx had discovered, including the historical inevitability of capitalism followed by a transition to socialism.
Despite this, Lenin also believed the transition could be effected faster by voluntary action.

In 1936 Karl Popper published an influential paper on "The Poverty of Historicism", published as a book in 1957, that attacked the doctrine of historical inevitability.
The historian Isaiah Berlin, author of "Historical Inevitability", also argued forcibly against this view, going as far as to say that some choices are entirely free and cannot be predicted scientifically.
Berlin presented his views in a 1953 lecture at the London School of Economics, published soon afterwards. When speaking he referred to Ludwig Wittgenstein's views, but the published version speaks approvingly of Karl Popper, which caused a stir among academics.

Thomas Carlyle has espoused the "heroic view" of history, famously saying in his essay on the Norse god Odin in his book "On heroes, hero-worship, & the heroic in history" that "No great man lives in vain. The History of the world is but the Biography of great men ... We do not now call our great men Gods, nor admire "without" limit; ah no, "with" limit enough! But if we have no great men, or do not admire at all,— that were a still worse case."
Carlyle's historical philosophy was based on the "Great Man theory", saying, "Universal History, the history of what man has accomplished in the world ... [is] at bottom the History of the Great Men who have worked here."
An extreme believer in individuality, he also believed that the masses of people should let themselves be guided by the great leaders of men.
Talking of poets he said,

More recently, in his 1943 book "The Hero in History", the pragmatist scholar Sidney Hook asserts:

Hook recognizes the relevance of the environment within which the "great man" or "hero" acted, but asserts that this can provide the backdrop but never the plot of the "dramas of human history". and distinguish life and species

There have been rankings of the significance of major historical figures. For example, Cesar A. Hidalgo and colleagues at the MIT Media Lab has calculated the memorability of historical figures using data such as the number of language editions for which there are articles for each person, the pageviews received, and other factors. These lists are available at MIT's Pantheon project.

It is sometimes hard to discern whether apparently historical figures from the earliest periods did in fact exist, due to the lack of records. Even with more recent personages, stories or anecdotes about the person often accumulate that have no basis in fact. Although the external aspects of a historical figure may be well documented, their inner nature can only be a subject of speculation. With religious figures, often the subjects of voluminous literature, separating "fact" from "belief" can be difficult if not impossible.

With older texts it can be difficult to be sure whether a person in the text is, in fact, a historical figure. "Wisdom literature" from early middle-eastern cultures (such as the Book of Job), mainly consist of verbal expositions or discussions that must be considered the work of the author, rather than the character supposedly speaking. It may still be possible to identify a figure in such texts with a historical figure known from some other context, and the text may be taken as informative about this figure, even if not verified by an independent source. On the other hand, a text may include realistic settings and references to historical people, while the central character may or may not be a historical figure.

Napoleon spoke of history as being a fable which had been agreed upon:– "la fable convenue qu'on appellera l'histoire". Great figures of the past have stories told about them which grow in the telling, and so become myths and legends which may dominate or displace the more prosaic historical facts about them. For example, some ancient chroniclers said that the Emperor Nero fiddled while Rome burned, but Tacitus disputed this by saying the stories were just malicious rumours. Similarly, there is no good evidence that Marie-Antoinette ever said "let them eat cake", or that Lady Godiva rode naked through the streets of Coventry.

Thomas Carlyle pointed out that even to the person living it, every life "remains in so many points unintelligible". The historian must struggle when writing biographies, "the very facts of which, to say nothing of the purport of them, we know not, and cannot know!"
Some psychologists have sought to understand the personalities of historical figures through clues about the way in which they were raised. However, this theoretical psychoanalytic approach is not supported empirically. An alternative approach, favored by psychobiographers such as William Runyan, is to explain the personality of the historical figure in terms of their life history. This approach has the advantage of recognizing that personality may evolve over time in response to events.

With historical religious figures, fact and belief may be difficult to disentangle.
There are cultural differences in the treatment of historical figures. Thus the Chinese can recognise that Mencius or Confucius were historical individuals, while also endowing them with sanctity. In Indian Hinduism, on the other hand, figures such as Krishna or Rama are almost always seen as embodiments of gods rather than as historical people. The Nirvana Sutra states: "Do not rely on the man but on the Dharma." A teacher such as Gautama Buddha is thus treated almost exclusively as a lesser god rather than a historical figure.

E. P. Sanders, author of "The Historical Figure of Jesus", called Jesus of Nazareth "one of the most important figures in human history". Various writers have struggled to present "historical" views of Jesus, as opposed to views distorted by belief. When writing about this subject, a historian who relies only on sources other than the New Testament may be criticized for implying that it is not a sufficient source of information about the subject.

The theologian Martin Kähler is known for his work "Der sogenannte historische Jesus und der geschichtliche, biblische Christus" (The so-called historical Jesus, and the historic, biblical Christ). He clearly distinguished between "the Jesus of history" and "the Christ of faith". Some historians openly admit bias, which may anyway be unavoidable. Paul Hollenback says he writes about the historical Jesus, "...in order to overthrow, not simply correct, the mistake called Christianity." Another historian who has written about Jesus, Frederick Gaiser, says, "historical investigation is part and parcel of biblical faith."

A historical figure may be interpreted to support political aims.
In France in the first half of the seventeenth century, there was an outpouring of writing about Joan of Arc, including seven biographies, three plays and an epic poem. Joan had become a symbol of national pride and the Catholic faith, helping unite a country that had been divided by the recent wars of religion. The reality of the historical Joan was subordinated to the need for a symbol of feminine strength, Christian virtue and resistance to the English.
George Bernard Shaw, introducing his 1923 play "Saint Joan", discussed representations of Joan by other authors.
He felt that William Shakespeare's depiction in "Henry VI, Part 1" was constrained from making her a "beautiful and romantic figure" by political considerations. Voltaire's version in his poem "La Pucelle d'Orléans" was also flawed by Voltaire's biases and Friedrich Schiller's play "Die Jungfrau von Orleans" "is not about Joan at all, and can hardly be said to pretend to be."

A historical figure may be used to validate a politician's claim to authority,
where the modern leader shapes and exploits the ideas associated with the historical figure,
which they are presumed to have inherited.
Thus Jesse Jackson has frequently evoked the spirit of Martin Luther King, Jr..
Fidel Castro often presented himself as following the path defined by José Martí.
Hugo Chávez of Venezuela has frequently identified himself with the historical figure Simón Bolívar, the liberator of South America from Spanish rule.

Hegel believed in the role of the state in guaranteeing individual liberties, and his views were therefore rejected by the German National Socialists, who considered him dangerously liberal and perhaps a proto-Marxist. On the other hand, Adolf Hitler identified himself as a Hegelian world historical figure, and justified his actions on this basis.

Plato used historical figures in his writing, but only to illustrate his points. Xenophon used Cyrus the Great in the same way.
When Plato apparently quotes Socrates in "The Republic", it is only to add dramatic effect to the presentation of his own thought. For this reason, Plato's writings on Socrates tell us little, at least directly, about Socrates. The historical figure is used only as a device for communicating Plato's ideas. In classical Rome, students of rhetoric had to master the "suasoria" — a form of declamation in which they wrote the soliloquy of a historical figure who was debating a critical course of action. For example, the poet Juvenal wrote a speech for the dictator Sulla, in which he was counselled to retire. The poet Ovid enjoyed this exercise more than the other final challenge — the "controversia".

The German philosopher Friedrich Nietzsche wrote an influential essay "On the Uses and Disadvantages of History for Life". He said "the unhistorical and historical are necessary in equal measure for the health of an individual, of a people and of a culture."
Nietzsche identifies three approaches to history, each with dangers.
The monumental approach describes the glories of the past, often focusing on heroic figures like Elizabeth I of England or Louis Pasteur. 
By treating these figures as models, the student is tempted to consider that there can be nobody of such stature today.
The antiquarian view examines the past in minute and reverent detail, turning its back on the present.
The critical approach challenges traditional views, even though they may be valid.

Historical figures may today be simulated as animated pedagogical agents to teach history and foreign culture. An example is Freudbot, which acted the part of Sigmund Freud for psychology students. When a variety of simulated character types were tried as educational agents, students rated historical figures as the most engaging.
There are gender differences in the perception of historical figures. When modern US schoolchildren were asked to roleplay or illustrate historical stereotypes, boys tended to focus upon male figures exclusively while girls showed more varied family groupings.

Using historical figures in marketing communicationsn and in branding is a new area of marketing research but historical figures’ names were used to promote products as early as in the Middle Ages. 

Historical figure brand is using famous historical person in branding, for instance Mozartkugel, Chopin (vodka) or Café Einstein https://www.cafeeinstein.com/. 

Historical figure is a person who lived in the past and whose deeds exerted a significant impact on other people’s lives and consciousness. These figures are attributed with certain features that are a compilation of the actual values they proclaimed and the manner they were perceived by others. This perception evolves and subsequent generations read the biography of a given historical figure in their own way through their own knowledge and experience. In order to determine the popularity of the commercialisation of historical figures, a study was conducted at the beginning of 2014 on the number of trademark protection applications filed with the Patent Office of the Republic of Poland as a measure of entrepreneurs’ interest in this activity. The names of 300 most prominent Polish historical figures were considered. The study showed that over 21% of the names analysed were recorded in the trademark register. 1,033 trademark protection applications were filed for 64 names out of the 300 historical figures investigated [Aldona Lipka, 2015,]. The greatest number of trademark protection applications were recorded for Mieszko (295), followed by Nicolaus Copernicus (250), John III Sobieski (94) and Chopin (81).

There is a huge body of historical fiction, where the text includes both imaginary and factual elements. In early English literature, Robin Hood was a fictional character, but the historical King Richard I of England also appears.
William Shakespeare wrote plays about people who were historical figures in his day, such as "Julius Caesar". He did not present these people as pure history, but dramatised their lives as a commentary about the people and politics of his own time.
Napoleon figured in Victor Hugo's 1862 classic "Les Misérables".
There are many more examples.

The compiler of a survey of historical novels in the 1920s claimed that the "appearance of reality ... is the great charm of the historical novel." He went on to assert, regarding novels about periods of which little is known, that "the danger is that the very elements which add to our interest in the tale as such will go far to mislead us in our conception of the period dealt with".
Traditionally the treatment of historical figures in fiction was realistic in style and respectful of fact. A historical novel would be true to the facts known about the period in which the novel is set, a biographical novel would follow the facts that are known about the protagonist's life, and a "roman à clef" would try to give an accurate interpretation of what is known about a public figure's private life. In each genre, the novelist would avoid introducing any elements that were clearly in conflict with the facts.

A writer may be handicapped by his readers' preconceptions about a historical person, which may or may not be accurate, and the facts about the historical person may also conflict with the novelist's plot requirements.
According to the Marxist philosopher György Lukács in his 1937 book on "The Historical Novel", "The 'world-historical individual' can only figure as a minor character in the [historical] novel because of the complexity and intricacy of the whole social-historical process."
As Jacobs observes, the "realist aesthetic" of the historical novel "assumes that a recognizable historical figure in fiction must not 'do things' its model did not do in real life; it follows that historical figures can be used only in very limited ways."
The author of a traditional historical novel should therefore focus more on the people who have been lost to history.
A novelist such as Sir Walter Scott or Leo Tolstoy ("War and Peace") would describe historical events accurately. They would give rein to their imagination only in scenes that were not significant historically, when interactions with fictional characters could safely be introduced.

More recently, however, starting with works such as "The Confessions of Nat Turner", and "Sophie's Choice" by William Styron,
the novelist has felt more free to introduce much larger amounts of purely imaginary detail about historical people.
E. L. Doctorow illustrates this different attitude when discussing his book "Ragtime": "Certain details were so delicious that I was scrupulous about getting them right. Others ... demanded to be mythologized." This reflects a changing attitude about the distinction between "fact" and "truth", expressed by Ursule Molinaro when he makes his Cassandra say, "I've come as close to the truth as facts would let me ... facts oppress the truth, which can breathe freely only in poetry & art."

Many films have depicted historical figures. Often the way in which the films interpret these figures and their times reflects the social and cultural values of the period in which the film was made.
Historical figures are familiar to the general reader and so may be used in speculative fiction so that readers marvel at their appearance in novel settings or with a fresh perspective.
For example, the time traveler The Doctor has encountered numerous historical figures such as Marco Polo and Queen Elizabeth I in his adventures.
They appeared most frequently when the television series first started, as it was directed at children and the use of historical figures in historical settings was intended to be educational.




</doc>
<doc id="53372308" url="https://en.wikipedia.org/wiki?curid=53372308" title="National memory">
National memory

National memory is a form of collective memory defined by shared experiences and culture. It is an integral part to national identity.

It represents one specific form of cultural memory, which makes an essential contribution to national group cohesion. Historically national communities have drawn upon commemorative ceremonies and monuments, myths and rituals, glorified individuals, objects, and events in their own history to produce a common narrative.

According to Lorraine Ryan, national memory is based on the public's reception of national historic narratives and the ability of people to affirm the legitimacy of these narratives.
National memory typically consists of a shared interpretation of a nation's past. Such interpretations can vary and sometimes compete. They can get challenged and augmented by a range of interest groups, fighting to have their histories acknowledged, documented and commemorated and reshape national stories. Often national memory is adjusted to offer a politicized vision of the past to make a political position appear consistent with national identity. Furthermore, it profoundly affects how historical facts are perceived and recorded and may circumvent or appropriate facts. A repertoire of discursive strategies functions to emotionalize national narrative and nationalize personal pasts.

National memory has been used calculatedly by governments for dynastic, political, religious and cultural purposes since as early as the sixteenth century.

Marketing of memory by the culture industry and its instrumentalisation for political purposes can both be seen as serious threats to the objective understanding of a nation's past.

Lorraine Ryan notes that individual memory both shapes and is shaped by national memory, and that there is a competition between the dominant and individual memories of a nation.

Hyung Park states that the nation is continuously revived, re-imagined, reconstituted, through shared memories among its citizens.

National memories may also conflict with the other nations' collective memory.

Reports that are narrated in terms of national memory characterize the past in ways that merge the past, the present and the future into "a single ongoing tale".

Pierre Nora argues that a "democratisation of history" allows for emancipatory versions of the past to surface:

However, national history being passed on by the culture industry, such as by historical films, can be seen as serious threats to the objective understanding of a nation's past.

Nations' memories can be shared across nations via media such as the Internet.

National memory can be a force of cohesion as well as division and conflict. It can foster constructive national reforms, international communities and agreements, dialogue as well as deepen problematic courses and rhetoric.

Identity crisis can occur due to bad memories (such as national crimes) or the absence of a belief in a shared past.

Often new developments, processes, problems and events are made sense of and contextualized by drawing from national memory.

Critical history or historic memory cuts from national memory's tradition centric to national heritage and orients itself towards a specialized study of history in a more sociological manner.

It has been proposed that the unthinkable ought not to be unmasked but that instead what made it thinkable should be reconstructed and that the difficulty of discussing the non-places or the bad places of national memory make it necessary to include forgetfulness and amnesia in the concept.

National memory may lead to questioning the nation as it is as well as its identity and imply a societal negotiation of what the country wishes to be as a nation. To understand the links between memory, forgetfulness, identity and the imaginary construction of the nation analysis of the discourse in the places of memory is fundamental as in all writings of national history an image of the nation is being restructured.



</doc>
<doc id="21014942" url="https://en.wikipedia.org/wiki?curid=21014942" title="Glossary of history">
Glossary of history

This glossary of history is a list of definitions of terms and concepts relevant to the study of history and its related fields and sub-disciplines, including both prehistory and the period of human history.




</doc>
<doc id="6830515" url="https://en.wikipedia.org/wiki?curid=6830515" title="Index of history articles">
Index of history articles

History is the study of the past. When used as the name of a field of study, "history" refers to the study and interpretation of the record of humans, families, and societies as preserved primarily through written sources. This is a list of history topics covered on English Wikipedia:





























</doc>
<doc id="1827281" url="https://en.wikipedia.org/wiki?curid=1827281" title="List of historical classifications">
List of historical classifications

Historical classification groups the various history topics into different categories according to subject matter as shown below.






See also Periodization.






Although there is arguably some intrinsic bias in history studies (with national bias perhaps being the most significant), history can also be studied from ideological perspectives, which practitioners feel are often ignored, such as:

A form of historical speculation known commonly as counterfactual history has also been adopted by some historians as a means of assessing and exploring the possible outcomes if certain events had not occurred or had occurred in a different way. This is somewhat similar to the alternate history genre in fiction.

Lists of false or dubious historical resources and historical myths that were once popular and widespread, or have become so, have also been prepared.


</doc>
<doc id="26334944" url="https://en.wikipedia.org/wiki?curid=26334944" title="Auxiliary sciences of history">
Auxiliary sciences of history

Auxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. "History" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.

Auxiliary sciences of history include, but are not limited to:



</doc>
<doc id="7370562" url="https://en.wikipedia.org/wiki?curid=7370562" title="Biography">
Biography

A biography, or simply bio, is a detailed description of a person's life. It involves more than just the basic facts like education, work, relationships, and death; it portrays a person's experience of these life events. Unlike a profile or curriculum vitae (résumé), a biography presents a subject's life story, highlighting various aspects of his or her life, including intimate details of experience, and may include an analysis of the subject's personality.

Biographical works are usually non-fiction, but fiction can also be used to portray a person's life. One in-depth form of biographical coverage is called legacy writing. Works in diverse media, from literature to film, form the genre known as biography.

An authorized biography is written with the permission, cooperation, and at times, participation of a subject or a subject's heirs. An autobiography is written by the person himself or herself, sometimes with the assistance of a collaborator or ghostwriter.

At first, biographical writings were regarded merely as a subsection of history with a focus on a particular individual of historical importance. The independent genre of biography as distinct from general history writing, began to emerge in the 18th century and reached its contemporary form at the turn of the 20th century.

One of the earliest biographers was Cornelius Nepos, who published his work "Excellentium Imperatorum Vitae" ("Lives of outstanding generals") in 44 BC. Longer and more extensive biographies were written in Greek by Plutarch, in his "Parallel Lives", published about 80 A.D. In this work famous Greeks are paired with famous Romans, for example the orators Demosthenes and Cicero, or the generals Alexander the Great and Julius Caesar; some fifty biographies from the work survive. Another well-known collection of ancient biographies is "De vita Caesarum" ("On the Lives of the Caesars") by Suetonius, written about AD 121 in the time of the emperor Hadrian.

In the early Middle Ages (AD 400 to 1450), there was a decline in awareness of the classical culture in Europe. During this time, the only repositories of knowledge and records of the early history in Europe were those of the Roman Catholic Church. Hermits, monks, and priests used this historic period to write biographies. Their subjects were usually restricted to the church fathers, martyrs, popes, and saints. Their works were meant to be inspirational to the people and vehicles for conversion to Christianity (see Hagiography). One significant secular example of a biography from this period is the life of Charlemagne by his courtier Einhard.

In Medieval Islamic Civilization (c. AD 750 to 1258), similar traditional Muslim biographies of Muhammad and other important figures in the early history of Islam began to be written, beginning the Prophetic biography tradition. Early biographical dictionaries were published as compendia of famous Islamic personalities from the 9th century onwards. They contained more social data for a large segment of the population than other works of that period. The earliest biographical dictionaries initially focused on the lives of the prophets of Islam and their companions, with one of these early examples being "The Book of The Major Classes" by Ibn Sa'd al-Baghdadi. And then began the documentation of the lives of many other historical figures (from rulers to scholars) who lived in the medieval Islamic world.
By the late Middle Ages, biographies became less church-oriented in Europe as biographies of kings, knights, and tyrants began to appear. The most famous of such biographies was "Le Morte d'Arthur" by Sir Thomas Malory. The book was an account of the life of the fabled King Arthur and his Knights of the Round Table. Following Malory, the new emphasis on humanism during the Renaissance promoted a focus on secular subjects, such as artists and poets, and encouraged writing in the vernacular.

Giorgio Vasari's "Lives of the Artists" (1550) was the landmark biography focusing on secular lives. Vasari made celebrities of his subjects, as the "Lives" became an early "bestseller". Two other developments are noteworthy: the development of the printing press in the 15th century and the gradual increase in literacy.

Biographies in the English language began appearing during the reign of Henry VIII. John Foxe's "Actes and Monuments" (1563), better known as "Foxe's Book of Martyrs", was essentially the first dictionary of the biography in Europe, followed by Thomas Fuller's "The History of the Worthies of England" (1662), with a distinct focus on public life.

Influential in shaping popular conceptions of pirates, "A General History of the Pyrates" (1724), by Charles Johnson, is the prime source for the biographies of many well-known pirates.

A notable early collection of biographies of eminent men and women in the United Kingdom was "Biographia Britannica" (1747-1766) edited by William Oldys. 

The American biography followed the English model, incorporating Thomas Carlyle's view that biography was a part of history. Carlyle asserted that the lives of great human beings were essential to understanding society and its institutions. While the historical impulse would remain a strong element in early American biography, American writers carved out a distinct approach. What emerged was a rather didactic form of biography, which sought to shape the individual character of a reader in the process of defining national character.

The first modern biography, and a work which exerted considerable influence on the evolution of the genre, was James Boswell's "The Life of Samuel Johnson", a biography of lexicographer and man-of-letters Samuel Johnson published in 1791. While Boswell's personal acquaintance with his subject only began in 1763, when Johnson was 54 years old, Boswell covered the entirety of Johnson's life by means of additional research. Itself an important stage in the development of the modern genre of biography, it has been claimed to be the greatest biography written in the English language. Boswell's work was unique in its level of research, which involved archival study, eye-witness accounts and interviews, its robust and attractive narrative, and its honest depiction of all aspects of Johnson's life and character - a formula which serves as the basis of biographical literature to this day.

Biographical writing generally stagnated during the 19th century - in many cases there was a reversal to the more familiar hagiographical method of eulogizing the dead, similar to the biographies of saints produced in Medieval times. A distinction between mass biography and literary biography began to form by the middle of the century, reflecting a breach between high culture and middle-class culture. However, the number of biographies in print experienced a rapid growth, thanks to an expanding reading public. This revolution in publishing made books available to a larger audience of readers. In addition, affordable paperback editions of popular biographies were published for the first time. Periodicals began publishing a sequence of biographical sketches.

Autobiographies became more popular, as with the rise of education and cheap printing, modern concepts of fame and celebrity began to develop. Autobiographies were written by authors, such as Charles Dickens (who incorporated autobiographical elements in his novels) and Anthony Trollope, (his "Autobiography" appeared posthumously, quickly becoming a bestseller in London), philosophers, such as John Stuart Mill, churchmen – John Henry Newman – and entertainers – P. T. Barnum.

The sciences of psychology and sociology were ascendant at the turn of the 20th century and would heavily influence the new century’s biographies. The demise of the "great man" theory of history was indicative of the emerging mindset. Human behavior would be explained through Darwinian theories. "Sociological" biographies conceived of their subjects' actions as the result of the environment, and tended to downplay individuality. The development of psychoanalysis led to a more penetrating and comprehensive understanding of the biographical subject, and induced biographers to give more emphasis to childhood and adolescence. Clearly these psychological ideas were changing the way biographies were written, as a culture of autobiography developed, in which the telling of one's own story became a form of therapy. The conventional concept of heroes and narratives of success disappeared in the obsession with psychological explorations of personality.
British critic Lytton Strachey revolutionized the art of biographical writing with his 1918 work "Eminent Victorians", consisting of biographies of four leading figures from the Victorian era: Cardinal Manning, Florence Nightingale, Thomas Arnold, and General Gordon. Strachey set out to breathe life into the Victorian era for future generations to read. Up until this point, as Strachey remarked in the preface, Victorian biographies had been "as familiar as the "cortège" of the undertaker", and wore the same air of "slow, funereal barbarism." Strachey defied the tradition of "two fat volumes...of undigested masses of material" and took aim at the four iconic figures. His narrative demolished the myths that had built up around these cherished national heroes, whom he regarded as no better than a "set of mouth bungled hypocrites". The book achieved worldwide fame due to its irreverent and witty style, its concise and factually accurate nature, and its artistic prose.

In the 1920s and '30s, biographical writers sought to capitalize on Strachey's popularity by imitating his style. This new school featured iconoclasts, scientific analysts, and fictional biographers and included Gamaliel Bradford, André Maurois, and Emil Ludwig, among others. Robert Graves ("I, Claudius", 1934) stood out among those following Strachey's model of "debunking biographies." The trend in literary biography was accompanied in popular biography by a sort of "celebrity voyeurism", in the early decades of the century. This latter form's appeal to readers was based on curiosity more than morality or patriotism. By World War I, cheap hard-cover reprints had become popular. The decades of the 1920s witnessed a biographical "boom."

The feminist scholar Carolyn Heilbrun observed that women's biographies and autobiographies began to change character during the second wave of feminist activism. She cited Nancy Milford's 1970 biography "Zelda", as the "beginning of a new period of women's biography, because "[only] in 1970 were we ready to read not that Zelda had destroyed Fitzgerald, but Fitzgerald her: he had usurped her narrative." Heilbrun named 1973 as the turning point in women's autobiography, with the publication of May Sarton's "Journal of a Solitude," for that was the first instance where a woman told her life story, not as finding "beauty even in pain" and transforming "rage into spiritual acceptance," but acknowledging what had previously been forbidden to women: their pain, their rage, and their "open admission of the desire for power and control over one's life."

In recent years, multimedia biography has become more popular than traditional literary forms. Along with documentary biographical films, Hollywood produced numerous commercial films based on the lives of famous people. The popularity of these forms of biography have led to the proliferation of TV channels dedicated to biography, including A&E, The Biography Channel, and The History Channel.

CD-ROM and online biographies have also appeared. Unlike books and films, they often do not tell a chronological narrative: instead they are archives of many discrete media elements related to an individual person, including video clips, photographs, and text articles. Biography-Portraits were created in 2001, by the German artist Ralph Ueltzhoeffer. Media scholar Lev Manovich says that such archives exemplify the database form, allowing users to navigate the materials in many ways. General "life writing" techniques are a subject of scholarly study.

In recent years, debates have arisen as to whether all biographies are fiction, especially when authors are writing about figures from the past. President of Wolfson College at Oxford University, Hermione Lee argues that all history is seen through a perspective that is the product of our contemporary society and as a result biographical truths are constantly shifting. So the history biographers write about will not be the way that it happened; it will be the way they remembered it. Debates have also arisen concerning the importance of space in life-writing.

Daniel R. Meister in 2017 argues that:

Biographical research is defined by Miller as a research method that collects and analyses a person's whole life, or portion of a life, through the in-depth and unstructured interview, or sometimes reinforced by semi-structured interview or personal documents. It is a way of viewing social life in procedural terms, rather than static terms. The information can come from "oral history, personal narrative, biography and autobiography” or "diaries, letters, memoranda and other materials". The central aim of biographical research is to produce rich descriptions of persons or "conceptualise structural types of actions", which means to "understand the action logics or how persons and structures are interlinked". This method can be used to understand an individual’s life within its social context or understand the cultural phenomena.

There are many largely unacknowledged pitfalls to writing good biographies, and these largely concern the relation between firstly the individual and the context, and, secondly, the private and public. Paul James writes:
Several countries offer an annual prize for writing a biography such as the:





</doc>
<doc id="60209193" url="https://en.wikipedia.org/wiki?curid=60209193" title="Historical significance">
Historical significance

Historical significance is a secondary history concept and an important topic which defines and influences many things in human life. Historical significance is a form of importance decided by judgement against sets of criteria.

Knowledge of dates and events is the "primary content" of history and can be considered "first-order knowledge" as a concept. Historical significance is an aspect of the "study" of primary content, therefore historical significance is a secondary or "second-order knowledge" concept. Historical significance is a largely overlooked aspect of history studies but it is a tool for guiding students to understand how cultural background affects perception of history and the preferences they have.

Historical significance is a central topic to history study and defines what is remembered about the past. Historical significance can be defined as importance. When history is taught to students, particular examples are emphasised because they are considered more significant than others. Historical significance is subjective and open to challenge.

Historical significance determines a present view of the past and many prominent aspects of contemporary culture (images on stamps and banknotes, street names, history books, etc). 

Assessing historical significance is a form of judgement. Valid judgement is a comparison to reasoned criteria. Judgement of historical significance could be summed up in two questions: 


Criteria is not always available to assess a judgement of historical significance. However, notable examples include UNESCO criteria, for inclusion of world heritage sites, a site must "bear a unique or at least exceptional testimony to a cultural tradition or to a civilization"

Criteria do not necessarily prevent disagreement. Values change between one person and another. Ultimately the key is perception. Lucien Febvre wrote, "At the heart of history, there are sentiments" The true understanding of significance is in understanding your thoughts and feelings about a subject.

Determining historical significance is a form of assessment. Dependable assessment relies on comparison to criteria. Criteria are designed to achieve an objective view over knowledge, by avoiding cultural and defensive interpretations, and forcing the reader to analyze and think carefully. Clear objective criteria provide an eventual ability to self-assess the direction of historical interpretation. 

A "Five Rs" criteria for assessing historical significance is proposed by Christine Counsell of the University of Cambridge Faculty of Education in page 114 of her book "Teaching History"

Historian David Fulton published the following example in "The Guided Teacher to Teaching and Learning History":


Geoffrey Partington published the following criteria in "The idea of an historical education":


Lis Cercadillo published the following criteria in "“Maybe They Haven't Decided Yet What Is Right”: English and Spanish Perspectives on Teaching Historical Significance":








</doc>
<doc id="3402732" url="https://en.wikipedia.org/wiki?curid=3402732" title="Outline of history">
Outline of history

The following outline is provided as an overview of and topical guide to history:

History – discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented (the beginning of recorded history).
History can be described as all of the following:



Auxiliary sciences of history – scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Auxiliary sciences of history include, but are not limited to:


History by period





Regional history


Era

















</doc>
<doc id="60593966" url="https://en.wikipedia.org/wiki?curid=60593966" title="List of National Junior Classical League conventions">
List of National Junior Classical League conventions

This is a list of National Junior Classical League conventions.


</doc>
<doc id="2118843" url="https://en.wikipedia.org/wiki?curid=2118843" title="Progress">
Progress

Progress is the movement towards a refined, improved, or otherwise desired state or, in the context of progressivism, the idea that advancements in technology, science, and social organization can result in an improved human condition; the latter may happen as a result of direct human action, as in social enterprise or through activism, or as a natural part of sociocultural evolution.

The concept of progress was introduced in the early 19th-century social theories, especially social evolution as described by Auguste Comte and Herbert Spencer. It was present in the Enlightenment's philosophies of history. As a goal, social progress has been advocated by varying realms of political ideologies with different theories on how it is to be achieved.

Specific indicators for measuring progress can range from economic data, technical innovations, change in the political or legal system, and questions bearing on individual life chances, such as life expectancy and risk of disease and disability.

GDP growth has become a key orientation for politics and is often taken as a key figure to evaluate a politician's performance. However, GDP has a number of flaws that make it a bad measure of progress, especially for developed countries. For example, environmental damage is not taken into account nor is the sustainability of economic activity. Wikiprogress has been set up to share information on evaluating societal progress. It aims to facilitate the exchange of ideas, initiatives and knowledge. HumanProgress.org is another online resource that seeks to compile data on different measures of societal progress.

The Social Progress Index is a tool developed by the International Organization Imperative Social Progress, which measures the extent to which countries cover social and environmental needs of its citizenry. There are fifty-two indicators in three areas or dimensions: Basic Human Needs, and Foundations of Wellbeing and Opportunities which show the relative performance of nations.

Indices that can be used to measure progress include:

Scientific progress is the idea that the scientific community learns more over time, which causes a body of scientific knowledge to accumulate. The chemists in the 19th century knew less about chemistry than the chemists in the 20th century, and they in turn knew less than the chemists in the 21st century. Looking forward, today's chemists reasonably expect that chemists in future centuries will know more than they do. 

This process differs from non-science fields, such as human languages or history: the people who spoke a now-extinct language, or who lived through a historical time period, can be said to have known different things from the scholars who studied it later, but they cannot be said to know less about their lives than the modern scholars. Some valid knowledge is lost through the passage of time, and other knowledge is gained, with the result that the non-science fields do not make scientific progress towards understanding their subject areas.

From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented as a progressive accumulation of knowledge, in which true theories replaced false beliefs. Some more recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in terms of competing paradigms or conceptual systems in a wider matrix of intellectual, cultural, economic and political trends. These interpretations, however, have met with opposition for they also portray the history of science as an incoherent system of incommensurable paradigms, not leading to any scientific progress, but only to the illusion of progress.

Aspects of social progress, as described by Condorcet, have included the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty.

How progress improved the degraded status of women in traditional society was a major theme of historians starting in the Enlightenment and continuing to today. British theorists William Robertson (1721–1793) and Edmund Burke (1729–1797), along with many of their contemporaries, remained committed to Christian- and republican-based conceptions of virtue, while working within a new Enlightenment paradigm. The political agenda related beauty, taste, and morality to the imperatives and needs of modern societies of a high level of sophistication and differentiation. Two themes in the work of Robertson and Burke—the nature of women in 'savage' and 'civilized' societies and 'beauty in distress'—reveals how long-held convictions about the character of women, especially with regard to their capacity and right to appear in the public domain, were modified and adjusted to the idea of progress and became central to an enlightened affirmation of modern European civilization.

Classics experts have examined the status of women in the ancient world, concluding that in the Roman Empire, with its superior social organization, internal peace, and rule of law, allowed women to enjoy a somewhat better standing than in ancient Greece, where women were distinctly inferior. The inferior status of women in traditional China has raised the issue of whether the idea of progress requires a thoroughgoing reject of traditionalism—a belief held by many Chinese reformers in the early 20th century.

Historians Leo Marx and Bruce Mazlish asking, "Should we in fact abandon the idea of progress as a view of the past," answer that there is no doubt "that the status of women has improved markedly" in cultures that have adopted the Enlightenment idea of progress.

Modernization was promoted by classical liberals in the 19th and 20th centuries, who called for the rapid modernization of the economy and society to remove the traditional hindrances to free markets and free movements of people. During the Enlightenment in Europe social commentators and philosophers began to realize that people "themselves" could change society and change their way of life. Instead of being made completely by gods, there was increasing room for the idea that people themselves "made their own society"—and not only that, as Giambattista Vico argued, "because" people made their own society, they could also fully comprehend it. This gave rise to new sciences, or proto-sciences, which claimed to provide new scientific knowledge about what society was like, and how one may change it for the better.

In turn, this gave rise to progressive opinion, in contrast with conservational opinion. The social conservationists were skeptical about panaceas for social ills. According to conservatives, attempts to radically remake society normally make things worse. Edmund Burke was the leading exponent of this, although later-day liberals like Hayek have espoused similar views. They argue that society changes organically and naturally, and that grand plans for the remaking of society, like the French Revolution, National Socialism and Communism hurt society by removing the traditional constraints on the exercise of power.

The scientific advances of the 16th and 17th centuries provided a basis for Francis Bacon's book the New Atlantis. In the 17th century, Bernard le Bovier de Fontenelle described progress with respect to arts and the sciences, saying that each age has the advantage of not having to rediscover what was accomplished in preceding ages. The epistemology of John Locke provided further support and was popularized by the Encyclopedists Diderot, Holbach, and Condorcet. Locke had a powerful influence on the American Founding Fathers. The first complete statement of progress is that of Turgot, in his "A Philosophical Review of the Successive Advances of the Human Mind" (1750). For Turgot, progress covers not only the arts and sciences but, on their base, the whole of culture—manner, mores, institutions, legal codes, economy, and society. Condorcet predicted the disappearance of slavery, the rise of literacy, the lessening of inequalities between the sexes, reforms of harsh prisons and the decline of poverty.

John Stuart Mill's (1806–1873) ethical and political thought demonstrated faith in the power of ideas and of intellectual education for improving human nature or behavior. For those who do not share this faith the idea of progress becomes questionable.

Alfred Marshall (1842–1924), a British economist of the early 20th century, was a proponent of classical liberalism. In his highly influential "Principles of Economics" (1890), he was deeply interested in human progress and in what is now called "sustainable development." For Marshall, the importance of wealth lay in its ability to promote the physical, mental, and moral health of the general population. After World War II, the modernization and development programs undertaken in the Third World were typically based on the idea of progress.

In Russia the notion of progress was first imported from the West by Peter the Great (1672–1725). An absolute ruler, he used the concept to modernize Russia and to legitimize his monarchy (unlike its usage in Western Europe, where it was primarily associated with political opposition). By the early 19th century, the notion of progress was being taken up by Russian intellectuals and was no longer accepted as legitimate by the tsars. Four schools of thought on progress emerged in 19th-century Russia: conservative (reactionary), religious, liberal, and socialist—the latter winning out in the form of Bolshevist materialism.

The intellectual leaders of the American Revolution, such as Benjamin Franklin, Thomas Paine, Thomas Jefferson and John Adams, were immersed in Enlightenment thought and believed the idea of progress meant that they could reorganize the political system to the benefit of the human condition; both for Americans and also, as Jefferson put it, for an "Empire of Liberty" that would benefit all mankind. In particular, Adams wrote “I must study politics and war, that our sons may have liberty to study mathematics and philosophy. Our sons ought to study mathematics and philosophy, geography, natural history and naval architecture, navigation, commerce and agriculture in order to give their children a right to study painting, poetry, music, architecture, statuary, tapestry and porcelain.”

Juan Bautista Alberdi (1810–1884) was one of the most influential political theorists in Argentina. Economic liberalism was the key to his idea of progress. He promoted faith in progress, while chiding fellow Latin Americans for blind copying of American and European models. He hoped for progress through promotion of immigration, education, and a moderate type of federalism and republicanism that might serve as a transition in Argentina to true democracy.

In Mexico, José María Luis Mora (1794–1850) was a leader of classical liberalism in the first generation after independence, leading the battle against the conservative trinity of the army, the church, and the "hacendados". He envisioned progress as both a process of human development by the search for philosophical truth and as the introduction of an era of material prosperity by technological advancement. His plan for Mexican reform demanded a republican government bolstered by widespread popular education free of clerical control, confiscation and sale of ecclesiastical lands as a means of redistributing income and clearing government debts, and effective control of a reduced military force by the government. Mora also demanded the establishment of legal equality between native Mexicans and foreign residents. His program, untried in his lifetime, became the key element in the Mexican Constitution of 1857.

In Italy, the idea that progress in science and technology would lead to solutions for human ills was connected to the nationalism that united the country in 1860. The Piedmontese Prime Minister Camillo Cavour envisaged the railways as a major factor in the modernization and unification of the Italian peninsula. The new Kingdom of Italy, formed in 1861, worked to speed up the processes of modernization and industrialization that had begun in the north, but were slow to arrive in the Papal States and central Italy, and were nowhere in sight in the "Mezzogiorno" (that is, Southern Italy, Sicily, and Sardinia). The government sought to combat the backwardness of the poorer regions in the south and work towards augmenting the size and quality of the newly created Italian army so that it could compete on an equal footing with the powerful nations of Europe. In the same period, the government was legislating in favour of public education to fight the great problem of illiteracy, upgrade the teaching classes, improve existing schools, and procure the funds needed for social hygiene and care of the body as factors in the physical and moral regeneration of the race.

In China, in the 20th century the Kuomintang or Nationalist party, which ruled from the 1920s to the 1940s, advocated progress. The Communists under Mao Zedong adopted western models and their ruinous projects caused mass famines. After Mao's death, however, the new regime led by Deng Xiaoping (1904–1997) and his successors aggressively promoted modernization of the economy using capitalist models and imported western technology. This was termed the "Opening of China" in the west, and more broadly encompasses Chinese economic reform.

Among environmentalists, there is a continuum between two opposing poles. The one pole is optimistic, progressive, and business-oriented, and endorses the classic idea of progress. For example, bright green environmentalism endorses the idea that new designs, social innovations and green technologies can solve critical environmental challenges. The other is pessimistic in respect of technological solutions, warning of impending global crisis (through climate change or peak oil, for example) and tends to reject the very idea of modernity and the myth of progress that is so central to modernization thinking. Similarly, Kirkpatrick Sale, wrote about progress as a myth benefiting the few, and a pending environmental doomsday for everyone. An example is the philosophy of Deep Ecology.

Sociologist Robert Nisbet said that "No single idea has been more important than ... the Idea of Progress in Western civilization for three thousand years", and defines five "crucial premises" of the idea of progress:

Sociologist P. A. Sorokin said, "The ancient Chinese, Babylonian, Hindu, Greek, Roman, and most of the medieval thinkers supporting theories of rhythmical, cyclical or trendless movements of social processes were much nearer to reality than the present proponents of the linear view". Unlike Confucianism and to a certain extent Taoism, that both search for an ideal past, the Judeo-Christian-Islamic tradition believes in the fulfillment of history, which was translated into the idea of progress in the modern age. Therefore, Chinese proponents of modernization have looked to western models. According to Thompson, the late Qing dynasty reformer, Kang Youwei, believed he had found a model for reform and "modernisation" in the Ancient Chinese Classics.

Philosopher Karl Popper said that progress was not fully adequate as a scientific explanation of social phenomena.
More recently, Kirkpatrick Sale, a self-proclaimed neo-luddite author, wrote exclusively about progress as a myth, in an essay entitled "Five Facets of a Myth".

Iggers (1965) says that proponents of progress underestimated the extent of man's destructiveness and irrationality, while critics misunderstand the role of rationality and morality in human behavior.

In 1946, psychoanalyst Charles Baudouin claimed modernity has retained the "corollary" of the progress myth, the idea that the present is superior to the past, while at the same time insisting that it is free of the myth:
A cyclical theory of history was adopted by Oswald Spengler (1880–1936), a German historian who wrote "The Decline of the West" in 1920. World War I, World War II, and the rise of totalitarianism demonstrated that progress was not automatic and that technological improvement did not necessarily guarantee democracy and moral advancement. British historian Arnold J. Toynbee (1889–1975) felt that Christianity would help modern civilization overcome its challenges.

The Jeffersonians said that history is not exhausted but that man may begin again in a new world. Besides rejecting the lessons of the past, they Americanized the idea of progress by democratizing and vulgarizing it to include the welfare of the common man as a form of republicanism. As Romantics deeply concerned with the past, collecting source materials and founding historical societies, the Founding Fathers were animated by clear principles. They saw man in control of his destiny, saw virtue as a distinguishing characteristic of a republic, and were concerned with happiness, progress, and prosperity. Thomas Paine, combining the spirit of rationalism and romanticism, pictured a time when America's innocence would sound like a romance, and concluded that the fall of America could mark the end of 'the noblest work of human wisdom.'

Historian J. B. Bury wrote in 1920:

In the postmodernist thought steadily gaining ground from the 1980s, the grandiose claims of the modernizers are steadily eroded, and the very concept of social progress is again questioned and scrutinized. In the new vision, radical modernizers like Joseph Stalin and Mao Zedong appear as totalitarian despots, whose vision of social progress is held to be totally deformed. Postmodernists question the validity of 19th-century and 20th-century notions of progress—both on the capitalist and the Marxist side of the spectrum. They argue that both capitalism and Marxism over-emphasize technological achievements and material prosperity while ignoring the value of inner happiness and peace of mind. Postmodernism posits that both dystopia and utopia are one and the same, overarching grand narratives with impossible conclusions.
Some 20th-century authors refer to the "Myth of Progress" to refer to the idea that the human condition will inevitably improve. In 1932, English physician Montague David Eder wrote: "The myth of progress states that civilization has moved, is moving, and will move in a desirable direction. Progress is inevitable... Philosophers, men of science and politicians have accepted the idea of the inevitability of progress." Eder argues that the advancement of civilization is leading to greater unhappiness and loss of control in the environment. The strongest critics of the idea of progress complain that it remains a dominant idea in the 21st century, and shows no sign of diminished influence. As one fierce critic, British historian John Gray (b. 1948), concludes:
Recently the idea of progress has been generalized to psychology, being related with the concept of a goal, that is, progress is understood as "what counts as a means of advancing towards the end result of a given defined goal."

Historian J. B. Bury said that thought in ancient Greece was dominated by the theory of world-cycles or the doctrine of eternal return, and was steeped in a belief parallel to the Judaic "fall of man," but rather from a preceding "Golden Age" of innocence and simplicity. Time was generally regarded as the enemy of humanity which depreciates the value of the world. He credits the Epicureans with having had a potential for leading to the foundation of a theory of progress through their materialistic acceptance of the atomism of Democritus as the explanation for a world without an intervening deity.
Robert Nisbet and Gertrude Himmelfarb have attributed a notion of progress to other Greeks. Xenophanes said "The gods did not reveal to men all things in the beginning, but men through their own search find in the course of time that which is better." Plato's Book III of "The Laws" depicts humanity's progress from a state of nature to the higher levels of culture, economy, and polity. Plato's "The Statesman" also outlines a historical account of the progress of mankind.

During the Medieval period, science was to a large extent based on Scholastic (a method of thinking and learning from the Middle Ages) interpretations of Aristotle's work. The Renaissance of the 15th, 16th and 17th Centuries changed the mindset in Europe towards an empirical view, based on a pantheistic interpretation of Plato. This induced a revolution in curiosity about nature in general and scientific advance, which opened the gates for technical and economic advance. Furthermore, the individual potential was seen as a never-ending quest for being God-like, paving the way for a view of Man based on unlimited perfection and progress.

In the Enlightenment, French historian and philosopher Voltaire (1694–1778) was a major proponent. At first Voltaire's thought was informed by the idea of progress coupled with rationalism. His subsequent notion of the historical idea of progress saw science and reason as the driving forces behind societal advancement.

Immanuel Kant (1724–1804) argued that progress is neither automatic nor continuous and does not measure knowledge or wealth, but is a painful and largely inadvertent passage from barbarism through civilization toward enlightened culture and the abolition of war. Kant called for education, with the education of humankind seen as a slow process whereby world history propels mankind toward peace through war, international commerce, and enlightened self-interest.

Scottish theorist Adam Ferguson (1723–1816) defined human progress as the working out of a divine plan, though he rejected predestination. The difficulties and dangers of life provided the necessary stimuli for human development, while the uniquely human ability to evaluate led to ambition and the conscious striving for excellence. But he never adequately analyzed the competitive and aggressive consequences stemming from his emphasis on ambition even though he envisioned man's lot as a perpetual striving with no earthly culmination. Man found his happiness only in effort.

Some scholars consider the idea of progress that was affirmed with the Enlightenment, as a secularization of ideas from early Christianity, and a reworking of ideas from ancient Greece.

In the 19th century, Romantic critics charged that progress did not automatically better the human condition, and in some ways could make it worse. Thomas Malthus (1766–1834) reacted against the concept of progress as set forth by William Godwin and Condorcet because he believed that inequality of conditions is "the best (state) calculated to develop the energies and faculties of man". He said, "Had population and food increased in the same ratio, it is probable that man might never have emerged from the savage state". He argued that man's capacity for improvement has been demonstrated by the growth of his intellect, a form of progress which offsets the distresses engendered by the law of population.

German philosopher Friedrich Nietzsche (1844–1900) criticized the idea of progress as the 'weakling's doctrines of optimism,' and advocated undermining concepts such as faith in progress, to allow the strong individual to stand above the plebeian masses. An important part of his thinking consists of the attempt to use the classical model of 'eternal recurrence of the same' to dislodge the idea of progress.

Iggers (1965) argues there was general agreement in the late 19th century that the steady accumulation of knowledge and the progressive replacement of conjectural, that is, theological or metaphysical, notions by scientific ones was what created progress. Most scholars concluded this growth of scientific knowledge and methods led to the growth of industry and the transformation of warlike societies into an industrial and pacific one. They agreed as well that there had been a systematic decline of coercion in government, and an increasing role of liberty and of rule by consent. There was more emphasis on impersonal social and historical forces; progress was increasingly seen as the result of an inner logic of society.

Marx developed a theory of historical materialism. He describes the mid-19th-century condition in "The Communist Manifesto" as follows:

Furthermore, Marx described the process of social progress, which in his opinion is based on the interaction between the productive forces and the relations of production:

Capitalism is thought by Marx as a process of continual change, in which the growth of markets dissolve all fixities in human life, and Marx admits that capitalism is progressive and non-reactionary. Marxism further states that capitalism, in its quest for higher profits and new markets, will inevitably sow the seeds of its own destruction. Marxists believe that, in the future, capitalism will be replaced by socialism and eventually communism.

Many advocates of capitalism such as Schumpeter agreed with Marx's analysis of capitalism as a process of continual change through creative destruction, but, unlike Marx, believed and hoped that capitalism could essentially go on forever.

Thus, by the beginning of the 20th century, two opposing schools of thought—Marxism and liberalism—believed in the possibility and the desirability of continual change and improvement. Marxists strongly opposed capitalism and the liberals strongly supported it, but the one concept they could both agree on was modernism, a trend of thought which affirms the power of human beings to make, improve and reshape their society, with the aid of scientific knowledge, technology and practical experimentation.



</doc>
<doc id="60411901" url="https://en.wikipedia.org/wiki?curid=60411901" title="Sources for the Quran">
Sources for the Quran

Scholars have been able to point out several pre-existing sources for the Quran. Some scholars have calculated that one third of the Quran has pre-Islamic Christian origins. The most famous pre-Islamic source for the Quran is the Bible, which predates the Quran by several centuries. Quran contains references to more than 50 people in the Bible, and while the stories told in each book are generally comparable, important differences sometimes emerge. Other sources that have been identified, are various Apocryphal writings, like the various infancy gospels, Protoevangelium of James, Gospel of Pseudo-Matthew, Syriac Infancy Gospel, as well as several circulating Jewish myths, which were built upon the Hebrew Bible. This reliance on so many pre-existing sources is sometimes used to argue against the Muslim doctrine of the divine inspiration of the Quran. Critics like Norman Geisler view the reliance on these pre-Islamic sources as one evidence that Quran is of purely human origins. Comparisons between Mormonism and Islam have also been made.

Quran contains references to more than 50 people in the Bible, which predates it by several centuries. Stories related in the Quran usually focus more on the spiritual significance of events than details. The stories are generally comparable, but there are differences. One of the most famous differences, is the Islamic view of Jesus' crucifixion. Quran maintains that Jesus was not actually crucified and did not die on the cross. The general Islamic view supporting the denial of crucifixion was probably influenced by Manichaenism (Docetism), which holds that someone else was crucified instead of Jesus, while concluding that Jesus will return during the end-times.
Despite these views, scholars have maintained, that the Crucifixion of Jesus is a fact of history and not disputed.

When looking at the narratives of Jesus found in the Quran, some themes are found in pre-Islamic sources such as the Infancy Gospels about Christ. Much of the qur'anic material about the selection and upbringing of Mary parallels much of the "Protovangelium of James", with the miracle of the palm tree and the stream of water being found in the "Gospel of Pseudo-Matthew". In Pseudo-Matthew, the flight to Egypt is narrated similarly to how it is found in Islamic lore, with Syriac translations of the Protoevangelium of James and The Infancy Story of Thomas being found in pre-Islamic sources.
John Wansbrough believes that the Quran is a redaction in part of other sacred scriptures, in particular the Judaeo-Christian scriptures. 
There are also Quranic parallels with the Syriac Infancy Gospel, particularly about Jesus talking as a baby in the cradle.
The Syriac infancy gospel contains this story:
and the Quranic parallel is found in Surah 19:29-34
Several narratives rely on Jewish Midrash Tanhuma legends, like the narrative of Cain learning to bury the body of Abel in Surah 5:31. Surah 5:32, when discussing the legal and moral applications to the story of Cain and Abel, relies heavily on the Jewish Mishnah tradition, and quotes from Sanhedrin 4:5:

Quran also employs popular legends about Alexander the Great called Dhul-Qarnayn ("he of the two horns") in the Quran.
The story of Dhul-Qarnayn has its origins in legends of Alexander the Great current in the Middle East in the early years of the Christian era. According to these the Scythians, the descendants of Gog and Magog, once defeated one of Alexander's generals, upon which Alexander built a wall in the Caucasus mountains to keep them out of civilised lands (the basic elements are found in Flavius Josephus). The legend went through much further elaboration in subsequent centuries before eventually finding its way into the Quran through a Syrian version.

The reasons behind the name "Two-Horned" are somewhat obscure: the scholar al-Tabari (839-923 CE) held it was because he went from one extremity ("horn") of the world to the other, but it may ultimately derive from the image of Alexander wearing the horns of the ram-god Zeus-Ammon, as popularised on coins throughout the Hellenistic Near East. The wall Dhul-Qarnayn builds on his northern journey may have reflected a distant knowledge of the Great Wall of China (the 12th century scholar al-Idrisi drew a map for Roger of Sicily showing the "Land of Gog and Magog" in Mongolia), or of various Sassanid Persian walls built in the Caspian area against the northern barbarians, or a conflation of the two.

Dhul-Qarneyn also journeys to the western and eastern extremities ("qarns", tips) of the Earth. In the west he finds the sun setting in a "muddy spring", equivalent to the "poisonous sea" which Alexander found in the Syriac legend. In the Syriac original Alexander tested the sea by sending condemned prisoners into it, but the Quran changes this into a general administration of justice. In the east both the Syrian legend and the Quran have Alexander/Dhul-Qarneyn find a people who live so close to the rising sun that they have no protection from its heat.

"Qarn" also means "period" or "century", and the name Dhul-Qarnayn therefore has a symbolic meaning as "He of the Two Ages", the first being the mythological time when the wall is built and the second the age of the end of the world when Allah's shariah, the divine law, is removed and Gog and Magog are to be set loose. Modern Islamic apocalyptic writers, holding to a literal reading, put forward various explanations for the absence of the wall from the modern world, some saying that Gog and Magog were the Mongols and that the wall is now gone, others that both the wall and Gog and Magog are present but invisible.



</doc>
<doc id="559305" url="https://en.wikipedia.org/wiki?curid=559305" title="Ritualization">
Ritualization

Ritualization is a behavior that occurs typically in a member of a given species in a highly stereotyped fashion and independent of any direct physiological significance. It is found, in differing forms, both in non-human animals and in humans.

Konrad Lorenz, working with greylag geese and other animals such as water shrews, showed that ritualization was an important process in their development. He showed that the geese obsessively displayed a reflexive motor pattern of egg retrieval when stimulated by the sight of an egg outside their nest. Similarly, in the shrews, Lorenz showed that once they had become used to jumping over a stone in their path, they went on jumping at that place after the stone was taken away. This sort of behaviour is analogous to obsessive-compulsive disorder in humans. 

Oskar Heinroth in 1910 and Lorenz from 1935 onwards studied the triumph ceremony in geese; Lorenz described it as becoming a fixed ritual. It involves a rolling behaviour (of the head and neck) and cackling with the head stretched forward, and occurs only among geese that know each other, meaning within a family or between mates. The triumph ceremony appears in varied situations, such as when mates meet after having been separated, when disturbed, or after an attack. The behaviour is now known also in other species, such as Canada goose.

Ritualization is associated with the work of Catherine Bell. Bell, drawing on the Practice Theory of Pierre Bourdieu, has taken a less functional view of ritual with her elaboration of ritualization. 

More recently scholars interested in the cognitive science of religion such as Pascal Boyer, Pierre Liénard, and William W. McCorkle, Jr. have been involved in experimental, ethnographic, and archival research on how ritualized actions might inform the study of ritualization and ritual forms of action. Boyer, Liénard, and McCorkle argue that ritualized compulsions are in relation to an evolved cognitive architecture where social, cultural, and environmental selection pressures stimulate "hazard-precaution" systems such as predation, contagion, and disgust in human minds. McCorkle argued that these ritualized compulsions (especially in regard to dead bodies vis-à-vis, mortuary behavior) were turned into ritual scripts by professional guilds only several thousand years ago with advancement in technology such as the domestication of plants and animals, literacy, and writing.


</doc>
<doc id="2105018" url="https://en.wikipedia.org/wiki?curid=2105018" title="Disinhibition">
Disinhibition

In psychology, disinhibition is a lack of restraint manifested in disregard of social conventions, impulsivity, and poor risk assessment. Disinhibition affects motor, instinctual, emotional, cognitive, and perceptual aspects with signs and symptoms similar to the diagnostic criteria for mania. Hypersexuality, hyperphagia, and aggressive outbursts are indicative of disinhibited instinctual drives.

According to Grafman et al. "disinhibition" is a lack of restraint manifested in several ways, affecting motor, instinctual, emotional, cognitive, and perceptual aspects with signs and symptoms e.g. impulsivity, disregard for others and social norms, aggressive outbursts, misconduct and oppositional behaviors, disinhibited instinctual drives including risk taking behaviors and hypersexuality. Disinhibition is a common symptom following brain injury, or lesions, particularly to the frontal lobe and primarily to the orbitofrontal cortex. The neuropsychiatric sequelae following brain injuries could include diffuse cognitive impairment, with more prominent deficits in the rate of information processing, attention, memory, cognitive flexibility, and problem solving. Prominent impulsivity, affective instability, and disinhibition are seen frequently, secondary to injury to frontal, temporal, and limbic areas. In association with the typical cognitive deficits, these sequelae characterize the frequently noted "personality changes" in TBI (Traumatic Brain Injury) patients. Disinhibition syndromes, in brain injuries and insults including brain tumors, strokes and epilepsy range from mildly inappropriate social behavior, lack of control over one's behaviour to the full-blown mania, depending on the lesions to specific brain regions. Several studies in brain traumas and insults have demonstrated significant associations between disinhibition syndromes and dysfunction of orbitofrontal and basotemporal cortices, affecting visuospatial functions, somatosensation, and spatial memory, motoric, instinctive, affective, and intellectual behaviors.

Disinhibition syndromes have also been reported with mania-like manifestations in old age with lesions to the orbito-frontal and basotemporal cortex involving limbic and frontal connections (orbitofrontal circuit), especially in the right hemisphere. Behavioral disinhibition as a result of damage to frontal lobe could be seen as a result of consumption of alcohol and central nervous system depressants drugs, e.g. benzodiazepines that disinhibit the frontal cortex from self-regulation and control. It has also been argued that ADHD, hyperactive/impulsive subtype have a general behavioural disinhibition beyond impulsivity and many morbidities or complications of ADHD, e.g. conduct disorder, anti-social personality disorder. substance abuse and risk taking behaviours are all consequences of untreated behavioural disinhibition.

Within the realm of classical (Pavlovian) conditioning, "disinhibition" is a fundamental process of associative learning characterized by the recurrence of a conditioned response after extinction trials have eliminated said response elicited by the presentation of a novel stimulus. The following process best illustrates this form of disinhibition:
Disinhibition is the temporary increase in strength of an extinguished response due to an unrelated stimulus effect. This differs from spontaneous recovery, which is the temporary increase in strength of a conditioned response, which is likely to occur during extinction after the passage of time. These effects occur during both classical and operant conditioning.

Clinical terms sometimes gain a broader usage and meaning in society outside of their original technical definition. The concept of disinhibition is being applied with some regularity in news articles as an explanation for how youth communicate differently when using the media of instant messaging, text messaging, and posting content on social networking sites. Because technology may provide a perceived buffer from regular consequences and an actual buffer from traditional social cues, people will say and do things through technology that they would not say and do face-to-face.

Individuals who show disinhibited behaviour tend to have this as part of a cluster of challenging behaviours including verbal aggression, physical aggression, socially inappropriate behaviour, sexual disinhibition, wandering, and repetitive behavior.

Disinhibited behaviour occurs when people do not follow the social rules about what or where to say or do something. People who are disinhibited may come across as rude, tactless or even offensive. For example, a person with a brain injury may make a comment about how ugly another person is, or a person with dementia may have lost their social manners and look as though they are deliberately harassing another person.

The reasons why these behaviors may occur include: 

Positive Behaviour Support (PBS) is a treatment approach that looks at the best way to work with each individual with disabilities. A behavioural therapist conducts a functional analysis of behaviour which helps to determine ways to improve the quality of life for the person and does not just deal with problem behaviour.

PBS also acknowledges the needs of support staff and includes strategies to manage crises when they arise. The following model is a brief guide to staff to remind them of key things to think about when planning support for a person with disabilities. There are two main objectives: reacting situationally when the behaviour occurs, and then acting proactively to prevent the behaviour from occurring.

Reactive strategies include:

Proactive strategies to prevent problems can include:

Broadly speaking, when the behaviour occurs, assertively in a nonjudgemental, clear, unambiguous way provide feedback that the behaviour is inappropriate, and say what you prefer instead. For example, "Jane, you're standing too close when you are speaking to me, I feel uncomfortable, please take a step back", or "I don't like it when you say I look hot in front of your wife, I feel uncomfortable, I am your Attendant Carer/Support Worker, I am here to help you with your shopping". Then re-direct to the next activity. Any subsequent behaviour ignore. Then generally, as almost all behaviour is communication, understand what the behaviour is trying to communicate and look at ways to have the need met in more appropriate ways.




</doc>
<doc id="2618295" url="https://en.wikipedia.org/wiki?curid=2618295" title="Flâneur">
Flâneur

Flâneur (), from the French noun "flâneur", means "stroller", "lounger", "saunterer", or "loafer". "Flânerie" is the act of strolling, with all of its accompanying associations. A near-synonym is 'boulevardier'. He is an ambivalent figure of urban riches representing the ability to wander detached from society with no other purpose than to be an acute observer of society.

The flâneur was, first of all, a literary type from 19th-century France, essential to any picture of the streets of Paris. The word carried a set of rich associations: the man of leisure, the idler, the urban explorer, the connoisseur of the street. It was Walter Benjamin, drawing on the poetry of Charles Baudelaire, who made this figure the object of scholarly interest in the 20th century, as an emblematic archetype of urban, modern experience. Following Benjamin, the flâneur has become an important symbol for scholars, artists and writers. Recent scholarship has also proposed the flâneuse, a female equivalent to the flâneur.

Flâneur in English is via French from the Old Norse verb flana "to wander with no purpose".

The terms of "flânerie" date to the 16th or 17th century, denoting strolling, idling, often with the connotation of wasting time. But it was in the 19th century that a rich set of meanings and definitions surrounding the "flâneur" took shape.

The "flâneur" was defined in a long article in Larousse's "Grand dictionnaire universel du XIXe siècle" (in the 8th volume, from 1872). It described the "flâneur" in ambivalent terms, equal parts curiosity and laziness and presented a taxonomy of "flânerie"—"flâneurs" of the boulevards, of parks, of the arcades, of cafés, mindless "flâneurs" and intelligent "flâneurs".

By then, the term had already developed a rich set of associations. Sainte-Beuve wrote that to "flâne" "is the very opposite of doing nothing". Honoré de Balzac described "flânerie" as "the gastronomy of the eye". Anaïs Bazin wrote that "the only, the true sovereign of Paris is the "flâneur"". Victor Fournel, in "Ce qu'on voit dans les rues de Paris" ("What One Sees in the Streets of Paris", 1867), devoted a chapter to "the art of "flânerie"". For Fournel, there was nothing lazy in "flânerie". It was, rather, a way of understanding the rich variety of the city landscape. It was a moving photograph ("un daguerréotype mobile et passioné") of urban experience.

In the 1860s, in the midst of the rebuilding of Paris under Napoleon III and the Baron Haussmann, Charles Baudelaire presented a memorable portrait of the "flâneur" as the artist-poet of the modern metropolis:

Drawing on Fournel, and on his analysis of the poetry of Baudelaire, Walter Benjamin described the "flâneur" as the essential figure of the modern urban spectator, an amateur detective and investigator of the city. More than this, his "flâneur" was a sign of the alienation of the city and of capitalism. For Benjamin, the "flâneur" met his demise with the triumph of consumer capitalism.

In these texts, the "flâneur" was often juxtaposed to the figure of the "badaud", the gawker or gaper. Fournel wrote: "The "flâneur" must not be confused with the "badaud"; a nuance should be observed there…. The simple "flâneur" is always in full possession of his individuality, whereas the individuality of the "badaud" disappears. It is absorbed by the outside world…which intoxicates him to the point where he forgets himself. Under the influence of the spectacle which presents itself to him, the "badaud" becomes an impersonal creature; he is no longer a human being, he is part of the public, of the crowd."

In the decades since Benjamin, the "flâneur" has been the subject of a remarkable number of appropriations and interpretations. The figure of the "flâneur" has been used—among other things—to explain modern, urban experience, to explain urban spectatorship, to explain the class tensions and gender divisions of the nineteenth-century city, to describe modern alienation, to explain the sources of mass culture, to explain the postmodern spectatorial gaze. And it has served as a source of inspiration to writers and artists.

The female counterpart of the "flâneur", the "passante" (French for 'walker', 'passer-by'), appears in particular in the work of Marcel Proust. He portrayed several of his female characters as elusive, passing figures, who tended to ignore his obsessive (and at times possessive) view of them. Increasing freedoms and social innovations such as industrialisation later allowed the "passante" to become an active participant in the 19th century metropolis, as women's social roles expanded away from the domestic and the private and into the public and urban spheres.

While Baudelaire characterized the "flâneur" as a "gentleman stroller of city streets," he saw the "flâneur" as having a key role in understanding, participating in, and portraying the city. A "flâneur" thus played a double role in city life and in theory, that is, while remaining a detached observer. This stance, simultaneously part of and apart from, combines sociological, anthropological, literary, and historical notions of the relationship between the individual and the greater populace.

In the period after the 1848 Revolution in France, during which the Empire was reestablished with clearly bourgeois pretensions of "order" and "morals", Baudelaire began asserting that traditional art was inadequate for the new dynamic complications of modern life. Social and economic changes brought by industrialization demanded that the artist immerse himself in the metropolis and become, in Baudelaire's phrase, "a botanist of the sidewalk". David Harvey asserts that "Baudelaire would be torn the rest of his life between the stances of "flâneur" and dandy, a disengaged and cynical voyeur on the one hand, and man of the people who enters into the life of his subjects with passion on the other".

The observer–participant dialectic is evidenced in part by the dandy culture. Highly self-aware, and to a certain degree flamboyant and theatrical, dandies of the mid-nineteenth century created scenes through self-consciously outrageous acts like walking turtles on leashes down the streets of Paris. Such acts exemplify a "flâneur"'s active participation in and fascination with street life while displaying a critical attitude towards the uniformity, speed, and anonymity of modern life in the city.

The concept of the "flâneur" is important in academic discussions of the phenomenon of modernity. While Baudelaire's aesthetic and critical visions helped open up the modern city as a space for investigation, theorists such as Georg Simmel began to codify the urban experience in more sociological and psychological terms. In his essay "The Metropolis and Mental Life", Simmel theorized that the complexities of the modern city create new social bonds and new attitudes towards others. The modern city was transforming humans, giving them a new relationship to time and space, inculcating in them a "blasé attitude," and altering fundamental notions of freedom and being:

Writing in 1962, Cornelia Otis Skinner suggested that there was no English equivalent of the term: "there is no Anglo-Saxon counterpart of that essentially Gallic individual, the deliberately aimless pedestrian, unencumbered by any obligation or sense of urgency, who, being French and therefore frugal, wastes nothing, including his time which he spends with the leisurely discrimination of a gourmet, savoring the multiple flavors of his city."

The concept of the "flâneur" has also become meaningful in architecture and urban planning, describing people who are indirectly and unintentionally affected by a particular design they experience only in passing.

In 1917, the Swiss writer Robert Walser published a short story called "Der Spaziergang" ("The Walk"), a veritable outcome of the flâneur literature.

Walter Benjamin adopted the concept of the urban observer both as an analytical tool and as a lifestyle. From his Marxist standpoint, Benjamin describes the "flâneur" as a product of modern life and the Industrial Revolution without precedent, a parallel to the advent of the tourist. His "flâneur" is an uninvolved but highly perceptive bourgeois dilettante. Benjamin became his own prime example, making social and aesthetic observations during long walks through Paris. Even the title of his unfinished "Arcades Project" comes from his affection for covered shopping streets.

In the context of modern-day architecture and urban planning, designing for "flâneurs" is one way to approach issues of the psychological aspects of the built environment.

The "flâneur's" tendency toward detached but aesthetically attuned observation has brought the term into the literature of photography, particularly street photography. The street photographer is seen as one modern extension of the urban observer described by nineteenth century journalist Victor Fournel before the advent of the hand-held camera:

The most notable application of "flâneur" to street photography probably comes from Susan Sontag in her 1977 collection of essays, "On Photography". She describes how, since the development of hand-held cameras in the early 20th century, the camera has become the tool of the "flâneur":

The flâneur concept is not limited to someone committing the physical act of a peripatetic stroll in the Baudelairian sense, but can also include a "complete philosophical way of living and thinking", and a process of navigating erudition as described by Nassim Nicholas Taleb's essay on "why I walk". Taleb further set this term with a positive connotation referring to anyone pursuing open, flexible plans, in opposition to the negative "touristification", which he defines as the pursuit of an overly orderly plan. Louis Menand, in seeking to describe the poet T.S. Eliot's relationship to English literary society and his role in the formation of modernism, describes Eliot as a flâneur. Moreover, in one of Eliot's well-known poems "The Lovesong of J. Alfred Prufrock", the protagonist takes the reader for a journey through his city in the manner of a flâneur.

In "De Profundis", Oscar Wilde writes from prison about his life regrets, stating "I let myself be lured into long spells of senseless and sensual ease. I amused myself with being a flaneur, a dandy, a man of fashion. I surrounded myself with the smaller natures and the meaner minds."




</doc>
<doc id="9057549" url="https://en.wikipedia.org/wiki?curid=9057549" title="Cultural mediation">
Cultural mediation

Cultural mediation describes a profession that studies the cultural differences between people, using the data in problem solving.
It is one of the fundamental mechanisms of distinctly human development according to cultural–historical psychological theory introduced by Lev Vygotsky and developed in the work of his numerous followers worldwide.

Vygotsky investigated child development and how this was guided by the role of culture and interpersonal communication. Vygotsky observed how higher mental functions developed through social interactions with significant people in a child's life, particularly parents, but also other adults. Through these interactions, a child came to learn the habits of mind of her/his culture, including speech patterns, written language, and other symbolic knowledge through which the child derives meaning and affects a child's construction of his or her knowledge. This key premise of Vygotskian psychology is often referred to as "cultural mediation". The specific knowledge gained by a child through these interactions also represented the shared knowledge of a culture. This process is known as internalization.

The easiest way to understand mediation is to start with an example and follow with the Vygotskian principles behind it.

At a North American girl's fourth birthday, she sits at the table with friends and family. As the candles on her birthday cake are lit and it is placed on the table, the child gains a feeling of deeply felt joy. This is not only because she knows the cake is sweet and she likes sweet food, nor that the candles' sparkling is pleasing to her eyes. While these would be sufficient reason to arouse an emotional response in an ape, there are mental processes in a four-year-old that extend well beyond this. She patiently waits as her family and friends sing "Happy Birthday to You". The joy is not in the cake itself but in the cake's specific meaning to her. It is a sign that today is a special day for her in which she is the center of attention and that her friends and family are praising her. It's also a sign that she is bigger and as such has higher status among her peers. It's not just a cake, it is a birthday cake and, more specifically, it is her own. The true significance of the birthday cake then, is not in its physical properties at all, but rather in the significance bestowed upon it by the culture the daughter is growing into. This is not restricted to such artifacts as a birthday cake. A classroom, a game of soccer, a fire engine are all first and foremost cultural artifacts from which children derive meaning.

This example can help us understand Vygotsky's approach to human development. Like animals, we have lower mental functions tied closely to biological processes. In our birthday cake example, a toddler may well have reached out to take a handful of cream from the cake as soon as she saw it and the four-year-old may have been tempted to do the same. In humans, however, lower mental functions facilitate a new line of development qualitatively unique to humans. Vygotsky referred to this as the higher mental functions. The lower mental functions cannot be equated to those of an ape as they are interwoven with the line of higher mental functions and are essential to them.
However, it is this higher line of development that explains the birthday cake example with profound insight.

From the perspective of an individual child's development, the higher psychological line of development is one guided by the development of "tools" and "signs" within the culture. In our example above, the birthday cake is much more than a source of nourishment, it is a sign with much deeper and broader meaning. The sign "mediates" between the immediate sensory input and the child's response, and in so doing allows for a moment of reflection and self-regulation that would not otherwise be possible. To the extent that these signs can be used to influence or change our physical or social environment they are tools. Even the birthday cake can be considered as a tool in that the parents use it to establish that their daughter is now older and has a new status in society.

The cake is a sophisticated example. Tools and signs can be much simpler, such as an infant pointing to an object she desires. At first she may simply be trying to reach the object, but the mother's response of passing the object helps the infant realize that the action of pointing is a tool to change the environment according to her needs. It is from these simple inter-subjective beginnings that the world of meaning in the child mediated by tools and signs, including language, develops.

A fundamental premise of Vygotsky's therefore, is that tools and signs are first and foremost shared between individuals in society and only then can they be internalized by individuals developing in the society as is reflected in this famous quote:



</doc>
<doc id="907874" url="https://en.wikipedia.org/wiki?curid=907874" title="Psychological contract">
Psychological contract

A psychological contract, a concept developed in contemporary research by organizational scholar Denise Rousseau, represents the mutual beliefs, perceptions and informal obligations between an employer and an employee. It sets the dynamics for the relationship and defines the detailed practicality of the work to be done. It is distinguishable from the formal written contract of employment which, for the most part, only identifies mutual duties and responsibilities in a generalized form.

Although Rousseau's 1989 article as highlighted by Coyle-Shapiro "was very influential in guiding contemporary research", the concept of psychological contract was first introduced by Argyris (1960)—"Since the foremen realize the employees in this system will tend to produce optimally under passive leadership, and since the employees agree, a relationship may be hypothesized to evolve between the employees and the foremen which might be called the "psychological work contract". The employee will maintain the high production, low grievances, etc., if the foremen guarantee and respect the norms of the employee informal culture (i.e., let the employees alone, make certain they make adequate wages, and have secure jobs)".

Psychological contracts are defined by the relationship between an employer and an employee where there are unwritten mutual expectations for each side. A psychological contract is rather defined as a philosophy, not a formula or devised plan. Characterizing a psychological contract through qualities like respect, compassion, objectivity, and trust. Psychological contracts are formed by beliefs about exchange agreements and may arise in a large variety of situations that are not necessary employer-employee. However, it is most significant in its function as defining the workplace relationship between employer and employee. In this capacity, the psychological contract is an essential, yet implicit agreement that defines employer-employee relationships. These contracts can cause virtuous and vicious circles in some circumstances. Multiple scholars define the psychological contract as a perceived exchange of agreement between an individual and another party. The psychological contract is a type of social exchange relationship. Parallels are drawn between the psychological contract and social exchange theory because the relationship's worth is defined through a cost-benefit analysis. The implicit nature of the psychological contract makes it difficult to define, although there is some general consensus on its nature. This consensus identifies psychological contracts as "promissory, implicit, reciprocal, perceptual, and based on expectations."

These psychological contracts can be impacted by many things like mutual or conflicting morals and values between employer and employee, external forces like the nudge theory, and relative forces like the Adams' equity theory.

The psychological contract came to be identified in 1960 by Argyris. However, only within the last ten to fifteen years has it become more popular and more research been done on the subject. As studies in industrial relations developed and grew more complex, it was revealed that employees are more likely to perform better in certain work environments. The early works of Frederick Winslow Taylor focused on how to enhance worker efficiency. Building upon this, Douglas McGregor developed Theory X and Theory Y to define two contrasting types of management styles that were each effective in attaining a certain goal. These differing management types hold different psychological contracts between employer and employee, as described in more detail under "formation of the psychological contract."

Works by Denise M. Rousseau and later went more in-depth on the details and perspectives of the psychological contract. Sandra L. Robinson indicated employees commonly reported a breach of the psychological contract within several years of beginning their position, and that the effects of contract breach negatively effected employee productivity and retention.

Maslach, Schaufeli and Leiter stated in 2001:

Psychological contract formation is a process whereby the employer and the employee or prospective employee develop and refine their mental maps of one another. According to the outline of phases of psychological contract formation, the contracting process begins before the employment itself, and develops throughout the course of employment. As the employment relationship grows the psychological contract also grows and is reinforced over time. However, the psychological contract is effective only if it is consented to on a voluntary basis. It is also useful in revealing what incentives workers may expect to receive in return for their employment. There are two types of contracts depending on the nature. These are relational psychological contracts and transactional psychological contracts. 
The content of psychological contracts varies widely depending on several factors including management style, as discussed earlier. It also depends on the type of profession and differs widely based on stage in career; for example, between graduates and managers. Denise Rousseau is credited with outlining these 5 phases of contract formation:

The employment relationship emerges through the interpersonal relationships formed in the workplace. How employers, supervisors and managers behave on a day-to-day basis is not determined by the legal contract. Employees slowly negotiate what they must do to satisfy their side of the bargain, and what they can expect in return. This negotiation is sometimes explicit, e.g. in appraisal or performance review sessions, but it more often takes the form of behavioral action and reaction through which parties explore and draw the boundaries of mutual expectation. Hence, the psychological contract determines what the parties will, or will not do and how it will be done. When the parties' expectations match each other, performance is likely to be good and satisfaction levels will be high. So long as the values and loyalty persist, trust and commitment will be maintained. 

The map followed by the parties is the development of an individualized career path that makes only reasonable demands on the employee, with adequate support from managers and co-workers, for a level of remuneration that is demonstrably fair for a person of that age, educational background, and experience. Motivation and commitment will be enhanced if transfers and promotions follow the agreed path in a timely fashion.

The psychological contract changes over time. Since an employee's level of work changes as they advance in their career, the psychological contract that was established when they first began their career changes, too. As an employee is promoted throughout their career they expect more from their psychological contract because they are putting more of themselves into their work. Each stage of a career creates another editing process to the contract. The stages include apprentice, colleague, mentor, sponsor, exploration, establishment, maintenance, and disengagement. The details of each step are as follows:
Studies from Canadian adjunct professor and psychology researcher Yani Likongo demonstrated that sometimes in organizations an idiosyncratic psychological contract is built between the employee and his direct supervisor in order to create an "informal deal" regarding work-life balance. These "deals" support the idea of a constructivist approach including both the employer and the employee, based on a give-and-take situation for both of them. Similarities are drawn between the psychological contract and social exchange theory in that the relationship's worth is defined through a cost-benefit analysis. The employee's attitude toward changes in the company which lead to changes in the psychological contract . An employee's attitude toward change in the job is directly linked to the employee's psychological contract with the manager or employer. An employee's attitude and mindset about what changes could benefit them in what ways could affect the psychological contract they have with the manager.

If managed effectively, the relationship will foster mutual trust between the parties, matching the objectives and commitments of the organization to those of their employees. But a negative psychological contract can result in employees becoming disenchanted, demotivated and resentful of authoritarianism within the organization. This will result in an increasingly inefficient workforce whose objectives no longer correspond to the organization they work for. The main cause of disappointment tends to be that middle managers are protective of their status and security in the eyes of their superiors, and this can introduce conflicts of interest when they are required to fulfill their obligations to their subordinates.

Psychological Contracts are largely reliant on promises between the employer and employee, with trust being the basis for the social exchange. A breach in the Psychological Contract occurs if employees perceive that their firm, or its agents, have failed to deliver on what they perceive was promised, or vice versa. Employees or employers who perceive a breach are likely to respond negatively as it may oftentimes result in an immediate response of mistrust from the other side. Responses may occur in the form of reduced loyalty, commitment, and organizational citizenship behaviors. These feelings typically increase negative tension in the environment. Perceptions that one's psychological contract has been breached may arise shortly after the employee joins the company or even after years of satisfactory service. A breach in the contract may occur when the organizational changes are not necessarily beneficial for employees because of extenuating factors such as globalization and fast-changing markets. The impact may be localized and contained, but if morale is more generally affected, the performance of the organization may be diminished. The risk for breach may be reduced when the organization knows and respects the contracts of the employees. Further, if the activities of the organization are perceived as being unjust or immoral, e.g. aggressive downsizing or outsourcing causing significant unemployment, its public reputation and brand image may also be damaged.



</doc>
<doc id="10439920" url="https://en.wikipedia.org/wiki?curid=10439920" title="Human Universals">
Human Universals

Human Universals is a book by Donald Brown, an American professor of anthropology (emeritus) who worked at the University of California, Santa Barbara. It was published by McGraw Hill in 1991. Brown says human universals, "comprise those features of culture, society, language, behavior, and psyche for which there are no known exception."

According to Brown, there are many universals common to all human societies.

Steven Pinker lists all Brown's universals in the appendix of his book "The Blank Slate". The list includes several hundred universals, and notes Brown's later article on human universals in "The MIT Encyclopedia of the Cognitive Sciences". Brown's universals are not all unique to humans, and many are realized differently in different societies.

The list is seen by Brown (and Pinker) to be evidence of mental adaptations to communal life in our species' evolutionary history. The issues raised by Brown's list are essentially darwinian. They occur in Darwin's "Descent of Man" (1871) and "The Expression of the Emotions in Man and Animals" (1872), and in Huxley's "Evidence as to Man's Place in Nature" (1863). The list gives little emphasis to the issues of aggression, physical conflict and warfare, which have an extensive literature in ethology. Brown's list does have conflict and its mediation as items. He also makes note of the fact that human males are more prone to violence and aggression than females.




</doc>
<doc id="9196294" url="https://en.wikipedia.org/wiki?curid=9196294" title="Tend and befriend">
Tend and befriend

Tend-and-befriend is a behavior exhibited by some animals, including humans, in response to threat. It refers to protection of offspring (tending) and seeking out the social group for mutual defense (befriending). In evolutionary psychology, tend-and-befriend is theorized as having evolved as the typical female response to stress, just as the primary male response was fight-or-flight. The tend-and-befriend theoretical model was originally developed by Dr. Shelley E. Taylor and her research team at the University of California, Los Angeles and first described in a "Psychological Review" article published in the year 2000.

According to the Polyvagal Theory developed by Dr. Stephen Porges, the "Social Nervous System" is an affiliative neurocircuitry that prompts affiliation, particularly in response to stress. This system regulates social approach behavior. A biological basis for this regulation appears to be oxytocin.

Oxytocin has been tied to a broad array of social relationships and activities, including peer bonding, sexual activity, and affiliative preferences. Oxytocin is released in humans in response to a broad array of stressors, especially those that may trigger affiliative needs. Oxytocin promotes affiliative behavior, including maternal tending and social contact with peers. Thus, affiliation under stress serves tending needs, including protective responses towards offspring. Affiliation may also take the form of befriending, namely seeking social contact for one's own protection, the protection of offspring, and the protection of the social group. These social responses to threat reduce biological stress responses, including lowering heart rate, blood pressure, and hypothalamic pituitary adrenal axis (HPA) stress activity, such as cortisol responses.

Women are more likely to respond to stress through tending and befriending than men. Paralleling this behavioral sex difference, estrogen enhances the effects of oxytocin, whereas androgens inhibit oxytocin release.

Female stress responses that increased offspring survival would have led to higher fitness and thus were more likely to be passed on through natural selection. In the presence of threats, protecting and calming offspring while blending into the environment may have increased chances of survival for mother and child. When faced with stress, females often respond by tending to offspring, which in turn reduces stress levels. Studies conducted by Repetti (1989) show that women respond to highly stressful workdays by providing more nurturing behaviors towards their children. In contrast, fathers who experienced stressful workdays were more likely to withdraw from their families or were more interpersonally conflictual that evening at home. Furthermore, physical contact between mothers and their offspring following a threatening event decreased HPA activity and sympathetic nervous system arousal. Oxytocin, released in response to stressors, may be the mechanism underlying the female caregiving response. Studies of ewes show that administration of oxytocin promoted maternal behavior. Breastfeeding in humans, which is associated with maternal oxytocin release, is physiologically calming to both mothers and infants.

Tend-and-befriend is a critical, adaptive strategy that would have enhanced reproductive success among female cooperative breeders. Cooperative breeders are group-living animals where infant and juvenile care from non-mother helpers are essential to offspring survival. Cooperative breeders include wolves, elephants, many nonhuman primates, and humans. Among all primates and most mammals, endocrinological and neural processes lead females to nurture infants, including unrelated infants, after being exposed long enough to infant signals. Non-mother female wolves and wild dogs sometimes begin lactating to nurse the alpha female's pups.

Humans are born helpless and altricial, mature slowly, and depend on parental investment well into their young adult lives, and often even later. Humans have spent most of human evolution as hunter-gatherer foragers. Among foraging societies without modern birth control methods, women have high parity, tending to give birth about every four years during their reproductive lifespan. When mothers give birth, they often have multiple dependent children in their care, who rely on adults for food and shelter for eighteen or more years. Such a reproductive strategy would not have been able to evolve if women did not have help from others. Allomothers (helpers who are not a child's mother) protect, provision, carry, and care for children. Allomothers are usually a child's aunts, uncles, fathers, grandmothers, siblings, and other women in the community. Even in modern Western societies, parents often rely on family members, friends, and babysitters to help care for children. Burkart, Hrdy, and Van Schaik (2009) argue that cooperative breeding in humans may have led to the evolution of psychological adaptations for greater prosociality, enhanced social cognition, and cognitive abilities for cooperative purposes, including willingness to share mental states and shared intentionality. These cognitive, prosocial processes brought on by cooperative breeding may have led to the emergence of culture and language.

Group living provides numerous benefits, including protection from predators and cooperation to achieve shared goals and access to resources. Women create, maintain, and use social networks—especially friendships with other women—to manage stressful conditions. During threatening situations, group members can be a source of support and protection for women and their children. Research shows that women are more likely to seek the company of others in times of stress, compared to men. Women and adolescent girls report more sources of social support and are more likely to turn to same-sex peers for support than men or boys are. Cross-culturally, women and girls tend to provide more frequent and effective support than men do, and they are more likely to seek help and support from other female friends and family members. Women tend to affiliate with other women under stressful situations. However, when women were given a choice to either wait alone or to affiliate with an unfamiliar man before a stressful laboratory challenge, they chose to wait alone. Female-female social networks can provide assistance for childcare, exchange of resources, and protection from predators, other threats, and other group members. Smuts (1992) and Taylor et al. (2000) argue that female social groups also provide protection from male aggression.

Human and animal studies (reviewed in Taylor et al., 2000) suggest that oxytocin is the neuroendocrine mechanism underlying the female "befriend" stress response. Oxytocin administration to rats and prairie voles increased social contact and social grooming behaviors, reduced stress, and lowered aggression. In humans, oxytocin promotes mother-infant attachments, romantic pair bonds, and friendships. Social contact or support during stressful times leads to lowered sympathetic and neuroendocrine stress responses. Although social support downregulates these physiological stress responses in both men and women, women are more likely to seek social contact during stress. Furthermore, support from another female provides enhanced stress-reducing benefits to women. However, a review of female aggression noted that "The fact that OT [oxytocin] enhances, rather than diminishes, attention to potential threat in the environment casts doubt on the popular ‘tend-and-befriend’ hypothesis which is based on the presumed anxiolytic effect of OT".

According to Taylor (2000), affiliative behaviors and tending activities reduce biological stress responses in both parents and offspring, thereby reducing stress-related health threats. "Befriending" may lead to substantial mental and physical health benefits in times of stress. Social isolation is associated with significantly enhanced risk of mortality, whereas social support is tied to positive health outcomes, including reduced risk of illness and death.

Women have higher life expectancies from birth in most countries where there is equal access to medical care. In the United States, for example, this difference is almost 6 years. One hypothesis is that men's responses to stress (which include aggression, social withdrawal, and substance abuse) place them at risk for adverse health-related consequences. In contrast, women's responses to stress, which include turning to social sources for support, may be protective to health.

Group living and affiliation with multiple unrelated others of the same sex (who do not share genetic interests) also presents the problem of competing for access to limited resources, such as social status, food, and mates. Interpersonal stress is the most common and distressing type of stress for women. Although the befriending stress response may be especially activated for women under conditions of resource scarcity, resource scarcity also entails more intense competition for these resources. In environments with a female-biased sex ratio, where males are a more limited resource, female-to-female competition for mates is intensified, sometimes even resorting to violence. Although male crime rates far exceed those of females, arrests for assault among females follow a similar age distribution as in males, peaking for females in the late teens to mid-twenties. Those are ages in which females are at peak reproductive potential and experience the most mating competition. However, the benefits of affiliation would have outweighed the costs in order for tend-and-befriend to have evolved.

Rates of aggression between human males and females may not differ, but the patterns of aggression between the sexes do differ. Although females in general are less physically aggressive, they tend to engage in as much or even more indirect aggression (e.g. social exclusion, gossip, rumors, denigration). When experimentally primed with a mating motive or status competition motive, men were more willing to become directly aggressive towards another man, whereas women were more likely to indirectly aggress against another woman in an aggression-provoking situation. However, experimentally priming people with a resource competition motive increased direct aggression in both men and women. Consistent with this result, rates of violence and crime are higher among males and females under conditions of resource scarcity. In contrast, resource competition did not increase direct aggression in either men or women when they were asked to imagine that were married and had a young child. The costs of physical injury to a parent would also entail costs to his or her family.

Lower variance in reproductive success and higher costs of physical aggression may explain the lower rates of physical aggression among human females compared to males. Females are in general more likely to produce offspring in their lifetimes than males. Therefore, they have less to gain from fighting and the risk of injury or death would produce greater fitness cost for females. The survival of young children depended more on maternal than paternal care, which underscores the importance of maternal safety, survival, and risk aversion. Infants' primary attachment is to their mother, and maternal death increased the chances of childhood mortality in foraging societies by fivefold, compared to threefold in the cases of paternal death. Therefore, women respond to threats by tending and befriending, and female aggression is often indirect and covert in nature to avoid retaliation and physical injury.

Women befriend others not only for protection, but also to form alliances to compete with outgroup members for resources, such as food, mates, and social and cultural resources (e.g. status, social positions, rights and responsibilities). Informational warfare is the strategic competitive tactics taking the form of indirect, verbal aggression directed towards rivals. Gossip is one such tactic, functioning to spread information that would damage the reputation of a competitor. There are several theories regarding gossip, including social bonding and group cohesion. However, consistent with informational warfare theory, the content of gossip is relevant to the context in which competition is occurring. For example, when competing for a work promotion, people were more likely to spread negative work-related information about a competitor to coworkers. Negative gossip also increases with resource scarcity and higher resource value. In addition, people are more likely to spread negative information about potential rivals but more likely to pass on positive information about family members and friends.

As mentioned above, befriending can serve to protect women from threats, including harm from other people. Such threats are not limited to physical harm but also include reputational damage. Women form friendships and alliances in part to compete for limited resources, and also in part to protect themselves from relational and reputational harm. The presence of friends and allies can help deter malicious gossip, due to an alliance's greater ability to retaliate, compared to a single individual's ability. Studies by Hess and Hagen (2009) show that the presence of a competitor's friend reduced people's tendencies to gossip about the competitor. This effect was stronger when the friend was from the same competitive social environment (e.g. same workplace) than when the friend was from a nonrelevant social environment. Friends increase women's perceived capabilities for inflicting reputational harm on a rival as well as perceptions of defensive capabilities against indirect aggression.

This theory is based in evolutionary psychology, a field which has generated significant criticism for its promotion of gender determinism.





</doc>
<doc id="11132183" url="https://en.wikipedia.org/wiki?curid=11132183" title="Normative social influence">
Normative social influence

Normative social influence is a type of social influence that leads to conformity. It is defined in social psychology as "...the influence of other people that leads us to conform in order to be liked and accepted by them." The power of normative social influence stems from the human identity as a social being, with a need for companionship and association. Normative social influence involves a change in behaviour that is deemed necessary in order to fit in a particular group. The need for a positive relationship with the people around leads us to conformity. This fact often leads to people exhibiting public compliance—but not necessarily private acceptance—of the group's social norms in order to be accepted by the group. Social norms refers to the unwritten rules that govern social behavior. These are customary standards for behavior that are widely shared by members of a culture.

In 1955, Solomon Asch conducted his classic conformity experiments in an attempt to discover if people still conform when the right answer is obvious. Specifically, he asked participants in his experiment to judge the similarity of lines, an easy task by objective standards. Using accomplices to the plot, also known as confederates, Asch created the illusion that an entire group of participants believed something that was clearly false (i.e., that dissimilar lines were actually similar). When in this situation, participants conformed over a third of the time on trials where the confederates gave blatantly false answers. When asked to make the judgments in private, participants gave the right answer more than 98% of the time. Asch's results cannot be explained by informational social influence, because in this case, the task was easy and the correct answer was obvious. Thus, participants were not necessarily looking to others to figure out the right answer, as informational social influence predicts. Instead, they were seeking acceptance and avoiding disapproval. Follow-up interviews with participants of the original Asch studies confirmed this. When participants were asked why they conformed, many provided reasons other than a need for accuracy.

In more current research, Schultz (1999) found that households that received more normative messages describing the frequency and amount of weekly recycling, began to have a direct impact on both the households frequency and amount of their curbside recycling. The sudden change was due to the fact that "the other neighbors'" recycling habits had a direct normative effect on the household to change their recycling behaviors. Similar results were apparent in another study in which researchers were able to increase household energy conservation through the use of normative messages. Participants in this conservation study did not believe that such normative messages could influence their behavior; they attributed their conservation efforts to environmental concerns or social responsibility needs. Thus, normative social influence can be a very powerful, yet unconscious, motivator of behavior.

Lastly, different studies have illustrated the consequences of deviation from a group's influence. In a study by Schachter (1951), participants were placed in groups and asked to discuss what to do with a juvenile delinquent they had read about. A "deviant" was instructed by the experimenter to take a stand strongly opposing that of the rest of the group and to hold this position in the midst of any arguments from other members. After the conclusion of the discussions, participants chose to reject this deviant the most, considering him the least desirable of the members, and relegating him to the least important tasks. Recent work by Berns et al. (2005) examined the physiological effects of deviation by using fMRI to scan participants' brains as they completed an object rotation task with other "participants", who were really confederates. The researchers were interested in examining participants' brain activity when they were under the pressure to conform to an incorrect group majority. The amygdala region (which is associated with negative emotions) was activated when participants sought to break off from the influence of the majority; providing evidence for the fact that resisting normative social influence can often lead to negative emotional consequences for individuals.

Latane's social impact theory posits that three factors influence the extent to which we conform to group norms: personal importance, immediacy, and size. As the group becomes more important to a person, physically closer to him/her, and larger in number, Social Impact Theory predicts that conformity to group norms will increase. However, the size of the group only affects conformity to an extent—as a group expands past 3–5 members, the effect levels off.

When a group is unanimous in its support of a norm, an individual feels greater pressure to follow suit. However, even a small break in unanimity can lead to decrease in the power of such normative influence. In Asch's study, when even one other confederate dissented from the majority and provided the correct answer, the participant answered incorrectly on fewer trials (about a fourth less). In addition, participants experienced positive emotions towards such dissenters. A similar reduction in conformity even occurred when the dissenting confederate provided an answer that was false (but still different from that of the majority).

In some versions of the experiment, Asch had dissenting confederates eventually rejoin the majority opinion after several trials; when this occurred, participants experienced greater pressure from normative influence and conformed as if they had never had the dissenter on their side. However, when the conditions were altered and the dissenting confederate left the room after several trials, the participants did not experience a similar pressure to conform as they had when the confederate rejoined the majority—they made less mistakes than they had in the condition where the confederate rejoined the others.

The pressure to bend to normative influence increases for actions performed in public, whereas this pressure decreases for actions done in private. In another variation of the Asch study, the researchers allowed the participant to privately write down his answer after all of the confederates had publicly stated their answers; this variation reduced the level of conformity among participants. In addition, the control condition of the Asch study revealed that participants were almost perfectly accurate when answering independently.

It is possible for a vocal minority to stem the normative influence of a larger majority. In the versions of the Asch study where a dissenter was inserted into the group (see Unanimity section), his presence as a minority member gave the participant the confidence to exert his independence to a greater extent. However, as soon as the dissenter waffled on his opinions and rejoined the majority, participant conformity increased. Thus, a minority must consistently stand by its beliefs to be effective.

In addition, there are other factors that increase the power of the minority: when the majority is forced to think about the beliefs and perspective of the minority, when the majority and minority are similar to one another, and when the minority exhibits some willingness to compromise and be flexible, although there is debate over the degree to which consistency and compromise should be balanced.

It is often the case that whereas a majority influences public compliance with a norm, a minority can engender private acceptance of a new norm, with the end result often being conversion (public and private acceptance of a norm).

There is a distinction between individualistic (e.g., United States) and collectivistic (e.g., Japan) cultures. While some predict that collectivistic cultures would exhibit stronger conformity under normative social influence, this is not necessarily the case—the identity of the group acts as a potential moderator. Because collectivists emphasize the importance of in-group members (e.g., family and friends), normative pressure from in-groups can lead to higher conformity than pressures from strangers.

Many have long wondered whether there is a gender gap in conformity under normative influence, with women possibly conforming more than men. A meta-analysis by Eagly and Carli (1981) shows that this gap is small, and driven by public vs. private situations. Women do conform (slightly) more under normative influence than do men when in public situations as opposed to private ones. Eagly and Carli found that male researchers reported higher levels of conformity among female participants than did female researchers; the authors speculate that each gender could be implicitly biased towards portraying itself in a positive light, thus leading to actions (e.g., setting up experimental conditions under which males or females may be more comfortable) that might favor one gender over the other.

In many cases, normative social influence serves to promote social cohesion. When a majority of group members conform to social norms, the group generally becomes more stable. This stability translates into social cohesion, which allows group members to work together toward a common understanding, or "good," but also has the unintended impact of making the group members less individualistic.

Fashion choices are often impacted by normative social influence. To feel accepted by a particular crowd, men and women often dress similarly to individuals in that group. Fashion conformity promotes social cohesion within the group, and can be a result of both conscious and unconscious motivations. Similar to fashion conformity, both the male and female view of the ideal body image is often affected by normative social influence. Social media and marketing helps to portray what is commonly considered the current view of physical attractiveness by the masses. As each generation defines the ideal female figure, women feel the pressure to conform to avoid disapproval of others. Likewise, as society continues to define the ideal male body type as muscular and fit, men also come under pressure to conform, as well, often leading to changes in eating habits to reach that ideal.



</doc>
<doc id="11537515" url="https://en.wikipedia.org/wiki?curid=11537515" title="Civil inattention">
Civil inattention

Civil inattention is the process whereby strangers who are in close proximity demonstrate that they are aware of one another, without imposing on each other – a recognition of the claims of others to a public space, and of their own personal boundaries.

Civil inattention is the term introduced by Erving Goffman to describe the care taken to maintain public order among strangers and thus to make anonymised life in cities possible. Rather than either ignoring or staring at others, civil inattention involves the unobtrusive and peaceful scanning of others so as to allow for neutral interaction. Through brief eye contact with an approaching stranger, we both acknowledge their presence and foreclose the possibility of more personal contact or of conversation.

Civil inattention is thus a means of making privacy possible within a crowd through culturally accepted forms of self-distancing. Seemingly (though not in reality) effortless, such civility is a way of shielding others from personal claims in public – an essential feature of the abstract, impersonal relationships demanded by the open society.

Civil inattention can lead to feelings of loneliness or invisibility, and it reduces the tendency to feel responsibility for the well-being of others. Newcomers to urban areas are often struck by the impersonality of such routines, which they may see as callous and uncaring, rather than as necessary for the peaceful co-existence of close-packed millions.

Goffman noted that "when men and women cross each other's path at close quarters, the male will exercise the right to look for a second or two at the female ... Civil inattention, then, can here involve a degree of role differentiation regarding obligations". Such a public double standard has been challenged by feminists, who resent the expectation that female appearance/behavior may be routinely commented on. Such behavior may then escalate into staring, stalking and insulting harassment, revealing the costs a breach of civil inattention may bring.

Goffman saw many classic indications of madness as violations of the norm of civil inattention speaking to strangers, or shying away from every passing glance. 




</doc>
<doc id="11963403" url="https://en.wikipedia.org/wiki?curid=11963403" title="Phi complex">
Phi complex

The phi complex is a brain rhythm in the awake human brain that appears to serve various social functions. Phi is one of several brain rhythms in the awake human brain that coordinate human behavior. "Phi" operates in the 10-Hz band (ten oscillations per second), and is located above the right centro-parietal cortex. It consists of two components, one favoring independent behaviors, and the other favoring interpersonal coordination between people.

The brain wave patterns of the phi complex are consistent with those of waves produced in the human mirror neuron system. The phi complex may reflect the influence of one person upon another's behavior, with phi-1 expressing the inhibition of the human mirror neuron system and phi-2 its enhancement.

Researchers from Florida Atlantic University (FAU), in one of the first studies in the field of social neuroscience, have identified neural signatures of effective, real-time coordination between people. FAU researchers have recorded, measured and analyzed both behavior and brain activity simultaneously in two interacting humans that could provide insight into neurological disorders, such as autism and schizophrenia. The research used the conceptual framework and methods of coordination dynamics.

Using specially designed dual-electroencephalogram recordings, Tognoli and her colleagues tested the brain activity of two people simultaneously performing continuous finger motion. Initially, two subjects were asked to rhythmically wag their fingers at their own preferential pace, but were prevented from seeing each other's hands. Then the barrier placed between them was then removed, so they could see each other while continuing to wag their fingers. When subjects were allowed to see one another's fingers moving, they sometimes adjusted their own movements and synchronized, and sometimes they did not, behaving independently.

The researchers believe that the use of dual EEG recordings to observe the phi complex could help scientists better understand what triggers leader/follower behaviors and male/female relationships. The researchers reported that 62% of respondents synchronized their movements upon being exposed to one another's finger-wagging, but 38% were unaffected and continued gesturing independently.

The study demonstrated a clear reduction in occipital lobe alpha wave and mu wave rhythms during social interaction. The evident suppression was independent of whether or not behavior was coordinated. In contrast, a pair of oscillatory components (phi-1 and phi-2) above the right centro-parietal cortex distinguished effective from ineffective coordination. An increase of phi-1 favored independent behavior and increase of phi-2 favored coordinated behavior.

According to Kelso "What this research suggests is that a unique pattern can be seen in the brains of two people interacting and that these brain activities distinguish independence from cooperation. This new brain rhythm that we have discovered and termed the 'phi complex' actually distinguishes when you're socially interacting and when you're not."




</doc>
<doc id="11088164" url="https://en.wikipedia.org/wiki?curid=11088164" title="Ultracrepidarianism">
Ultracrepidarianism

Ultracrepidarianism is the habit of giving opinions and advice on matters outside of one's knowledge. The term "ultracrepidarian" was first publicly recorded in 1819 by the essayist William Hazlitt in an open "Letter to William Gifford", the editor of the "Quarterly Review": "You have been well called an Ultra-Crepidarian critic." It was used again four years later in 1823, in the satire by Hazlitt's friend Leigh Hunt, "Ultra-Crepidarius: a Satire on William Gifford".

The term draws from a famous comment purportedly made by Apelles, a famous Greek artist, to a shoemaker who presumed to criticise his painting. The Latin phrase ""Sutor, ne ultra crepidam", as set down by Pliny and later altered by other Latin writers to "Ne ultra crepidam judicaret"", can be taken to mean that a shoemaker ought not to judge beyond his own soles. That is to say, critics should only comment on things they know something about. The saying remains popular in several languages, as in the English, "A cobbler should stick to his last", the Spanish, "Zapatero a tus zapatos" ("Shoemaker, to your shoes"), the Dutch, "Schoenmaker, blijf bij je leest", the Danish "Skomager, bliv ved din læst", and the German, "Schuster, bleib bei deinen Leisten" (the last three in English, "cobbler, stick to your last").





</doc>
<doc id="1459474" url="https://en.wikipedia.org/wiki?curid=1459474" title="Dishonesty">
Dishonesty

Dishonesty is to act without honesty. It is used to describe a lack of probity, cheating, lying, or being deliberately deceptive or a lack in integrity, knavishness, perfidiosity, corruption or treacherousness. Dishonesty is the fundamental component of a majority of offences relating to the acquisition, conversion and disposal of property (tangible or intangible) defined in criminal law such as fraud.

Dishonesty has had a number of definitions. For many years, there were two views of what constituted dishonesty in English law. The first contention was that the definitions of dishonesty (such as those within the Theft Act 1968) described a course of action, whereas the second contention was that the definition described a state of mind. A clear test within the criminal law emerged from "R v Ghosh" (1982) 75 CR App. R. 154. The Court of Appeal held that dishonesty is an element of "mens rea", clearly referring to a state of mind, and that overall, the test that must be applied is hybrid, but with a subjective bias which "looks into the mind" of the person concerned and establishes what he was thinking. The test is two-stage:

But this decision was criticised, and over-ruled, by the UK Supreme Court in the case of Ivey v Genting Casinos (UK) Ltd t/a Crockfords [2017] UKSC 67. The position as a result is that the court must form a view of what the defendant's belief was of the relevant facts (but it is no longer necessary to consider whether the person concerned believed that what he did was dishonest at the time).

Where dishonesty is an issue in civil cases, the trend in English Law is for only the actions to be tested objectively and not to apply any test as to the subjective state of mind of the actor. Now that the decision in Ghosh has been over-ruled the same legal test applies in English law in civil and criminal cases.

The Theft Act 1968 contains a single definition for dishonesty which is intended to apply to all the substantive offences. Yet, rather than defining what dishonesty is, s2 describes what it is not, allowing a jury to take a flexible approach, thus:

s2(1). A person’s appropriation of property belonging to another is not to be regarded as dishonest:

(a) if he appropriates the property in the belief that he has in law the right to deprive the other of it, on behalf of himself or of a third person; or

(b) if he appropriates the property in the belief that he would have the other’s consent if the other knew of the appropriation and the circumstances of it; or

(c) (except where the property came to him as trustee or personal representative) if he appropriates the property in the belief that the person to whom the property belongs cannot be discovered by taking reasonable steps.

s2(2). A person’s appropriation of property belonging to another may be dishonest notwithstanding that he is willing to pay for the property.

For the purposes of the deception offences, dishonesty is a separate element to be proved. The fact that a defendant knowingly deceives the owner into parting with possession of property does not, of itself, prove the dishonesty. This distinguishes between "obtaining by a dishonest deception" and "dishonestly obtains by a deception".

Debtor's dishonesty or dishonesty to creditors is a felony in Finland and Sweden. It is an abuse of the bankruptcy process, where the debtor attempts to prevent the recovery of assets.

In Finnish law, the felonies of debtor's dishonesty ("velallisen epärehellisyys") and aggravated debtor's dishonesty ("törkeä velallisen epärehellisyys") are defined. A debtor is dishonest if "1) he destroys his or her property, 2) gives away or otherwise surrenders his or her property without acceptable reason, 3) transfers his or her property abroad in order to place it beyond the reach of his or her creditors or 4) increases his or her liabilities without basis, and thus causes his or her insolvency or essentially worsens his or her state of insolvency". The felony is considered aggravated if "1) considerable benefit is sought, 2) considerable or particularly substantial damage is caused to the creditors, or 3) the offence is committed in a particularly methodical manner". The punishment is fine or imprisonment for at most two years, and four months at minimum and four years at maximum if aggravated. It is essential that there is a direct cause and effect between a debtor's deliberate action and the insolvency; mere poor management or accidental losses are not grounds for conviction. Taking into account judicial practice, the best defense is to claim a lack of deliberate intent, and demonstrate that the actions were reasonable at the time and not intended to cause insolvency. Explicit fraud and embezzlement, involving concealment or presenting fraudulent liabilities, are defined separately, as are the less serious deceitfulness and violation by a debtor.

An example was a case involving the former CEO of a bank as the debtor. The debtor was ordered to pay FIM 1.8 million in damages due to reckless lending that had led to a bankruptcy of the bank. However, the debtor kept multiple credit accounts overdrawn by withdrawing large sums of cash, which he claimed were for daily expenses and frequent travel abroad. Thus, garnishment was not possible, because he could claim that he had no net worth. The court found it unlikely that such sums could be spent on daily expenses, but were in fact stashed somewhere, and convicted the debtor of aggravated debtor's dishonesty.

In Swedish law, dishonesty to creditors ("oredlighet mot borgenärer") and aggravated dishonesty to creditors ("grov oredlighet mot borgenärer") carry a sentence of up to two years and half to six years of imprisonment, respectively.




</doc>
<doc id="14511650" url="https://en.wikipedia.org/wiki?curid=14511650" title="Impulsivity">
Impulsivity

In psychology, impulsivity (or impulsiveness) is a tendency to act on a whim, displaying behavior characterized by little or no forethought, reflection, or consideration of the consequences. Impulsive actions are typically "poorly conceived, prematurely expressed, unduly risky, or inappropriate to the situation that often result in undesirable consequences," which imperil long-term goals and strategies for success. Impulsivity can be classified as a multifactorial construct. A functional variety of impulsivity has also been suggested, which involves action without much forethought in appropriate situations that can and does result in desirable consequences. "When such actions have positive outcomes, they tend not to be seen as signs of impulsivity, but as indicators of boldness, quickness, spontaneity, courageousness, or unconventionality" Thus, the construct of impulsivity includes at least two independent components: first, acting without an appropriate amount of deliberation, which may or may not be functional; and second, choosing short-term gains over long-term ones.

Impulsivity is both a facet of personality and a major component of various disorders, including ADHD, substance use disorders, bipolar disorder, antisocial personality disorder, and borderline personality disorder. Abnormal patterns of impulsivity have also been noted instances of acquired brain injury and neurodegenerative diseases. Neurobiological findings suggest that there are specific brain regions involved in impulsive behavior, although different brain networks may contribute to different manifestations of impulsivity, and that genetics may play a role.

Many actions contain both impulsive and compulsive features, but impulsivity and compulsivity are functionally distinct. Impulsivity and compulsivity are interrelated in that each exhibits a tendency to act prematurely or without considered thought and often include negative outcomes. Compulsivity may be on a continuum with compulsivity on one end and impulsivity on the other, but research has been contradictory on this point. Compulsivity occurs in response to a perceived risk or threat, impulsivity occurs in response to a perceived immediate gain or benefit, and, whereas compulsivity involves repetitive actions, impulsivity involves unplanned reactions.

Impulsivity is a common feature of the conditions of gambling and alcohol addiction. Research has shown that individuals with either of these addictions discount delayed money at higher rates than those without, and that the presence of gambling and alcohol abuse lead to additive effects on discounting.

For many years it was understood that impulsivity is a trait but with further analysis it can be found that there were five traits that can lead to impulsive actions.

Two main types of urgency 

Two main types of low consciousness

UPDATE NEEDED to reflect updated version of DSM (DSM-V, 2013.).

Attention deficit-hyperactivity disorder (ADHD) is a multiple component disorder involving inattention, impulsivity, and hyperactivity. The Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR) breaks ADHD into three subtypes according to the behavioral symptoms:

Predominantly hyperactive-impulsive type symptoms may include:

and also these manifestations primarily of impulsivity:

Prevalence of the disorder worldwide is estimated to be between 4% and 10%, with reports as low as 2.2% and as high as 17.8%. Variation in rate of diagnoses may be attributed to differences between populations (i.e., culture), and differences in diagnostic methodologies. Prevalence of ADHD among females is less than half that of males, and females more commonly fall into the inattentive subtype.

Despite an upward trend in diagnoses of the inattentive subtype of ADHD, impulsivity is commonly considered to be the central feature of ADHD, and the impulsive and combined subtypes are the major contributors to the societal costs associated with ADHD. The estimated cost of illness (COI) for a child with ADHD is $14,576 (in 2005 dollars) annually. Prevalence of ADHD among prison populations is significantly higher than that of the normal population.
In both adults and children, ADHD has a high rate of comorbidity with other mental health disorders such as learning disability, conduct disorder, anxiety disorder, major depressive disorder, bipolar disorder, and substance use disorders.

The precise genetic and environmental factors contributing to ADHD are relatively unknown, but endophenotypes offer a potential middle ground between genes and symptoms. ADHD is commonly linked to "core" deficits involving "executive function," "delay aversion," or "activation/arousal" theories that attempt to explain ADHD through its symptomology. Endophenotypes, on the other hand, purport to identify potential behavioral markers that correlate with specific genetic etiology. There is some evidence to support deficits in response inhibition as one such marker. Problems inhibiting prepotent responses are linked with deficits in pre-frontal cortex (PFC) functioning, which is a common dysfunction associated with ADHD and other impulse-control disorders.

Evidence based psychopharmacological and behavioral interventions exist for ADHD.

Impulsivity appears to be linked to all stages of substance abuse.

The acquisition phase of substance abuse involves the escalation from single use to regular use. Impulsivity may be related to the acquisition of substance abuse because of the potential role that instant gratification provided by the substance may offset the larger future benefits of abstaining from the substance, and because people with impaired inhibitory control may not be able to overcome motivating environmental cues, such as peer pressure. "Similarly, individuals that discount the value of delayed reinforcers begin to abuse alcohol, marijuana, and cigarettes early in life, while also abusing a wider array of illicit drugs compared to those who discounted delayed reinforcers less."

Escalation or dysregulation is the next and more severe phase of substance abuse. In this phase individuals "lose control" of their addiction with large levels of drug consumption and binge drug use. Animal studies suggest that individuals with higher levels of impulsivity may be more prone to the escalation stage of substance abuse.

Impulsivity is also related to the abstinence, relapse, and treatment stages of substance abuse. People who scored high on the Barratt Impulsivity Scale (BIS) were more likely to stop treatment for cocaine abuse. Additionally, they adhered to treatment for a shorter duration than people that scored low on impulsivity. Also, impulsive people had greater cravings for drugs during withdrawal periods and were more likely to relapse. This effect was shown in a study where smokers that test high on the BIS had increased craving in response to smoking cues, and gave into the cravings more quickly than less impulsive smokers. Taken as a whole the current research suggests that impulsive individuals are less likely to abstain from drugs and more likely to relapse earlier than less impulsive individuals.

While it is important to note the effect of impulsivity on substance abuse, the reciprocating effect whereby substance abuse can increase impulsivity has also been researched and documented. The promoting effect of impulsivity on substance abuse and the effect of substance abuse on increased impulsivity creates a positive feedback loop that maintains substance seeking behaviors. It also makes conclusions about the direction of causality difficult. This phenomenon has been shown to be related to several substances, but not all. For example, alcohol has been shown to increase impulsivity while amphetamines have had mixed results.

Substance use disorder treatments include prescription of medications such as acamprosate, buprenorphine, disulfiram, LAAM, methadone, and naltrexone., as well as effective psychotherapeutic treatment like 
behavioral couples therapy, CBT, contingency management, motivational enhancement therapy, and relapse prevention.

Impulsive overeating spans from an episode of indulgence by an otherwise healthy person to chronic binges by a person with an eating disorder.

Consumption of a tempting food by non-clinical individuals increases when self-regulatory resources are previously depleted by another task, suggesting that it is caused by a breakdown in self control. Impulsive eating of unhealthy snack foods appears to be regulated by individual differences in impulsivity when self-control is weak and by attitudes towards the snack and towards healthy eating when self-control is strong. There is also evidence that greater food consumption occurs when people are in a sad mood, although it is possible that this is due more to emotional regulation than to a lack of self-control. In these cases, overeating will only take place if the food is palatable to the person, and if so individual differences in impulsivity can predict the amount of consumption.

Chronic overeating is a behavioral component of binge eating disorder, compulsive overeating, and bulimia nervosa. These diseases are more common for women and may involve eating thousands of calories at a time. Depending on which of these disorders is the underlying cause, an episode of overeating can have a variety of different motivations. Characteristics common among these three disorders include low self-esteem, depression, eating when not physically hungry, preoccupation with food, eating alone due to embarrassment, and feelings of regret or disgust after an episode. In these cases, overeating is not limited to palatable foods.

Impulsivity differentially affects disorders involving the over control of food intake (such as anorexia nervosa) and disorders involving the lack of control of food intake (such as bulimia nervosa). Cognitive impulsivity, such as risk-taking, is a component of many eating disorders, including those that are restrictive. However, only people with disorders involving episodes of overeating have elevated levels of motoric impulsivity, such as reduced response inhibition capacity.

One theory suggests that binging provides a short-term escape from feelings of sadness, anger, or boredom, although it may contribute to these negative emotions in the long-term. Another theory suggests that binge eating involves reward seeking, as evidenced by decreased serotonin binding receptors of binge-eating women compared to matched-weight controls and predictive value of heightened reward sensitivity/drive in dysfunctional eating.

Treatments for clinical-grade overeating include cognitive behavioral therapy to teach people how to track and change their eating habits and actions, interpersonal psychotherapy to help people analyze the contribution of their friends and family in their disorder, and pharmacological therapies including antidepressants and SSRIs.

Impulse buying consists of purchasing a product or service without any previous intent to make that purchase. It has been speculated to account for as much as eighty percent of all purchases in the United States.

There are several theories pertaining to impulsive buying. One theory suggests that it is exposure combining with the speed that a reward can be obtained that influences an individual to choose lesser immediate rewards over greater rewards that can be obtained later. For example, a person might choose to buy a candy bar because they are in the candy aisle even though they had decided earlier that they would not buy candy while in the store.

Another theory is one of self-regulation which suggests that the capacity to refrain from impulsive buying is a finite resource. As this capacity is depleted with repeated acts of restraint susceptibility to purchasing other items on impulse increases.

Finally, a third theory suggests an emotional and behavioral tie between the purchaser and the product which drives both the likelihood of an impulsive purchase as well as the degree that a person will retroactively be satisfied with that purchase result. Some studies have shown a large number of individuals are happy with purchases made on impulse (41% in one study) which is explained as a preexisting emotional attachment which has a positive relationship both with the likelihood of initiating the purchase as well as mitigating post purchase satisfaction. As an example, when purchasing team-related college paraphernalia a large percentage of those purchases are made on impulse and are tied to the degree with which a person has positive ties to that team.

Impulsive buying is seen both as an individual trait in which each person has a preconditioned or hereditary allotment, as well as a situational construct which is mitigated by such things as emotion in the moment of the purchase and the preconditioned ties an individual has with the product.

Psychotherapy and pharmacological treatments have been shown to be helpful interventions for patients with impulsive-compulsive buying disorder.

Impulse control disorder (ICDs) are a class of DSM diagnoses that do not fall into the other diagnostic categories of the manual (e.g. substance use disorders), and that are characterized by extreme difficulty controlling impulses or urges despite negative consequences. Individuals suffering from an impulse control disorder frequently experience five stages of symptoms: compelling urge or desire, failure to resist the urge, a heightened sense of arousal, succumbing to the urge (which usually yields relief from tension), and potential remorse or feelings of guilt after the behavior is completed. Specific disorders included within this category include intermittent explosive disorder, kleptomania, pathological gambling, pyromania, trichotillomania (hair pulling disorder), and impulse control disorders not otherwise specified (ICD NOS). ICD NOS includes other significant difficulties that seem to be related to impulsivity but do not meet the criteria for a specific DSM diagnosis.

There has been much debate over whether or not the ICDs deserve a diagnostic category of their own, or whether they are in fact phenomenologically and epidemiologically related to other major psychiatric conditions like obsessive-compulsive disorder (OCD), affective disorders, and addictive disorders. In fact, the ICD classification is likely to change with the release of the DSM-V in May 2013. In this new revision the ICD NOS will likely be reduced or removed; proposed revisions include reclassifying trichotillomania (to be renamed hair-pulling disorder) and skin picking disorder as obsessive-compulsive and related disorders, moving Intermittent Explosive Disorder under the diagnostic heading of disruptive, impulse control, and conduct disorders, and gambling disorder may be included in addiction and related disorders.

The role of impulsivity in the ICDs varies. Research on kleptomania and pyromania are lacking, though there is some evidence that greater kleptomania severity is tied to poor executive functioning.

Trichotillomania and skin picking disorder seem to be disorders that primarily involve motor impulsivity, and will likely be classified in the DSM-V within the obsessive-compulsive and related disorders category.

Pathological gambling, in contrast, seems to involve many diverse aspects of impulsivity and abnormal reward circuitry (similar to substance use disorders) that has led to it being increasingly conceptualized as a non-substance or behavioral addiction. Evidence elucidating the role of impulsivity in pathological gambling is accumulating, with pathological gambling samples demonstrating greater response impulsivity, choice impulsivity, and reflection impulsivity than comparison control samples. Additionally, pathological gamblers tend to demonstrate greater response perseveration (compulsivity) and risky decision making in laboratory gambling tasks compared to controls, though there is no strong evidence suggesting that attention and working memory are impaired in pathological gamblers. These relations between impulsivity and pathological gambling are confirmed by brain function research: pathological gamblers demonstrate less activation in the frontal cortical regions (implicated in impulsivity) compared to controls during behavioral tasks tapping response impulsivity, compulsivity, and risk/reward. Preliminary, though variable, findings also suggest that striatal activation is different between gamblers and controls, and that neurotransmitter differences (e.g. dopamine, serotonin, opioids, glutamate, norepinephrine) may exist as well.

Individuals with Intermittent Explosive Disorder, also known as impulsive aggression, have exhibited serotonergic abnormalities and show differential activation in response to emotional stimuli and situations. Notably, Intermittent Explosive Disorder is not associated with a higher likelihood of diagnosis with any of the other ICDs but is highly comorbid with disruptive behavior disorders in childhood. Intermittent Explosive Disorder is likely to be re-classified in the DSM-V as under the heading of disruptive, impulse control, and conduct disorders.

These sorts of impulse control disorders are most often treated using certain types of psychopharamcological interventions (e.g. antidepressants) and behavioral treatments like cognitive behavioral therapy.

According to the ego (or cognitive) depletion theory of impulsivity, self-control refers to the capacity for altering one's own responses, especially to bring them into line with standards such as ideals, values, morals, and social expectations, and to support the pursuit of long-term goals. Self-control enables a person to restrain or override one response, thereby making a different response possible.
A major tenet of the theory is that engaging in acts of self-control draws from a limited "reservoir" of self-control that, when depleted, results in reduced capacity for further self-regulation. Self-control is viewed as analogous to a muscle: Just as a muscle requires strength and energy to exert force over a period of time, acts that have high self-control demands also require strength and energy to perform. Similarly, as muscles become fatigued after a period of sustained exertion and have reduced capacity to exert further force, self-control can also become depleted when demands are made of self-control resources over a period of time. Baumeister and colleagues termed the state of diminished self-control strength ego depletion (or cognitive depletion).

The strength model of self-control asserts that:

Empirical tests of the ego-depletion effect typically adopt dual-task paradigm. Participants assigned to an experimental ego-depletion group are required to engage in two consecutive tasks requiring self-control. Control participants are also required to engage in two consecutive tasks, but only the second task requires self-control. The strength model predicts that the performance of the experimental-group on the second self-control task will be impaired relative to that of the control group. This is because the finite self-control resources of the experimental participants will be diminished after the initial self-control task, leaving little to draw on for the second task.

The effects of ego depletion do not appear to be a product of mood or arousal. In most studies, mood and arousal has not been found to differ between participants who exerted self-control and those who did not. Likewise, mood and arousal was not related to final self-control performance. The same is true for more specific mood items, such as frustration, irritation, annoyance, boredom, or interest as well. Feedback about success and failure of the self-control efforts does not appear to affect performance. In short, the decline in self-control performance after exerting self-control appears to be directly related to the amount of self-control exerted and cannot be easily explained by other, well-established psychological processes.

Dual process theory states that mental processes operate in two separate classes: automatic and controlled. In general, automatic processes are those that are experiential in nature, occur without involving higher levels of cognition, and are based on prior experiences or informal heuristics. Controlled decisions are effortful and largely conscious processes in which an individual weighs alternatives and makes a more deliberate decision.

Dual process theories at one time considered any single action/thought as either being automatic or controlled. However, currently they are seen as operating more along a continuum as most impulsive actions will have both controlled and automatic attributes. Automatic processes are classified according to whether they are meant to inhibit or to facilitate a thought process. For example, in one study researchers offered individuals a choice between a 1:10 chance of winning a prize and a 10 in 100 chance. Many participants chose one of the choices over the other without identifying that the chances inherent in each were the same as they saw either only 10 chances total as more beneficial, or of having 10 chances to win as more beneficial. In effect impulsive decisions can be made as prior information and experiences dictate one of the courses of action is more beneficial when in actuality careful consideration would better enable the individual to make a more informed and improved decision.

Intertemporal choice is defined as "decisions with consequences that play out over time". This is often assessed using the relative value people assign to rewards at different points in time, either by asking experimental subjects to choose between alternatives or examining behavioral choices in a naturalistic setting.

Intertemporal choice is commonly measured in the laboratory using a "delayed discounting" paradigm, which measures the process of devaluing rewards and punishments that happen in the future. In this paradigm, subjects must choose between a smaller reward delivered soon and a larger reward delivered at a delay in the future. Choosing the smaller-sooner reward is considered impulsive. By repeatedly making these choices, indifference points can be estimated. For example, if someone chose $70 now over $100 in a week, but chose the $100 in a week over $60 now, it can be inferred that they are indifferent between $100 in a week and an intermediate value between $60 and $70. A delay discounting curve can be obtained for each participant by plotting their indifference points with different reward amounts and time delays. Individual differences in discounting curves are affected by personality characteristics such as self-reports of impulsivity and locus of control; personal characteristics such as age, gender, IQ, race, and culture; socioeconomic characteristics such as income and education; and many other variables. Lesions of the nucleus accumbens core subregion or basolateral amygdala produce shifts towards choosing the smaller-sooner reward, suggesting the involvement of these brain regions in the preference for delayed reinforcers. There is also evidence that the orbitofrontal cortex is involved in delay discounting, although there is currently debate on whether lesions in this region result in more or less impulsivity.

Economic theory suggests that optimal discounting involves the exponential discounting of value over time. This model assumes that people and institutions should discount the value of rewards and punishments at a constant rate according to how delayed they are in time. While economically rational, recent evidence suggests that people and animals do not discount exponentially. Many studies suggest that humans and animals discount future values according to a hyperbolic discounting curve where the discount factor decreases with the length of the delay (for example, waiting from today to tomorrow involves more loss of value than waiting from twenty days to twenty-one days). Further evidence for non-constant delay discounting is suggested by the differential involvement of various brain regions in evaluating immediate versus delayed consequences. Specifically, the prefrontal cortex is activated when choosing between rewards at a short delay or a long delay, but regions associated with the dopamine system are additionally activated when the option of an immediate reinforcer is added. Additionally, intertemporal choices differ from economic models because they involve anticipation (which may involve a neurological "reward" even if the reinforcer is delayed), self-control (and the breakdown of it when faced with temptations), and representation (how the choice is framed may influence desirability of the reinforcer), none of which are accounted for by a model that assumes economic rationality.

One facet of intertemporal choice is the possibility for preference reversal, when a tempting reward becomes more highly valued than abstaining only when immediately available. For example, when sitting home alone, a person may report that they value the health benefit of not smoking a cigarette over the effect of smoking one. However, later at night when the cigarette is immediately available, their subjective value of the cigarette may rise and they may choose to smoke it.

A theory called the "primrose path" is intended to explain how preference reversal can lead to addiction in the long run. As an example, a lifetime of sobriety may be more highly valued than a lifetime of alcoholism, but, at the same time, one drink now may be more highly valued than not drinking now. Because it is always "now," the drink is always chosen, and a paradoxical effect occurs whereby the more-valued long-term alternative is not achieved because the more-valued short-term alternative is always chosen. This is an example of complex ambivalence, when a choice is made not between two concrete alternatives but between one immediate and tangible alternative (i.e., having a drink) and one delayed and abstract alternative (i.e., sobriety).

Similarities between humans and non-human animals in intertemporal choice have been studied. Pigeons and rats also discount hyperbolically; tamarin monkeys do not wait more than eight seconds to triple the amount of a food reward. The question arises as to whether this is a difference of homology or analogy—that is, whether the same underlying process underlies human-animal similarities or whether different processes are manifesting in similar patterns of results.

Inhibitory control, often conceptualized as an executive function, is the ability to inhibit or hold back a prepotent response. It is theorized that impulsive behavior reflects a deficit in this ability to inhibit a response; impulsive people may find it more difficult to inhibit action whereas non-impulsive people may find it easier to do so. There is evidence that, in normal adults, commonly used behavioral measures of inhibitory control correlate with standard self-report measures of impulsivity.

Inhibitory control may itself be multifaceted, evidenced by numerous distinct inhibition constructs that can be measured in different ways, and relate to specific types of psychopathology. Joel Nigg developed a useful working taxonomy of these different types of inhibition, drawing heavily from the fields of cognitive and personality psychology Nigg's eight proposed types of inhibition include the following:

Suppression of a stimulus that elicits an interfering response, enabling a person to complete the primary response. Interference control can also refer to suppressing distractors.

Interference control has been measured using cognitive tasks like the stroop test, flanker tasks, dual task interference, and priming tasks. Personality researchers have used the Rothbart effortful control measures and the conscientiousness scale of the Big Five as inventory measures of interference control. Based on imaging and neural research it is theorized that the anterior cingulate, the dorsolateral prefrontal/premotor cortex, and the basal ganglia are related to interference control.

Cognitive inhibition is the suppression of unwanted or irrelevant thoughts to protect working memory and attention resources.

Cognitive inhibition is most often measured through tests of directed ignoring, self-report on one's intrusive thoughts, and negative priming tasks. As with interference control, personality psychologists have measured cognitive inhibition using the Rothbart Effortful Control scale and the Big Five Conscientiousness scale. The anterior cingulate, the prefrontal regions, and the association cortex seem to be involved in cognitive inhibition.

Behavioral Inhibition is the suppression of prepotent response.

Behavioral inhibition is usually measured using the Go/No Go task, Stop signal task, and reports of suppression of attentional orienting. Surveys that are theoretically relevant to behavioral inhibition include the Rothbart effortful control scale, and the Big Five Conscientiousness dimension. The rationale behind the use of behavioral measures like the Stop signal task is that "go" processes and "stop processes" are independent, and that, upon "go" and "stop" cues, they "race" against each other; if the go process wins the race, the prepotent response is executed, whereas if the stop processes wins the race, the response is withheld. In this context, impulsivity is conceptualized as a relatively slow stop process. The brain regions involved in behavioral inhibition appear to be the lateral and orbital prefrontal regions along with premotor processes.

Oculomotor Inhibition is the effortful suppression of reflexive saccade.

Oculomotor inhibition is tested using antisaccade and oculomotor tasks. Also, the Rothbart effortful control measure and the Big Five Conscientiousness dimension are thought to tap some of the effortful processes underlying the ability to suppress saccade. The frontal eye fields and the dorsolateral prefrontal cortex are involved in oculomotor inhibition.

Motivational inhibition and response in the face of punishment can be measured using tasks tapping inhibition of primary response, modified go/no go tasks, inhibition of competing response, and emotional Stroop tasks. Personality psychologists also use the Gray behavioral inhibition system measure, the Eysenck scale for neurotic introversion, and the Zuckerman Neuroticism-Anxiety scale. The Septal-hippocampal formation, cingulate, and motor systems seem to be the brain areas most involved in response to punishment.

Response to novelty has been measured using the Kagan behavioral inhibition system measure and scales of neurotic introversion. The amygdaloid system is implicated in novelty response.

Suppression of recently inspected stimuli for both attention and oculomotor saccade is usually measured using attentional and oculomotor inhibition of return tests. The superior colliculus and the midbrain, oculomotor pathway are involved in suppression of stimuli.

Information at locations that are not presently being attended to is suppressed, while attending elsewhere.

This involves measures of covert attentional orienting and neglect, along with personality scales on neuroticism. The posterior association cortex and subcortical pathways are implicated in this sort of inhibition.

Recent psychology research also yields out the condition of impulsivity in relation to peoples' general goal setting. It is possible these action and inaction goals are underlying people's behavioral differences in their daily lives since they can demonstrate "patterns comparable to natural variation in overall activity levels". More specifically, the level of impulsivity and mania people have might positive correlated with favorable attitudes about and goals of general action while negatively respond to favorable attitudes about and goals of general inaction.

The "Barratt Impulsiveness Scale" (BIS) is one of the oldest and most widely used measures of impulsive personality traits. The first BIS was developed in 1959 by Dr. Ernest Barratt. It has been revised extensively to achieve two major goals: (1) to identify a set of "impulsiveness" items that was orthogonal to a set of "anxiety" items as measured by the Taylor Manifest Anxiety Scale (MAS) or the Cattelll Anxiety Scale, and (2) to define impulsiveness within the structure of related personality traits like Eysenck's Extraversion dimension or Zuckerman's Sensation-Seeking dimension, especially the disinhibition subfactor. The BIS-11 with 30 items was developed in 1995. According to Patton and colleagues, there are 3 subscales (Attentional Impulsiveness, Motor Impulsiveness, and Non-Planning Impulsiveness) with six factors:

"The Eysenck Impulsiveness Scale" (EIS) is a 54-item yes/no questionnaire designed to measure impulsiveness. Three subscales are computed from this measure: Impulsiveness, Venturesomeness, and Empathy. Impulsiveness is defined as "behaving without thinking and without realizing the risk involved in the behavior". Venturesomeness is conceptualized as "being conscious of the risk of the behavior but acting anyway" The questionnaire was constructed through factor analysis to contain items that most highly loaded on impulsiveness and venturesomeness. The EIS is a widely used and well-validated measure.

The "Dickman Impulsivity Inventory" was first developed in 1990 by Scott J. Dickman. This scale is based on Dickman's proposal that there are two types of impulsivity that are significantly different from one another. This includes functional impulsivity which is characterized by quick decision making when it is optimal, a trait that is often considered to be a source of pride. The scale also includes dysfunctional impulsivity which is characterized by making quick decisions when it is not optimal. This type of impulsivity is most often associated with life difficulties including substance abuse problems and other negative outcomes.

This scale includes 63 items of which 23 are related to dysfunctional impulsivity, 17 are related to functional impulsivity, and 23 are filler questions that relate to neither construct. This scale has been developed into a version for use with children as well as into several languages. Dickman showed there is no correlation between these two tendencies across individuals, and they also have different cognitive correlates.

The "UPPS Impulsive Behavior Scale" is a 45-item self-report questionnaire that was designed to measure impulsivity across dimensions of the Five Factor Model of personality. The UPPS includes 4 sub-scales: lack of premeditation, urgency, lack of perseverance, and sensation-seeking.

UPPS-P Impulsive Behavior Scale (UPPS-P) is a revised version of the UPPS, including 59 items. It assesses an additional personality pathway to impulsive behavior, Positive Urgency, in addition to the four pathways assessed in the original version of the scale: Urgency (now Negative Urgency), (lack of) Premeditation, (lack of) Perseverance, and Sensation Seeking

UPPS-P short version (UPPS-Ps) is 20-item scale that evaluates five different impulsivity facets (4 items per dimension).

UPPS-R Interview is a semi-structured interview that measures the degree to which individuals exhibit the various components of impulsivity assessed by the UPPS-P.

"Lifetime History of Impulsive Behaviors" (LHIB). is a 53-item questionnaire designed to assess lifetime history of impulsive behavior (as opposed to impulsive tendencies) as well as the level of distress and impairment associated with these behaviors. The assessment battery was designed to measure the following six dimensions: (a) impulsivity, (b) sensation seeking, (c) trait anxiety, (d) state depression, (e) empathy, and (f) social desirability. The LHIB consists of scales for clinically significant impulsivity, non-clinically significant impulsivity, and impulsivity related distress/impairment.

"Behavioral Inhibition System/Behavioral Activation System (BIS/BAS)" was developed based on the Gray's biopsychological theory of personality which suggests that there are two general motivational systems that underlie behavior and affect: BIS and BAS. This 20-item self-report questionnaire is designed to assess dispositional BIS and BAS sensitivities.

"Impulsive/Premeditated Aggression Scale (IPAS)" is a 30-item self-report questionnaire. Half of the items describe impulsive aggression and half the items describe premeditated aggression. Aggressive behavior has traditionally been classified into two distinct subtypes, impulsive or premeditated. Impulsive aggression is defined as a hair-trigger aggressive response to provocation with loss of behavioral control. Premeditated aggression is defined as a planned or conscious aggressive act, not spontaneous or related to an agitated state. The IPAS is designed to characterize aggressive behavior as predominately impulsive or predominately premeditated in nature. Those subjects who clustered on the impulsive factor showed a broad range of emotional and cognitive impairments; those who clustered on the premeditated factor showed a greater inclination for aggression and anti-social behaviour.

The "Padua Inventory (PI)" consists of 60 items describing common obsessional and compulsive behavior and allows investigation of such problems in normal and clinical subjects.

A wide variety of behavioral tests have been devised for the assessment of impulsivity in both clinical and experimental settings. While no single test is a perfect predictor or a sufficient replacement for an actual clinical diagnosis, when used in conjunction with parent/teacher reports, behavioral surveys, and other diagnostic criteria, the utility of behavioral paradigms lies in their ability to narrow in on specific, discrete aspects of the impulsivity umbrella. Quantifying specific deficits is of use to the clinician and the experimenter, both of whom are generally concerned with obtaining objectively measurable treatment effects.

One widely recognizable test for impulsivity is the delay of gratification paradigm commonly known as the 'marshmallow test'. Developed in the 1960s to assess 'willpower' and self-control in preschoolers, the marshmallow test consists of placing a single marshmallow in front of a child and informing them that they will be left alone in the room for some duration. The child is told that if the marshmallow remains uneaten when the experimenter returns, they will be awarded a second marshmallow, both of which can then be eaten.

Despite its simplicity and ease of administration, evidence from longitudinal studies suggests that the number of seconds preschoolers wait to obtain the second marshmallow is predictive of higher SAT scores, better social and emotional coping in adolescence, higher educational achievement, and less cocaine/crack use.

Like the marshmallow test, delay discounting is also a delay of gratification paradigm. It is designed around the principle that the subjective value of a reinforcer decreases, or is 'discounted,' as the delay to reinforcement increases. Subjects are given varying choices between smaller, immediate rewards and larger, delayed rewards. By manipulating reward magnitude and/or reward delay over multiple trials, 'indifference' points can be estimated whereby choosing the small, immediate reward, or the large, delayed reward are about equally likely. Subjects are labeled impulsive when their indifference points decline more steeply as a function of delay compared to the normal population (i.e., greater preference for immediate reward). Unlike the marshmallow test, delay discounting does not require verbal instruction and can be implemented on non-human animals.

Two common tests of response inhibition used in humans are the go/no-go task, and a slight variant known as the stop signal reaction time (SSRT) test. During a go/no-task, the participant is trained over multiple trials to make a particular response (e.g., a key-press) when presented with a 'go' signal. On some trials, a 'stop' signal is presented just prior to, or simultaneously with the 'go' signal, and the subject must inhibit the impending response.

The SSRT test is similar, except that the 'stop' signal is presented after the 'go' signal. This small modification increases the difficulty of inhibiting the 'go' response, because the participant has typically already initiated the 'go' response by the time the 'stop' signal is presented. The participant is instructed to respond as fast as possible to the 'go' signal while maintaining the highest possible inhibition accuracy (on no-go trials). During the task, the time at which the 'stop' signal is presented (the stop signal delay or SSD) is dynamically adjusted to match the time after the 'go' signal at which the participant is just able/unable to inhibit their 'go' response. If the participant fails to inhibit their 'go' response, the 'stop' signal is moved slightly closer to the original 'go' signal, and if the participant successfully inhibits their 'go' response, the 'stop' signal is moved slightly ahead in time. The SSRT is thus measured as the average 'go' response time minus the average 'stop' signal presentation time (SSD).

The balloon analogue risk task (BART) was designed to assess risk-taking behavior. Subjects are presented with a computer depiction of a balloon that can be incrementally inflated by pressing a response key. As the balloon inflates, the subject accumulates rewards with each new key-press. The balloon is programmed with a constant probability of popping. If the balloon pops, all rewards for that balloon are lost, or the subject may choose to stop inflating and 'bank' the reward for that balloon at any time. Therefore, more key-presses equate to greater reward, but also greater probability of popping and cancelling rewards for that trial. The BART assumes that those with an affinity for 'risk-taking' are more likely to pop the balloon, earning less reward overall than the typical population.

The Iowa gambling task (IGT) is a test originally meant to measure decision making specifically within individuals who have ventromedial prefrontal cortex damage. The concept of impulsivity as relates to the IGT is one in which impulsive decisions are a function of an individual's lack of ability to make rational decisions over time due to an over amplification of emotional/somatic reward. In the IGT individuals are provided four decks of cards to choose from. Two of these decks provide much higher rewards but the deductions are also much higher while the second two decks have lower rewards per card but also much lower deductions. Over time anyone who chooses predominantly from the high rewards decks will lose money while those who choose from the smaller rewards decks will gain money.

The IGT uses hot and cold processes in its concept of decision making. Hot decision making involves emotional responses to the material presented based on motivation related to reward and punishment. Cold processes occur when an individual uses rational cognitive determinations when making decisions. Combined an individual should gain a positive emotional reaction when choices have beneficial consequences and will have negative emotional responses tied to choices that have greater negative consequences. In general, healthy responders to the IGT will begin to drift to the lower gain decks as they realize that they are gaining more money than they lose both through an ability to recognize that one is more consistently providing rewards as well as through the emotions related to winning consistently. However, those who have emotional deficits will fail to recognize that they are losing money over time and will continue to be more influenced by the exhilaration of higher value rewards without being influenced by the negative emotions of the loses associated with them.

For more information concerning these process refer to the Somatic marker hypothesis

Differential reinforcement of low response rate (DRL) described by Ferster and Skinner is used to encourage low rates of responding. It is derived from research in operant conditioning that provides an excellent opportunity to measure the hyperactive child's ability to inhibit behavioral responding. Hyperactive children were relatively unable to perform efficiently on the task, and this deficit endured regardless of age, IQ, or experimental condition. Therefore, it can be used to discriminate accurately between teacher rated and parent rated hyperactive and nonhyperactive children. In this procedure, responses that occur before a set time interval has passed are not reinforced and reset the time required between behaviors.

In a study, a child was taken to the experimental room and told that they were going to play a game in which they had a chance to win a lot of M&M's. Every time they made the light of the reward indicator by pressing a red button, they would earn an M&M's. However, they had to wait a while (6 seconds) before they could press it to get another point. If they had pressed the button too soon, then they would have not gotten a point, and the light would not go on, and they had to wait a while before they could press it to get another point.

Researchers have also observed that subjects in a time-based situation will often engage in a sequence or chain of behaviors between reinforceable responses. This is because this collateral behavior sequence helps the subject "wait out" the required temporal delay between responses.

Other common impulsivity tasks include the Continuous performance task (CPT), 5-choice serial reaction time task (5-CSRTT), Stroop task, and Matching Familiar Figures Task.

Although the precise neural mechanisms underlying disorders of impulse control are not fully known, the prefrontal cortex (PFC) is the brain region most ubiquitously implicated in impulsivity. Damage to the prefrontal cortex has been associated with difficulties preparing to act, switching between response alternatives, and inhibiting inappropriate responses. Recent research has uncovered additional regions of interest, as well as highlighted particular subregions of the PFC, that can be tied to performance in specific behavioral tasks.

Excitotoxic lesions in the nucleus accumbens core have been shown to increase preference for the smaller, immediate reward, whereas lesions to the nucleus accumbens shell have had no observable effect. Additionally, lesions of the basolateral amygdala, a region tied closely to the PFC, negatively affect impulsive choice similarly to what is observed in the nucleus accumbens core lesions. Moreover, dorsal striatum may also be involved in impulsive choice in an intricate manner.

The 'orbital frontal cortex' is now thought to play a role in disinhibiting, and injury to other brain structures, such as to the right inferior frontal gyrus, a specific subregion of the PFC, has been associated with deficits in stop-signal inhibition.

As with delay discounting, lesion studies have implicated the core region of the nucleus accumbens in response inhibition for both DRL and 5-CSRTT. Premature responses in the 5-CSRTT may also be modulated by other systems within the ventral striatum. In the 5-CSRTT, lesions of the anterior cingulate cortex have been shown to increase impulsive responding, and lesions to the prelimbic cortex impair attentional performance.

Patients with damage to the ventromedial frontal cortex exhibit poor decision-making and persist in making risky choices in the Iowa Gambling Task.

The primary pharmacological treatments for ADHD are methylphenidate (Ritalin) and amphetamine. Both methylphenidate and amphetamines block re-uptake of dopamine and norepinephrine into the pre-synaptic neuron, acting to increase post-synaptic levels of dopamine and norepinephrine. Of these two monoamines, increased availability of dopamine is considered the primary cause for the ameliorative effects of ADHD medications, whereas increased levels of norepinephrine may be efficacious only to the extent that it has downstream, indirect effects on dopamine. 
The effectiveness of dopamine re-uptake inhibitors in treating the symptoms of ADHD has led to the hypothesis that ADHD may arise from low tonic levels of dopamine (particularly in the fronto-limbic circuitry), but evidence in support of this theory is mixed.

There are several difficulties when it comes to trying to identify a gene for complex traits such as impulsivity, such as genetic heterogeneity. Another difficulty is that the genes in question might sometimes show incomplete penetrance, "where a given gene variant does not always cause the phenotype". Much of the research on the genetics of impulsivity-related disorders, such as ADHD, is based on family or linkage studies. There are several genes of interest that have been studied in an attempt to find the major genetic contributors to impulsivity. Some of these genes are:

While impulsivity can take on pathological forms (e.g. substance use disorder, ADHD), there are less severe, non-clinical forms of problematic impulsivity in many people's daily lives. Research on the different facets of impulsivity can inform small interventions to change decision making and reduce impulsive behavior For example, changing cognitive representations of rewards (e.g. making long term rewards seem more concrete) and/or creating situations of "pre-commitment" (eliminating the option of changing one's mind later) can reduce the preference for immediate reward seen in delay discounting.

"Brain training" interventions include laboratory-based interventions (e.g. training using tasks like go/no go) as well as community, family, and school based interventions that are ecologically valid (e.g. teaching techniques for regulating emotions or behaviors) and can be used with individuals with non-clinical levels of impulsivity. Both sorts of interventions are aimed at improving executive functioning and self-control capacities, with different interventions specifically targeting different aspects of executive functioning like inhibitory control, working memory, or attention. Emerging evidence suggests that brain training interventions may succeed in impacting executive function, including inhibitory control. Inhibitory control training specifically is accumulating evidence that it can help individuals resist temptation to consume high calorie food and drinking behavior. Some have voiced concerns that the favorable results of studies testing working memory training should be interpreted with caution, claiming that conclusions regarding changes to abilities are measured using single tasks, inconsistent use of working memory tasks, no-contact control groups, and subjective measurements of change.

Behavioral, psychosocial, and psychopharmacological treatments for disorders involving impulsivity are common.

"Psychopharmacological intervention" in disorders of impulsivity has shown evidence of positive effects; common pharmacological interventions include the use of stimulant medication, selective serotonin reuptake inhibitors (SSRIs) and other antidepressants. ADHD has a well-established evidence base supporting the use of stimulant medication for the reduction of ADHD symptoms. Pathological gambling has also been studied in drug trials, and there is evidence that gambling is responsive to SSRIs and other antidepressants. Evidence based pharmacological treatment for trichotillomania is not yet available, with mixed results of studies investigating the use of SSRIs, though Cognitive Behavioral Therapy (CBT) has shown positive effects. Intermittent Explosive Disorder is most often treated with mood stabilizers, SSRIs, beta blockers, alpha agonists, and anti-psychotics (all of which have shown positive effects). There is evidence that some pharmacological interventions are efficacious in treating substance use disorders, though their use can depend on the type substance that is abused. Pharmacological treatments for SUD include the use of acamprosate, buprenorphine, disulfiram, LAAM, methadone, and naltrexone.

"Behavioral interventions" also have a fairly strong evidence base in impulse control disorders. In ADHD, the behavioral interventions of behavioral parent training, behavioral classroom management, and intensive peer-focused behavioral interventions in recreational settings meet stringent guidelines qualifying them for evidence based treatment status. In addition, a recent meta-analysis of evidence-based ADHD treatment found organization training to be a well-established treatment method. Empirically validated behavioral treatments for substance use disorder are fairly similar across substance use disorders, and include behavioral couples therapy, CBT, contingency management, motivational enhancement therapy, and relapse prevention. Pyromania and kleptomania are understudied (due in large part to the illegality of the behaviors), though there is some evidence that psychotherapeutic interventions (CBT, short term counseling, day treatment programs) are efficacious in treating pyromania, while kleptomania seems to be best impacted using SSRIs. Additionally, therapies including CBT, family therapy, and social skill training have shown positive effects on explosive aggressive behaviors.



</doc>
<doc id="14240521" url="https://en.wikipedia.org/wiki?curid=14240521" title="McGuire's Motivations">
McGuire's Motivations

McGuire’s Psychological Motivations is a classification system that organizes theories of motives into 16 categories. The system helps marketers to isolate motives likely to be involved in various consumption situations.

McGuire first divided the motivation into two main categories using two criteria:

Then for each division in each category he stated there is two more basic elements.





</doc>
<doc id="15879259" url="https://en.wikipedia.org/wiki?curid=15879259" title="Boreout">
Boreout

Boredom burnout syndrome is a psychological disorder that causes physical illness.
This theory was first expounded in 2007 in "Diagnose Boreout", a book by Peter Werder and Philippe Rothlin, two Swiss business consultants.

The first symptoms of boredom at work are demotivation, anxiety, and sadness. In the long term, boreout will develop, generating a strong feeling of self-deprecation, which can turn into depression.

The consequences of boreout for employees are numerous both psychologically and physically and more or less serious. On the psychological level, boredom, dissatisfaction, and permanent frustration gradually lead the victim of a boreout into a vicious circle. They gradually lose the will to act at the professional level and at the personal level. To the loss of self-esteem is added the constant anxiety of being discovered. The boreout victim lives with the constant fear that their supervisor, colleagues, or friends will discover their inactivity and duplicity. A state of constant sadness takes hold of the employee, provoking crises of tears for no particular reason. Being constantly confronted with the emptiness of their professional life and their uselessness in society, the employee is in great pain. The suffering all the more accentuated because it cannot be shared and if it is, is not understood. This can lead to serious mental disorders such as personality destruction or even depression or suicide. Boreout is also a trigger for physical diseases such as certain types of epilepsy caused by stress or exhaustion, severe sleep disorders, hand and voice tremors, shingles, and ulcers.

On the physical side, according to the British "Bored to death" study, employees who are bored at work are two to three times more likely to be victims of cardiovascular events than those whose employment is stimulating. The permanent anxiety in which the employee lives exhausts him physically. Fatigue is constant despite physical inactivity. Boreout can lead to eating disorders such as untimely nibbling or loss of appetite. Some people may use alcohol or drugs to overcome their discomfort and thus develop a harmful addiction.

According to Peter Werder and Philippe Rothlin, the absence of meaningful tasks, rather than the presence of stress, is many workers' chief problem. Boreout consists of three elements: boredom, lack of challenge, and lack of interest. These authors disagree with the common perceptions that a demotivated employee is lazy; instead, they claim that the employee has lost interest in work tasks. Those suffering from boreout are "dissatisfied with their professional situation" in that they are frustrated at being prevented, by institutional mechanisms or obstacles as opposed to by their own lack of aptitude, from fulfilling their potential (as by using their skills, knowledge, and abilities to contribute to their company's development) and/or from receiving official recognition for their efforts.

The authors suggest that the reason for researchers' and employers' overlooking the magnitude of boreout-related problems is that they are underreported because revealing them exposes a worker to the risk of social stigma and adverse economic effects. (By the same token, many managers and co-workers consider an employee's level of workplace stress to be indicative of that employee's status in the workplace.)

There are several reasons boreout might occur. The authors note that boreout is unlikely to occur in many non-office jobs where the employee must focus on finishing a specific task (e.g., a surgeon) or helping people in need (e.g., a childcare worker or nanny). In terms of group processes, it may well be that the boss or certain forceful or ambitious individuals with the team take all the interesting work leaving only a little of the most boring tasks for the others. Alternatively, the structure of the organization may simply promote this inefficiency. Of course, few if any employees (even among those who would prefer to leave) want to be fired or laid off, so the vast majority are unwilling and unlikely to call attention to the dispensable nature of their role. 

As such, even if an employee has very little work to do, s/he gives the appearance of "looking busy" (e.g., ensuring that a work-related document is open on one's computer, covering one's desk with file folders, and carrying briefcases (whether empty or loaded) from work to one's home and vice versa).

Werder and Rothlin cite research into time wasting at work carried out by AOL and salary.com in 2005. The survey of 10,000 employees showed that the average worker frittered away 2.09 hours per eight-hour day outside their break time on non-work related tasks. The reason most often cited for this behavior (by 33% of subjects; see study methodology for whether subjects could cite more than one reason) was management's failure to assign specific tasks to specific employees.

The authors note that the main response of many companies to these problems is to increase their monitoring and surveillance. Internet use may be monitored and a number of websites (e.g., video game websites or social networking sites) may be blocked. However, the authors argue that these monitoring and surveillance methods are neither effective nor conducive to a productive and fulfilling working environment. First of all, tech-savvy employees can get around some of the monitoring and surveillance methods (e.g., by using encrypted proxies that carry no target-specific information in their URLs). 

Even if employers block both sites used for personal business (e.g., social-networking and web-based email sites) and sites configured as proxies, employees can circumvent the block entirely with devices with data plans such as smartphones. As well, if employers monitor employees' telephone use, whether by tracking numbers dialed and/or by tracking time spent on the phone, employees can simply use their personal phones to make calls, whether at their desk or (if individual offices are "bugged") by slipping into an area not monitored.

The symptoms of boreout lead employees to adopt coping or work-avoidance strategies that create the appearance that they are already under stress, suggesting to management both that they are heavily "in demand" as workers and that they should not be given additional work: "The boreout sufferer's aim is to look busy, to not be given any new work by the boss and, certainly, not to lose the job."

Boreout strategies include:

Consequences of boreout for employees include dissatisfaction, fatigue as well as ennui and low self-esteem, while for the business itself there are the problems of an unnecessary financial burden, high levels of sick leave and low company loyalty. The paradox of boreout is that despite hating the situation, employees feel unable to ask for more challenging tasks, to raise the situation with superiors or even look for a new job. The authors do however propose a solution: first, one must analyse one's personal job situation, then look for a solution within the company and finally if that does not help, look for a new job. If all else fails, turning to friends, family, or other co-workers for support can be extremely beneficial until any of the previously listed options become viable.





</doc>
<doc id="13625251" url="https://en.wikipedia.org/wiki?curid=13625251" title="Leisure satisfaction">
Leisure satisfaction

"Leisure refers to activities that a person voluntarily engages in when they are free from any work, social or familial responsibilities." Leisure satisfaction is the positive perceptions or feelings that an individual forms, elicits and gains as a result of engaging in leisure activities and choices. What can contribute to leisure satisfaction is to what degree an individual is currently satisfied with their leisure experiences and activities. An individual might attain positive feelings of contentment and happiness that result from the satisfaction of needs. Participation in leisure activities and leisure satisfaction are inextricably linked to an individual's health. Caldwell (2005) suspects that leisure activities may be associated with a number of defensive traits that enhance a person's resiliency to negative life experiences. Some aspects of leisure activities that can act as protective factors include: "[the activity] being personally meaningful, intrinsically interesting and/or challenging; offering social support and friendships; contributing to a sense of competence and/or self efficacy; offering a sense of personal control, choice and self-determination; and being relaxing and/or distracting the individual from negative life events." Leisure activities, although ranging in types, have also proven to be beneficial to health cross-culturally.

In a study by Hribernik and Mussap (2010), leisure satisfaction was found to predict unique variance in life satisfaction, supporting its inclusion as a distinct life domain contributing to subjective well-being. Additionally, relationship status interacted with age group and gender on differences in leisure satisfaction. The relationship between leisure satisfaction and life satisfaction, however, was reduced when considering the impact of core affect (underlying mood state). This suggests that leisure satisfaction may primarily be influenced by an individual's subjective well-being level as represented by core affect. This has implications for possible limitations in the extent to which leisure satisfaction may be improved beyond pre-existing levels of well-being and mood in individuals.

In another study conducted by Brajsa-Zagnec et al. (2010), subjective well being (SWB) was defined as a combination of an individual's emotional reactions, satisfaction with specific aspects of one's life, and satisfaction with one's whole life. Many studies have been conducted to determine what specific leisure activities are linked to SWB. Research identifies other groups of leisure activities ranging from three to eleven to sixteen groups. There is no overall agreement regarding what specific groups of leisure activities predict SWB, but some researchers agree that leisure activities contribute to SWB and that the relationship between the two is complex.

Data was collected from a group of Croatian citizens ranging across various age groups. The participants estimated their SWB and time spent participating in leisure activities. These leisure activities included active socializing and going out (sports, going to clubs, eating dinner out etc.), visiting cultural events (reading books, going to concerts, going to movies etc.), and family and home activities (going to church, visiting family, watching television etc.). The results of the study found specific leisure activities to be a predictor of SWB across age groups. For people ages 31–60 participation in visiting cultural events, family leisure activities, and active socializing and going out contributed to SWB. A significant positive correlation was found between family leisure activities and SWB of men and women across different age groups. This study concluded that participation in leisure activities lead to SWB, though the importance of such specific leisure activities vary across different age and genders. Essentially, people may improve their SWB by participating in leisure activities, especially in family and home activities.

A study conducted by London et al. (1977) was about job and leisure satisfaction contributing to quality of life (QOL). QOL was determined by asking the participants "How do you feel about your life as a whole" twice during the run of the study. As well, the participants were asked to fill out a survey that measured feelings about leisure, work, and life. It was found that activities that had to do with families and people they socialize with was significant to QOL. Overall non-job related activities (leisure activities) can be more important and a better predictor of QOL as opposed to variables of job related activities. People should consider the importance of the amount of time spent in leisure activities.

In a study conducted by Agate et al. 2009, participants required a child and parent from a family to fill out an online survey which measured the amount of involvement in family leisure activities and the satisfaction with involvement of family leisure activities. It was found that families' involvement in leisure activities is the best predictor for overall family life satisfaction, even more than the amount of time spent together. In early adolescent years, the amount and the satisfaction of family leisure experiences are important to the perceptions of satisfaction with family life that the adolescents will develop later on in life. Zabriskie and McCormick (2003), the study that Agate modeled his off of, also concluded that involvement in family leisure activities was the single strongest predictor of satisfaction with family life. As well, benefits of participation in family leisure activities are better communication skills among the family, better problem solving strategies, development of life and social skills, and better overall satisfaction with family life. Overall research has provided evidence for significant correlations between leisure satisfaction and satisfaction with family life.

Not much empirical data has been collected regarding leisure activities and marital satisfaction. But with the research that has been conducted, it has been found that leisure activities influence marital satisfaction positively. Happily married men and women were likely to value spending time together, enjoy activities done together, and agree on recreation needs. Some research has focused on the compatibility between married couples and this influence on the types of leisure activities they choose and the relationship between their overall marital satisfactions. Some research that focused on the relationship between leisure companionship and marital satisfaction found these couples tended to participate in activities that both partners enjoyed.

In a study conducted by Orthner et al. (1975), the overall amount of time spent together in leisure activities is positively related to marital satisfaction for both males and females. Participants filled out a questionnaire measuring the pattern of interaction between spouses during leisure time and how much time is spent in individual, parallel, and joint leisure activities. It was found that wives spent more time alone in leisure activities than husbands across the marital career, but this individual participation was negatively correlated with marriage satisfaction. As well, the scores for marriage satisfaction were more stable for wives than husbands over time. Overall, this study concluded that the benefit of participating in leisure activities as a married couple is improved communication. Marital participation in leisure activities is the most critical during the first years of marriage and after 18–23 years of marriage when the marital relationship is reestablishing itself.

Research regarding compatibility in spouses and the relationship between leisure activities and marital satisfaction have found that the couples who are less compatible are more prone to pursue leisure activities separately than highly compatible couples. This study concluded that the more spouses liked the leisure activities they were participating in, the higher their marital satisfaction was. Essentially the results of this study determined that couples participating in leisure activities together positively correlated with their marriage satisfaction.

The importance of leisure activities has been studied in various aspects of life. One of the most prevalent aspects of life studied with importance of leisure satisfaction is for people with psychological issues. Some psychological issues can consist of common concerns such as stress or more complex concerns such as clinical disorders. Whether a person experiences stressors at work, through depression or brain injuries, leisure satisfaction may ease the stress regardless of the type. 
Stress in the workforce is a common issue many people face in their lifetime, however, leisure activities may help lower a person's stress levels and increase their satisfaction. When someone engages in enjoyable leisure activities, their moods tend to increase, which in turn, allows them to better accept everyday stressors. When faced with difficult job situations one must be able to achieve adequate free time to truly enjoy their leisure activity of choice. Another important aspect of leisure activity is the type performed, whether is it an active or passive activity. Joudrey & Wallace (2009) conducted a study that statistically found the importance of active leisure activity. Passive leisure activities were suggested to give workers an ability to "escape", which in end could cause depressive moods. However, workers participating in active leisure showed considerably higher levels of mental health.

People with mental disabilities often lack the ability or confidence to participate in social events, such as leisure activities. However, studies such as the one conducted by Lloyd, King, Lampe, & McDougall (2001) have been performed to prove the true importance of leisure among patients with mental disabilities. The results from their study showed a strong positive relationship between leisure satisfaction and the patients' met needs. The results basically state that the more leisure patients experienced, the more likely they felt their social, education and psychological needs were met. The study concluded that leisure is as important for people with psychiatric disabilities as it is for the general public. As Lloyd, King, Lampe, & McDougall (2001) explained, the general public may view an event, such as a leisure activity, as unsatisfactory, but to a mental patient a leisure activity can greatly raise their average happiness. Prvu, J. (1999) conducted a study among brain injury patients. Results showed that patients involved in the leisure activity program that helped increase leisure skills and knowledge of community resources also provided patients with an increase in self-confidence and leisure participation, which in turn increased leisure satisfaction. Leisure activity can be a significant factor in lowering a person's level of depressive symptoms.



</doc>
<doc id="12319843" url="https://en.wikipedia.org/wiki?curid=12319843" title="Relational disorder">
Relational disorder

According to Michael First of the DSM-5 working committee the focus of a relational disorder, in contrast to other DSM-IV disorders, "is on the relationship rather than on any one individual in the relationship".

Relational disorders involve two or more individuals and a disordered "juncture", whereas typical Axis I psychopathology describes a disorder at the individual level. An additional criterion for a relational disorder is that the disorder cannot be due solely to a problem in one member of the relationship, but requires pathological interaction from each of the individuals involved in the relationship.

For example, if a parent is withdrawn from one child but not another, the dysfunction could be attributed to a relational disorder. In contrast, if a parent is withdrawn from both children, the dysfunction may be more appropriately attributable to a disorder at the individual level.

First states that "relational disorders share many elements in common with other disorders: there are distinctive features for classification; they can cause clinically significant impairment; there are recognizable clinical courses and patterns of comorbidity; they respond to specific treatments; and they can be prevented with early interventions. Specific tasks in a proposed research agenda: develop assessment modules; determine the clinical utility of relational disorders; determine the role of relational disorders in the etiology and maintenance of individual disorders; and consider aspects of relational disorders that might be modulated by individual disorders."

The proposed new diagnosis defines a relational disorder as "persistent and painful patterns of feelings, behaviors, and perceptions" among two or more people in an important personal relationship, such a husband and wife, or a parent and children.

According to psychiatrist Darrel Regier, MD, some psychiatrists and other therapists involved in couples and marital counseling have recommended that the new diagnosis be considered for possible incorporation into the Diagnostic and Statistical Manual of Mental Disorders (DSM IV).

The idea of a psychology of relational disorders is far from new. According to Adam Blatner, MD, some of the early psychoanalysts alluded to it more or less directly, and the history of marital couple therapy began with a few pioneers in 1930s. J.L. Moreno, the inventor of psychodrama and a major pioneer of group psychotherapy and social psychology, noted the idea that relationships could be "sick" even if the people involved were otherwise "healthy," and even vice versa: Otherwise "sick" people could find themselves in a mutually supportive and "healthy" relationship.

Moreno's ideas may have influenced some of the pioneers of family therapy, but also there were developments in general science, namely, cybernetic theory, developed in the mid-1940s, and noting the nature of circularity and feedback in complex systems. By the 1950s, the idea that relationships themselves could be problematic became quite apparent. So, diagnostically, in the sense not of naming a disease or disorder, but just helping people think through what was really going on, the idea of relational disorder was nothing new.

The majority of research on relational disorders concerns three relationship systems: adult children and their parents, minor children and their parents, and the marital relationship. There is also an increasing body of research on problems in dyadic gay relationships and on problematic sibling relationships.

Marital disorders are divided into "Marital Conflict Disorder Without Violence" and "Marital Abuse Disorder (Marital Conflict Disorder With Violence)." Couples with marital disorders sometimes come to clinical attention because the couple recognize long-standing dissatisfaction with their marriage and come to the clinician on their own initiative or are referred by a health care professional. Secondly, there is serious violence in the marriage which is "usually the husband battering the wife". In these cases the emergency room or a legal authority often is the first to notify the clinician.

Most importantly, marital violence "is a major risk factor for serious injury and even death and women in violent marriages are at much greater risk of being seriously injured or killed" (National Advisory Council on Violence Against Women 2000). The authors of this study add that "There is current considerable controversy over whether male-to-female marital violence is best regarded as a reflection of male psychopathology and control or whether there is an empirical base and clinical utility for conceptualizing these patterns as relational."

Recommendations for clinicians making a diagnosis of "Marital Relational Disorder" should include the assessment of actual or "potential" male violence as regularly as they assess the potential for suicide in depressed patients. Further, "clinicians should not relax their vigilance after a battered wife leaves her husband, because some data suggest that the period immediately following a marital separation is the period of greatest risk for the women.

Many men will stalk and batter their wives in an effort to get them to return or punish them for leaving. Initial assessments of the potential for violence in a marriage can be supplemented by standardized interviews and questionnaires, which have been reliable and valid aids in exploring marital violence more systematically."
The authors conclude with what they call "very recent information" on the course of violent marriages which suggests that "over time a husband's battering may abate somewhat, but perhaps because he has successfully intimidated his wife."

The risk of violence remains strong in a marriage in which it has been a feature in the past. Thus, treatment is essential here; the clinician cannot just wait and watch. The most urgent clinical priority is the protection of the wife because she is the one most frequently at risk, and clinicians must be aware that supporting assertiveness by a battered wife may lead to more beatings or even death.

In some cases, men are abuse victims of their wives; there is not exclusively male-on-female physical violence, although this is more common than female-on-male violence.

Research on parent–child abuse bears similarities to that on marital violence, with the defining characteristic of the disorder being physical aggression by a parent toward a child. The disorder is frequently concealed by parent and child, but may come to the attention of the clinician in several ways, from emergency room medical staff to reports from child protection services.

Some features of abusive parent–child relationships that serve as a starting point for classification include: (a) the parent is physically aggressive with a child, often producing physical injury, (b) parent–child interaction is coercive, and parents are quick to react to provocations with aggressive responses, and children often reciprocate aggression, (c) parents do not respond effectively to positive or prosocial behavior in the child, (d) parents do not engage in discussion about emotions, (e) parent engages in deficient play behavior, ignores the child, rarely initiates play, and does little teaching, (f) children are insecurely attached and, where mothers have a history of physical abuse, show distinctive patterns of disorganized attachment, and (g) parents relationship shows coercive marital interaction patterns.

Defining the relational aspects of these disorders can have important consequences. For example, in the case of early appearing feeding disorders, attention to relational problems may help delineate different types of clinical problems within an otherwise broad category. In the case of conduct disorder, the relational problems may be so central to the maintenance, if not the etiology, of the disorder that effective treatment may be impossible without recognizing and delineating it.




</doc>
<doc id="10930626" url="https://en.wikipedia.org/wiki?curid=10930626" title="Description error">
Description error

A description error or selection error is an error, or more specifically a human error, that occurs when a person performs the correct action on the wrong object due to insufficient specification of an action which would have led to a desired result. This commonly happens when similar actions lead to different results. A typical example is a panel with rows of identical switches, where it is easy to carry out a correct action (flip a switch) on a wrong switch due to their insufficient differentiation.

This error can be very disorienting and usually causes a brief loss of situation awareness or automation surprise if noticed right away. But much worse, if it goes unnoticed, it could cause more serious problems. So allowances such as clearly highlighting a selected item should be made in interaction design.

Donald Norman describes the subject in his book "The Design of Everyday Things". There he describes how user-centered design can help account for human limitations that can lead to errors like description errors. James Reason also covers the subject in his book "Human Error".



</doc>
<doc id="5465383" url="https://en.wikipedia.org/wiki?curid=5465383" title="Endemic warfare">
Endemic warfare

Endemic warfare is a state of continual or frequent warfare, such as is found in some tribal societies (but is not limited to tribal societies). 

Ritual fighting (or ritual battle or ritual warfare) permits the display of courage, masculinity and the expression of emotion while resulting in relatively few wounds and even fewer deaths. Thus such a practice can be viewed as a form of conflict-resolution and/or as a psycho-social exercise.
Native Americans often engaged in this activity, but the frequency of warfare in most hunter-gatherer cultures is a matter of dispute.

Warfare is known to several tribal societies, but some societies develop a particular emphasis of warrior culture (such as the Nuer of South Sudan, the Māori of New Zealand, the Dugum Dani of Papua, the Yanomami (dubbed "the Fierce People") of the Amazon, or the Germanic tribes of Iron Age Europe.)

Communal societies are well capable of escalation to all-out wars of annihilation between tribes. Thus, in Amazonas, there was perpetual animosity between the neighboring tribes of the Jívaro. A fundamental difference between wars enacted within the same tribe and against neighboring tribes is such that "wars between different tribes are in principle wars of extermination".

The Yanomami of Amazonas traditionally practiced a system of escalation of violence in several discrete stages. 
The chest-pounding duel, the side-slapping duel, the club fight, and the spear-throwing fight. Further escalation results in raiding parties with the purpose of killing at least one member of the hostile faction. Finally, the highest stage of escalation is "Nomohoni" or all-out massacres brought about by treachery.

Similar customs were known to the Dugum Dani and the Chimbu of New Guinea, the Nuer of Sudan and the North American Plains Indians. Among the Chimbu and the Dugum Dani, pig theft was the most common cause of conflict, even more frequent than abduction of women, while among the Yanomamö, the most frequent initial cause of warfare was accusations of sorcery. Warfare serves the function of easing intra-group tensions and has aspects of a game, or "overenthusiastic football". Especially Dugum Dani "battles" have a conspicuous element of play, with one documented instance of a battle interrupted when both sides were distracted by throwing stones at a passing cuckoo dove.





</doc>
<doc id="973778" url="https://en.wikipedia.org/wiki?curid=973778" title="Pedant">
Pedant

A pedant is a person who is excessively concerned with formalism, accuracy, and precision, or one who makes an ostentatious and arrogant show of learning.

Pedantry is related to personality. One study found that extroverts were more tolerant of typing mistakes than introverts.

The English language word "pedant" comes from the French "pédant" (used in 1566 in Darme & Hatzfeldster's "Dictionnaire général de la langue française") or its older mid-15th century Italian source "pedante", "teacher, schoolmaster". (Compare the Spanish "pedante.") The origin of the Italian "pedante" is uncertain, but several dictionaries suggest that it was contracted from the medieval Latin "pædagogans," present participle of "pædagogare", "to act as pedagogue, to teach" (Du Cange). The Latin word is derived from Greek , "paidagōgós", "child" + "to lead", which originally referred to a slave who escorted children to and from school but later meant "a source of instruction or guidance".

The term in English is typically used with a negative connotation to refer to someone who is over-concerned with minutiae and whose tone is condescending. Thomas Nashe wrote in "Have with you to Saffron-walden" (1596), page 43: "O, tis a precious apothegmaticall [terse] Pedant, who will finde matter inough to dilate a whole daye of the first inuention [invention] of Fy, fa, fum". However, when the word was first used by Shakespeare in "Love's Labour's Lost" (1598), it simply meant "teacher".

Obsessive–compulsive personality disorder is in part characterized by a form of pedantry that is excessively concerned with the correct following of rules, procedures, and practices. Sometimes the rules that OCPD sufferers obsessively follow are of their own devising, or are corruptions or reinterpretations of the letter of actual rules.

Pedantry can also be an indication of specific developmental disorders. In particular, people with Asperger syndrome often have behaviour characterized by pedantic speech.




</doc>
<doc id="1064845" url="https://en.wikipedia.org/wiki?curid=1064845" title="Turncoat">
Turncoat

A turncoat is a person who shifts allegiance from one loyalty or ideal to another, betraying or deserting an original cause by switching to the opposing side or party. In political and social history, this is distinct from being a traitor, as the switch mostly takes place under the following circumstances:

From a military perspective, opposing armies generally wear uniforms of contrasting colors to prevent incidents of Friendly fire. Thus the term "turn-coat" indicates that an individual has changed sides and his uniform coat to one matching the color of his former enemy. For example, in the English Civil War during the 17th century, Oliver Cromwell's soldiers turned their coats inside out to match the colours of the Royal army (see Examples below).

Even in a modern historical context "turncoat" is often synonymous with the term "renegade", a term of religious origins having its origins in the Latin word "renegare" (to deny). Historical currents of great magnitude have periodically caught masses of people, along with their leaders, in their wake. In such a dire situation new perspectives on past actions are laid bare and the question of personal treason becomes muddled. One example would be the situation that led to the Act of Abjuration or "Plakkaat van Verlatinghe", signed on July 26, 1581 in the Netherlands, an instance where changing sides was given a positive meaning.

The first written use of the term meaning was by J. Foxe in "Actes & Monumentes" in 1570: "One who changes his principles or party; a renegade; an apostate." Cited 1571*

"Turncoat" could also have a more literal origin. According to the Rotuli Chartarum 1199–1216 two barons changed fealty from William Marshal, 1st Earl of Pembroke, to King John. In other words, they turned their coats (of arms) from one lord to another, hence turncoat.

A mass-shift in allegiance by a population may take place during military occupation, after a nation has been defeated in war or after a major social upheaval, such as a revolution. Following the initial traumatic times many of the citizens of the area in question quickly embrace the cause of the victors to benefit from the new system. This shift of allegiance is often done without much knowledge about the new order that is replacing the former one. In the face of fear and insecurity, the prime motive for a turncoat to draw away from former allegiances may be mere survival.

Often the leaders are the first to change loyalties, for they have had access to privileged information and are more aware of the hopelessness of the situation for their former cause. This is especially apparent in dictatorships and authoritarian states when most of the population has been fed propaganda and triumphalism and has been kept in the dark about important turns of events.

As time goes by, along with the embracing of life under the new circumstances comes a need of burying and rewriting the past by concealing evidence. The fear of the past coming to upset the newly found stability is always present in the mind of the turncoat. The past is rewritten and whitewashed to cover former deeds. When successful, this activity results in the distortion and falsification of historical events.

Even after the death of a turncoat his family and friends may wish to keep uncomfortable secrets from the past out of the light. There is a fear of loss of prestige as well as a wish to honor the memory of a family member from the part of those who have experienced the positive side of the person.

In certain countries, individuals and organizations have actively investigated the past to bring turncoats to justice to face their responsibilities.

There were many turncoats in:




</doc>
<doc id="19974664" url="https://en.wikipedia.org/wiki?curid=19974664" title="Institute for the Study of Human Knowledge">
Institute for the Study of Human Knowledge

The Institute for the Study of Human Knowledge (ISHK) is a non-profit educational charity and publisher established in 1969 by the noted and award-winning psychologist and writer Robert E. Ornstein and based in Los Altos, California, in the USA. Its watchword is "public education: health and human nature information."

Robert Ornstein, psychologist, writer and professor at Stanford University, founded and chairs ISHK. He has published over 25 books on the mind and won over a dozen awards from organizations over the years, including the American Psychological Association and the United Nations Educational, Scientific and Cultural Organization (UNESCO). His work has been featured in a 1974 "Time" magazine article entitled "Hemispheric Thinker". Ornstein is best known for his research on the hemispheric specialization of the brain and the advancement of understanding into how we think. He has also contributed to the London-based Institute for Cultural Research set up by his associate, the writer and Sufi teacher, Idries Shah. The ICR and its associate, Octagon Press have since been replaced by The Idries Shah Foundation and ISF Publishing, respectively.

Ornstein's "The Psychology of Consciousness" (1972) was enthusiastically received by the academic psychology community.
More recent works include "The Right Mind" (1997), described as "a cutting edge picture of how the two sides of the brain work".

Robert Ornstein died in December 2018.

ISHK's primary aim is public education, by providing new information on health and human nature through its book service, through its children's imprint Hoopoe Books and adult imprint Malor Books, which includes the works of Robert Ornstein. Hoopoe Books focuses on publishing traditional children's stories from Afghanistan, Central Asia and the Middle East, including works by Idries Shah, such as "The Lion Who Saw Himself in the Water."

The Institute also operates philanthropic projects, including Share Literacy, which provides books for children; support for caregivers; training and support for teachers, and independent program evaluation. Through its Share Literacy Program, Hoopoe Books has partnered with other organizations to give books away to children in low-income areas. It also provides books free of charge to lending libraries.

ISHK has worked with organizations such as The Institute for Cross-cultural Exchange to provide children in Afghanistan with desperately needed books for distribution to schools, orphanages and libraries throughout the country, in order to address the literacy crisis.

Events organized by ISHK include a symposium in 2006 on "The Core of Early Christian Spirituality: Its Relevance to the World Today" which featured presentations by Elaine Pagels, well known for her studies and writing on the Gnostic Gospels ("Beyond Belief: A Different View of Christianity"); New Testament scholar Bart D. Ehrman ("Jesus and the Apocalyptic Vision"), and scholar of religion and Professor, Marvin Meyer ("Magdalene in the Gnostic Gospels: From the Gospel of Mary to the DaVinci Code, Mary Magdelene in History and Culture"). In 1976, Robert Ornstein and Idries Shah presented a seminar, "Traditional Esoteric Psychologies in Contemporary Life", in cooperation with The New School, New York City.

In 2010, ISHK set up a web site for a project entitled "The Human Journey". It aims to "follow humanity from our origins in Eastern Africa and the Middle East to the present day, with an eye to what comes next."




</doc>
<doc id="20270864" url="https://en.wikipedia.org/wiki?curid=20270864" title="Environmental enrichment">
Environmental enrichment

Environmental enrichment is the stimulation of the brain by its physical and social surroundings. Brains in richer, more stimulating environments have higher rates of synaptogenesis and more complex dendrite arbors, leading to increased brain activity. This effect takes place primarily during neurodevelopment, but also during adulthood to a lesser degree. With extra synapses there is also increased synapse activity, leading to an increased size and number of glial energy-support cells. Environmental enrichment also enhances capillary vasculation, providing the neurons and glial cells with extra energy. The neuropil (neurons, glial cells, capillaries, combined together) expands, thickening the cortex. Research on rodent brains suggests that environmental enrichment may also lead to an increased rate of neurogenesis.

Research on animals finds that environmental enrichment could aid the treatment and recovery of numerous brain-related dysfunctions, including Alzheimer's disease and those connected to aging, whereas a lack of stimulation might impair cognitive development. Moreover, this research also suggests that environmental enrichment leads to a greater level of cognitive reserve, the brain's resilience to the effects of conditions such as aging and dementia.

Research on humans suggests that lack of stimulation delays and impairs cognitive development. Research also finds that attaining and engaging in higher levels of education, environments in which people participate in more challenging cognitively stimulating activities, results in greater cognitive reserve.

Donald O. Hebb in 1947 found that rats raised as pets performed better on problem solving tests than rats raised in cages. His research, however, did not investigate the brain nor use standardized impoverished and enriched environments. Research doing this first was started in 1960 at the University of California, Berkeley by Mark Rosenzweig, who compared single rats in normal cages, and those placed in ones with toys, ladders, tunnels, running wheels in groups. This found that growing up in enriched environments affected enzyme cholinesterase activity. This work led in 1962 to the discovery that environmental enrichment increased cerebral cortex volume. In 1964, it was found that this was due to increased cerebral cortex thickness and greater synapse and glial numbers.

Also starting around 1960, Harry Harlow studied the effects of maternal and social deprivation on rhesus monkey infants (a form of environmental stimulus deprivation). This established the importance of social stimulation for normal cognitive and emotional development.

Rats raised with environmental enrichment have thicker cerebral cortices (3.3–7%) that contain 25% more synapses. This effect of environmental richness upon the brain occurs whether it is experienced immediately following birth, after weaning, or during maturity. When synapse numbers increase in adults, they can remain high in number even when the adults are returned to impoverished environment for 30 days suggesting that such increases in synapse numbers are not necessarily temporary. However, the increase in synapse numbers has been observed generally to reduce with maturation. Stimulation affects not only synapses upon pyramidal neurons (the main projecting neurons in the cerebral cortex) but also stellate ones (that are usually interneurons). It also can affect neurons outside the brain in the retina.

Environmental enrichment affects the complexity and length of the dendrite arbors (upon which synapses form). Higher-order dendrite branch complexity is increased in enriched environments, as can the length, in young animals, of distal branches.

Synapses in animals in enriched environments show evidence of increased synapse activation. Synapses tend to also be much larger. Gamma oscillations become larger in amplitude in the hippocampus. This increased energy consumption is reflected in glial and local capillary vasculation that provides synapses with extra energy.
These energy related changes to the neuropil are responsible for increasing the volume of the cerebral cortex (the increase in synapse numbers contributes in itself hardly any extra volume).

Part of the effect of environmental enrichment is providing opportunities to acquire motor skills. Research upon “acrobatic” skill learning in the rat shows that it leads to increased synapse numbers.

Environmental enrichment during pregnancy has effects upon the fetus such as accelerating its retinal development.

Environmental enrichment can also lead to the formation of neurons (at least in rats) and reverses the loss of neurons in the hippocampus and memory impairment following chronic stress. However, its relevance has been questioned for the behavioral effects of enriched environments.

Enriched environments affect the expression of genes in the cerebral cortex and the hippocampus that determine neuronal structure. At the molecular level, this occurs through increased concentrations of the neurotrophins NGF, NT-3, and changes in BDNF. This alters the activation of cholinergic neurons, 5-HT, and beta-adrenolin. Another effect is to increase proteins such as synaptophysin and PSD-95 in synapses. Changes in Wnt signaling have also been found to mimic in adult mice the effects of environmental enrichment upon synapses in the hippocampus. Increase in neurons numbers could be linked to changes in VEGF.

Research in animals suggests that environmental enrichment aids in recovery from an array of neurological disorders and cognitive impairments. There are two mains areas of focus: neurological rehabilitation and cognitive reserve, the brain's resistance to the effects of exposure to physical, natural, and social threats. Although most of these experiments used animal subjects, mainly rodents, researchers have pointed to the affected areas of animal brains to which human brains are most similar and used their findings as evidence to show that humans would have comparable reactions to enriched environments. The tests done on animals are thus meant to represent human simulations for the following list of conditions.

A study conducted in 2011 led to the conclusion that environmental enrichment vastly improves the cognitive ability of children with autism. The study found that autistic children who receive olfactory and tactile stimulation along with exercises that stimulated other paired sensory modalities clinically improved by 42 percent while autistic children not receiving this treatment clinically improved by just 7 percent. The same study also showed that there was significant clinical improvement in autistic children exposed to enriched sensorimotor environments, and a vast majority of parents reported that their child's quality of life was much better with the treatment. A second study confirmed its effectiveness. The second study also found after 6 months of sensory enrichment therapy, 21% of the children who initially had been given an autism classification, using the Autism Diagnostic Observation Schedule, improved to the point that, although they remained on the autism spectrum, they no longer met the criteria for classic autism. None of the standard care controls reached an equivalent level of improvement. The therapy using the methodologies is titled Sensory Enrichment Therapy.

Through environmental enrichment, researchers were able to enhance and partially repair memory deficits in mice between ages of 2 to 7 months with characteristics of Alzheimer's disease. Mice in enriched environments performed significantly better on object recognition tests and the Morris Water Maze than they had when they were in standard environments. It was thus concluded that environmental enrichment enhances visual and learning memory for those with Alzheimer's. Furthermore, it has been found that mouse models of Alzheimer's disease that were exposed to enriched environment before amyloid onset (at 3 months of age) and then returned to their home cage for over 7 months, showed preserved spatial memory and reduced amyloid deposition at 13 months old, when they are supposed to show dramatic memory deficits and amyloid plaque load. These findings reveal the preventive, and long-lasting effects of early life stimulating experience on Alzheimer-like pathology in mice and likely reflect the capacity of enriched environment to efficiently stimulate the cognitive reserve.

Research has indicated that environmental enrichment can help relieve motor and psychiatric deficits caused by Huntington's disease. It also improves lost protein levels for those with the disease, and prevents striatal and hippocampal deficits in the BDNF, located in the hippocampus. These findings have led researchers to suggest that environmental enrichment has a potential to be a possible form of therapy for those with Huntington's.

Multiple studies have reported that environmental enrichment for adult mice helps relieve neuronal death, which is particularly beneficial to those with Parkinson's disease. A more recent study shows that environmental enrichment particularly affects the nigrostriatal pathway, which is important for managing dopamine and acetylcholine levels, critical for motor deficits. Moreover, it was found that environmental enrichment has beneficial effects for the social implications of Parkinson's disease.

Research done in animals has shown that subjects recovering in an enriched environment 15 days after having a stroke had significantly improved neurobehavioral function. In addition these same subjects showed greater capability of learning and larger infarct post-intervention than those who were not in an enriched environment. It was thus concluded that environmental enrichment had a considerable beneficial effect on the learning and sensorimotor functions on animals post-stroke. A 2013 study also found that environmental enrichment socially benefits patients recovering from stroke. Researchers in that study concluded that stroke patients in enriched environments in assisted-care facilities are much more likely to be engaging with other patients during normal social hours instead of being alone or sleeping.

A 2008 study found that environmental enrichment was significant in aiding recovery of motor coordination and some recovery of BDNF levels in female mice with conditions similar to those of Rett syndrome. Over the course of 30 weeks female mice in enriched environments showed superior ability in motor coordination to those in standard conditions. Although they were unable to have full motor capability, they were able to prevent a more severe motor deficit by living in an enriched environment. These results combined with increased levels of BDNF in the cerebellum led researchers to conclude that an enriched environment that stimulates areas of the motor cortex and areas of the cerebellum having to do with motor learning is beneficial in aiding mice with Rett syndrome.

A recent study found that adult rats with amblyopia improved visual acuity two weeks after being placed into an enriched environment. The same study showed that another two weeks after ending environmental enrichment, the rats retained their visual acuity improvement. Conversely, rats in a standard environment showed no improvement in visual acuity. It was thus concluded that environmental enrichment reduces GABA inhibition and increases BDNF expression in the visual cortex. As a result, the growth and development of neurons and synapses in the visual cortex were much improved due to the enriched environment.

Studies have shown that with the help of environmental enrichment the effects of sensory deprivation can be corrected. For example, a visual impairment known as "dark-rearing" in the visual cortex can be prevented and rehabilitated. In general, an enriched environment will improve, if not repair, the sensory systems animals possess.

During development, gestation is one of the most critical periods for exposure to any lead. Exposure to high levels of lead at this time can lead to inferior spatial learning performance. Studies have shown that environmental enrichment can overturn damage to the hippocampus induced by lead exposure. Learning and spatial memory that are dependent on the long-term potentiation of the hippocampus are vastly improve as subjects in an enriched environments had lower levels of lead concentration in their hippocampi. The findings also showed that enriched environments result in some natural protection of lead-induced brain deficits.

Research has indicated that animals suffering from spinal cord injuries showed significant improvement in motor capabilities even with a long delay in treatment after the injury when exposed to environmental enrichment. Social interactions, exercise, and novelty all play major roles in aiding the recovery of an injured subject. This has led to some suggestions that the spinal cord has a continued plasticity and all efforts must be made for enriched environments to stimulate this plasticity in order to aid recovery.

Maternal deprivation can be caused by the abandonment by a nurturing parent at a young age. In rodents or nonhuman primates, this leads to a higher vulnerability for
stress-related illness. Research suggests that environmental enrichment can reverse the effects of maternal separation on stress reactivity, possibly by affecting the hippocampus and the prefrontal cortex.

In all children, maternal care is one of the significant influences for hippocampal development, providing the foundation for stable and individualized learning and memory. However, this is not the case for those who have experienced child neglect. Researchers determined that through environmental enrichment, a neglected child can partially receive the same hippocampal development and stability, albeit not at the same level as that of the presence of a parent or guardian. The results were comparable to those of child intervention programs, rendering environmental enrichment a useful method for dealing with child neglect.

Decreased hippocampal neurogenesis is a characteristic of aging. Environmental enrichment increases neurogenesis in aged rodents by potentiating neuronal differentiation and new cell survival. As a result, subjects exposed to environmental enrichment aged better due to superior ability in retaining their levels of spatial and learning memory.

Research has shown that mice exposed to environmental enrichment are less affected by the consequences of cocaine exposure in comparison with those in standard environments. Although the levels of dopamine in the brains of both sets of mice were relatively similar, when both subjects were exposed to the cocaine injection, mice in enriched environment were significantly less responsive than those in standard environments. It was thus concluded that both the activating and rewarding effects are suppressed by environmental enrichment and early exposure to environmental enrichment can help prevent drug addiction.

Though environmental enrichment research has been mostly done upon rodents, similar effects occur in primates, and are likely to affect the human brain. However, direct research upon human synapses and their numbers is limited since this requires histological study of the brain. A link, however, has been found between educational level and greater dendritic branch complexity following autopsy removal of the brain.

MRI detects localized cerebral cortex expansion after people learn complex tasks such as mirror reading (in this case in the right occipital cortex), three-ball juggling (bilateral mid-temporal area and left posterior intraparietal sulcus), and when medical students intensively revise for exams (bilaterally in the posterior and lateral parietal cortex). Such changes in gray matter volume can be expected to link to changes in synapse numbers due to the increased numbers of glial cells and the expanded capillary vascularization needed to support their increased energy consumption.

Children that receive impoverished stimulation due to being confined to cots without social interaction or reliable caretakers in low quality orphanages show severe delays in cognitive and social development. 12% of them if adopted after 6 months of age show autistic or mildly autistic traits later at four years of age. Some children in such impoverished orphanages at two and half years of age still fail to produce intelligible words, though a year of foster care enabled such children to catch up in their language in most respects. Catch-up in other cognitive functioning also occurs after adoption, though problems continue in many children if this happens after the age of 6 months

Such children show marked differences in their brains, consistent with research upon experiment animals, compared to children from normally stimulating environments. They have reduced brain activity in the orbital prefrontal cortex, amygdala, hippocampus, temporal cortex, and brain stem. They also showed less developed white matter connections between different areas in their cerebral cortices, particularly the uncinate fasciculus.

Conversely, enriching the experience of preterm infants with massage quickens the maturating of their electroencephalographic activity and their visual acuity. Moreover, as with enrichment in experimental animals, this associates with an increase in IGF-1.

Another source of evidence for the effect of environment stimulation upon the human brain is cognitive reserve (a measure of the brain’s resilience to cognitive impairment) and the level of a person’s education. Not only is higher education linked to a more cognitively demanding educational experience, but it also correlates with a person’s general engagement in cognitively demanding activities. The more education a person has received, the less the effects of aging, dementia, white matter hyperintensities, MRI-defined brain infarcts, Alzheimer's disease, and traumatic brain injury. Also, aging and dementia are less in those that engage in complex cognitive tasks. The cognitive decline of those with epilepsy could also be affected by the level of a person’s education.




</doc>
<doc id="13774844" url="https://en.wikipedia.org/wiki?curid=13774844" title="Religious behaviour">
Religious behaviour

Religious behaviours are behaviours motivated by religious beliefs. Religious actions are also called 'ritual' and religious avoidances are called taboos or ritual prohibitions.

The two best known religious actions are prayer and sacrifice. The most general religious action is prayer. It can be done quietly by a person all alone, but people can also pray in groups using songs. Sacrifice is also a widely spread religious action. Prayer and sacrifice often form the basis of other, more complicated religious actions like pilgrimage, processions, or consulting an oracle. Many rituals are connected to a certain purpose, like initiation, ritual purification and preparation for an important happening or task. Among these are also the so-called rituals of transition, which occur at important moments of the human life cycle, like birth, adulthood/marriage, sickness and death. A special religious action is spirit possession and religious ecstasy. Religious specialists, such as priests, vicars, rabbis, imams and pandits are involved in many religious actions.

A religious avoidance is when a person desists from something or from some action for religious reasons. It can be food or drink that one does not touch because of one's religion for some time (fast). This abstinence can also be for a longer time. Some people do not have sex (celibacy). Or one avoids contact with blood, or dead animals. Well known examples are: Jews and Muslims do not eat pork; the celibacy of Catholic priests; the purity rules of Hinduism and Judaism.

These avoidances, or 'taboos', are often about food and drink.

Religious avoidances are often not easily recognisable as (part of) religious behaviour. When asked, the believers often do not motivate this kind of behaviour explicitly as religious but say the avoidance for health reasons, ethical reasons, or because it is hygienic.

Religious behaviour is seldom studied for itself. When it is given attention at all, it is usually studied as an illustration of the religious images, like in comparative religion and cultural anthropology, or as part of the study of man in the social sciences.




</doc>
<doc id="20853872" url="https://en.wikipedia.org/wiki?curid=20853872" title="Health action process approach">
Health action process approach

The health action process approach (HAPA) is a psychological theory of health behavior change, developed by Ralf Schwarzer, Professor of Psychology at the Free University of Berlin, Germany.

Health behavior change refers to a replacement of health-compromising behaviors (such as sedentary behavior) by health-enhancing behaviors (such as physical exercise). To describe, predict, and explain such processes, theories or models are being developed. Health behavioural change theories are designed to examine a set of psychological constructs that jointly aim at explaining what motivates people to change and how they take preventive action.

HAPA is an open framework of various motivational and volitional constructs that are assumed to explain and predict individual changes in health behaviors such as quitting smoking or drinking, and improving physical activity levels, dental hygiene, seat belt use, breast self-examination, dietary behaviors, and avoiding drunk driving. HAPA suggests that the adoption, initiation, and maintenance of health behaviors should be conceived of as a structured process including a motivation phase and a volition phase. The former describes the intention formation while the latter refers to planning, and action (initiative, maintenance, recovery). The model emphasizes the particular role of perceived self-efficacy at different stages of health behavior change.

Models that describe health behavior change can be distinguished in terms of the assumption whether they are continuum-based or stage-based. A continuum (mediator) model claims that change is a continuous process that leads from lack of motivation via action readiness either to successful change or final disengagement. Research on such mediator models are reflected by path diagrams that include distal and proximal predictors of the target behavior. On the other hand, the stage approach assumes that change is non-linear and consists of several qualitative steps that reflect different mindsets of people. A two-layer framework that can be applied either as a continuum or as a stage model is HAPA. It includes self-efficacy, outcome expectancies, and risk perception as distal predictors, intention as a middle-level mediator, and volitional factors (such as action planning) as the most proximal predictors of behavior. "See Self-efficacy."

Good intentions are more likely to be translated into action when people plan when, where, and how to perform the desired behavior. Intentions foster planning, which in turn facilitates behavior change. Planning was found to mediate the intention-behavior relation. A distinction has been made between action planning and coping planning. Coping planning takes place when people imagine scenarios that hinder them to perform their intended behavior, and they develop one or more plans to cope with such a challenging situation.

HAPA is designed as a sequence of two continuous self-regulatory processes, a goal-setting phase (motivation) and a goal-pursuit phase (volition). The second phase is subdivided into a pre-action phase and an action phase. Thus, one can superimpose these three phases (stages) on the continuum (mediator) model as a second layer, and regard the stages as moderators. This two-layer architecture allows to switch between the continuum model and the stage model, depending on the given research question.

HAPA has five major principles that make it distinct from other models.

"Principle 1: Motivation and volition". The first principle suggests that one should divide the health behavior change process into two phases. There is a switch of mindsets when people move from deliberation to action. First comes the motivation phase in which people develop their intentions. Afterwards, they enter the volition phase.

"Principle 2: Two volitional phases". In the volition phase there are two groups of individuals: those who have not yet translated their intentions into action, and those who have. There are inactive as well as active persons in this phase. In other words, in the volitional phase one finds intenders as well as actors who are characterized by different psychological states. Thus, in addition to health behavior change as a continuous process, one can also create three categories of people with different mindsets depending on their current point of residence within the course of health behavior change: preintenders, intenders, and actors. The assessment of stages is done by behavior-specific stage algorithms.

"Principle 3: Postintentional planning". Intenders who are in the volitional preactional stage are motivated to change, but do not act because they might lack the right skills to translate their intention into action. Planning is a key strategy at this point. Planning serves as an operative mediator between intentions and behavior.

"Principle 4: Two kinds of mental simulation". Planning can be divided into action planning and coping planning. Action planning pertains to the when, where, and how of intended action. Coping planning includes the anticipation of barriers and the design of alternative actions that help to attain one's goals in spite of the impediments. The separation of the planning construct into two constructs, action planning and coping planning, has been found useful as studies have confirmed the discriminant validity of such a distinction. Action planning seems to be more important for the initiation of health behaviors, whereas coping planning is required for the initiation and maintenance of actions as well.

"Principle 5: Phase-specific self-efficacy". Perceived self-efficacy is required throughout the entire process. However, the nature of self-efficacy differs from phase to phase. This difference relates to the fact that there are different challenges as people progress from one phase to the next one. Goal setting, planning, initiation, action, and maintenance pose challenges that are not of the same nature. Therefore, one should distinguish between preactional self-efficacy, coping self-efficacy, and recovery self-efficacy. Sometimes the terms task self-efficacy instead of preaction self-efficacy, and maintenance self-efficacy instead of coping and recovery self-efficacy are preferred.

When it comes to the design of interventions, one can consider identifying individuals who reside either at the motivational stage or the volitional stage. Then, each group becomes the target of a specific treatment that is tailored to this group. Moreover, it is theoretically meaningful and has been found useful to subdivide further the volitional group into those who perform and those who only intend to perform. In the postintentional preactional stage, individuals are labeled "intenders", whereas in the actional stage they are labeled "actors". Thus, a suitable subdivision within the health behavior change process yields three groups: nonintenders, intenders, and actors. The term "stage" in this context was chosen to allude to the stage theories, but not in the strict definition that includes irreversibility and invariance. The terms "phase" or "mindset" may be equally suitable for this distinction. The basic idea is that individuals pass through different mindsets on their way to behavior change. Thus, interventions may be most efficient when tailored to these particular mindsets. For example, nonintenders are supposed to benefit from confrontation with outcome expectancies and some level of risk communication. They need to learn that the new behavior (e.g., becoming physically active) has positive outcomes (e.g., well-being, weight loss, fun) as opposed to the negative outcomes that accompany the current (sedentary) behavior (such as developing an illness or being unattractive). In contrast, intenders should not benefit from such a treatment because, after setting a goal, they have already moved beyond this mindset. Rather, they should benefit from planning to translate their intentions into action. Finally, actors do not need any treatment at all unless one wants to improve their relapse prevention skills. Then, they should be prepared for particular high-risk situations in which lapses are imminent. Preparation can be exercised by teaching them to anticipate such situations and by acquiring the necessary levels of perceived recovery self-efficacy.
There are quite a few randomized controlled trials that have examined the notion of stage-matched interventions based on HAPA, for example in the context of dietary behaviors, physical activity,
and dental hygiene.





</doc>
<doc id="1239477" url="https://en.wikipedia.org/wiki?curid=1239477" title="Eccentricity (behavior)">
Eccentricity (behavior)

Eccentricity (also called quirkiness) is unusual or odd behavior on the part of an individual. This behavior would typically be perceived as unusual or unnecessary, without being demonstrably maladaptive. Eccentricity is contrasted with normal behavior, the nearly universal means by which individuals in society solve given problems and pursue certain priorities in everyday life. People who consistently display benignly eccentric behavior are labeled as "eccentrics".

From Medieval Latin "eccentricus", derived from Greek "ekkentros", "out of the center", from "ek"-, "ex"- "out of" + "kentron", "center". "Eccentric" first appeared in English essays as a neologism in 1551 as an astronomical term meaning "a circle in which the earth, sun, etc. deviates from its center." Five years later, in 1556, an adjective form of the word was used. In 1685, the definition evolved from the literal to the figurative, and "eccentric" is noted to have begun being used to describe unconventional or odd behavior. A noun form of the word – a person who possesses and exhibits these unconventional or odd qualities and behaviors – appeared by 1832.

Eccentricity is often associated with genius, intellectual giftedness, or creativity. People may perceive the individual's eccentric behavior as the outward expression of their unique intelligence or creative impulse. In this vein, the eccentric's habits are incomprehensible not because they are illogical or the result of madness, but because they stem from a mind so original that it cannot be conformed to societal norms. English utilitarian thinker John Stuart Mill () wrote that "the amount of eccentricity in a society has generally been proportional to the amount of genius, mental vigour, and moral courage which it contained," and mourned a lack of eccentricity as "the chief danger of the time". Edith Sitwell () wrote that eccentricity is "often a kind of innocent pride", also saying that geniuses and aristocrats are called eccentrics because "they are entirely unafraid of and uninfluenced by the opinions and vagaries of the crowd". Eccentricity is also associated with great wealth. What would be considered signs of insanity in a poor person, some may accept as eccentricity in wealthy people.

A person who is simply in a "fish out of water" situation is not, by the strictest definition, an eccentric since (presumably) he or she may be ordinary by the conventions of his or her native environment.

Eccentrics may or may not comprehend the standards for normal behavior in their culture. They are simply unconcerned by society's disapproval of their habits or beliefs. Many of history's most brilliant minds have displayed some unusual behaviors and habits.

Some eccentrics are pejoratively considered "cranks" rather than geniuses. Eccentric behavior is often considered whimsical or quirky, although it can also be strange and disturbing. Many individuals previously considered merely eccentric, such as aviation magnate Howard Hughes, have recently been retrospectively diagnosed as actually having had mental disorders (obsessive–compulsive disorder in Hughes' case).

Other people may have an eccentric taste in clothes, or eccentric hobbies or collections they pursue with great vigor. They may have a pedantic and precise manner of speaking, intermingled with inventive wordplay.

Many individuals may even manifest eccentricities consciously and deliberately in an attempt to differentiate themselves from societal norms or enhance a sense of inimitable identity. Given the overwhelmingly positive stereotypes (at least in popular culture and especially with fictional characters) often associated with eccentricity, as detailed above, certain individuals seek to be associated with this sort of character type. However, this is not always successful as eccentric individuals are not necessarily charismatic and the individual in question may simply be dismissed by others as just seeking attention.

"Extravagance" is a kind of eccentricity, related to abundance and wastefulness; refer to description in "hyperbole".

Psychologist David Weeks believes people with a mental illness "suffer" from their behavior while eccentrics are quite happy. He even states eccentrics are "less" prone to mental illness than everyone else.

According to Weeks' study, there are several distinctive characteristics that often differentiate a healthy eccentric person from a regular person or someone who has a mental illness. The first five characteristics on Weeks' list are found in most people regarded as eccentric:




</doc>
<doc id="14833451" url="https://en.wikipedia.org/wiki?curid=14833451" title="Impulse (psychology)">
Impulse (psychology)

An impulse is a wish or urge, particularly a sudden one. It can be considered as a normal and fundamental part of human thought processes, but also one that can become problematic, as in a condition like obsessive-compulsive disorder, borderline personality disorder, and attention deficit hyperactivity disorder.

The ability to control impulses, or more specifically control the desire to act on them, is an important factor in personality and socialization. Deferred gratification, also known as impulse control is an example of this, concerning impulses primarily relating to things that a person wants or desires.


</doc>
<doc id="21482040" url="https://en.wikipedia.org/wiki?curid=21482040" title="Loner">
Loner

A loner is a person who avoids or does not actively seek human interaction. There are many reasons for solitude, intentional or otherwise. Intentional reasons include being introverted, spiritual, mystic, or religious considerations, or personal philosophies. Unintentional reasons involve being highly sensitive, shy, past trauma or events, or having various mental disorders.

The modern term "loner" can be used with a negative connotation in the belief that human beings are social creatures and those that do not participate are deviant. Being a loner is sometimes depicted culturally as a positive personality trait, as indicative of being independent and responsible. Someone who is a recluse or solitary in the context of relationship status can be referred to by various terms, including, singleton, nonwedder, as well as gender-specific and pejorative terms such as "dried-fish woman" or incel. Loners are sometimes labeled with demeaning stereotypes such as "misanthrope" as well as the ramifications of such a perception, such as being perceived as an alien outcast or misfit.

There are different types of loner, including individuals that prefer solitude and are content to have very limited social interaction. A second type includes individuals that are forced into isolation because they are rejected by society. This individual typically experiences loneliness. A third type of loner is an individual that likes to be social and has lots of social interactions, but prefers solitude without feelings of loneliness. 

The first type of loner often does not feel lonely even when they are alone, at least, not in the same way as would a social person who found themselves isolated. However, these are very broad generalizations and it is not uncommon for loners to experience both of these dimensions at some point. Being a loner can sometimes be a symptom of certain mental illnesses such as depression, schizophrenia, or related to autism. People with autism, for instance, may have difficulty with social interactions and limited or restrictive interests and routines which would make it more likely for the person to be a loner. Being a loner is also sometimes associated with fully functional individuals who have certain atypical personality traits, such as alexithymia. The characteristics of loners are sometimes attributed to non-human animals such as the leopard, an animal whose behaviour is usually defined by being solitary.

While expressing a desire to be alone, loners may not reject human contact entirely. An example would be the person who shuns any social interaction with colleagues beyond what is necessary for fulfilling his or her job description (mainly for practical reasons and to avoid further complicating one's professional relationships) but who is highly charismatic during parties or social gatherings with people outside work or school, or vice versa.

Somebody who can be a loner would also fit the criteria for introversion. This may be due to both innate personality traits as well as life experiences. Loners often attend movies and other public events alone, exhibiting their strength and inward focus on enjoying life without needing others. More solitary hobbies and interests such as reading, art, and meditation are common among loners.





</doc>
<doc id="4811617" url="https://en.wikipedia.org/wiki?curid=4811617" title="Taunting">
Taunting

A taunt is a battle cry, sarcastic remark, gesture, or insult intended to demoralize the recipient, or to anger them and encourage reactionary behaviors without thinking. Taunting can exist as a form of social competition to gain control of the target's cultural capital (i.e. status). In sociological theory, the control of the three social capitals is used to produce an advantage in the social hierarchy as to enforce one's own position in relation to others. Taunting is committed by either directly, or indirectly encouraging others to taunt the target. The target may give a response in kind to maintain status, as in fighting words and trash-talk.

Taunts are also a genre of folklore.

The act of taunting can be learned by observation and improvisation. It usually follows linear thought, correlating or building in some manner to the target of taunting. Things such as the victim's appearance, intelligence, mannerisms, education, background, past offences, etc. can otherwise be insulted. When used in this manner, the effectiveness of a taunt at provoking a response varies depending on how the specific insult relates to its victim (or their sense of self), to what level of offence they regard the taunt, and how well the victim can control their emotions when responding.

In the Eastern US and modern Britain the chant "Nyah nyah nyah nyah nyah nyah", sung to the tune of "Bye, baby Bunting" is insult among children. In the American South this is often used as "Nanny nanny boo-boo" and repeated with words such as "You ca-an't catch me". In Hebrew, the taunt is "Na na, banana" or "Na-na-na banana" (means the same as in English). In French, the taunt uses syllables often rendered "Nananananère," and Swedish-speaking children use the phrase "Du kan inte ta mig" ("You cannot catch me"). In Croatia, children sing in that tune: "Ulovi me, ulovi me, kupit ću ti novine. Novine su skupe, poljubi me u dupe.", which means: "Catch me, catch me, [if you do that] I'll buy you a newspaper. Newspapers are pricey, kiss my tushie."

Certain movements of one's body are, in many cultures, interpreted as a taunt. These can be expressed through the eyes, hands, fingers, head and other areas of the body.

A gesture in Japanese culture, made by pulling a lower eyelid down to expose the red underneath.

A raised, clenched fist is used as a gesture of defiance by a number of groups. It is usually considered to be hostile, yet without any sexual, scatological, or notionally offensive connotations.

The crotch-grab is done almost exclusively by males. It is, as the name suggests, a grabbing (or one-handed cupping and lifting) of the penis and testicles - usually through clothing. In Italy the sign is by no means purely a taunt, being also an apotropaic gesture of considerable antiquity employed, since the days of Ancient Rome, to ward off the evil eye or bad luck and also to attract good luck. It is, in this context, an invocation of the benign powers of fertility embodied in the male genitalia and, as such, lies at the root of the magical intent expressed symbolically in the fascinum and probably also the cornicello. Despite recent prudish rulings by the Italian legal system, the (public) crotch-grab is still resorted to by more traditionally-minded Italian men as a means of deflecting the ill-luck threatened by objects or people related to death and burial and (more esoterically) the unlucky number 17 (said to be unlucky because it a) resembles a man hanging from a gibbet and b) because when written XVII in Roman numerals is an anagram of 'vixi' - 'I lived', a verb form considered unlucky because of its frequent occurrence in ancient Roman funerary inscriptions).

The cutthroat gesture is performed by drawing the hand, or a finger or two, across the throat. It represents slitting the throat with a knife, and means that the gesturer or someone else is metaphorically being killed. It is rarely if ever used literally to refer to death, though it is occasionally used as a theatrical threat ("I'm going to kill you"). The gesture earned a great deal of national notoriety in the NFL during the 1999 season in which several players did the cutthroat gesture.

The dickhead gesture is made by holding a hand to one's forehead, the thumb and fingers usually forming a "C" shape, and then moving the hand forward and backward in an arc. The image suggested is a large penis growing out of the forehead.

A more insulting form is where the thumb and finger are brought closer together so the gap between them is smaller and more elongated with the hand moved in the same manner. The image suggests a small penis growing out of the forehead.

The finger gesture is a gesture consisting of a fist with the middle finger extended. It is universally understood as "fuck you" due to its resemblance to the penis. It is certainly thousands of years old, being referred to in Ancient Roman literature as the "digitus infamis" or "digitus impudicus". Performing this gesture is also called "flipping the bird" in countries where "the finger" is used. In other regions, "flipping the bird" refers to the raising of the middle and index finger with the back of the hand directed at the recipient. It can also mean "Victor" in some countries, which is not to be mistaken for the "Peace" gesture. The "Peace" gesture is done with the palm facing the recipient of the gesture. In Britain, this is also the case - however, if the palm faces inwards (towards the person doing the "peace" sign), it is an offensive gesture in Britain, though not considered quite as rude as to "the finger" (but still very rude and offensive).

The "loser" gesture used in some countries is performed by raising the index finger and thumb of one's right hand perpendicular to each other and then placing them on one's forehead with index finger pointing upward. So placed, the fingers form the letter "L" from the perspective of a viewer and signify the name-calling insult "loser" directed toward the person being spoken to or spoken about.

Done by holding up the hand with the index, middle and pinky finger, implying the act of putting two fingers in a woman's vagina and one in her anus.

Often sticking one's tongue out at another is seen as mocking the other. A variation of this is also known as blowing a raspberry. It can also be wagged in a manner suggesting cunnilingus which is usually seen as highly vulgar.

The turkey face gesture is when you take your hand and put your thumb on your nose, wriggle your head back and forth and do the same thing with the hand. Cocking a snook is an old British taunting gesture in which the thumb of one hand is on the nose and the extended fingers are wiggled.

The insulting version of the gesture (with the palm inwards) is often compared to the offensive gesture known as "the finger". The "two-fingered salute", as it is also known, is commonly performed by flicking the V upwards from wrist or elbow. The V sign, when the palm is facing toward the person giving the sign, has long been an insulting gesture in England, and later in the rest of the United Kingdom; its use is largely restricted to the UK, Ireland, Australia, and New Zealand. It is frequently used to signify defiance (especially to authority), contempt or derision.

The wanker gesture is made with a loose fist (with all fingers forming a cylindrical shape), and shaken up and down (or sometimes, back and forth) at the wrist, suggesting masturbation.
A picture of the young Tony Blair, later the British Prime Minister (1997–2007), using the wanker gesture became widely available (although copyrighted) in 2007.

In Monty Python's Monty Python and the Holy Grail the French Knight taunts King Arthur and his companions with a series of increasingly ludicrous insults, culminating in "Your mother was a hamster and your father smelt of elderberries".

Versions of the Endemol quiz show 1 vs. 100 based on the United States version (Australia, and to an extent, France) are known for the contestants and mob taunting each other.

Similar game shows, such as Weakest Link, are built on taunting a defeated player.

Some video games feature the ability to taunt an opponent. In the context of role-playing games, a "taunt" command is often used by the tank to draw the AI opponents' attention to the player's character, saving other more fragile characters from its attacks so that they can perform specialized roles. In a first-person shooter context or 1 vs 1 games, a "taunt" command is essentially a virtual incarnation of a verbal taunt, and may include either purpose-programmed sounds and gestures or a symbolic ritual insult that players have adopted to show dominance. In the context of wrestling or fighting games, a taunt may serve the purpose of building energy or stamina.

In games not featuring a dedicated "taunt" command, players have devised other ways, within the controls of the game, to taunt or harass opponents of other skill levels. In a racing game, for example, a player far in the lead might come to a stop before the finish line to watch their competitors begin to catch up, only to accelerate again and take the checkered flag when the opponents draw near. Multiplayer FPS games have given rise to the practice of corpse humping or tea bagging, which involves the "crouch" command present in a typical FPS's control scheme. In FPS games which allow hand-to-hand attacks, another common way of taunting opponents is to kill an opponent while one is not armed with a gun, through the use of some sort of melee attack such as a punch or knifing; "Quake III Arena" makes this form of "taunting" explicit by having the announcer loudly call out "Humiliation!" whenever a player is killed by another player's gauntlet.

In the online PC game sensation "World of Warcraft", the classes of Warrior and Druid have the ability named "Taunt" and "Growl" respectively and used to focus the attack of an enemy non playing character (NPC) onto the Warrior or Druid who have used this ability. These classes also have an ability which focuses the attacks of all creatures in an area, commonly referred to as Area of Effect taunt (AOE). The Warrior ability is called "Challenging shout" and the Druid ability is called "Challenging roar". The Paladin and Death Knight classes possess taunts as well. Taunting isn't limited to game skills; it exists also as an emote (by typing /taunt in the game's chat feature). This variant of taunting has also been featured in turn-based RPGs, such as "", in which tank characters can taunt to force all enemies to attack him/her, while having the health and/or defense to withstand all of such attacks so that allies can avoid every attack except area of effect ones for a limited number of turns.

In the "Super Smash Bros." series, characters have a brief taunt that can be performed by pressing a button. Luigi's is the only taunt that damages the foe, except in Super Smash Bros Brawl and Super Smash Bros Ultimate, where Solid Snake also possesses a damaging taunt. Kirby's taunt removes his current "transformation". Since "Super Smash Bros. Brawl", characters have had 3 different taunts each. If a Down Taunt is executed very quickly, Star Fox characters, Pit, and Solid Snake can contact allies on their home stages (known as a Smash Taunt), while (in Brawl), Samus Aran can remove her power suit if the Up and Down taunts are repeated very quickly.

In the online game Team Fortress 2, each character possesses a unique taunt depending on the weapon he is holding, an example of which is when the Demoman lifts his crotch armor to reveal a piece of paper with a smiley-face on it, shouting "Ka-BOOM!" Some taunts can kill opponents, and two can restore health. There are also special taunts, including some that require a partner to execute, that can be obtained as items.

In the "Saints Row" games, players can design their own character and choose from numerous taunts in order to make his/her enemies or pedestrians feel bad, thus engaging in a fist fight. This feature is also available in the second game, now that the character has the ability to speak in which he/she can trash-talk his/her pedestrians and enemy gangs nearby. Performing taunts will earn the player respect.

In "MotorStorm" game, drivers can perform taunts to other drivers in order to humiliate them. These can be performed by any vehicle the player derives, but so do other rivals. In "", the use of taunt has been expanded upon and these are done by performing the infamous "the finger" next to another driver. Other taunts can be performed; however, doing this can make the rival attack the player, thus causing the player to crash.

In some Pokémon video games, there is a Dark-type move called Taunt. Once it has been used, it only allows the other player to use moves that can inflict damage, instead of using moves that affect stats, status, weather, etc.

In The Secret of Monkey Island, the main character learns taunts and retorts in a process called insult sword fighting. The learned retorts are used to counter a second set of taunts later. The sequels feature several similar insult games.

In Heroes of Newerth, users who pre-purchased before the open beta began, can use the ability "Taunt" to possibly score a "smackdown" announcement. It happens when the taunting player lands the killing blow on the taunted target. However, if the taunted target kills the taunter, "HUMILIATION" announcement is displayed.

In Mirror's Edge, the main character can pull the V sign, if you execute a certain combination.

In "Grand Theft Auto V" and "Grand Theft Auto Online" the protagonists can pull the finger to the pedestrians when driving a vehicle while unarmed.

In the Dragon Ball fighting game , Taunt is a Super Skill used to force enemies to lock-on to the user turning the opponent's attention to the user. When performed the user places their hands on the sides of their hand and make faces while dancing from side-to-side.



</doc>
<doc id="9128611" url="https://en.wikipedia.org/wiki?curid=9128611" title="Infomania">
Infomania

Infomania is the debilitating state of information overload, caused by the combination of a backlog of information to process (usually in e-mail), and continuous interruptions from technologies like phones, instant messaging, and e-mail. It is also defined as an obsessive need to constantly check social media, online news, and emails to acquire knowledge. There is a constant need to know what is going on at all times because of a fear of missing out (FOMO). This can affect how well someone can operate at work or in the classroom, as well as the possibility of becoming addicted to the technology used to obtain information. With the new technological age, information has become easier to obtain, therefore infomania has become more common. A typical symptom of infomania is checking e-mail frequently during vacation.

The term "infomania" has been used since the 1980s, but has only recently been used as a term for a psychological debility. To date, the term infomania is not used to refer to any recognized psychological disorder, and infomania is not generally recognized as causing significant impairment.

The term was coined by Elizabeth M. Ferrarini, the author of "Confessions of an Infomaniac" (1984) and "Infomania: The Guide to Essential Electronic Services" (1985). "Confessions" was an early book about life online. It was excerpted in "Cosmopolitan" in 1982.

In 2005, Dr. Glenn Wilson conducted an experimental study which described effects of information overload on problem solving ability. The 80 volunteers carried out problem solving tasks in a quiet space and then while being bombarded with new emails and phone calls that they could not answer. Results showed a reduction in IQ by an average of 10 points during the bombardment session, but not everyone was affected to the same extent; men were distracted more than women. In 2010, Dr. Glenn Wilson published a clarifying note about the study in which he documented the limited size of the study and stated the results were "widely misrepresented in the media".

Wilson compares working while having an incoming of calls and email can reduce someone’s ability to focus as much as losing a night’s sleep. Not only can it affect one’s ability to function below their full potential at a job or in class, but it has been found that it can become addicting using technology as well. For example, how often have you found yourself on your phone checking work emails during a lunch with family on the weekend? This is just one of many examples of the addiction effect of infomania.

There have not been any long-term studies on the effects of infomania, but studies on Fear of Missing Out, which involves compulsively checking in on the experiences of others via social media show the effects of constant interruptions. A study by Gloria Mark at UC Irvine concluded interruptions result in "more stress, higher frustration, time pressure and effort" and it took an average of 23 minutes to return to an original task after an interruption.




</doc>
<doc id="22505590" url="https://en.wikipedia.org/wiki?curid=22505590" title="Uncle Tom syndrome">
Uncle Tom syndrome

Uncle Tom syndrome is a concept in psychology. It refers to a coping skill where individuals use passivity and submissiveness when confronted with a threat, leading to subservient behaviour and appeasement, while concealing their true thoughts and feelings. The term "Uncle Tom" comes from the title character of Harriet Beecher Stowe's novel "Uncle Tom's Cabin", where the African American slave Tom is beaten to death by a cruel white master for refusing to betray the whereabouts of two other slaves.

In the American racial context, "Uncle Tom" is a pejorative term for African-Americans that give up or hide their ethnic or gender outlooks, traits, and practices, in order to be accepted into the mainstream. 

In race minority literature Uncle Tom syndrome refers to African Americans that, as a necessary survival technique, opt to appear docile, non-assertive, and happy-go-lucky. Especially during slavery, African Americans used passivity and servility for the avoidance of retaliation and for self-preservation. 

In a broader context, the term may refer to a minority's strategy of coping with oppression from socially, culturally or economically dominant groups involving suppression of aggressive feelings and even identification with the oppressor, leading to "forced assimilation/acculturation" of the cultural minority.


</doc>
<doc id="165390" url="https://en.wikipedia.org/wiki?curid=165390" title="Venality">
Venality

Venality is a vice associated with being bribeable, cruel, selfish, or of selling one's services or power, especially when people are intended to act in a decent way instead. In its most recognizable form, dishonesty, venality causes people to lie and steal for their own personal advantage, and is related to bribery and nepotism, among other vices.

Venality in its mild form is a vice notable especially among those with government or military careers. For example, the Ancien Régime in France from the 1500s through the late 1700s, was notorious for the venality of many government officials. In these fields, one is ideally supposed to act with justice and honor and not accept bribes. That ensures that the organization is not susceptible to manipulation by self-interested parties.

In contrast to the previous interpretation, dishonesty is not specifically expressed in the literal meaning, but is often implied. The condition of failing to act justly is not a literal component of the word's meaning either. By definition, committing "venal" acts does not indicate "stealing" or "lying", but rather suggests a consensual arrangement, perhaps without conscience or regard for consequences, but is not synonymous with stealing. While bribery could be related, nepotism clearly has no literal similarity or correlation with venality. Though venality is generally used as a pejorative term, an individual or entity could be venal (or mercenary) and not be corrupt or unethical. One could perform one's duties or job in a perfunctory manner in order to collect a wage or payment, or prostitute one's time or skills for monetary or material gain, without necessarily being dishonest.

Much contemporary use of the words venal or venality is applied to modern professional athletes, particularly baseball, basketball, American football, and soccer players all around the world. The implication being that the highly paid players are essentially "hired guns" with no allegiance to any team or city, and are motivated solely by the acquisition of material wealth.

For people to accept settlements and legislation, the acts of the government must be seen as just. This perception enhances the legitimacy of the government. Venality is a term often used with reference to pre-revolutionary France, where it describes the then-widespread practice of selling administrative positions within the government to the highest bidder, especially regarding the Nobles of the Robe.

Thus, for example, venality was a charge for which, in part, Danton and others were executed during the Reign of Terror.


</doc>
<doc id="13561452" url="https://en.wikipedia.org/wiki?curid=13561452" title="Philosophy of desire">
Philosophy of desire

In philosophy, desire has been identified as a philosophical problem since Antiquity. In Plato's "The Republic", Socrates argues that individual desires must be postponed in the name of the higher ideal.

Within the teachings of Buddhism, craving is thought to be the cause of all suffering. By eliminating craving, a person can attain ultimate happiness, or Nirvana. While on the path to liberation, a practitioner is advised to "generate desire" for skillful ends.

In Aristotle's "De Anima" the soul is seen to be involved in motion, because animals desire things and in their desire, they acquire locomotion. Aristotle argued that desire is implicated in animal interactions and the propensity of animals to motion. But Aristotle acknowledges that desire cannot account for all purposive movement towards a goal. He brackets the problem by positing that perhaps reason, in conjunction with desire and by way of the imagination, makes it possible for one to apprehend an object of desire, to see it as desirable. In this way reason and desire work together to determine what is a good object of desire. This resonates with desire in the chariots of Plato's "Phaedrus", for in the "Phaedrus" the soul is guided by two horses, a dark horse of passion and a white horse of reason. Here passion and reason, as in Aristotle, are also together. Socrates does not suggest the dark horse be done away with, since its passions make possible a movement towards the objects of desire, but he qualifies desire and places it in a relation to reason so that the object of desire can be discerned correctly, so that we may have the right desire. Aristotle distinguishes desire into two aspects of appetition, and volition. Appetition, or appetite, is a longing for or seeking after something.

Aristotle makes the distinction as follows:

In "Passions of the Soul", René Descartes writes of the passion of desire as an agitation of the soul that projects desire, for what it represents as agreeable, into the future. Desire in Immanuel Kant can represent things that are absent and not only objects at hand. Desire is also the preservation of objects already present, as well as the desire that certain effects not appear, that what affects one adversely be curtailed and prevented in the future. Moral and temporal values attach to desire in that objects which enhance one's future are considered more desirable than those that do not, and it introduces the possibility, or even necessity, of postponing desire in anticipation of some future event, anticipating Sigmund Freud's text "Beyond the Pleasure Principle". See also, the pleasure principle in psychology.

In his "Ethics", Baruch Spinoza declares desire to be "the very essence of man," in the "Definitions of the Affects" at the end of Part III. An early example of desire as an ontological principle, it applies to all things or "modes" in the world, each of which has a particular vital "striving" (sometimes expressed with the Latin "conatus") "to persist in existence" (Part III, Proposition 7). Different striving beings have different levels of power, depending on their capacity to persevere in being. Affects, or emotions which are divided into the "joyful" and the "sad", alter our level of power or striving: joy is a passage "from a lesser to a greater perfection" or degree of power (III Prop. 11 Schol.), just as sadness is the opposite. Desire, qualified by the imagination and the intellect, is an attempt to maximize power, to "strive to imagine those things that increase or aid the body's power of acting." (III Prop. 12). Spinoza ends the "Ethics" by a proposition that both moral virtue and spiritual blessedness are a direct result of essential power to exist, i.e. desire (Part V Prop. 42).

In "A Treatise on Human Nature", David Hume suggests that reason is subject to passion. Motion is put into effect by desire, passions, and inclinations. It is desire, along with belief, that motivates action. Immanuel Kant establishes a relation between the beautiful and pleasure in "Critique of Judgment". He says "I can say of every representation that it is at least possible (as a cognition) it should be bound up with a pleasure. Of representation that I call pleasant I say that it actually excites pleasure in me. But the beautiful we think as having a necessary reference to satisfaction." Desire is found in the representation of the object.

Georg Wilhelm Friedrich Hegel begins his exposition of desire in "Phenomenology of Spirit" with the assertion that "self-consciousness is the state of desire () in general." It is in the restless movement of the negative that desire removes the antithesis between itself and its object, "...and the object of immediate desire is a living thing...", and object that forever remains an independent existence, something other. Hegel's inflection of desire via stoicism becomes important in understanding desire as it appears in Marquis de Sade. Stoicism in this view has a negative attitude towards "...otherness, to desire, and work."

Reading Maurice Blanchot in this regard, in his essay "Sade's Reason", the libertine is one, of a type that sometimes intersects with a Sadean man, who finds in stoicism, solitude, and apathy the proper conditions. Blanchot writes, "...the libertine is thoughtful, self-contained, incapable of being moved by just anything." Apathy in de Sade is opposition not to desire but to its spontaneity. Blanchot writes that in Sade, "for passion to become energy, it is necessary that it be constricted, that it be mediated by passing through a necessary moment of insensibility, then it will be the greatest passion possible." Here is stoicism, as a form of discipline, through which the passions pass. Blanchot says, "Apathy is the spirit of negation, applied to the man who has chosen to be sovereign." Dispersed, uncontrolled passion does not augment one's creative force but diminishes it.

In his "Principia Ethica", British philosopher G. E. Moore argued that two theories of desire should be clearly distinguished. The hedonistic theory of John Stuart Mill states that pleasure is the sole object of all desire. Mill suggests that a desire for an object is caused by an idea of the possible pleasure that would result from the attainment of the object. The desire is fulfilled when this pleasure is achieved. On this view, the pleasure is the sole motivating factor of the desire. Moore proposes an alternative theory in which an actual pleasure is already present in the desire for the object and that the desire is then for that object and only indirectly for any pleasure that results from attaining it. 
"In the first place, plainly, we are not always conscious of expecting pleasure, when we desire a thing. We may only be conscious of the thing which we desire, and may be impelled to make for it at once, without any calculation as to whether it will bring us pleasure or pain. In the second place, even when we do expect pleasure, it can certainly be very rarely pleasure only which we desire. On Moore's view, Mill's theory is too non-specific as to the objects of desire. Moore provides the following example: 
"For instance, granted that, when I desire my glass of port wine, I have also an idea of the pleasure I expect from it, plainly that pleasure cannot be the only object of my desire; the port wine must be included in my object, else I might be led by my desire to take wormwood instead of wine . . . If the desire is to take a definite direction, it is absolutely necessary that the idea of the object, from which the pleasure is expected, should also be present and should control my activity."

For Charles Fourier, following desires (like passions or in Fourier's own words 'attractions') is a means to attain harmony.

Within the teachings of Siddhartha Gautama (Buddhism), craving is thought to be the cause of all suffering that one experiences in human existence. The extinction of this craving leads one to ultimate happiness, or Nirvana. Nirvana means "cessation", "extinction" (of suffering) or "extinguished", "quieted", "calmed"; it is also known as "Awakening" or "Enlightenment" in the West. The Four Noble Truths were the first teaching of Gautama Buddha after attaining Nirvana. They state that suffering is an inevitable part of life as we know it. The cause of this suffering is attachment to, or craving for worldly pleasures of all kinds and clinging to this very existence, our "self" and the things or people we—due to our delusions—deem the cause of our respective happiness or unhappiness. The suffering ends when the craving and desire ends, or one is freed from all desires by eliminating the delusions, reaches "Enlightenment".

While greed and lust are always unskillful, desire is ethically variable—it can be skillful, unskillful, or neutral. In the Buddhist perspective, the enemy to be defeated is craving rather than desire in general.

Jacques Lacan's "désir" follows Freud's concept of "Wunsch" and it is central to Lacanian theories. For the aim of the talking cure—psychoanalysis—is precisely to lead the analysand or patient to uncover the truth about their desire, but this is only possible if that desire is articulated, or spoken. Lacan said that "it is only once it is formulated, named in the presence of the other, that desire appears in the full sense of the term." "That the subject should come to recognize and to name his/her desire, that is the efficacious action of analysis. But it is not a question of recognizing something which would be entirely given. In naming it, the subject creates, brings forth, a new presence in the world." "[W]hat is important is to teach the subject to name, to articulate, to bring desire into existence." Now, although the truth about desire is somehow present in discourse, discourse can never articulate the whole truth about desire: whenever discourse attempts to articulate desire, there is always a leftover, a surplus.

In "The Signification of the Phallus" Lacan distinguishes desire from need and demand. Need is a biological instinct that is articulated in demand, yet demand has a double function, on one hand it articulates need and on the other acts as a demand for love. So, even after the need articulated in demand is satisfied, the demand for love remains unsatisfied and this leftover is desire. For Lacan "desire is neither the appetite for satisfaction nor the demand for love, but the difference that results from the subtraction of the first from the second" (article cited). Desire then is the surplus produced by the articulation of need in demand. Lacan adds that "desire begins to take shape in the margin in which demand becomes separated from need." Hence desire can never be satisfied, or as Slavoj Žižek puts it "desire's "raison d'être" is not to realize its goal, to find full satisfaction, but to reproduce itself as desire."

It is also important to distinguish between desire and the drives. Even though they both belong to the field of the Other (as opposed to love), desire is one, whereas the drives are many. The drives are the partial manifestations of a single force called desire (see "The Four Fundamental Concepts of Psychoanalysis"). If one can surmise that "objet petit a" is the object of desire, it is not the object towards which desire tends, but the cause of desire. For desire is not a relation to an object but a relation to a lack (manque). Then desire appears as a social construct since it is always constituted in a dialectical relationship.




</doc>
<doc id="901369" url="https://en.wikipedia.org/wiki?curid=901369" title="Pet peeve">
Pet peeve

A pet peeve, pet aversion, or pet hate is a minor annoyance that an individual identifies as particularly irritating to them, to a greater degree than would be expected based on the experience of others. The phrase analogizes that feeling of annoyance as a pet animal that one does not wish to give up, despite its objective lack of importance.

The noun "peeve", meaning an annoyance, is believed to have originated in the United States early in the twentieth century, derived by back-formation from the adjective "peevish", meaning "ornery or ill-tempered", which dates from the late 14th-century. 

The term "pet peeve" was introduced to a wide readership in the single-panel comic strip "The Little Pet Peeve" in the Chicago Tribune during the period 1916–1920. The strip was created by cartoonist Frank King, who also created the long-running "Gasoline Alley" strip. King's "little pet peeves" were humorous critiques of generally thoughtless behaviors and nuisance frustrations. Examples included people reading the inter-titles in silent films aloud, cracking an egg only to smell that it's gone rotten, back-seat drivers, and rugs that keep catching the bottom of the door and bunching up. King's readers submitted topics, including theater goers who unwrap candy in crinkly paper during a live performance, and (from a 12 year old boy) having his mother come in to sweep when he has the pieces of a building toy spread out on the floor.

Pet peeves often involve specific behaviors of someone close, such as a spouse or significant other. These behaviors may involve disrespect, manners, personal hygiene, relationships, and family issues. A key aspect of a pet peeve is that it may well seem acceptable or insignificant to others, while the person is likewise not bothered by things that might upset others. For example, a supervisor may have a pet peeve about people leaving the lid up on the copier, when others interrupt when speaking, or their subordinates having messy desks. 

"Pet Peeve" is the twenty-ninth book of the Xanth series by fantasy author Piers Anthony.



</doc>
<doc id="8837954" url="https://en.wikipedia.org/wiki?curid=8837954" title="Feminine psychology">
Feminine psychology

Feminine psychology is an approach that focuses on social, economic, and political issues confronting women all throughout their lives. It can be considered a reaction to male-dominated theories such as Sigmund Freud's view of female sexuality. The groundbreaking works of Karen Horney argued that male realities cannot describe female psychology or define their gender because they are not informed by girls' or women's experiences. Theorists, thus, claim this new approach is required. There is the position that women's social existence is crucial in understanding their psychology. For instance, it is claimed that some characteristics of female psychology emerge to comply with the given social order defined by men and not necessarily because it is the nature of their gender or psychology. 

The feminine psychology is often attributed to the pioneering work of Horney, who was the first woman to present a paper on feminine psychology at an international meeting. She famously contradicted Sigmund Freud's psychoanalytic theory, arguing that it is male-dominated and, therefore, harbored biases and phalocentric views. For this reason, such theory - Horney claimed - cannot describe femininity because it is informed by male reality and not by actual female experience. For instance, there is the case of Freud's proposition that the female personality tends to exhibit the so-called penis envy, where a girl interprets her failure to possess a penis as a punishment for wrongdoing and later blames her mother responsible. As Freud stated, "she has seen it and knows that she is without it and wants to have it." Horney argued that it is not penis envy but a basic anxiety, hostility, and anger towards the opposite-sex parent, whom she views as competition for the affection of the same-sex parent, thus, a direct threat to her safety and security. Her view, which formed a significant part of her feminine psychology theory, is that this aspect should be resolved based on interpersonal dynamics (e.g. differences in social power) than sexual dynamics.

Horney countered the Freudian concept with her own "womb envy" theory where men envy women's capability to bear children and they compensate for it with achievement and success. She deconstructed penis envy and described it as nothing more than women wanting to express their own natural needs for success and the security that is characteristic of both sexes. There is an analogy that describes Horney's feminine psychology as optimistic in the sense of world and life affirmation in comparison with Freud's pessimism oriented towards world and life negation. In the fourteen papers she wrote about feminine psychology, Horney offered a new way of thinking about women, underscoring how they should not gain value through their husbands, children, and family.

One dynamic outlined by feminine psychologists is the balancing act that women partake in between the more traditional role of motherhood and the more modern one of a career woman. Balancing the roles means attempting to satisfy both the need for personal achievement and the need for love and emotional security.

This does not mean that the roles contradict each other. The additional income from work may both relieve some stress and give the mother the ability to provide greater advantages (education, healthcare) to her children. Working also allows women to feel as though they are making a contribution to society beyond the family. A more fulfilled mother, in most cases, will be a better mother. Although this is true, many children feel neglected by their mothers when they are more focused on their career Twenty three percent of mothers feel that they are not spending enough time with their children, but believe that their children will become more independent and understanding once they get older.

A lot has changed throughout the years as mothers and fathers both feel the pressure of balancing both work and family life. Fifty six percent of mothers say that handling work and family life is difficult for them as they are doing more than just housework and child care. Adding to the burden is society's concept that the mother's role is to spend more time with their children. A study conducted by the Pew Research Center indicates that 42% of respondents believe that a mother who works part-time is an ideal scenario while 16% thinks that working full-time is ideal for mothers, and the rest think that mothers should stay at home.

In addition, fathers have always been viewed as the bread makers in the family. However, times have changed and they are now more included in the parenting role. Fathers spend more time at home and they engage in taking care of their children and helping around the house a lot more than they did a century ago. For example, 50% of working fathers state that it is extremely difficult to balance work and taking care of their children (Pew Research Center 2012). The Pew Research Center also asked parents to rate themselves as good or bad parents. It was found that most mothers rated themselves a lot higher than fathers did and working mothers rated themselves a lot higher than non-working mothers did (Pew Research Center 2012).

According to a study conducted by Dr. Jennifer Stuart, sometimes the history of the woman affects how she chooses to balance the two roles, or if she will balance at all. Specifically, Stuart asserts that the primary determinant is a woman's "quality of her relationship with her mother. Women whose mothers fostered feelings of both warm attachment and confident autonomy may find ways to enjoy their children and/or work, often modifying work and family environments in ways that favor both".

Some women have no choice other than to work while raising children because of financial need. Others work for personal fulfillment. In either case, women are making compromises in their careers so that they can balance paid work and motherhood responsibilities. They are cutting back hours and accepting lower pay or a lower job status. In order to make the compromise, they have chosen to be satisfied with being average rather than being a top performer in the workplace (Kapur, 2004).

What mothers have to remember, according to Dr. Ramon Resa, is that "children are fairly resilient and will adapt to whatever changes are required. They are also astute at sensing unhappiness, disappointment and apathy" (Resa, 2009). There is no harm in trying any path in order to find fulfillment, because no decision is permanent and can be changed as the situation warrants.





</doc>
<doc id="8768899" url="https://en.wikipedia.org/wiki?curid=8768899" title="Obstructionism">
Obstructionism

Obstructionism is the practice of deliberately delaying or preventing a process or change, especially in politics.

An obstructionist causes problems. Neuman and Baron (1998) identify obstructionism as one of the three dimensions that encompass the range of workplace aggression. In this context, obstructionism is "behaviors intended to hinder an employee from performing their job or the organization from accomplishing its objectives.".

Obstructionism or policy of obstruction denotes the deliberate interference with the progress of a legislation by various means such as filibustering or slow walking which may depend on the respective parliamentary procedures.

Obstructionism can also take the form of widespread agreement to oppose policies from the other side of a political debate or dispute.

In September 2010, Jon Stewart of "The Daily Show" announced the Rally to Restore Sanity and/or Fear, an event dedicated to ending political obstructionism in American mass media.
The most common tactic is the filibuster which consists of extending the debate upon a proposal in order to delay or completely prevent a vote on its passage. 

Another form of parliamentary obstruction practiced in the United States and other countries is called "slow walking". It specifically refers to the extremely slow speed with which legislators walk to the podium to cast their ballots. For example, in Japan this tactic is known as a "cow walk", and in Hawaii it's known as a "Devil's Gambit". Consequently, slow walking is also used as a synonym for obstructionism itself.

John O'Connor Power, Joe Biggar, Frank Hugh O'Donnell, and Charles Stewart Parnell, Irish nationalists; all were famous for making long speeches in the British House of Commons. In a letter to Cardinal Cullen, 6 August 1877, The O'Donoghue, MP for County Kerry, denounced the obstruction policy: "It is Fenianism in a new form." The tactic deadlocked legislation and 'the autumn Session of 1882 was entirely devoted to the reform of the Rules of Procedure with a view to facilitating the despatch of business.' Sir Leslie Ward's "Spy" cartoon of John O'Connor Power appeared in "Vanity Fair"s "Men of the Day" series, 25 December 1886, and was captioned "the brains of Obstruction".

Two other famous obstructionists are Jesse Helms and Mme Flemington.

Newt Gingrich brought obstructionism to the United States Congress "crippling the congress" in an attempt to make the minority rule.

John Boehner "chalks up his theatrical obstructionism to the reality of being minority leader" further developing obstructionist tactics.

Mitch McConnell has been accused of obstructionism for his tactics during the Obama administration.

Chuck Schumer has been labeled as obstructionist for similar tactics during the current Trump administration.

Jane Stanford, 'That Irishman The Life and Times of John O'Connor Power', Part Two, 'Parliamentary Manoeuvres', pp 77–84, 'A Change of Government, pp 105–107.


</doc>
<doc id="2396037" url="https://en.wikipedia.org/wiki?curid=2396037" title="Opportunism">
Opportunism

Opportunism is the conscious policy and practice of taking advantage of circumstances – with little regard for principles or with what the consequences are for others. Opportunist actions are expedient actions guided primarily by self-interested motives. The term can be applied to individual humans and living organisms, groups, organizations, styles, behaviours, and trends.

Opportunism or "opportunistic behavior" is an important concept in such fields of study as biology, transaction cost economics, game theory, ethics, psychology, sociology and politics.

Opportunism is the conscious policy and practice of taking selfish advantage of circumstances. 

Although in many societies opportunism often has a strong negative moral connotation, it may also be defined more neutrally as putting self-interest before other interests when there is an opportunity to do so, or flexibly adapting to changing circumstances to maximize self-interest (though usually in a way that negates some principle previously followed). 

Opportunism is sometimes defined as the ability to capitalize on the mistakes of others: to exploit opportunities created by the errors, weaknesses or distractions of opponents to one's own advantage.

Taking a realistic or practical approach to a problem can involve "weak" forms of opportunism. For the sake of doing something that will work, or that successfully solves the problem, a previously agreed principle is knowingly compromised or disregarded - with the justification that alternative actions would, overall, have a worse effect.

In choosing or seizing opportunities, human opportunism is most likely to occur where:


Criticism of opportunism usually refers to a situation where beliefs and principles are tested or challenged.

Human opportunism should not be confused with "seeking opportunities" as such, or "making use of opportunities when they arise". Opportunism refers rather to a "specific way" of responding to opportunities, which involves the element of self-interestedness plus disregard for relevant (ethical) principles, or for intended or previously agreed goals, or for the shared concerns of a group.

Somewhat confusingly, though, opportunism is sometimes also redefined by businessmen simply as "the theory of discovering and pursuing opportunities". These businessmen are motivated by their dislike for the idea that there could ever be anything "wrong" with capitalizing on opportunities. According to this redefinition, "opportunism" is a euphemism for "entrepreneurship".

In the early 19th century, the term "opportunist" as noun or adjective was already known and used in several European languages, but initially it rarely referred to political processes or to a political tendency. The English term "opportunism" is possibly borrowed originally from the Italian expression "opportunismo". In 19th-century Italian politics, it meant "exploiting the prevailing circumstances or opportunities to gain immediate advantage for oneself or one's own group". However, it is more likely that the English expression was directly borrowed from the French term, when it began to refer specifically to the opportunist Republicans, since the term first entered the English language in the early 1870s. In this sense the meaning "opportunism" has mutated: from those who claimed to advocate a principle (in the original French case, an amnesty for the Communards) but said that the time was not yet "opportune", to what may be thought of as the opposite - those who act without principle.

In Latin, "opportunus" means opportune or favourable ("opportunitas" = opportunity); the word itself is a contraction of "ob portus" ("toward the harbour/entrance") or "oppositum portus" ("facing the harbour/entrance").

As a style of human behaviour, opportunism has the connotation of a lack of integrity, or doing something that is out of character (inconsistent). The underlying thought is that the price of the unrestrained pursuit of selfishness is behavioural inconsistency. Thus, opportunism involves compromising some or other principle normally upheld. However, the boundary between "legitimate self-interest" and "undesirable (or anti-social) selfishness" can be difficult to define; the definition may depend on one's point of view, or position in life.

Some people regard an opportunist stance positively as a legitimate choice, which can be the lesser evil. Thus, the British Conservative statesman Stanley Baldwin is supposed to have quipped:

Life can be viewed as presenting "an endless series of opportunities", where the pattern of one's responses defines who or what one is (i.e. individual identity). It can also be viewed as a striving to realize or express certain principles. However, the moral dilemma implied by opportunism concerns the "conflict" of self-interest with the interests of others, or with following a principle: either to do what one wants, or to do "what is the right thing to do". Thus, substantively, opportunism refers to someone who acts on opportunities in a self-interested, biased or one-sided manner that "conflicts" or "contrasts" in some way with one or more general rule, law, norm, or principle.

The fact that the self-interested action evokes this conflict, often implies that the tendency to use opportunities to advantage is "excessive" or "improper", the corollary being a deficiency of character or at least a lack of propriety. Hence the term opportunism often has the pejorative connotation of morally unsound behaviour, or behaviour that sacrifices a greater good for the sake of gaining an advantage for oneself or one's own group. Moralists may have a distaste for opportunism, insofar as opportunism implies the violation of a moral principle.

It is often difficult for an outsider to understand why an action or an idea is (or is not) "opportunist", because the outsider does not know the whole story, or the whole context, or the true intention behind it. The way things appear can give an impression which is quite different from the real motivation that is behind it.

In human behaviour generally, opportunism concerns the relationship between what people do, and their basic principles when faced with opportunities and challenges. The opportunist seeks to gain personal advantage when an opportunity presents itself, putting self-interest ahead of some other interest, in a way contrary either to a "previously established" principle or another principle that ought to have "higher priority". Hence opportunist behaviour is usually regarded "at least" as questionable or dubious, and "at most" as unjustifiable or completely illegitimate. Opportunism is regarded as unhealthy, as a disorder or as a character deficiency, if selfishly pursuing an opportunity is blatantly anti-social (involves disregard for the needs, wishes and interests of others). However, behaviour can also be regarded as "opportunist" by scholars without any particular moral evaluation being made or implied (simply as a type of self-interested behaviour).

The sociology and psychology of human opportunism is somewhat related to the study of gambling behaviour, and centres on the way people respond to risk and opportunity, and what kind of motivation and organizational culture is involved. Both the element of risk and opportunity play a role. To be opportunist in behaviour, a person or group must:


Thus, the opportunity exploited for selfish ends can itself exist either because an action is taken, or because of deliberate inaction (when action should really have been taken). The propensity to engage in such kinds of behaviours depends a great deal on the presence of absence of personal characteristics such as integrity, moral character, personal insight or self-awareness, personal flexibility and balance. It also depends on the ability to judge the consequences of different courses of action correctly. Strong emotions and desires may also play a role, and much may depend on how permissive a person, group or organization is (see permissive society). These factors influence the capacity to know "where to draw the line" appropriately, and regulate one's own behaviour so that it remains consistent. Much also depends on the beliefs people happen to have about themselves and the world they live in, and on the morale of an organization.

Whatever the opportunist's exact motive, it always involves the element of selfishness. Psychologically, it follows that opportunism always assumes a basic ability to make one's own choices, and decide to act in a way that serves one's own interest. In turn, that presupposes at least some basic self-motivation, inner direction, inventiveness and behavioural freedom; subjectively, an opportunist must at least be able to recognize and respond to opportunities when they are there.

Personalities and beliefs are shaped by the specific environment where they form. It is likely that the possibilities for opportunist behaviour are promoted in contexts where there is not only an incentive to engage in them, but also where it is also extremely difficult for some reason to remain behaviourally consistent, or where ordinary constraints on behaviour are lacking. In that case, opportunist behaviour does not seem to have much adverse effect or consequence, at least in the short term, compared to the much greater benefits of engaging in it. Eight main contexts are referred to in the literature:


Opportunist behaviour is also strongly influenced by the organizational context in which it occurs.


In professional ethics, the concept of opportunism plays a role in defining criteria for professional integrity. In providing a service, a professional may have personal discretion (choice or leeway) about how to provide the service. Professionals may, to a great extent, make their own judgements, interpretations, and decisions about the exact approach to take—without an explicit rule that they must perform in a specific way. Such a situation can be exploited with opportunist motives that are contrary to the stated ethics of a profession. Consequently, it becomes necessary—for the sake of preserving professional integrity—to explicate "guiding norms" that define the boundaries of acceptable practice, or to divide up roles in such a way that different people in an organization can effectively check and control what their colleagues actually do ("to keep them honest").

The term intellectual opportunism—the pursuit of intellectual opportunities with a selfish, ulterior motive not consistent with relevant principles—refers to certain self-serving tendencies of the human intellect, often involving professional producers and disseminators of ideas, who work with idea-formation all the time. The phenomenon of intellectual opportunism is frequently associated by its critics with careerism. When human knowledge becomes a tradeable good in a market of ideas, all sorts of opportunities arise for huckstering, swindling, haggling and hustling with information in ways which are regarded as unprincipled, dubious or involve deceit of some sort.

The intellectual opportunist "adapts" his intellectual concerns, pursuits and utterances to "fit with the trend/fashion" or "fit the situation" or "with what sells" – with the (ulterior) motive of gaining personal popularity/support, protecting intellectual coherence, obtaining personal credit, acquiring privilege or status, persuading others, ingratiating himself, taking advantage or making money. Normally this assumes some degree of intellectual flexibility, agility or persuasiveness.

Sexual opportunism is the selfish pursuit of sexual opportunities for their own sake when they arise, often with the negative moral connotation that in some way it "takes advantage" of others, or "makes use" of, or "exploits", other persons for sexual purposes. Sexual opportunism is sometimes also defined as the use of sexual favours for selfish purposes "quite unrelated" to the sexual activity, in which case taking a sexual opportunity is merely the "means" to achieve a quite different purpose, for example to advance one's career or obtain status or money. This may be accepted or tolerated, or it may be criticized because the concerns of others are not adequately taken into consideration (or because it is contrary to authentic sexual love).

To the extent that the feelings, wishes, intentions, purposes, interests or norms of others are not adequately considered in the pursuit of sexual gratification, it then conflicts with some or other principle for appropriate behaviour, and it may involve deceit or dishonesty (for example, the deliberate exploitation of sexual innocence). In that case, the sexual opportunist is considered to lack sexual and/or personal integrity. In a clinical or scientific sense, sexual opportunism is often straightforwardly described as observable sexual promiscuity or the observable propensity to engage in casual sex, whatever the motive.

In the theory of evolution, "evolutionary opportunism" refers to a specific pattern of development in the history of a species. The behaviour, culture or body part of a species that long ago evolved to serve a particular purpose or function may subsequently lend itself to a very "different" positive purpose or function that helps the species to survive. Thus, in a new stage of evolution, a long-existing behaviour, culture, or physical characteristic can respond to a wholly new opportunity and acquire a new role. It turns out to have "new" advantages or potential benefits the species previously never used—and, therefore, the species retains an adaptation even if the "original" purpose it served is long gone.

In biology, an opportunist "organism" is generally defined as a species that can live and thrive in variable environmental conditions, and sustain itself from a number of different food sources, or can rapidly take advantage of favorable conditions when they arise, because the species is behaviorally sufficiently flexible. Such species can for example postpone reproduction, or stay dormant, until conditions make growth and reproduction possible. In the biological disciplines, opportunistic behavior is studied in fields such as evolutionary biology, ecology, epidemiology, and etiology, where moral or judgmental overtones do not apply (see also opportunistic pathogens, opportunistic predation, phoresis, and parasitism).

In microbiology, opportunism refers to the ability of a normally non-pathogenic microorganism to act as a pathogen in certain circumstances. Opportunist "micro-organisms" (such as bacteria, viruses, fungi, and protozoa) are ones that, when they invade the host organism, can cause infection in the host organism, but cause real disease only if the natural defenses, resistance or immune system of the host organism are lowered (see opportunistic infection). In macrobiology, opportunist behaviour by an organism generally means that it is able to seize and use "diverse" opportunities in its environment to survive and grow. If one single opportunity or need occurs, the organism can "improvise" a response to it with whatever resources it has available, even if what it can do is "not the best possible" strategy.

Some animals also show this behavior for group-foraging. In other words, they try to optimize the feeding intake of their colony. The Australian stingless bee Tetragonula carbonaria, for instance, has several workers search for an area full of rich resources, and will then recruit heavily in this area until the resources are depleted."

The term "opportunism" is often used in politics and political science, and by activists campaigning for a cause. The political philosophy of Niccolò Machiavelli as described in "The Prince" is often regarded as a classic manual of opportunist scheming. Political opportunism is interpreted in different ways, but usually refers to one or more of the following:


Typically, opportunist political behavior is criticized for being short-sighted or narrow-minded. Most politicians are "opportunists" to some extent at least (they aim to use political opportunities creatively to their advantage, and have to try new initiatives), but the controversies surrounding the concept concern the exact relationship between "seizing a political opportunity" and the "political principles" being espoused. The term "political opportunism" is often used in a pejorative sense, mainly because it connotes the "abandonment" of principles or compromising political goals. Political integrity typically demands an appropriate combination of principled positions and political flexibility that produces a morally consistent behavior in specific circumstances. There are four main sources of political opportunism: suivisme (a specific political methodology that is applied to maintain or increase political influence), populism, risk management, and "means become ends".

There exists no agreed general, scientific definition or theory of economic opportunism; the literature usually considers only specific cases and contexts. Market trade supplies no universal morality of its own, except the law of contract and basic practical requirements to settle transactions, while at the same time legal rules, however precise in their formulation, cannot control every detail of transactions and the interpretation (or implications) thereof. Since economic opportunism must be assessed against some relevant norm or principle, controversy about what that norm or principle should be, makes a general definition difficult.

Market trade is compatible with a great variety of moral norms, religions and political systems, and indeed supporters of the free market claim that this is exactly its advantage: people can choose their own values, buying and selling as they wish within a basic legal framework accepted by all. People would not normally trade, if they did not expect to gain something by it; the fact that they do trade normally presupposes at least a respect for the basic rights of the party being traded with. Nevertheless, the gains or benefits of trading activity (and indeed the losses), although entirely legal, might be distributed very "unequally" or in ways not anticipated by previous understandings, and thus accusations of "economic opportunism" can arise nevertheless in many different settings. If this is the case, relevant trading obligations (or civil obligations) are usually considered as not being (fully) met or honored, in the pursuit of economic self-interest. Greed is frequently mentioned as a primary motive for economic opportunism.

Glenn R. Parker claims that the five most discussed examples of "economic' opportunism are:


In transaction cost economics, opportunism means self-interest seeking with guile, involving some kind of deliberate deceit and the absence of moral restraint. It could involve deliberately withholding or distorting important business information, shirking (doing less work than agreed), or failing to fulfill formal or informal promises and obligations. It occurs in trading activities especially where rules and sanctions are lacking, and where the opportunist actor has great power to influence an outcome by the attitude he assumes in practice. However, others argue that this reflects a narrow view of economic opportunism, because there are many more ways that economic actors can take selfish advantage of other economic actors, even if they do not violate the law.

In game theory, opportunism concerns the contradictory relationships between altruistic and self-interested behaviour, where the different kinds of common and sectional interests existing in a situation are used mainly to make gains for oneself. If some actors in a game are placed at a disadvantage in some way, for any reason, it becomes an opportunity for other actors to capitalize on that fact, by using the disadvantage of others to improve their own position – under conditions where actors both compete and cooperate in different areas. Two classic cases discussed in game theory where opportunism is often involved are the free rider problem and the prisoner's dilemma. In this game-theoretical sense, Paul Seabright defines opportunism as "the behaviour of those who seek to benefit from the efforts of others without contributing anything themselves." Game theory can, for example, model the effects of information asymmetry, where people have unequal access to relevant information, so that those who "do know" can take advantage of those who "don't know".

From a game-theoretical perspective, opportunism is objectively a "problem", if the pursuit of self-interest – "in conflict with" other interests at stake – has an "undesirable or unwanted result" for some actors or most of them. However, in principle examples could also be constructed where opportunist behaviour unintentionally serves other, broader interests (such as when, in their rush to take selfish advantage of a situation, the opportunist actors create more opportunities for other actors at the same time – the "bandwaggon" or "food chain" effect; see also Pareto optimality). In game theory, therefore, opportunism is "not" defined as being "intrinsically" and "necessarily" always a good thing or a bad thing; it could be either. Usually though, it is assumed, that the game theorist is able to "stand outside" the different interests being studied, to view the situation objectively – in a detached, uninvolved, impartial and unbiased way.

Kenneth Arrow explains that markets require trust to operate effectively, but that trust may not be spontaneously generated by market activity:

Social opportunism refers to the use of opportunities for social contact only for selfish purposes or motives. Because it is only selfish, the implication is usually that obligations to other participants in the given social setting are not (fully) met or honoured. The social opportunist participates in a group, cooperates with it or associates with it, not primarily because he wants to "contribute", give or share something to the group, or because he values being part of it as an intrinsic good, but only because he wants to get some advantage out of the participation for himself. Consequently, the participation by the opportunist is substantively only a "means" that serves some other, selfish purpose. This may be tolerated, to the extent that the selfish purpose of the opportunist is compatible with, or does not conflict with, the goals and intentions of the group. It may be regarded as undesirable and unwanted, or indeed a breach of trust or good faith, if that is not the case.

Groups, gatherings, associations, or organizations that operate on the basis of voluntary or involuntary association, or in an atmosphere of mutual trust, may provide resources or contacts to their participants that are:

Thus, to use those resources or contacts for some selfish aim, paradoxically the social opportunist necessarily has to gain entry, join in and participate socially; there is no other way to gain access to or extract what he wants for himself. Some social groupings may welcome social opportunists, because they can serve a useful function, or can be persuaded (perhaps with group pressure) to change their ways through participation. Other social groupings may try to prevent social opportunism, by imposing strict preconditions of participation to ward off opportunists, or with the aid of rules prohibiting opportunist behaviour.

Karl Marx provided no substantive theory of opportunism; insofar as he used the term, he meant a tactic of convenience or expediency used for self-serving motives, involving some or other kind of political, economic or intellectual trick. Nevertheless, some Marxists claim that Marx's theory of capitalism does imply a substantive theory of opportunism. Its main claim is that opportunism is not simply an aberration or impediment to the efficient functioning of capitalism, but an "integral and necessary characteristic" of it; capitalist market activity "promotes" opportunist moves in all sorts of ways. Five kinds of factors are usually cited:


Taken together, these five factors make it difficult for any individual or group to reconcile self-interest with the general interest, genuinely and durably, and it means that moral double standards are very pervasive. In turn, that creates a total environment where opportunism can flourish – including "within" the socialist movement. In fact, "opportunism" as a political term began to be used widely among Marxists, when the parliamentarians from the leading party of the Second International, the German Social Democratic Party, voted in favour of the war credits necessary at the beginning of World War I. Marxist critics argued that this policy was a total abandonment of socialist principles, especially the principle of anti-militarism and the international solidarity of the working class. Since that time, opportunism has been often defined by Marxists as a policy that puts special interests ahead of the interests of the working class

Legal opportunism is a wide area of human activity, which refers generally to a type of abuse of the "proper intention" of legal arrangements (the "spirit of the law" as distinguished from the letter of the law). More specifically, it refers to deliberately manipulating legal arrangements for purposes they were not meant for, guided by self-interested motives. Usually, legal opportunism is understood to occur "legally": it is itself not necessarily a "crime" (a violation of the law or an unlawful act), but it could be considered "immoral" ("there ought to be a law against it"). The general effect of legal opportunism, if it really occurs, is that it discredits the rule of law or destroys the legitimacy of particular legal rules in the eyes of the people affected by them. Inversely, if people perceive a legal framework as arbitrary, obstructive or irrelevant, they are tempted to search for opportunities to find ways "around the law", without formally "breaking" the law.

Typical of legal opportunists is that they accept or approve of the application of legal rules when it suits their own interest but reject or disapprove of the application when the rules are against their interest (or if taking self-interested action would mean "breaking" the law). The law should serve them, and not the other way around; or, there is "one rule for them, and another rule for other people." Often, legal opportunism is enabled because a rule must be "interpreted" in order to "apply" it, where the chosen interpretation is precisely the one that favours one's self-interest. Since there are many dubious ways to manipulate the applicability of legal rules and procedures for selfish purposes, a "general" definition of legal opportunism (one which covers all cases) is exceptionally difficult. Legal opportunism can involve practices such as the following:


Spiritual opportunism refers to the exploitation of spiritual ideas (or of the spirituality of others, or of spiritual authority): for personal gain, partisan interests or selfish motives. Usually the implication is that doing so is unprincipled in some way, although it may cause no harm and involve no abuse. In other words, religion becomes a means to achieve something that is alien to it, or things are projected into religion that do not belong there.

If a religious authority acquires influence over the "hearts and minds" of people who are believers in a religion, and therefore can "tap into" the most intimate and deepest-felt concerns of believers, it can also gain immense power from that. This power can be used in a self-interested manner, exploiting opportunities to benefit the position of the religious authority or its supporters in society. This could be considered as inconsistent with the real intentions of the religious belief, or it might show lack of respect for the spiritual autonomy of others. The "good faith" of people is then taken advantage of, in ways that involve some kind of deceit, or some dubious, selfish motive.

The term spiritual opportunism is also used in the sense of casting around for suitable spiritual beliefs borrowed and cobbled together in some way to justify, condemn or "make sense of" particular ways of behaving, usually with some partisan or ulterior motive. This may not be abusive, but it often gives rise to criticisms or accusations that the given spiritual beliefs:


Supporters of traditional religions such as Christianity, Islam, Hinduism or Buddhism sometimes complain that people (such as New Age enthusiasts) seek out spiritual beliefs "that serve only themselves", as a form of "spiritual opportunism". Such complaints are often highly controversial, because people are considered to have the right to their own spiritual beliefs (they may not have that right, to the extent that they are socially excluded unless they profess certain spiritual beliefs, but they may only subscribe "formally" or "outwardly" to them).

Spiritual opportunism sometimes refers also to the practice of proselytizing one's spiritual beliefs when any opportunity to do so arises, for the purpose of winning over, or persuading others, about the superiority of these beliefs. In this context, the spiritual opportunist may engage in various actions, themselves not directly related to the spiritual beliefs, with the specific aim of convincing others of the superiority of his own belief system – it may effectively amount to "buying their support".



</doc>
<doc id="6937839" url="https://en.wikipedia.org/wiki?curid=6937839" title="Evasion (ethics)">
Evasion (ethics)

In ethics, evasion is an act that deceives by stating a true statement that is irrelevant or leads to a false conclusion. For instance, a man knows that another man is in a room in the building because he heard him, but in answer to a question, says, "I have not seen him," thereby falsely implying that he does not know.

Evasion is described as a way to fulfill an obligation to tell the truth while keeping secrets from those not entitled to know the truth, but is considered unethical unless there are grave reasons for withholding the truth. Evasions are closely related to equivocations and mental reservations; indeed, some statements fall under both descriptions.

Question dodging is a rhetorical technique involving the intentional avoidance of answering a question. This may occur when the person questioned either does not know the answer and wants to avoid embarrassment, or when the person is being interrogated or questioned in debate, and wants to avoid giving a direct response. Overt question dodging can sometimes be employed humorously, in order to sidestep giving a public answer in a political discussion: when a reporter asked Mayor Richard J. Daley why Hubert Humphrey had lost the state of Illinois in the 1968 presidential election, Daley replied "He lost it because he didn't get enough votes." Often the aim of dodging a question is to make it seem as though the question was fulfilled, leaving the person who asked the question feeling satisfied with the answer, unaware that the question was not properly answered. A false accusation of question dodging can sometimes be made as a disingenuous tactic in debate, in the informal fallacy of the loaded question. A common way out of this argument is not to answer the question (e.g. with a simple 'yes' or 'no'), but to challenge the assumption behind the question. This can lead the person questioned to be accused of "dodging the question".

In the context of political discourse, evasion is a technique of equivocation that is important for face management.

Peter Bull identified the following evasion techniques for answering questions:




</doc>
<doc id="990772" url="https://en.wikipedia.org/wiki?curid=990772" title="Food fight">
Food fight

A food fight is a form of chaotic collective behavior, in which food is thrown at others in the manner of projectiles. These projectiles are not made nor meant to harm others, but to simply ignite a fight filled with spontaneous food throwing. Food fights may be impromptu examples of rebellion or violence; however, they can also be planned events. In organized food fights, the food "weapons" are usually all of one kind, or of a limited variety. An impromptu food fight will use whatever food is on hand.

Though usually associated with juvenile settings such as schools, food fights have a long history throughout the world as a form of festive public entertainment or pastime. They have traditionally been popular since the early Middle Ages in Europe during seasonal festivals, especially in the summertime. For example, Spanish "La Tomatina" is still regularly held every August in Valencian town of Buñol, in which participants pelt each other with tomatoes, as is Battle of the Oranges held in the Italian town of Ivrea where, as the name would suggest, the oranges are used instead. Food fights occur at the meetings of the Legislative Yuan of Taiwan.
Food fights have also become a common element in slapstick comedy, with the pie in the face gag being especially well-known. Food fights are frequently featured in children's television and books, usually as an example of childish, destructive or reckless behavior.


</doc>
<doc id="862496" url="https://en.wikipedia.org/wiki?curid=862496" title="Assertiveness">
Assertiveness

Assertiveness is the quality of being self-assured and confident without being aggressive. In the field of psychology and psychotherapy, it is a learnable skill and mode of communication. "Dorland's Medical Dictionary" defines assertiveness as: 

During the second half of the 20th century, assertiveness was increasingly singled out as a behavioral skill taught by many personal development experts, behavior therapists, and cognitive behavioral therapists. Assertiveness is often linked to self-esteem. The term and concept was popularized to the general public by books such as "Your Perfect Right: A Guide to Assertive Behavior" (1970) by Robert E. Alberti and Michael L. Emmons and "When I Say No, I Feel Guilty: How To Cope Using the Skills of Systematic Assertiveness Therapy" (1975) by Manuel J. Smith

Joseph Wolpe originally explored the use of assertiveness as a means of "reciprocal inhibition" of anxiety, in his 1958 book on treating neurosis; and it has since been commonly employed as an intervention in behavior therapy. Assertiveness Training ("AT") was introduced by Andrew Salter (1961) and popularized by Joseph Wolpe. Wolpe's belief was that a person could not be both assertive and anxious at the same time, and thus being assertive would inhibit anxiety.
The goals of assertiveness training include:

As a communication style and strategy, assertiveness is thus distinguished from both aggression and passivity. How people deal with personal boundaries, their own and those of other people, helps to distinguish between these three concepts. Passive communicators do not defend their own personal boundaries and thus allow aggressive people to abuse or manipulate them through fear. Passive communicators are also typically not likely to risk trying to influence anyone else. Aggressive people do not respect the personal boundaries of others and thus are liable to harm others while trying to influence them. A person communicates assertively by overcoming fear of speaking his or her mind or trying to influence others, but doing so in a way that respects the personal boundaries of others. Assertive people are also willing to defend themselves against aggressive people.

Assertive communication involves respect for the boundaries of oneself and others. It also presumes an interest in the fulfillment of needs and wants through cooperation.

According to the textbook "Cognitive Behavior Therapy" (2008), "Assertive communication of personal opinions, needs, and boundaries has been ... conceptualized as the behavioral middle ground, lying between ineffective passive and aggressive responses". Such communication "emphasizes expressing feelings forthrightly, but in a way that will not spiral into aggression".

If others' actions threaten one's boundaries, one communicates this to prevent escalation.

In contrast, "aggressive communication" judges, threatens, lies, breaks confidences, stonewalls, and violates others' boundaries.

At the opposite end of the dialectic is "passive communication". Victims may passively permit others to violate their boundaries. At a later time, they may come back and attack with a sense of impunity or righteous indignation.

Assertive communication attempts to transcend these extremes by appealing to the shared interest of all parties; it "focuses on the issue, not the person". Aggressive and/or passive communication, on the other hand, may mark a relationship's end, and reduce self-respect.

Assertive people tend to have the following characteristics:

Techniques of assertiveness can vary widely. Manuel Smith, in his 1975 book "When I Say No, I Feel Guilty", offered some of the following behaviors:

The "broken record" technique consists of simply repeating your requests or your refusals every time you are met with resistance. The term comes from vinyl records, the surface of which when scratched would lead the needle of a record player to loop over the same few seconds of the recording indefinitely. "As with a broken record, the key to this approach is repetition ... where your partner will not take no for an answer."

A disadvantage with this technique is that when resistance continues, your requests may lose power every time you have to repeat them. If the requests are repeated too often, it can backfire on the authority of your words. In these cases, it is necessary to have some sanctions on hand.

Fogging consists of finding some limited truth to agree with in what an antagonist is saying. More specifically, one can "agree in part" or "agree in principle".

Negative inquiry consists of requesting further, more specific criticism.

Negative assertion is agreement with criticism without letting up demand.

I-statements can be used to voice one's feelings and wishes from a personal position without expressing a judgment about the other person or blaming one's feelings on them.

Several research studies have identified assertiveness training as a useful tool in the prevention of alcohol-use disorders. Psychological skills in general including assertiveness and social skills have been posed as intervention for a variety of disorders with some empirical support.

In connection with gender theory, "Tannen argues that men and women would both benefit from learning to use the others' style. ... So, women would benefit from assertiveness training just as men would benefit from sensitivity training".

Assertiveness may be practiced in an unbalanced way, especially by those new to the process: "[One] problem with the concept of assertiveness is that it is both complex and situation-specific. ... Behaviors that are assertive in one circumstance may not be so in another". More particularly, while "unassertiveness courts one set of problems, over-assertiveness creates another." Assertiveness manuals recognize that "many people, when trying out assertive behaviour for the first time, find that they go too far and become aggressive."

In the late 1970s and early 1980s, in the heyday of assertiveness training, some so-called assertiveness training techniques were distorted and "people were told to do some pretty obnoxious things in the name of assertiveness. Like blankly repeating some request over and over until you got your way". Divorced from respect for the rights of others, so-called assertiveness techniques could be psychological tools that might be readily abused: The line between repeatedly demanding with sanctions ("broken record") versus coercive nagging, emotional blackmail, or bullying, could be a fine one, and the caricature of assertiveness training as "training in how to get your own way ... or how to become as aggressive as the next person" was perpetuated.




</doc>
<doc id="26511362" url="https://en.wikipedia.org/wiki?curid=26511362" title="Hypoactivity">
Hypoactivity

Hypoactivity is an inhibition of behavioral or locomotor activity.

Hypoactivity is a characteristic effect of sedative agents and many centrally acting anesthetics. Other drugs such as antipsychotics and mCPP also produce this effect, often as a side effect.

It may be a characteristic symptom of the inattentive type of ADHD (ADHD-PI) and sluggish cognitive tempo.



</doc>
<doc id="1101567" url="https://en.wikipedia.org/wiki?curid=1101567" title="Courtesy">
Courtesy

Courtesy (from the word "courteis", from the 12th century) is gentle politeness and courtly manners. In the Middle Ages in Europe, the behaviour expected of the nobility was compiled in courtesy books.

The apex of European courtly culture was reached in the Late Middle Ages and the Baroque period (i.e. roughly the four centuries spanning 1300–1700). 
The oldest courtesy books date to the 13th century, but they become an influential genre in the 16th, the most influential of them being "Il Cortegiano" (1508), which not only covered basic etiquette and decorum but also provided models of sophisticated conversation and intellectual skill.
The royal courts of Europe did, of course, persist well into the 18th century (and to some limited extent to the present day), but in the 18th century, the notion of "courtesy" was replaced by that of "gallantry", referring to an ideal emphasizing the display of affected sensitivity in direct contrast with the ideals of self-denial and dignified seriousness that were the Baroque norm. 
During the late medieval and early modern period, the bourgeois class tended to emulate the courtly etiquette of their betters. This changed in the 19th century, after the end of the Napoleonic Wars, with the emergence of a middle class with its own set of bourgeois etiquette, which in turn was mocked in the classist theory of Marxism as "petite bourgeoisie".

The analogue concept in the court culture of medieval India was known by the Sanskrit term "dakṣiṇya", literally meaning "right-handedness", but as in English "dexterity" having a figurative meaning of "apt, clever, appropriate", glossed as "kindness and consideration expressed in a sophisticated and elegant way".


</doc>
<doc id="4585349" url="https://en.wikipedia.org/wiki?curid=4585349" title="Self-actualization">
Self-actualization

Self-actualization is a term that has been used in various psychology theories, often in different ways. 

The term was originally introduced by the organismic theorist Kurt Goldstein for the motive to realize one's full potential: "the tendency to actualize itself as fully as possible is the basic drive ... the drive of self-actualization." Carl Rogers similarly wrote of "the curative force in psychotherapy "man's tendency to actualize himself, to become his potentialities" ... to express and activate all the capacities of the organism." 

The concept was brought most fully to prominence in Abraham Maslow's hierarchy of needs theory as the final level of psychological development that can be achieved when all basic and mental needs are essentially fulfilled and the "actualization" of the full personal potential takes place. Maslow defined self-actualization to be "the desire for self-fulfillment, namely the tendency for him [the individual] to become actualized in what he is potentially. This tendency might be phrased as the desire to become more and more what one is, to become everything that one is capable of becoming." Maslow used the term self-actualization to describe a desire, not a driving force, that could lead to realizing one's capabilities. He did not feel that self-actualization determined one's life; rather, he felt that it gave the individual a desire, or motivation to achieve budding ambitions. Maslow's usage of the term is now popular in modern psychology when discussing personality from the humanistic approach. A basic definition from a typical college textbook defines self-actualization according to Maslow simply as "the full realization of one's potential" and of one's "true self."

Classical Adlerian psychotherapy also promotes this level of psychological development by utilizing the foundation of a 12-stage therapeutic model to realistically satisfy the basic needs. This then leads to an advanced stage of "meta-therapy", creative living, and self/other/task-actualization. Gestalt therapy, acknowledging that 'Kurt Goldstein first introduced the concept of the "organism as a whole,"' which is built on the assumption that "every individual, every plant, every animal has only one inborn goal to actualize itself as it is."Kurt Goldstein's book, "" (1939), presented self-actualization as "the tendency to actualize, as much as possible, [the organism's] individual capacities" in the world. The tendency toward self-actualization is "the only drive by which the life of an organism is determined." However, for Goldstein self-actualization cannot be understood as a kind of goal to be reached sometime in the future. At any moment, the organism has the fundamental tendency to actualize all its capacities and its whole potential, as it is present in that exact moment, under the given circumstances. Under the influence of Goldstein, Abraham Maslow developed a hierarchical theory of human motivation in "Motivation and Personality" (1954).

Carl Rogers used the term "self-actualization" to describe something distinct from the concept developed by Maslow: the actualization of the individual's sense of 'self.' In Rogers' theory of person-centered therapy, self-actualization is the ongoing process of maintaining and enhancing the individual's self-concept through reflection, reinterpretation of experience, allowing the individual to recover, develop, change, and grow. Self-actualization is a subset of the overall organismic actualizing tendency, and begins with the infant learning to differentiate what is "self" and what is "other" within its "total perceptual field," as their full self-awareness gradually crystallizes. Interactions with significant others are key to the process of self-actualization:

The process of self-actualization is continuous as the individual matures into a socially competent, interdependent autonomy, and is ongoing throughout the life-cycle. When there is sufficient tension between the individual's sense of self and their experience, a psychopathological state of incongruence can arise, according to Rogers, "individuals are culturally conditioned, rewarded, reinforced, for behaviors which are in fact perversions of the natural directions of the unitary actualizing tendency." In Rogers' theory self-actualization is not the end-point; it is the process that can, in conducive circumstances (in particular the presence of positive self-regard and the empathic understanding of others), lead to the individual becoming more "fully-functioning."

Abraham Maslow's book, "Motivation and Personality" (1954), started a philosophical revolution out of which grew humanistic psychology. This changed the view of human nature from a negative point of view - man is a conditioned or tension reducing organism - to a more positive view in which man is motivated to realize his full potential, which is reflected in his hierarchy of needs and in his theory of Self-actualization. Self-actualization is at the top of Maslow's hierarchy of needs, and is described as becoming "'fully human' ... maturity or self-actualization." It's considered a part of Humanistic psychology, which is one of several methods used in psychology for studying, understanding, and evaluating personality. The humanistic approach was developed because other approaches, such as the psychodynamic approach made famous by Sigmund Freud, focused on unhealthy individuals that exhibited disturbed behavior; whereas the humanistic approach focuses on healthy, motivated people and tries to determine how they define the self while maximizing their potential. Stemming from this branch of psychology is Maslow's hierarchy of needs. According to Maslow, people have lower order needs that in general must be fulfilled before high order needs can be satisfied: 'five sets of needs physiological, safety, belongingness, esteem, and finally self-actualization'.

Theoretically, once these needs are met, an individual is primed for self-actualization. However, Maslow later suggested that there are two more phases an individual must progress through before self-actualization can take place. These include "the cognitive needs," where a person will desire knowledge and an understanding of the world around them, "the aesthetic needs," which include a need for "symmetry, order, and beauty." Once all these needs have been satisfied, the final stage of Maslow's hierarchy self actualization can take place. Maslow also added a further step beyond self-actualization, which is self-transcendence. Self transcendence occurs at the "very highest and most inclusive or holistic levels of human consciousness."

Maslow's writings are used as inspirational resources. The key to Maslow's writings is understanding that there are no quick routes to self-actualization: rather it is predicated on the individual having their lower deficiency needs met. Once a person has moved through feeling and believing that they are deficient, they naturally seek to grow into who they are, i.e. self-actualization. Elsewhere, however, Maslow (2011) and Carl Rogers (1980) both suggested necessary attitudes and/or attributes that need to be inside an individual as a pre-requisite for self-actualization. Among these are a real wish to be themselves, to be fully human, to fulfill themselves, and to be completely alive, as well as a willingness to risk being vulnerable and to uncover more "painful" aspects in order to learn about/grow through and integrate these parts of themselves (this has parallels with Jung's slightly similar concept of individuation).

Although their studies were initially biologically-centered (or focused around the more ordinary, psychological self-nature), both Maslow (2011) and Rogers (1980) became more open to "spirituality" and grew to accept a more open and "spiritual" conception of man before the end of their lives. Also, there have been many similarities and cross-references between various spiritual schools or groups (particularly Eastern spiritual ways) in the past 40 years. Sri Ramana Maharshi's description, that complete and spiritual self-realization is characterized by "being" ("sat"), "consciousness" ("chit") and "bliss" ("ananda"), can be seen as a reflection of humanistic thinking; the experience of a self-actualizing person partakes in these things to some degree: "beingness", "awareness", and "meaningful happiness", even if one can go further than self-actualization into self-transcendence.

As Abraham Maslow noted, the basic needs of humans must be met (e.g. food, shelter, warmth, security, sense of belonging) before a person can achieve self-actualization. Yet, Maslow argued that reaching a state of true self-actualization in everyday society was fairly rare. Research shows that when people live lives that are different from their true nature and capabilities, they are less likely to be happy than those whose goals and lives match. For example, someone who has inherent potential to be a great artist or teacher may never realize their talents if their energy is focused on attaining the basic needs of humans. As a person moves up Maslow's hierarchy of needs, they may eventually find themselves reaching the summitself-actualization. Maslow's hierarchy of needs begins with the most basic necessities deemed "the physiological needs" in which the individual will seek out items like food and water, and must be able to perform basic functions such as breathing and sleeping. Once these needs have been met, a person can move on to fulfilling "the safety needs", where they will attempt to obtain a sense of security, physical comforts and shelter, employment, and property. The next level is "the belongingness and love needs", where people will strive for social acceptance, affiliations, a sense of belongingness and being welcome, sexual intimacy, and perhaps a family. Next are "the esteem needs", where the individual will desire a sense of competence, recognition of achievement by peers, and respect from others.

A more explicit definition of self-actualization according to Maslow is "intrinsic growth of what is already in the organism, or more accurately of what is the organism itself ... self-actualization is growth-motivated rather than deficiency-motivated." This explanation emphasizes the fact that self-actualization cannot normally be reached until other lower order necessities of Maslow's hierarchy of needs are satisfied. While Goldstein defined self-actualization as a driving force, Maslow uses the term to describe personal growth that takes place once lower order needs have essentially been met, one corollary being that, in his opinion, "self-actualisation ... rarely happens ... certainly in less than 1% of the adult population." The fact that "most of us function most of the time on a level lower than that of self-actualization" he called the "psychopathology of normality".

While the theory is generally portrayed as a fairly rigid hierarchy, Maslow noted that the order in which these needs are fulfilled does not always follow this standard progression. For example, he notes that for some individuals, the need for self-esteem is more important than the need for love. For others, the need for creative fulfillment may supersede even the most basic needs.

Instead of focusing on what goes wrong with people, Maslow wanted to focus on human potential, and how we fulfill that potential. Maslow (1943, 1954) stated that human motivation is based on people seeking fulfillment and change through personal growth. Self-actualized people as those who were fulfilled and doing all they were capable of. It refers to the person's desire for self-fulfillment, namely, to the tendency for him to become actualized in what he is potentially. "The specific form that these needs will take will of course vary greatly from person to person. In one individual it may take the form of the desire to be an ideal mother, in another it may be expressed athletically, and in still another it may be expressed in painting pictures or in inventions." 

A "self-actualizer" is a person who is living creatively and fully using his or her potentials. "What a man can do, he must do." It refers to the desire for self-fulfillment, namely, to the tendency for him to become actualized in what he is potentially. Maslow based his theory partially on his own assumptions or convictions about human potential and partially on his case studies of historical figures whom he believed to be self-actualized, including Albert Einstein and Henry David Thoreau. He considered self-actualizing people to possess "an unusual ability to detect the spurious, the fake, and the dishonest in personality, and in general to judge people correctly and efficiently."Maslow examined the lives of each of these people in order to assess the common qualities that led each to become self-actualized. In his studies, Maslow found that self-actualizers really do share similarities. He also believed that each of these people had somehow managed to find their core-nature that is unique to them, and is one of the true goals of life. Whether famous or unknown, educated or not, rich or poor, self-actualizers tend to fit the following profile.

Maslow's self-actualizing characteristics are:


Maslow early noted his impression that "impulsivity, the unrestrained expression of any whim, the direct seeking for 'kicks' and for non-social and purely private pleasures ... is often mislabelled self-actualization." In this sense, "self-actualization" is little more than what Eric Berne described as the game of "'Self-Expression' ... based on the dogma 'Feelings are Good'".

Broader criticism from within humanistic psychology of the concept of self-actualization includes the danger that 'emphasis on the actualizing tendency ... can lead to a highly positive view of the human being but one which is strangely non-relational.' According to Fritz Perls there is also the risk of confusing ""self"-actualizing and self-"image" actualizing ... the curse of the ideal." For Perls, by conflating "the "virtue" of self-actualization and the "reality" of self-actualization," the latter becomes merely another measuring rod for the "topdog" the nagging conscience: "You tell me to do things. You tell me to be "real". You tell me to be self-actualized ... I don't have to be that good!" Barry Stevens remarked: 

According to Paul Vitz, this may be connected with the charge that "Rogers and Maslow both transform self-actualization from a descriptive notion into a moral norm." Although if it is indeed as good a reality as they purport, then a certain eagerness in their communication is understandable.

In general, during the early twenty-first-century, "the usefulness of the concepts of self and self-actualization continue to attract discussion and debate."

Also, there may be a common feeling that the possibility of 'self-actualization' is reserved for those people who have been lucky in life and don't have to struggle for their day-to-day survival in a dead-end job. Notwithstanding, Maslow (2011) suggested that it was very much about the attitude the individual brought to his/her life that might be the crucial catalyst for where one's life and self-growth goes. There are many examples of when people have been in basically the same circumstances, but have turned out very differently, which might indicate that attitude can have an enormous bearing upon one's fate; however, there is always the question: what "is" it that makes attitude different from person to person?



</doc>
<doc id="24538180" url="https://en.wikipedia.org/wiki?curid=24538180" title="Superficial charm">
Superficial charm

Superficial charm (or insincere charm or glib charm) is the tendency to be smooth, engaging, charming, slick and verbally facile.

The phrase often appears in lists of attributes of psychopathic personalities, such as in Hervey M. Cleckley's "The Mask of Sanity", and Robert D. Hare's Hare Psychopathy Checklist.

Classical rhetoric had early singled out the critical category of the superficial charmer, whose merit was purely verbal, without underlying substance.

Superficial charmers, in their more benign manifestations, can produce a variety of positive results, their conversational skills providing light-hearted entertainment in social settings through their ability to please.

Critics object that there are few objective criteria whereby to distinguish superficial from genuine charm, and that as part of the conventional niceties of politeness, we all regularly employ superficial charm in everyday life, conveying superficial solidarity and fictitious benevolence to all social interactions.

Associated expressions are "charm offensive", "turning on the charm" and "superficial smile".

Contemporary interest in superficial charm goes back to Hervey M. Cleckley's classic study (1941) of the sociopath: since his work it has become widely accepted that the sociopath/psychopath was characterised by superficial charm and a disregard for other people's feelings. According to Hare, "Psychopathic charm is not in the least shy, self-conscious, or afraid to say anything." 

Subsequent studies have refined, but not perhaps fundamentally altered, Cleckley's initial assessment. In the latest diagnostic review, Cleckley's mix of intelligence and superficial charm has been redefined to reflect a more deviant demeanour, talkative, slick, and insincere. A distinction can also be drawn between a subtle, self-effacing kind of sociopathic charm, and a more expansive, exhilarating spontaneity which serves to give the sociopath a sort of animal magnetism.

The authors of the book describe a five phase model of how a typical workplace psychopath climbs to and maintains power. In phase one (entry), the psychopath will use highly developed social skills and charm to obtain employment by an organisation. Corporate psychopaths within organizations may be singled out for rapid promotion because of their polish, charm, and cool decisiveness. Their superficial charm may be misinterpreted as charisma.

The term also occurs in Hotchkiss' discussion of narcissists: "Their superficial charm can be enchanting." For such figures, however, there is no substance behind the romantic gestures, which only serve to feed the narcissist's own ego. 

Narcissists are known as manipulative in a charming way, entrapping their victims through a façade of understanding into suspending their self-protective behaviour and lowering their personal boundaries. Closely related is the way impostors are able to make people fall in love with them to satisfy their narcissistic needs, without reciprocating in any real sense or returning their feelings.

Social chameleons have been described as adept in social intelligence, able to make a charming good impression, yet at the price of their own true motivations. Their ability to manage impressions well often leads to success in areas like the theatre, salesmanship, or politics and diplomacy. But when lacking a sense of their own inner needs, such superficial extraverts may end up (despite their charm) as rootless chameleons, endlessly taking their social cues from other people.

Similarly, for the histrionic personality, the attention seeking through superficial charm may only reinforce the splitting of the real self from the public presentation in a vicious circle.

A "charm offensive" is a related concept meaning a publicity campaign, usually by politicians, that attempts to attract supporters by emphasizing their charisma or trustworthiness. The first recorded use of the expression is in the California newspaper "The Fresno Bee Republican" in October 1956.

F. Scott Fitzgerald explored the destructive consequences of excess charm in stories like "Magnetism", maintaining that charm, for those who had it, had a life of its own, demanding constant use to keep it in peak condition.




</doc>
<doc id="19682421" url="https://en.wikipedia.org/wiki?curid=19682421" title="Behavioral contagion">
Behavioral contagion

Behavioral contagion is a type of social influence. It refers to the propensity for certain behavior exhibited by one person to be copied by others who are either in the vicinity of the original actor, or who have been exposed to media coverage describing the behavior of the original actor. It was originally used by Gustave Le Bon (1895) to explain undesirable aspects of behavior of people in crowds. A variety of behavioral contagion mechanisms were incorporated in models of collective human behavior.

The occurrence of behavioral contagion has been attributed to a variety of different factors, but the predominant theory is that of the reduction of restraints, put forth by Fritz Redl in 1949 and analyzed in depth by Ladd Wheeler in 1966. Even with the popularity of this theory, social psychologists acknowledge a number of factors that influence the likelihood of behavioral contagion occurring, such as deindividuation (Festinger, Pepitone, & Newcomb, 1952) and the emergence of social norms (Turner, 1964). Freedman, Birsky and Cavoukian (1980) have also focused on the effects of physical factors on contagion, in particular, density and number.

Ogunlade (1979, p. 205) describes behavioral contagion as a “spontaneous, unsolicited and uncritical imitation of another’s behavior” that occurs when certain variables are met: a) the observer and the model share a similar situation or mood (this is one way behavioral contagion can be readily applied to mob psychology); b) the model’s behavior encourages the observer to review his condition and to change it; c) the model’s behavior would assist the observer to resolve a conflict by reducing restraints, if copied; and d) the model is assumed to be a positive reference individual.

Behavioral contagion is a result of the reduction of fear or restraints – aspects of a group or situation which prevent certain behaviors from being performed. Restraints are typically group-derived, meaning that the “observer”, the individual wishing to perform a certain behavior, is constrained by the fear of rejection by the group, who would view this behavior as a “lack of impulse control”.

An individual (the “observer”) wants to perform some behavior, but that behavior would violate the unspoken and accepted rules of the group or situation they are in; these rules are the restraints preventing the observer from performing that action. Once the restraints are broken or reduced the observer is then “free” to perform the behavior his- or herself; this is achieved by the “intervention” of the model. The model is another individual, in the same group or situation as the observer, who performs the behavior which the observer wished to perform. Stephenson and Fielding (1971) describe this effect as “[Once] one member of a gathering has performed a commonly desired action, the payoffs for similar action or nonaction are materially altered. … [The] initiator, by his action, establishes an inequitable advantage over the other members of the gathering which they may proceed to nullify by following his example. ”

Density refers to the amount of space available to a person – high density meaning there is less space per person – and number refers to the size of the group. Freedman (1975) put forth the intensification theory, which posits that high density makes the other people in a group more salient features of the environment, this magnifying the individual’s reaction to them. Research has shown that high density does in fact increase the likelihood of contagion (Freedman, 1975; Freedman, Birsky, & Cavoukian, 1980). Number also has an effect on contagion, but to a lesser degree than density.

Stephenson and Fielding (1971) state that the identity of the model is a factor that influences contagion (p. 81). Depending on the behavior, sex of the model may be a factor in the contagion of that behavior being performed by other individuals – particularly in instances of adult models performing aggressive behavior in the presence of children-observers (Bandura, Ross, & Ross, 1963) {Imitation of film-mediated aggressive models}. In this particular series of experiments – Albert Bandura’s Bobo doll experiments from 1961 and 1963 – where the behavior of children was studied after the children watched an adult model punching a bobo doll and the model received a reward, a punishment, or there were no consequences, the analyses revealed that the male model influenced the participants’ behavior to a greater extent than did the female model; this was true for both the aggressive and the nonaggressive male models (p. 581).

Ogunlade (1979) found that extroverts, who are described as impulsive and sociable individuals, are more likely to be susceptible to contagion than introverted individuals, who are described as reserved and emotionally controlled.

Gino, Ayal and Ariely (2009) state that an important factor influencing contagion is the degree to which the observer identifies with the others of the group (p. 394). When identification with the rest of the group is strong, the behaviors of the others will have a larger influence.

Contagion is only one of a myriad of types of social influence.

Conformity is a type of social influence that is very similar to contagion. It is almost identical to another type of social influence, "pressures toward uniformity" (social pressures) (Festinger, 1954), which differ only in the research techniques they are associated with (Wheeler, 1966, p. 182).

Both conformity and contagion involve some sort of conflict, but differ in the roles other individuals play in that conflict. In conformity, the other individuals of the group try to pressure the observer into performing a behavior; the model then performs some other behavior in the vicinity of the observer. This results in the observer creating restraints against the pressured behavior and a conflict between the pressured behavior and the behavior performed by the model. In the end, the observer either performs the model’s behavior his-/herself, rejects the model, or pressures the model to perform the original pressured behavior (Wheeler, Table 1). In contagion, the model’s behavior results in the removing of restraints and the resolving of the conflict, while in conformity, the model’s behavior results in the creation of restraints and of the conflict.

Social facilitation, another type of social influence, is distinguished from contagion, as well as from conformity and social pressures, by the lack of any marked conflict. It is said to occur when the performance of an instinctive pattern of behavior by an individual acts as a releaser for the same behavior in others, and so initiates the same line of action in the whole group (Thorpe, 1956, p. 120). Bandura and Walters (1963, p. 79), give the example of an adult, who has lost the unique aspects of the dialect of the region where they were raised, returns for a visit and “regains” those previously lost patterns of speech. Starch (1911) referred to this phenomenon as an “unintentional or unconscious imitation”.

Imitation is different from contagion in that it is learned via reward and punishment and is generalized across situations. Imitation can also be a generic term for contagion, conformity, social pressures, and social facilitation.

While behavioral contagion is largely about how people might be affected by observations of the expressions or behavior of others, research has also found contagion in the context of a competition where mere awareness of an ongoing competition can have an influence on noncompetitors’ task performance, without any information about the actual behavior of the competitors. .

Behavioral contagion, largely discussed in the behaviors of crowds, and closely related to emotional contagion, plays a large role in gatherings of two or more people. In the original Milgram experiment on obedience, for example, where participants, who were in a room with only the experimenter, were ordered to administer increasingly more severe electrical shocks as punishment to a person in another room (from here on out, referred to as the “victim”), the conflict or social restraint experienced by the participants was the obligation to not disobey the experimenter – even when shocking the victim to the highest shock level given, a behavior which the participants saw as opposing their personal and social ideals (Milgram, 1965, p. 129).

Milgram also conducted two other experiments, replications of his original obedience experiment, with the intent being to analyze the effect of group behavior on participants: instead of the subject being alone with the experimenter, two confederates were utilized. In the first of the two experiments, “Groups for Disobedience,” the confederates defied the experimenter and refused to punish the victim (p. 130). This produced a significant effect on the obedience of the participants: in the original experiment, 26 of the 40 participants administered the maximum shock; in the disobedient groups experiment, only 4 of 40 participants administered the highest level of voltage (Table 1). Despite this high correlation between shock level administered and the obedience of the group in the disobedient groups experiment, there was no significant correlation for the second of the replicated experiments: “Obedient Groups”, where the confederates did not disobey the experimenter and, when the participant voiced angst regarding the experiment and wished to stop administering volts to the victim, the confederates voiced their disapproval (p. 133). Milgram concludes the study by remarking that “the insertion of group pressure in a direction opposite that of the experimenter’s commands produces a powerful shift toward the group. Changing the group movement does not yield a comparable shift in the [participant’s] performance. The group success in one case and failure in another can be traced directly to the configuration of motive and social forces operative in the starting situation.” That is, if the group’s attitudes are similar to or compatible with the participant’s/observer’s, there is a greater likelihood that the participant/observer will join with the group (p. 134).



</doc>
<doc id="29054097" url="https://en.wikipedia.org/wiki?curid=29054097" title="National Survey of Sexual Health and Behavior">
National Survey of Sexual Health and Behavior

The National Survey of Sexual Health and Behavior is a study of human sexual behavior conducted in the United States by the Center for Sexual Health Promotion at Indiana University in Bloomington. Articles based on the study were first released in a supplement to the October, 2010 issue of "Journal of Sexual Medicine". 

The survey, of about 6,000 subjects between the age of 14 and 94, living in the United States, showed a wide variety of sexual behavior. According to Debby Herbenick, PhD, of Indiana University in Bloomington, "Adult men and women rarely engage in just one sex act when they have sex."

Significant findings include use of condoms in about 25% of instances of vaginal sex by adults, about 33% if they were single, with teenagers using condoms 70 to 80% of the time. Only a low level of sexual activity among the approximately 800 teenagers surveyed was found with incidence increasing with age. It was discovered that about one third of women reported pain during intercourse. A discrepancy was discovered between men's perception that their female partner had experienced orgasm, about 85%, and women's self-reporting of 64%.

The studies were sponsored by Church and Dwight maker of Trojan condoms. The sponsor offered input with respect to gathering information regarding use of condoms, settling with a formulation which requested information on whether condoms were used or not during the last 10 sexual encounters of each respondent.

With respect to condom use results were encouraging especially with respect to teenagers. Ethnic populations impacted by HIV/AIDS showed a higher rate of use than then general population as did dating adults. Discrepancies remain between the level of use optimal for public health and reported rate of use particularly by people over 40.

Women reported less satisfaction with sexual activity than men with less pleasure, less arousal, and fewer orgasms. This was hypothesized by one of the researchers as being related to the greater incidence of pain also reported by women.



</doc>
<doc id="12116946" url="https://en.wikipedia.org/wiki?curid=12116946" title="Lawn dart effect">
Lawn dart effect

In psychology, the lawn dart effect occurs when fighter aircraft pilots accelerate horizontally at more than 1 standard gravity. The effect occurs when such extreme stimulation to the vestibular system leads to the perception that the aircraft is climbing, prompting the pilot to lower the aircraft's pitch attitude, or drop the nose. 



</doc>
<doc id="239414" url="https://en.wikipedia.org/wiki?curid=239414" title="Teasing">
Teasing

Teasing has multiple meanings and uses. In human interactions, teasing exists in three major forms: "playful", "hurtful", and "educative". Teasing can have a variety of effects, depending on how it is utilized and its intended effect. When teasing is unwelcome, it may be regarded as harassment or mobbing, especially in the work place and school, or as a form of bullying or emotional abuse. If done in public, it may be regarded as humiliation. Teasing can also be regarded as educative when it is used as a way of Informal learning. Adults in some of the Indigenous American Communities often tease children to playfully illustrate and teach them how their behaviour negatively affects the community. Children in many Indigenous American Communities also learn by observing what others do in addition to collaborating with them. Along with teasing, this form of informal learning is different from the ways that Western American children learn. Informal ways of child learning include mutual responsibility, as well as active collaboration with adults and peers. This differentiates from the more formal way of learning because it is not adult-oriented.

People may be teased on matters such as their appearance, weight, behavior, abilities, clothing, and intelligence. From the victim's point of view, this kind of teasing is often hurtful, irrespective of the intention of the teaser.

One may also tease an animal. Some animals, such as dogs and cats, may recognize this both as play or harassment.

A common form of teasing is verbal bullying or taunting. This behavior is intended to distract, disturb, offend, sadden, anger, bother, irritate, or annoy the recipient. Because it is hurtful, it is different from joking and is generally accompanied by some degree of social rejection. Teasing can also be taken to mean "To make fun of; mock playfully" or be sarcastic about and use sarcasm.

Dacher Keltner utilizes Penelope Brown's classic study on the difference between "on-record" and "off-record" communication to illustrate how people must learn to read others' tone of voice and facial expressions in order to learn appropriate responses to teasing.

A form of teasing that is usually overlooked is educational teasing. This form is commonly used by parents and caregivers in two Indigenous American Communities and Mexican Heritage communities to guide their children into responding with more Prosocial behavior. For example, when a parent teases a child who is throwing a tantrum for a piece of candy, the parent will pretend to give the child candy but then take it away and ask the child to correct their behavior before giving the child that piece of candy. In this way, the parent teaches the child the importance of maintaining self-control. When adults educate children through teasing, they are informally teaching the children. This type of learning is often overlooked because it is different from the way Western American Communities teach their children.

Another form of teasing is to pretend to give something which the other desires, or give it very slowly. This is usually done by arousing curiosity or desire, and may not actually involve the intent to satisfy or disclose. This form of teasing could be called "tantalizing", after the story of Tantalus. Tantalizing is generally playful among adults, although among children it can be hurtful, such as when one child acquires a possession of another's property and will not return it. It is also common in flirting and dating. For example, a man or woman who is interested in someone might reject an advance the first time in order to arouse interest and curiosity, and give in the second or third time.

Whether teasing is playful or hurtful or educative is largely subject to the interpretation of the person being teased. If the person being teased feels harmed, then the teasing is hurtful. A difference in power between people may also make the behavior hurtful rather than playful. Ultimately though, if someone perceives him or herself as the victim of teasing, and experiences the teasing as unpleasant, then it is considered hurtful. If parents' intentions are positive, as in many Indigenous American Communities, then teasing to the community can be seen as an educational tool. The child may or may not understand that in the moment. If the other person continues to do it after being asked to stop then it is a form of bullying or abuse.

Another way to look at teasing is as an honest reflection on differences, expressed in a joking fashion with the goal of "clearing the air". It can express a comfort with the other which can be comforting. As opposed to being nice to someone's face while making disparaging remarks behind their back, teasing can be a way to express differences in a direct fashion rather than internalizing them.

Some indigenous American communities use teasing to teach their children about the expectations and values of the community and to change negative behaviors. Teasing gives children a better understanding of how their behavior affects the people around them. Teasing in Indigenous American communities is used to learn community acceptance, humbleness, correcting a behavior and social control.

In some Mexican indigenous American communities, teasing is used in an effective educative way. Teasing is found more useful because it allows the child to feel and understand the relevant effect of their behavior instead of receiving out of context feedback. Some parents in Indigenous American communities believe it mildly embarrasses the children in a shared reference to give them a good sense of the consequences of their behavior. This type of teasing is thought to teach children to be less egocentric, teaches autonomy and responsibility to monitor their own behavior. Parental teasing also is practiced to encourage the child to think of their behavior in a social context. Some Indigenous American mothers have reported that this urges the children to understand how their behavior affects others around them. From examples in Eisenberg's article, parents use teasing as way of reinforcing relationships and participation in group/community activities (Prosocial behavior). Parents tease their children to be able to “control the behavior of the child and to have fun with them”.

An Inuit principal of learning that follows a similar teasing pattern is known as issumaksaiyuk, meaning to cause thought. Oftentimes, adults pose questions or hypothetical situations to the children (sometimes dangerous) but in a teasing, playful manner, often dramatizing their responses. These questions raise the child's awareness to issues surrounding their community, as well as give them a sense of agency within the community as a member capable of having an effect and creating change. Once the child begins to answer the questions reasonably, like an adult, the questions would stop.

In some Cherokee communities, teasing is a way of diffusing aggressive or hostile situations and teaching the individual about the consequences of their behavior. It allows the individual to feel how their behaviors are affecting others and control their behavior.

To tease, or to "be a tease" in a sexual sense can refer to the use of posture, language or other means of flirting to cause another person to become sexually aroused. Such teasing may or may not be a prelude to intercourse, an ambiguity which can lead to uncomfortable situations. In a more physical sense, it can also refer to sexual stimulation.

Teasing is also used to describe playing part of a song at a concert. Jam bands will often quote the main riff of another song during jams.

"Tease it" is also used as a slang term to smoke marijuana. The word "tease" can also be used as a noun to stand for marijuana.

In a very different context, hair can be teased, "ratted", or more correctly, "backcombed". As the name suggests, backcombing involves combing the hair backwards from end to root to intentionally tangle the strands to create volume. It can also be done excessively in sections to create dreadlocks.




</doc>
<doc id="3135680" url="https://en.wikipedia.org/wiki?curid=3135680" title="Masculine psychology">
Masculine psychology

Masculine psychology refers to an archetypal gender-related psychology supposedly of male human identity. It is examined through the lenses of history, which have produced standard culture, especially as it relates to gender studies. Gender differences are determined from a scientific and empirical approach, while the other approaches are theorized as they are aligned to the psychoanalytic tradition. Psychoanalytic approaches have cultivated concepts like masculinity and machismo because of biased analytic inclusions.

Enlightenment of modern history became an influencing discipline during the Renaissance. Intellectual approaches of modern philosophy shaped cultural customs as thinkers became fixated with creating a world through law, artistic representation, western expansion, social structure, and ultimately control.

The masculine psychology is the effect of gender differencing, which was closely governed resulting Europe's western political and social control. Conflict of social structures became evident in society as there were beliefs in liberty through common resistance, such as protest writing. Writers, artists, and liberalist alike rebelled from social structures through their forms of artistry.

The original languages of several religions have gender-specific pronouns and verb conjugations. As language was constructed by man, references to God often use the masculine pronoun "He" and refer to God as masculine; social modernity enabled hegemonic language.

In many religions, including Judaism, Sikhism, and Islam, God has traditionally been referred to by using masculine pronouns. However, in the original Hebrew and Aramaic languages of the Old Testament, God is referred to by a variety of names, in both singular and plural forms, and so it is not clear that the use of a masculine pronoun necessarily conveys actual gender. For example, in Sikhism the use of masculine pronouns is due to grammatical conventions and does not signify actual gender. In Islam, similarly, God is generally depicted as having a male gender, though there may be debate as to God's gender. As there is no neutral gender in Arabic, God is referred to in the masculine form by default, and it is universally understood that God (Allah) is not a woman.

In Mainstream Christianity God is understood as a Trinity of three persons in one God, consisting of Father, Son, and Holy Spirit. While in mainstream Christianity God is thought of in masculine terms, teachings generally state that God has no gender, except in his incarnation as Jesus Christ, due to the fact that He is a spiritual being. However, the names of Father and Son clearly imply masculinity, and the Gospel of John implies the masculinity of the Spirit by applying a masculine demonstrative pronoun to the grammatically neuter antecedent. Still, teachings regarding the gender and nature of God vary between Christian groups, and some clearly state that God is male. These include the teachings of the Church of Jesus Christ of Latter-day Saints, which explicitly describe God the Father as being male, corporeal, and separate in being from the Son, Jesus Christ. Unusually, Latter-day Saint teachings also indicate the existence of a Heavenly Mother, who is the wife of the Heavenly Father, and that together they are the spiritual parents of humankind. This reinforces the idea that God is analogous to our earthly fathers. However, these beliefs are not common among Christian groups, which generally see God as having both male and female attributes and therefore no need for a female counterpart.

In many polytheistic religions, there are both gods and goddesses clearly defined. This includes many pagan religions, such as those involving Greek mythology, Norse mythology, and Celtic mythology. This distinction also exists in other polytheisms. For example, Hinduism distinguishes between God and Goddess. There are three main male gods known as Tridev (Sanskrit:त्रिदेव) and their wives are worshiped as goddesses. In Hindu mythology, all the forces converge to form a female supreme power known as aadishakti (Sanskrit: आदिशक्ति.).

The western masculine psychology derived from religious social structures that created ideologies of masculinity (and femininity).

Among artists and scientists during the Renaissance, it was the prevailing belief that the study of the male form was in itself a study of God. Michelangelo's "David" is based upon this artistic discipline, which is known as "disegno". Under this discipline, sculpture is considered to be the finest form of art because it mimics divine creation. Because Michelangelo adhered to the concepts of "disegno", he worked under the premise that the image of "David" was already in the block of stone he was working on—in much the same way as the human soul is thought by some to be found within the physical body as concluded by psychoanalytic theory.

Masculine identities or concepts are developed differently across cultures and subcultures. Because masculinity can take varied forms, forms of manliness can be changed and utilized at once. Masculine identities are not limited to the archetypal male species, and can be fluid in constructing a new identity outside of binary lenses.

The quest for control and understanding became the Britain's Victorian period, and the creation of laws to fit societal structure was later the Enlightenment period. Laws stood in place to dictate order.

Sodomy was a 1789 law that enforced masculinity in western culture, and specifically, sexual acts by men was deemed illegal. As this law was never repealed since, sodomy laws began to be used in a new way during the late 1960's - distinctly against people who identify as gay. As the gay rights movement began to make headway, and the social condemnation of being gay began to weaken, social conservatives invoked sodomy laws as a justification for discrimination. The gradual liberalization of the Western sexuality, especially in Northern America, led government officials sodomy laws in most states. Several northern American states out-ruled sodomy in just 2003 and culture remains a continuity.

Throughout the quest of control, literature was used to rebel against standard ideologies, for ambiguous forms of same-sex love had so significant during the Victorian culture; "homoaffectional" literature became, both, more explicit in its sexuality and much less common as time progressed. The lack of acknowledging sexual preferences outside of the binary social constructs are effects of an unchallenged masculine psychology.

Social standards of manliness or masculinity have been challenged as much as they have been asserted in Western culture. The study of masculine psychology has brought about the publication of many books, poems, and journals.


Robert Bly inspired the mythopoetic men's movement with his 1990 book "" consisting of retellings of archetypal male myths and analysis of the Grimms' Fairy Tale "Iron John".


David Deida, a noted author and spiritual practitioner, published "The Way Of The Superior Man: A Spiritual Guide to Mastering the Challenges of Women, Work, and Sexual Desire" in 1997. In this book he argues "It is time to evolve beyond the (first-stage) macho jerk ideal, all spine and no heart. It is also time to evolve beyond the (second-stage) sensitive and caring wimp ideal, all heart and no spine. Heart and spine must be united in a single man, and then gone beyond in the fullest expression of love and consciousness possible, which requires a deep relaxation into the infinite openness of this present moment. And this takes a new kind of (third-stage) guts. This is the way of the superior man." Literary critiques of David Deida include the fact that he encourages men to express their sexuality in a manner that still reflects the same attitudes.


Susan Faludi, a noted feminist author, published "" in 1999. In this book she claims that in the 20th century men suffered from the breakdown of patriarchal structures.


Robert L. Moore and Douglas Gillette collaborated on a series of five books on male psychology and mythopoetic aspects of human development, including "King, Warrior, Magician, Lover", and a book exploring each of these four archetypes. The book and its authors are considered important parts of the men's movement in the latter part of the 20th century.


"The White Family of Man: Colonial Discourse and the Reinvention of Patriarchy" (1994). "This text chronicles the dangerous liaisons between gender, race and class that shaped British imperialism and its bloody dismantling. Spanning the century between Victorian Britain and the recent struggle for power in South Africa, the book takes up the complex relationships between race, sexuality, and more.


In his books "Phallos: Sacred Image of the Masculine" and "Castration and Male Rage", Monick correlates male sexuality and spirituality, arguing that the "phallos" (erect penis) is something of an existential God-image for men. He also presents his thesis that there is a difference between masculinity and patriarchy. The author also argues that there is a deep need within men to participate in a fraternity with men and to have their maleness recognized by other men, but that our society often does not take this into account. The author claims that what usually results is that these needs become frustrated and manifest themselves in often anti-social behavior and activities, such as hazing rituals.


Female Masculinity in Dee Rees's "Pariah" is viewed through many different facets as it explores identity and sexuality. This film offers multiple definitions of the monolithic black feminist that is often thought of in a singular aspect. "Pariah" goes against the concept of a binary heterosexist expression of sexuality, for it challenges the concepts of “masculinity” and “aggressiveness” in women. All women who choose this form of sexuality are not limited to one definition of the “third gender”. This film represents the attitudes of the black community towards the “third gender” by revealing that African Americans can offer complex identity formation, which validates humanism in a patriarchal society.


Shakespeare's works are politically influenced as the following plays comment on western patriarchal constructs such as family, monarch, nature, religion, law, and the Renaissance, respectively: "Titus Andronicus" (Family), "Richard III" (Monarchy), "A Midsummer’s Night’s Dream" (Nature), "Hamlet" (Religion), "Measure for Measure" (Law), "Antony and Cleopatra" (Renaissance).


Percy Bysshe Shelley, whose literary career was marked with controversy due conflicting social agendas, is a figures of English romanticism (following the Renaissance). "The Keepsake" (1828) reveals his ideas about religion, socialism, and free love, especially in his essay "On Love".


"We the Animals" (2014) highlights character psychology, familial structure, the learned behavior of masculinity, and it encourages one to imagine a utopian space in which civil and human rights are not tied to a compliance with heteronormative lifestyles.

The male fear of the feminine is a phenomenon that has been discussed since the 1930s. It was first introduced by the German psychoanalyst and critic of Freudian theory, Karen Horney (1932) in her paper titled "The Dread of Woman." Erich Neumann (1954), a German-born Jungian analyst, dedicated one essay to the discussion, titled "The fear of the feminine" (Orig: "Die Angst vor dem Weiblichen", 1959). Neumann regards "patriarchal normality as a form of fear of the feminine" (p. 261).

A later contributor is Chris Blazina, a psychodynamic psychologist and professor based at Tennessee State University. Blazina considers that "the fear of the feminine helps define what is masculine" (1997). In 1986, James O'Neil et al. theorized that the male fear of the feminine is a core aspect of the male psyche. He developed a 37-question psychometric test, a gender role conflict scale (GRCS), to measure the extent to which a man is in conflict with traditional masculine role values. This test is built upon the notion of the male fear of the feminine.

In 2003, Werner Kierski, a London-based German-born psychotherapist and researcher, associated with humanistic psychology and transpersonal and existential psychotherapy designed the first empirical research into the male fear of the feminine with the results published in 2007 and presented to the public at the 2007 annual conference of the American Men's Studies Association (AMSA) and at the 2007 research conference of the British Association for Counselling and Psychotherapy (BACP).

According to the various sources, the male fear of the feminine is connected to influences from their mothers and to cultural norms that prescribe how men must behave in order to feel accepted as men.

When men experience vulnerable feelings and other feelings that are associated with women, men can become frightened. According to Kierski (2007), the fear of the feminine then acts in two ways: a) Like an internal monitor to ensure that men stay within the boundaries of what is regarded as masculine, i.e. being action orientated, self-reliant, guarded, and seemingly independent; b) if a man fails to experience this and feels out of control, vulnerable or dependent, the fear of the feminine can act like a defense, leading to splitting off, repressing, or projecting those feelings.

Figure 1: Male fear of the feminine as an internal monitor and as a defense. Source: Werner Kierski.

Kierski's research claimed that men do acknowledge that male fear of the feminine can have a strong influence on both hetero- and homosexual men. The research has also indicated that there appears to be a link between fear of the feminine and men's negative views about counseling and psychotherapy. In addition, this research has identified four possible groups of experiences that lead to male fear of the feminine, which relate to internal and external triggers. These are: Experiencing vulnerability and uncertainty; women who are strong and competent; women who are angry or aggressive; women who are like their mothers.

Constructions of race and sexuality in the western world caused particular expressions of homophobic racism and sexual violence. Defying traditional gender expectations became unacceptable and spectacles are labeled as “other” for disregarding [gender] norms.

Issues of homophobia and gay bashing are of relevance to the study of masculine psychology. Every year, men (such as Matthew Shepard) die as a result of gay bashing. The victims of gay bashing attacks are most often homosexual males, or those who display what are commonly perceived as effeminate behaviors or mannerisms which are, when seen in males, often associated with homosexuality. Self-identified heterosexual males are usually the perpetrators of gay bashing attacks.

Sigmund Freud presented the thesis that everyone is at some level bisexual, and Alfred Kinsey research results claimed that as many as 37% of American males had engaged in homosexual activity. French-Canadian psychologist Guy Corneau says that despite Kinsey's research results, attitudes toward homosexuality have remained hostile.

The author says it is puzzling that we live in what he considers a male-dominated society, and yet very little work has been done to understand the archetypal basis of masculinity. He suggests that this may be due to a societal assumption of male superiority, founded on the belief that one should not question that which is deemed to be right and superior.

See also: Gay panic defense and Violence § Psychology and sociology



</doc>
<doc id="211430" url="https://en.wikipedia.org/wiki?curid=211430" title="Sexual attraction">
Sexual attraction

Sexual attraction is attraction on the basis of sexual desire or the quality of arousing such interest. Sexual attractiveness or sex appeal is an individual's ability to attract the sexual or erotic interests of other people, and is a factor in sexual selection or mate choice. The attraction can be to the physical or other qualities or traits of a person, or to such qualities in the context where they appear. The attraction may be to a person's aesthetics or movements or to their voice or smell, besides other factors. The attraction may be enhanced by a person's adornments, clothing, perfume or style. It can be influenced by individual genetic, psychological, or cultural factors, or to other, more amorphous qualities. Sexual attraction is also a response to another person that depends on a combination of the person possessing the traits and on the criteria of the person who is attracted.

Though attempts have been made to devise objective criteria of sexual attractiveness and measure it as one of several bodily forms of capital asset ("see erotic capital"), a person's sexual attractiveness is to a large extent a subjective measure dependent on another person's interest, perception, and sexual orientation. For example, a gay or lesbian person would typically find a person of the same sex to be more attractive than one of the other sex. A bisexual person would find either sex to be attractive. Asexuality refers to those who do not experience sexual attraction for either sex, though they may have romantic attraction (homoromantic, biromantic or heteroromantic) or a non-directed libido. Interpersonal attraction includes factors such as physical or psychological similarity, familiarity or possessing a preponderance of common or familiar features, similarity, complementarity, reciprocal liking, and reinforcement.

The ability of a person's physical and other qualities to create a sexual interest in others is the basis of their use in advertising, film, and other visual media, as well as in modeling and other occupations.

In evolutionary terms, the ovulatory shift hypothesis posits that female humans exhibit different sexual behaviours and desires at points in their menstrual cycle, as a means to ensure that they attract a high quality mate to copulate with during their most fertile time. Hormone levels throughout the menstrual cycle affect a woman's overt behaviours, influencing the way a woman presents herself to others during stages of her menstrual cycle, in attempt to attract high quality mates the closer the woman is to ovulation.

Human sexuality has many aspects. In biology, sexuality describes the reproductive mechanism and the basic biological drive that exists in all sexually reproducing species and can encompass sexual intercourse and sexual contact in all its forms. There are also emotional and physical aspects of sexuality. These relate to the bond between individuals, which may be expressed through profound feelings or emotions. Sociologically, it can cover the cultural, political, and legal aspects; philosophically, it can span the moral, ethical, theological, spiritual, and religious aspects.

Which aspects of a person's sexuality attract another is influenced by cultural factors; it has varied over time as well as personal factors. Influencing factors may be determined more locally among sub-cultures, across sexual fields, or simply by the preferences of the individual. These preferences come about as a result of a complex variety of genetic, psychological, and cultural factors.

A person's physical appearance has a critical impact on their sexual attractiveness. This involves the impact one's appearance has on the senses, especially in the beginning of a relationship:


As with other animals, pheromones may have an impact, though less significantly in the case of humans. Theoretically, the "wrong" pheromone may cause someone to be disliked, even when they would otherwise appear attractive. Frequently, a pleasant-smelling perfume is used to encourage the member of the opposite sex to more deeply inhale the air surrounding its wearer, increasing the probability that the individual's pheromones will be inhaled. The importance of pheromones in human relationships is probably limited and is widely disputed, although it appears to have some scientific basis.

Many people exhibit high levels of sexual fetishism and are sexually stimulated by other stimuli not normally associated with sexual arousal. The degree to which such fetishism exists or has existed in different cultures is controversial.

Pheromones have been determined to play a role in sexual attraction between people. They influence gonadal hormone secretion, for example, follicle maturation in the ovaries in females and testosterone and sperm production in males.

Research conducted by Donald G. Dutton and Arthur P. Aron in the 1970s aimed to find the relation between sexual attraction and high anxiety conditions. In doing so, 85 male participants were contacted by an attractive female interviewer at either a fear-arousing suspension bridge or a normal bridge. Conclusively, it was shown that the male participants who were asked by the female interviewer to perform the thematic apperception test (TAT) on the fear-arousing bridge, wrote more sexual content in the stories and attempted, with greater effort, to contact the interviewer after the experiment than those participants who performed the TAT on the normal bridge. In another test, a male participant, chosen from a group of 80, was given anticipated shocks. With him was an attractive female confederate, who was also being shocked. The experiment showed that the male's sexual imagery in the TAT was much higher when self shock was anticipated and not when the female confederate shock was anticipated.

People consciously or subconsciously enhance their sexual attractiveness or sex appeal for a number of reasons. It may be to attract someone with whom they can form a deeper relationship, for companionship, procreation, or an intimate relationship, besides other possible purposes. It can be part of a courtship process. This can involve physical aspects or interactive processes whereby people find and attract potential partners, and maintain a relationship. These processes, which involve attracting a partner and maintaining sexual interest, can include flirting, which can be used to attract the sexual attention of another to encourage romance or sexual relations, and can involve body language, conversation, joking, or brief physical contact.

Men have been found to have a greater interest in uncommitted sex compared to women. Some research shows this interest to be more sociological than biological. Men have a greater interest in visual sexual stimuli than women. However, additional trends have been found with a greater sensitivity to partner status in women choosing a sexual partner and men placing a greater emphasis on physical attractiveness in a potential mate, as well as a significantly greater tendency toward sexual jealousy in men and emotional jealousy in women.

Bailey, Gaulin, Agyei, and Gladue (1994) analyzed whether these results varied according to sexual orientation. In general, they found biological sex played a bigger role in the psychology of sexual attraction than orientation. However, there were some differences between homosexual and heterosexual women and men on these factors. While gay and straight men showed similar psychological interest in casual sex on markers of sociosexuality, gay men showed a larger number of partners in behaviour expressing this interest (proposed to be due to a difference in opportunity). Self-identified lesbian women showed a significantly greater interest in visual sexual stimuli than heterosexual women and judged partner status to be less important in romantic partnerships. Heterosexual men had a significantly greater preference for younger partners than homosexual men. People who identify as asexual may not be sexually attracted to anyone. Gray asexuality includes those who only experience sexual attraction under certain circumstances; for example, exclusively after an emotional bond has been formed. This tends to vary from person to person.

The ovulatory shift hypothesis refers to the idea that female humans tend to exhibit different sexual behaviours and desires at points in their cycle, as an evolutionarily adaptive means to ensure that a high quality male is chosen to copulate with during the most fertile period of the cycle. It is thought that, due to the length of time and the parental investment involved for a woman to reproduce, changes in female psychology during menstrual periods would help them make critical decisions in mating selection. For example, it has been suggested that women's sexual preferences shift toward more masculine physical characteristics during peak phases of fertility. In such, a symmetrical and masculine face outwardly indicates the reproductive value of a prospective mate.

There is evidence that women's mate preferences differ across the ovarian cycle. A meta analysis, investigating 50 studies about whether women's mate preferences for good gene-related male traits changed across the ovarian cycle found that women's preferences change across their cycle: Women show the greatest preference for good gene male traits at their most fertile window.

Female sexual preference for male face shapes has been shown to vary with the probability of conception. Findings showed that during a 'high conception' stage of the menstrual cycle, women were more attracted to men with less feminine/more masculine faces for short-term relationships. Unlike men, women's sexual arousal has been found to be generic—it is non-specific to either men or women. The aforementioned research suggests that there may be a possibility that female sexual arousal becomes more sex-specific during the most fertile points of the menstrual cycle.

In males, a masculine face has been positively correlated with fewer respiratory diseases and, as a consequence, masculine features offer a marker of health and reproductive success. The preference for masculine faces is only recorded in short-term mate choices. It is therefore suggested that females are attracted to masculine faces only during ovulation as masculinity reflects a high level of fitness, used to ensure reproductive success. Whilst such preferences may be of lesser importance today, the evolutionary explanation offers reasoning as to why such effects are recorded.

As well as masculinity, females are more sensitive to the scent of males who display high levels of developmental stability. An individual's developmental stability is a measurement of fluctuating asymmetry, defined as their level of deviation from perfect bilateral symmetry. In a comparison of female college students, the results indicated that those normally cycling were more receptive to the scent of shirts worn by symmetrical men when nearing peak fertility in their ovulatory cycle. The same women reported no such preference for the scent of symmetrical men when re-tested during non-fertile stages of the menstrual cycle. Those using the contraceptive pill, and therefore not following regular cyclical patterns, reported no such preference.

As with masculine faces, the ability to determine symmetry via scent was likely designed by natural selection to increase the probability of reproductive success through mating with a male offering strong genetics. This is evidenced in research focusing on traits of symmetrical males, who consistently record higher levels of IQ, coordination, social dominance, and consequently, greater reproductive fitness. As symmetry appears to reflect an abundance of desirable traits held by the male in question, it is self-evident that such males are more desirable to females who are seeking high quality mates. In such, during ovulation, females show a strong preference for symmetrical males as they are reaching peak fertility. As it would be advantageous for asymmetrical men to release a scent similar to that produced by symmetrical males, the female signal used to detect symmetry is presumed to be an honest one (asymmetrical males cannot fake it).

In addition to this, females have different behavioural preferences towards men during stages in their cycles. It has been found that women have a preference towards more masculine voices during the late-follicular, fertile phase of the menstrual cycle. They are particularly sensitive towards voice pitch and apparent vocal-tract length, which are testosterone-related traits. This effect has been found to be most significant in women who are less feminine (those with low E3G levels), in comparison to women with higher E3G levels. It has been suggested that this difference in preference is because feminine women (those with high E3G levels) are more successful at obtaining investment. It is not necessary for these women to change their mating preferences during their cycles. More masculine women may make these changes to enhance their chances of achieving investment.

Women have been found to report greater sexual attraction to men other than their own partners when near ovulation compared with the luteal phase. Women whose partners have high developmental stability have greater attraction to men other than their partners when fertile. This can be interpreted as women possessing an adaptation to be attracted to men possessing markers of genetic fitness, therefore sexual attraction depends on the qualities of her partner.

Hormone levels throughout the menstrual cycle affect a woman's behaviour in preferences and in their overt behaviours. The ornamentation effect is a phenomenon influenced by a stage of the menstrual cycle which refers to the way a woman presents herself to others, in a way to attract potential sexual partners. Studies have found that the closer women were to ovulation, the more provocatively they dress and the more attractive they are rated.

Similar to the function in animals, it is probable that this ornamentation is to attract potential partners and that a woman's motivations may vary across her cycle. Research into this relationship has discovered that women who were to attend a discothèque and rated their clothing as 'sexy' and 'bold,' also stated that their intention for the evening was to flirt and find a partner to go home with. Although direct causation cannot be stated, this research suggests that there is a direct link between a woman's ornamentation and her motivation to attract mates.

It is possible that women are sensitive to the changes in their physical attractiveness throughout their cycles, such that at their most fertile stages their levels of attractiveness are increased. Consequently, they choose to display their increased levels of attractiveness through this method of ornamentation.

During periods of hormonal imbalance, women exhibit a peak in sexual activity. As these findings have been recorded for female-initiated sexual activity and not for male-initiated activity, the causation appears to be hormonal changes during the menstrual cycle. In addition, studies have found that women report themselves to be significantly more flirtatious with men, other than their partners, during the most fertile stages of their cycle, as well as a greater desire to attend parties or nightclubs where there is the potential to meet male partners.

Research has also found that menstrual cycles affect sexual behaviour frequency in pre-menopausal women. For example, women who had weekly sexual intercourse with men had menstrual cycles with the average duration of 29 days, while women with less frequent sexual interactions tended to have more extreme cycle lengths.

Changes in hormones during a female's cycles affect the way she behaves and the way males behave towards her. Research has found that men are a lot more attentive and loving towards their partners when they are in the most fertile phase of their cycles, in comparison to when they are in the luteal phases. Men become increasingly jealous and possessive over their partners during this stage. It is highly likely that these changes in male behaviour is a result of the female partner's increased desire to seek and flirt with other males. Therefore, these behavioural adaptations have developed as a form of mate guarding, which increases the male's likelihood of maintaining the relationship and increasing chances of reproductive success.



</doc>
<doc id="21549439" url="https://en.wikipedia.org/wiki?curid=21549439" title="Hamiltonian spite">
Hamiltonian spite

Within the field of social evolution, Hamiltonian spite is a term for behaviours occurring among conspecifics that have a cost for the actor and a negative impact upon the recipient.

W. D. Hamilton published an influential paper on altruism in 1964 to explain why genetic kin tend to help each other. He argued that genetically related individuals are likely to carry the copies of the same alleles; thus, helping kin may ensure that copies of the actors' alleles pass onto next generations of both the recipient and the actor.

While this became a widely accepted idea, it was less noted that Hamilton published a later paper that modified this view. This paper argues that by measuring the genetic relatedness between any two (randomly chosen) individuals of a population several times, we can identify an average level of relatedness. Theoretical models predict that (1) it is adaptive for an individual to be altruistic to any other individuals that are more closely related to it than this average level, and also that (2) it is adaptive for an individual to be spiteful against any other individuals that are less closely related to it than this average level. The indirect adaptive benefits of such acts can surpass certain costs of the act (either helpful or harmful) itself. Hamilton mentioned birds and fishes exhibiting infanticide (more specifically: ovicide) as examples for such behaviours.

Briefly, an individual can increase the chance of its genetic alleles to be passed to the next generations either by helping those that are more closely related, or by harming those that are less closely related than relationship by chance.

Though altruism and spitefulness appear to be two sides of the same coin, the latter is less accepted among evolutionary biologists.

First, unlike the case with the beneficiary of an altruistic act, targets of aggression are likely to act in revenge: bites will provoke bites. Thus harming non-kin may be more costly than helping kins.

Second, presuming a panmictic population, the vast majority of pairs of individuals exhibit a roughly average level of relatedness. For a given individual, the majority of others are not worth helping or harming. While it is easy to identify the few most closely related ones (see: kin recognition), it is hard to identify the most genetically distant ones.

Most terrestrial vertebrates exhibit a certain degree of site fidelity, so levels of kinship tend to correlate negatively with spatial distance. While this may provide some cues to identify the least related individuals, it may also ensure that non-kin rarely if ever meet each other.

Many animal species exhibit infanticide, i.e. adults tend to kill the eggs or the offspring of conspecifics, even if they do not feed on them (in the absence of cannibalism). This form of spitefulness is relatively free from the threat of revenge – provided that the parents and relatives of the target are either weak or far away. Infanticide may not be a form of spite as in many cases the loss of offspring to the female brings it back into estrous providing a mating advantage to an infanticidal male. This is seen in lions. 

An individual carrying a long-lasting infection of virulent pathogens may benefit from (1) channelling the flow of pathogens from its own body away from its kin and (2) directing them toward non-kin conspecifics. The adaptive nature of this behaviour has been supported by the analysis of theoretical models and also by the analyses of the behavioural repertoire of different animal species. Thus, tuberculosis-infected European badgers and rabies-infected dogs equally tend to emigrate from their natal ranges before starting to distribute the pathogens. Similarly, wild herds of Asian elephants tend to defecate into drinkwater holes apparently to keep rival herds away.

Throughout human history, war often emerges as a costly form of aggression typically targeting the non-kin enemy. Naturally, most wars appear to be motivated by potential benefits other than the genetic. Nevertheless, the widespread occurrence of rape and infanticide during periods of war indicates Hamiltonian elements as well. Infanticide is a biologically spiteful action in that it costs the killer time and energy, and opens the killer to the threat of revenge, without any direct compensating benefits. Rape, by contrast, is not, in the strict definition of the word, biologically spiteful as it is likely to have a direct fitness increase on the rapist (by increasing his number of direct descendants in the next generation, should the victim fall pregnant).

It was recently suggested that spiteful motivations may also play a role in economic decisions.

Moreover, another paper argues that the motivation for biological aggression – i.e. using pathogens as weapons – is an inherent and common aspect of the human behavioural repertoire. This argument contradicts the widespread belief that biological weapons are rare and aberrant tools of warfare and it is supported by the following observations:


</doc>
<doc id="22579001" url="https://en.wikipedia.org/wiki?curid=22579001" title="Gender differences in social network service use">
Gender differences in social network service use

Men and women use social network services (SNSs) differently and with different frequencies. In general, several researchers have found that women tend to use SNSs more than men and for different and more social purposes.

Technologies, including communications technologies, have a long history of shaping and being shaped by the gender of their users. Although technologies used to perform housework have an apparent historical connection to gender in many cultures, a more ready connection to SNSs may be drawn with telephones as a communications technology readily and widely available in the home. Telephone use has long had gendered connections ranging from the widespread assumption that women simply talk more than men, and the employment of women as telephone operators. In particular, young women have been closely associated with extensive and trivial use of the telephone for purely social purposes. Similarly, women's use of and influence on the development of computers has been trivialized while significant developments in computers have been masculinized. Thus the idea that there may be both real and perceived differences in how men and women use SNSs – and that those uses may shape the SNSs – is neither new nor surprising and has historical analogues.

There is historical and contemporary evidence that current fears about young girls' online safety have historical antecedents such as telegraphs and telephones. Further, in many cases those historical reactions resulted in restrictions of girls' use of technology to protect them from predators, molesters, and other criminals threatening their innocence. Like current fears focused on computer use, particularly SNSs and other communication media, these fears are most intense when the medium enters the home. These fears have the potential to – at least temporarily – overwhelm the positive and empowering uses of these technologies. These historical fears are echoed in contemporary media accounts of youths' use of SNSs.

Finally, the histories of some SNSs themselves have ties with gender. For example, gay men were one of the earliest groups to join and use the early SNS Friendster.

Many studies have found that women are more likely to use either specific SNSs such as Facebook or MySpace or SNSs in general. In 2015, 73% of online men and 80% of online women used social networking sites. The gap in gender differences has become less apparent in LinkedIn. In 2015 about 26 percent of online men and 25% of online women used the business-and employee-oriented networking site.

Researchers who have examined the gender of users of multiple SNSs have found contradictory results. Hargittai's groundbreaking 2007 study examining race, gender, and other differences between undergraduate college student users of SNSs found that women were not only more likely to have used SNSes than men but that they were also more likely to have used many different services, including Facebook, MySpace, and Friendster; these differences persisted in several models and analyses. Although she only surveyed students at one institution – the University of Illinois at Chicago – Hargittai selected that institution intentionally as "an ideal location for studies of how different kinds of people use online sites and services." In contrast, data collected by the Pew Internet & American Life Project found that men were more likely to have multiple SNS profiles. Although the sample sizes of the two surveys are comparable – 1,650 Internet users in the Pew survey compared with 1,060 in Hargittai's survey – the data from the Pew survey are newer and arguably more representative of the entire adult United States population.

In general, women seem to use SNSs more to explicitly foster social connections. A study conducted by Pew research centers found that women were more avid users of social media. In November 2010, the gap between men and women was as high as 15%. Female participants in a multi-stage study conducted in 2007 to discover the motivations of Facebook users scored higher on scales for social connection and posting of photographs. Studies have also been conducted on the differences between females and males with regards to blogging. The Pew Research Center found that younger females are more likely to blog than males their own age, even males that are older than them. Similarly, in a study of blogs maintained in MySpace, women were found to be more likely to not only write blogs but also write about family, romantic relationships, friendships, and health in those blogs. A study of Swedish SNS users found that women were more likely to have expressions of friendship, specifically in the areas of (a) publishing photos of their friends, (b) specifically naming their best friends, and (c) writing poems to and about their friends. Women were also more likely to have expressions related to family relationships and romantic relationships. One of the key findings of this research is that those men who do have expressions of romantic relationships in their profile had expressions just as strong as the women. However, the researcher speculated that this may be in part due to a desire to publicly express heterosexual behaviors and mannerisms instead of merely expressing romantic feelings.

A large-scale study of gender differences in MySpace found that both men and women tended to have a majority of female Friends, and both men and women tended to have a majority of female "Top" Friends in the site.
A later study found women to author disproportionately many (public) comments in MySpace,
but an investigation into the role of emotion in public MySpace comments found that women both give and receive stronger positive emotion.
It was hypothesised that women are simply more effective at using social networking sites because they are better able to harness positive emotion.

A study focused on the influence of gender and personality on individuals’ use of online social networking websites such as Facebook, reported that men use social networking sites with the intention of forming new relationships, whereas, women use them more for relationship maintenance. (Muscanell and Guadagno, 2012)
In addition to this, women are more likely to use Facebook or MySpace to compare themselves to others and also to search for information. Men, however, are more likely to look at other people's profiles with in the intention to find friends. (Haferkamp et al., 2012) 

Privacy has been the primary topic of many studies of SNS users, and many of these studies have found differences between male and female SNS users, although some studies have found results contradictory to those found in other studies.

Some researchers have found that women are more protective of their personal information and more likely to have private profiles. Other researchers have found that women are less likely to post some types of information. Acquisti and Gross found that women in their sample were less likely to reveal their sexual orientation, personal address, or cell phone number. This is similar to Pew Internet & American Life research of children users of SNSs that found that boys and girls presented different views of privacy and behaviors, with girls being more concerned about and restrictive of information such as city, town, last name, and cell phone number that could be used to locate them. At least one group of researchers has found that women are less likely to share information that "identifies them directly – last name, cell phone number, and address or home phone number," linking that resistance to women's greater concerns about "cyberstalking", "cyberbullying", and security problems.

Despite these concerns about privacy, researchers have found that women are more likely to maintain up-to-date photos of themselves. Further, Kolek and Saunders found in their sample of college student Facebook users that women were more likely to not only post a photograph of themselves in their profile but that they were more likely to have a publicly viewable Facebook account (a contradictory finding compared to many other studies), post photos, and post photo albums.

Women were more likely to have: (a) a publicly viewable Facebook account, (b) more photo albums, (c) more photos, (d) a photo of themselves as their profile picture, (e) positive references to alcohol, partying, or drugs, and (f) more positive references to or about the institution or institution-related activities. In general, women were more likely to disclose information about themselves in their Facebook profile, with the primary exception of sharing their telephone number. Similarly, female respondents to Strano's study were more likely to keep their profile photo recent and choose a photo that made them appear attractive, happy, and fun-loving. Citing several examples, Strano opined that there may also be a difference in how men and women Facebook users display and interpret profile photos depicting relationships.

Privacy has also been a concern for the SnapChat app, which allows you to send messages either text or photo or video which then disappear. One study has shown that security is not a major concern for the majority of users and that most do not use Snapchat to send sensitive content (although up to 25% may do so experimentally). As part of their research almost no statistically significant gender differences were found.

Cyberbullying

Past research carried out to investigate if there are any gender differences in cyber-bulling has found that boys commit more cyber verbal bullying, cyber forgery and more violence based on hidden identity or presenting themselves as other person. (Sincek 2014).

Although men and women users of SNSs exhibit different behavior and motivations, they share some similarities. For example, one study that examined the veracity of information shared on SNSs by college students found that men and women were as likely to "provide accurate and complete information about their birthday, schedule of classes, partner's name, AIM, or political views."

In contradiction to several of the studies described above that found that women are more likely to be SNS users, at least one very reputable study has found that men and women are equally likely to be SNS users. Data gathered in December 2008 by the Pew Internet & American Life Project showed that the SNS users in their sample were equally divided among men and women. As mentioned above, the data from the Pew survey are newer and arguably more representative of the entire adult United States population than the data in much of the previously described research.

Some studies have found that traditional gender roles are present in SNSs, with men in this study conforming to traditional views of masculinity and the women to traditional views of femininity. Qualitative work with college student SNS users by Martínez Alemán and Wartman and Managgo "et al." have found similar results for both Facebook and MySpace users. Moreover, the work by Managgo "et al." discovered not only traditional gender roles and images but sexualisation of women users of MySpace. Similarly, research into the impact of comments in the profile of a Facebook users on that user's perceived attractiveness revealed a "sexual double standard", wherein negative statements resulted in male profile owners being judged more attractive and female profile owners less attractive. Finally, at least one study has found that men and women SNS users both left textual clues about their gender.

In February 2014, Facebook announced the vast expansion of options for choosing gender identities to list on profiles, ranging to up to 56 gender identity choices. In August 2014, Facebook followed up with allowing gender-neutral relationship identities for identifying family members.



</doc>
<doc id="22709409" url="https://en.wikipedia.org/wiki?curid=22709409" title="Workplace deviance">
Workplace deviance

Workplace deviance, in group psychology, may be described as the deliberate (or intentional) desire to cause harm to an organization – more specifically, a workplace. The concept has become an instrumental component in the field of organizational communication. More accurately, it can be seen as "voluntary behavior that violates institutionalized norms and in doing so threatens the well-being of the organization".

Employees often create a set of expectations about their workplace; people tend to make psychological contracts with their organizations. When his or her expectations are not met, the employee may "perceive a psychological contract breach by their employers". This "breach" of the psychological contract then presents potential problems, particularly in the workplace.

Workplace deviance may arise from the worker's perception that their organization has mistreated him or her in some manner. Employees then resort to misbehaving (or acting out) as a means of avenging their organization for the perceived wrongdoing. Workplace deviance may be viewed as a form of negative reciprocity. "A negative reciprocity orientation is the tendency for an individual to return negative treatment for negative treatment". In other words, the maxim "an eye for an eye" is a concept that some employees strongly feel is a suitable approach to their problem. However, what is critical in understanding employee deviance is that the employee perceives being wronged, whether or not mistreatment actually occurred.

Workplace deviance is also closely related to abusive supervision. Abusive supervision is defined as the "subordinates' perceptions of the extent to which their supervisors engage in the sustained display of hostile verbal and nonverbal behaviors". This could be when supervisors ridicule their employees, give them the silent treatment, remind them of past failures, fail to give proper credit, wrongfully assign blame or blow up in fits of temper. It may seem like employees who are abused by their supervisor will either directly retaliate or withdraw by quitting the job but in reality many strike out against their employer by engaging in organizational deviant behaviors. Since employees control many of the organization's resources, they often use, or abuse anything they can. This abuse of resources may come in the form of time, office supplies, raw materials, finished products or the services that they provide. This usually occurs in two steps. First step is that commitment is destroyed and employees stop caring about the welfare of the employer. The second step is that the abused employee will get approval (normally implied) of their coworkers to commit deviant acts.

Workplace experiences may fuel the worker to act out. Research has been conducted demonstrating that the perception of not being respected is one of the main causes for workplace deviance; workplace dissatisfaction is also a factor. According to Bolin and Heatherly, "dissatisfaction results in a higher incidence of minor offenses, but does not necessarily lead to severe offense". An employee who is less satisfied with his or her work may become less productive as their needs are not met. In the workplace, "frustration, injustices and threats to self are primary antecedents to employee deviance". Although workplace deviance does occur, the behavior is not universal. There are two preventive measures that business owners can use to protect themselves. The first is strengthening the employee's commitment by reacting strongly to abusive supervision so that the employee knows that the behavior is not accepted. Holding the employee at high esteem by reminding them of their importance, or setting up programs that communicate concern for the employee may also strengthen employee commitment. Providing a positive ethical climate can also help. Employers can do this by having a clear code of conduct that is applied to both managers and employees alike.

Workplace deviance may be expressed in various ways. Employees can engage in minor, extreme, nonviolent or violent behavior, which ultimately leads to an organization's decline in productivity. Interpersonal and organizational deviance are two forms of workplace deviance which are directed differently; however, both cause harm to an organization.

Interpersonal deviance can occur when misconduct "target(s) specific stakeholders such as coworkers". Behavior falling within this subgroup of employee deviance includes gossiping about coworkers and assigning blame to them. These minor (but unhealthy) behaviors, directed at others, are believed to occur as some employees perceive "a sense of entitlement often associated with exploitation". In other words, they feel the need to misbehave in ways that will benefit them.

Deviant behavior typically aimed directly at the organization is often referred to as organizational deviance. Organizational deviance encompasses production and property deviance. Workplace-deviant behavior may be expressed as tardiness or excessive absenteeism. These behaviors have been cited by some researchers as "withdraw(al) behaviors…such behaviors allow employees to withdraw physically and emotionally from the organization".

Employee silence is also considered a deviant behavior in the workplace, falling into the realms of both interpersonal and organizational deviance. Silence becomes employee deviance when "an employee intentionally or unintentionally withholds any kind of information that might be useful to the organization". The problem occurs if an employee fails to disclose important information, which detrimentally affects the effectiveness of the organization due to poor communication.

Coworker backstabbing occurs to some degree in many workplaces. It consists of an employee's doing something to another employee to get a "leg up" on the other employee. Strategies used for backstabbing include dishonesty, blame (or false accusation), discrediting others and taking credit for another's work. Motives for backstabbing include disregarding others' rights in favor of one's own gain, self-image management, revenge, jealousy, and personal reasons.

A novel form of workplace deviance has emerged in recent years, as technology becomes a bigger part of people's work lives. Internet workplace deviance (or "cyber loafing") has become another way for employees to avoid the tasks at hand. This includes surfing the web and doing non-work-related tasks on the internet such as chatting on social-networking sites, online shopping and other activities.

All behaviors in which deviant employees partake ultimately have a negative impact on the overall productivity of the organization. For this reason, all are considered production deviance. Production deviance is "behavior that violates formally prescribed organizational norms with respect to minimal quality and quantity of work to be accomplished as part of one's job".

More serious cases of deviant behavior harmful to an organization concern property deviance. Property deviance is "where employees either damage or acquire tangible assets…without authorization". This type of deviance typically involves theft but may include "sabotage, intentional errors in work, misusing expense accounts", among other examples.

Deviant behavior can be much more extreme, involving sexual harassment and even violence. All these deviant behaviors create problems for the organization. It is costly for an organization to pay employees who are not working efficiently.

The relationships employees have with their organization are crucial, as they can play an important role in the development of workplace deviance. Employees who perceive their organization or supervisor(s) as more caring (or supportive) have been shown to have a reduced incidence of workplace-deviant behaviors. Supervisors, managers and organizations are aware of this, and "assess their own behaviors and interactions with their employees and understand while they may not intend to abuse their employees they may be perceived as doing so…".

Organizational justice and the organizational climate are also critical, since the quality of the work experience can impact employee behavior in the workplace. Organizational justice may be organized into three subcategories: procedural, distributive and interactional justice.


Research indicates that procedural justice (combined with interactional justice) is beneficial in reducing workplace-deviant behavior. Employees who are consulted (and given an opportunity to be involved in the decision-making processes at their organization) are less likely to act out, since their voices are valued.

Workplace deviance is a phenomenon which occurs frequently within an organization. Ultimately, it is the managers' and the organization's responsibility to uphold the norms to which the organization wishes to adhere; it is the organization's job to create an ethical climate. If organizations have authority figures who demonstrate their ethical values, a healthier workplace environment is created. "Research has suggested that managers' behavior influences employee ethical decision-making". Employees who perceive themselves as being treated respectfully and valued are those less likely to engage in workplace deviance.




</doc>
<doc id="511676" url="https://en.wikipedia.org/wiki?curid=511676" title="Group polarization">
Group polarization

In social psychology, group polarization refers to the tendency for a group to make decisions that are more extreme than the initial inclination of its members. These more extreme decisions are towards greater risk if individuals' initial tendencies are to be risky and towards greater caution if individuals' initial tendencies are to be cautious. The phenomenon also holds that a group's attitude toward a situation may change in the sense that the individuals' initial attitudes have strengthened and intensified after group discussion, a phenomenon known as attitude polarization.

Group polarization is an important phenomenon in social psychology and is observable in many social contexts. For example, a group of women who hold moderately feminist views tend to demonstrate heightened pro-feminist beliefs following group discussion. Similarly, have shown that after deliberating together, mock jury members often decided on punitive damage awards that were either larger or smaller than the amount any individual juror had favored prior to deliberation. The studies indicated that when the jurors favored a relatively low award, discussion would lead to an even more lenient result, while if the jury was inclined to impose a stiff penalty, discussion would make it even harsher. Moreover, in recent years, the Internet and online social media have also presented opportunities to observe group polarization and compile new research. Psychologists have found that social media outlets such as Facebook and Twitter demonstrate that group polarization can occur even when a group is not physically together. As long as the group of individuals begins with the same fundamental opinion on the topic and a consistent dialogue is kept going, group polarization can occur.

Research has suggested that well-established groups suffer less from polarization, as do groups discussing problems that are well known to them. However, in situations where groups are somewhat newly formed and tasks are new, group polarization can demonstrate a more profound influence on the decision-making.

Attitude polarization, also known as belief polarization and polarization effect, is a phenomenon in which a disagreement becomes more extreme as the different parties consider evidence on the issue. It is one of the effects of "confirmation bias": the tendency of people to search for and interpret evidence selectively, to reinforce their current beliefs or attitudes. When people encounter ambiguous evidence, this bias can potentially result in each of them interpreting it as in support of their existing attitudes, widening rather than narrowing the disagreement between them.

The effect is observed with issues that activate emotions, such as political "hot button" issues. For most issues, new evidence does not produce a polarization effect. For those issues where polarization is found, mere thinking about the issue, without contemplating new evidence, produces the effect. Social comparison processes have also been invoked as an explanation for the effect, which is increased by settings in which people repeat and validate each other's statements. This apparent tendency is of interest not only to psychologists, but also to sociologists and philosophers.

Since the late 1960s, psychologists have carried out a number of studies on various aspects of attitude polarization.

In 1979, Charles Lord, Lee Ross and Mark Lepper performed a study in which they selected two groups of people, one group strongly in favor of capital punishment, the other strongly opposed. The researchers initially measured the strength with which people held their position. Later, both the pro- and anti-capital punishment people were put into small groups and shown one of two cards, each containing a statement about the results of a research project written on it. For example:Kroner and Phillips (1977) compared murder rates for the year before and the year after adoption of capital punishment in 14 states. In 11 of the 14 states, murder rates were lower after adoption of the death penalty. This research supports the deterrent effect of the death penalty.or:Palmer and Crandall (1977) compared murder rates in 10 pairs of neighboring states with different capital punishment laws. In 8 of the 10 pairs, murder rates were higher in the state with capital punishment. This research opposes the deterrent effect of the death penalty.The researchers again asked people about the strength of their beliefs about the deterrence effect of the death penalty, and, this time, also asked them about the effect that the research had had on their attitudes.

In the next stage of the research, the participants were given more information about the study described on the card they received, including details of the research, critiques of the research, and the researchers' responses to those critiques. The participants' degree of commitment to their original positions were re-measured, and the participants were asked about the quality of the research and the effect the research had on their beliefs. Finally, the trial was rerun on all participants using a card that supported the opposite position to that they had initially seen.

The researchers found that people tended to believe that research that supported their original views had been better conducted and was more convincing than research that didn't. Whichever position they held initially, people tended to hold that position more strongly after reading research that supported it. Lord "et al." point out that it is reasonable for people to be less critical of research that supports their current position, but it seems less rational for people to significantly increase the strength of their attitudes when they read supporting evidence. When people had read both the research that supported their views and the research that did not, they tended to hold their original attitudes "more" strongly than before they received that information. These results should be understood in the context of several problems in the implementation of the study, including the fact the researchers changed the scaling of the outcome of the variable, so measuring attitude change was impossible, and measured polarization using a subjective assessment of attitude change not a direct measure of how much change had occurred.

Group polarization and choice shifts are similar in many ways; however, they differ in one distinct way. Group polarization refers to attitude change on the individual level due to the influence of the group, and choice shift refers to the outcome of that attitude change; namely, the difference between the average group members' pre-group discussion attitudes and the outcome of the group decision.

Risky and cautious shifts are both a part of a more generalized idea known as group-induced attitude polarization. Though group polarization deals mainly with risk-involving decisions and/or opinions, discussion-induced shifts have been shown to occur on several non-risk-involving levels. This suggests that a general phenomenon of choice-shifts exists apart from only risk-related decisions. Stoner found that a decision is impacted by the values behind that circumstances of the decision. The study found that situations that normally favor the more risky alternative increased risky shifts. More so, situations that normally favor the cautious alternative increased cautious shifts. These findings also show the importance of previous group shifts. Choice shifts are mainly explained by largely differing human values and how highly these values are held by an individual. According to Moscovici et al. interaction within a group and differences of opinion are necessary for group polarization to take place. While an extremist in the group may sway opinion, the shift can only occur with sufficient and proper interaction within the group. In other words, the extremist will have no impact without interaction. Also, Moscovici et al. found individual preferences to be irrelevant; it is differences of opinion which will cause the shift. This finding demonstrates how one opinion in the group will not sway the group; it is the combination of all the individual opinions that will make an impact.

The study of group polarization can be traced back to an unpublished 1961 Master's thesis by MIT student James Stoner, who observed the so-called "risky shift". The concept of risky shift maintains that a group's decisions are riskier than the average of the individual decisions of members before the group met.

In early studies, the risky-shift phenomenon was measured using a scale known as the Choice-Dilemmas Questionnaire. This measure required participants to consider a hypothetical scenario in which an individual is faced with a dilemma and must make a choice to resolve the issue at hand. Participants were then asked to estimate the probability that a certain choice would be of benefit or risk to the individual being discussed. Consider the following example:

"Mr. A, an electrical engineer, who is married and has one child, has been working for a large electronics corporation since graduating from college five years ago. He is assured of a lifetime job with a modest, though adequate, salary and liberal pension benefits upon retirement. On the other hand, it is very unlikely that his salary will increase much before he retires. While attending a convention, Mr. A is offered a job with a small, newly founded company which has a highly uncertain future. The new job would pay more to start and would offer the possibility of a share in the owner- ship if the company survived the competition of the larger firms."
Participants were then asked to imagine that they were advising Mr. A. They would then be provided with a series of probabilities that indicate whether the new company that offered him a position is financially stable. It would read as following

"Please check the lowest probability that you would consider acceptable to make it worthwhile for Mr. A to take the new job."

____The chances are 1 in 10 that the company will prove financially sound.

____The chances are 3 in 10 that the company will prove financially sound.

____The chances are 5 in 10 that the company will prove financially sound.

____The chances are 7 in 10 that the company will prove financially sound.

____The chances are 9 in 10 that the company will prove financially sound.

____Place a check here if you think Mr. A should not take the new job no matter what the probabilities.

Individuals completed the questionnaire and made their decisions independently of others. Later, they would be asked to join a group to reassess their choices. Indicated by shifts in the mean value, initial studies using this method revealed that group decisions tended to be relatively riskier than those that were made by individuals. This tendency also occurred when individual judgments were collected after the group discussion and even when the individual post-discussion measures were delayed two to six weeks.

The discovery of the risky shift was considered surprising and counter-intuitive, especially since earlier work in the 1920s and 1930s by Allport and other researchers suggested that individuals made more extreme decisions than did groups, leading to the expectation that groups would make decisions that would conform to the average risk level of its members. The seemingly counter-intuitive findings of Stoner led to a spurt of research around the risky shift, which was originally thought to be a special case exception to the standard decision-making practice. Many people had concluded that people in a group setting would make decisions based on what they assumed to be the overall risk level of a group; because Stoner's work did not necessarily address this specific theme, and because it does seem to contrast Stoner's initial definition of risky shift, additional controversy arose leading researchers to further examine the topic. By the late 1960s, however, it had become clear that the risky shift was just one type of many attitudes that became more extreme in groups, leading Moscovici and Zavalloni to term the overall phenomenon "group polarization".

Subsequently, a decade-long period of examination of the applicability of group polarization to a number of fields in both lab and field settings began. There is a substantial amount of empirical evidence demonstrating the phenomenon of group polarization. Group polarization has been widely considered as a fundamental group decision-making process and was well established, but remained non-obvious and puzzling because its mechanisms were not fully understood.

Almost as soon as the phenomenon of group polarization was discovered, a number of theories were offered to help explain and account for it. These explanations were gradually narrowed down and grouped together until two primary mechanisms remained, social comparison and informational influence.

The social comparison theory, or normative influence theory, has been widely used to explain group polarization. According to the social comparison interpretation, group polarization occurs as a result of individuals' desire to gain acceptance and be perceived in a favorable way by their group. The theory holds that people first compare their own ideas with those held by the rest of the group; they observe and evaluate what the group values and prefers. In order to gain acceptance, people then take a position that is similar to everyone else's but slightly more extreme. In doing so, individuals support the group's beliefs while still presenting themselves as admirable group "leaders". The presence of a member with an extreme viewpoint or attitude does not further polarize the group. Studies regarding the theory have demonstrated that normative influence is more likely with judgmental issues, a group goal of harmony, person-oriented group members, and public responses.

Informational influence, or persuasive arguments theory, has also been used to explain group polarization, and is most recognized by psychologists today. The persuasive arguments interpretation holds that individuals become more convinced of their views when they hear novel arguments in support of their position. The theory posits that each group member enters the discussion aware of a set of items of information or arguments favoring both sides of the issue, but lean toward that side that boasts the greater amount of information. In other words, individuals base their individual choices by weighing remembered pro and con arguments. Some of these items or arguments are shared among the members while some items are unshared, in which all but one member has considered these arguments before. Assuming most or all group members lean in the same direction, during discussion, items of unshared information supporting that direction are expressed, which provides members previously unaware of them more reason to lean in that direction. Group discussion shifts the weight of evidence as each group member expresses their arguments, shedding light onto a number of different positions and ideas. Research has indicated that informational influence is more likely with intellective issues, a group goal of making correct decision, task-oriented group members, and private responses. Furthermore, research suggests that it is not simply the sharing of information that predicts group polarization. Rather, the amount of information and persuasiveness of the arguments mediate the level of polarization experienced.

In the 1970s, significant arguments occurred over whether persuasive argumentation alone accounted for group polarization. Daniel Isenberg's 1986 meta-analysis of the data gathered by both the persuasive argument and social comparison camps succeeded, in large part, in answering the questions about predominant mechanisms. Isenberg concluded that there was substantial evidence that both effects were operating simultaneously, and that persuasive arguments theory operated when social comparison did not, and vice versa.

While these two theories are the most widely accepted as explanations for group polarization, alternative theories have been proposed. The most popular of these theories is self-categorization theory. Self-categorization theory stems from social identity theory, which holds that conformity stems from psychological processes; that is, being a member of a group is defined as the subjective perception of the self as a member of a specific category. Accordingly, proponents of the self-categorization model hold that group polarization occurs because individuals identify with a particular group and conform to a prototypical group position that is more extreme than the group mean. In contrast to social comparison theory and persuasive argumentation theory, the self-categorization model maintains that inter-group categorization processes are the cause of group polarization 

Support for the self-categorization theory, which explains group polarization as conformity to a polarized norm, was found by Hogg, Turner, and Davidson in 1990. In their experiment, participants gave pre-test, post-test, and group consensus recommendations on three choice dilemma item-types (risky, neutral, or cautious). The researchers hypothesized that an ingroup confronted by a risky outgroup will polarize toward caution, an ingroup confronted by a caution outgroup will polarize toward risk, and an ingroup in the middle of the social frame of reference, confronted by both risky and cautious outgroups, will not polarize but will converge on its pre-test mean. The results of the study supported their hypothesis in that participants converged on a norm polarized toward risk on risky items and toward caution on cautious items. Another similar study found that in-group prototypes become more polarized as the group becomes more extreme in the social context. This further lends support to the self-categorization explanation of group polarization.

The rising popularity and increased number of online social media platforms, such as Facebook, Twitter and Instagram, has enabled people to seek out and share ideas with others who have similar interests and common values, making group polarization effects increasingly evident, particularly in generation Y and generation Z individuals. Owing to this technology, it is possible for individuals to curate their sources of information and the opinions to which they are exposed, thereby reinforcing and strengthening their own views while effectively avoiding information and perspectives with which they disagree.

One study analyzed over 30,000 tweets on Twitter regarding the shooting of George Tiller, a late term abortion doctor, where the tweets analyzed were conversations among pro-life and pro-choice advocates post shooting. The study found that like-minded individuals strengthened group identity whereas replies between different-minded individuals reinforced a split in affiliation.

In a study conducted by Sia et al. in 2002, group polarization was found to occur with online (computer-mediated) discussions. In particular, this study found that group discussions, conducted when discussants are in a distributed (cannot see one another) or anonymous (cannot identify one another) environment, can lead to even higher levels of group polarization compared to traditional meetings. This is attributed to the greater numbers of novel arguments generated (due to persuasive arguments theory) and higher incidence of one-upmanship behaviors (due to social comparison).

However, some research suggests that important differences arise in measuring group polarization in laboratory versus field experiments. A study conducted by Taylor & MacDonald in 2002 featured a realistic setting of a computer-mediated discussion, but group polarization did not occur at the level expected. The study's results also showed that groupthink occurs less in computer-mediated discussions than when people are face to face. Moreover, computer-mediated discussions often fail to result in a group consensus, or lead to less satisfaction with the consensus that was reached, compared to groups operating in a natural environment. Furthermore, the experiment took place over a two-week period, leading the researchers to suggest that group polarization may occur only in the short-term. Overall, the results suggest that not only may group polarization not be as prevalent as previous studies suggest, but group theories, in general, may not be simply transferable when seen in a computer-related discussion.

Group polarization has been widely discussed in terms of political behavior (see political polarization). Researchers have identified an increase in affective polarization among the United States electorate, and report that hostility and discrimination towards the opposing political party has increased dramatically over time.

Group polarization is similarly influential in legal contexts. A study that assessed whether Federal district court judges behaved differently when they sat alone, or in small groups, demonstrated that those judges who sat alone took extreme action 35% of the time, whereas judges who sat in a group of three took extreme action 65% of the time. These results are noteworthy because they indicate that even trained, professional decision-makers are subject to the influences of group polarization.

Group polarization has been reported to occur during wartime and other times of conflict and helps to account partially for violent behavior and conflict. Researchers have suggested, for instance, that ethnic conflict exacerbates group polarization by enhancing identification with the ingroup and hostility towards the outgroup. While polarization can occur in any type of conflict, it has its most damaging effects in large-scale inter-group, public policy, and international conflicts.

On a smaller scale, group polarization can also be seen in the everyday lives of students in higher education. A study by Myers in 2005 reported that initial differences among American college students become more accentuated over time. For example, students who do not belong to fraternities and sororities tend to be more liberal politically, and this difference increases over the course of their college careers. Researchers theorize that this is at least partially explained by group polarization, as group members tend to reinforce one another's proclivities and opinions.



</doc>
<doc id="7412739" url="https://en.wikipedia.org/wiki?curid=7412739" title="Mind games">
Mind games

Mind games is used to define three forms of competitive human behaviors:

The first known use of "mind game" is in 1963. The first known use of "head game" is in 1977.

Mind games in the sense of the struggle for prestige appear in everyday life in the fields of office politics, sport, and relationships. Played most intensely perhaps by Type A personalities, office mind games are often hard to identify clearly, as strong management blurs with over-direction, healthy rivalry with manipulative head-games and sabotage. The wary salesman will be consciously and unconsciously prepared to meet a variety of challenging mind games and put-downs in the course of their work.

The serious sportsman will also be prepared to meet a variety of gambits and head-games from their rivals, attempting meanwhile to tread the fine line between competitive psychology and paranoia.

In intimate relationships, mind games can be used to undermine one partner's belief in the validity of their own perceptions. Personal experience may be denied and driven from memory; and such abusive mind games may extend to denial of the victim's reality, social undermining, and downplaying the importance of the other partner's concerns or perceptions. Both sexes have equal opportunities for such verbal coercion, which may be carried out unconsciously as a result of the need to maintain one's own self-deception.

Eric Berne described a psychological game as an organized series of ulterior transactions taking place on twin levels, social and psychological, and resulting in a dramatic outcome when the two levels finally came to coincide. He described the opening of a typical game like flirtation as follows: "Cowboy: 'Come and see the barn'. Visitor: 'I've loved barns ever since I was a little girl'". At the social level a conversation about barns, at the psychological level one about sex play, the outcome of the game – which may be comic or tragic, heavy or light – will become apparent when a switch takes place and the ulterior motives of each become clear.

Between thirty and forty such games (as well as variations of each) were described and tabulated in Berne's best seller on the subject. According to one transactional analyst, "Games are so predominant and deep-rooted in society that they tend to become institutionalized, that is, played according to rules that everybody knows about and more or less agrees to. The game of Alcoholic, a five-handed game, illustrates this...so popular that social institutions have developed to bring the various players together" such as Alcoholics Anonymous and Al-anon.

Psychological games vary widely in degrees of consequence, ranging from first-degree games where losing involves embarrassment or frustration, to third-degree games where consequences are life threatening. Berne recognised however that "since by definition games are based on ulterior transactions, they must all have some element of exploitation", and the therapeutic ideal he offered was to stop playing games altogether.

Mind games for self-improvement fall into two main categories. There are mental exercises and puzzles to maintain or improve the actual working of the brain.

There is also the category of the self-empowering mind game, as in psychodrama, or mental and fantasy workshops – elements which might be seen as an ultimate outgrowth of yoga as a set of mental (and physical) disciplines.



</doc>
<doc id="26188644" url="https://en.wikipedia.org/wiki?curid=26188644" title="Victim playing">
Victim playing

Victim playing (also known as playing the victim, victim card, or self-victimization) is the fabrication of victimhood for a variety of reasons such as to justify abuse of others, to manipulate others, a coping strategy, or attention seeking.

Victim playing by abusers is either:

It is common for abusers to engage in victim playing. This serves two purposes:

Manipulators often play the victim role ("woe is me") by portraying themselves as victims of circumstances or someone else's behavior in order to gain pity or sympathy or to evoke compassion and thereby get something from someone. Caring and conscientious people cannot stand to see anyone suffering, and the manipulator often finds it easy and rewarding to play on sympathy to get cooperation.

While portraying oneself as a victim can be highly successful in obtaining goals over the short-term, this method tends to be less successful over time:

Victim playing is also: 

The language of "victim playing" has entered modern corporate life, as a potential weapon of all professionals. To define victim-players as dishonest may be an empowering response; as too may be awareness of how childhood boundary issues can underlie the tactic.

In the hustle of office politics, the term may however be abused so as to penalize the legitimate victim of injustice, as well as the role-player.

Transactional analysis distinguishes real victims from those who adopt the role in bad faith, ignoring their own capacities to improve their situation. Among the predictable interpersonal "games" psychiatrist Eric Berne identified as common among by victim-players are "Look How Hard I've Tried" and "Wooden Leg". 

R. D. Laing considered that "it will be difficult in practice to determine whether or to what extent a relationship is collusive" – when "the one person is predominantly the passive 'victim'", and when they are merely playing the victim. The problem is intensified once a pattern of victimization has been internalised, perhaps in the form of a double bind.

Object relations theory has explored the way possession by a false self can create a permanent sense of victimisation – a sense of always being in the hands of an external fate. 

To break the hold of the negative complex, and to escape the passivity of victimhood, requires taking responsibility for one's own desires and long-term actions.




</doc>
<doc id="30038492" url="https://en.wikipedia.org/wiki?curid=30038492" title="Narcissistic leadership">
Narcissistic leadership

Narcissistic leadership is a leadership style in which the leader is only interested in him/herself. Their priority is themselves – at the expense of their people/group members. This leader exhibits the characteristics of a narcissist: arrogance, dominance and hostility. It is a sufficiently common leadership style that it has acquired its own name. Narcissism is most often described as unhealthy and destructive. It has been described as "driven by unyielding arrogance, self-absorption, and a personal egotistic need for power and admiration".

A study published in the journal "Personality and Social Psychology Bulletin" suggests that when a group is without a leader, a narcissist is likely to take charge. Researchers have found that people who score high in narcissism tend to take control of leaderless groups. Freud considered "the "narcissistic" type... especially suited to act as a support for others, to take on the role of leaders and to... impress others as being 'personalities'.": one reason may be that "another person's narcissism has a great attraction for those who have renounced part of their own... as if we envied them for maintaining a blissful state of mind—an unassailable libidinal position which we ourselves have since abandoned."

According to the book "Narcissism: Behind the Mask", there are four basic types of leader with narcissists most commonly in type 3 although they may be in type 1:

Michael Maccoby stated that "psychoanalysts don't usually get close enough to [narcissistic leaders], especially in the workplace, to write about them."

According to Alan Downs, corporate narcissism occurs when a narcissist becomes the chief executive officer (CEO) or other leadership roles within the senior management team and gathers an adequate mix of codependents around him (or her) to support the narcissistic behavior. Narcissists profess company loyalty but are only really committed to their own agendas, thus organizational decisions are founded on the narcissist's own interests rather than the interests of the organization as a whole, the various stakeholders, or the society in which the organization operates. As a result, "a certain kind of charismatic leader can run a financially successful company on thoroughly unhealthy principles for a time. But ... the chickens always come home to roost".

Neville Symington has suggested that "one of the ways of differentiating a good-enough organisation from one that is pathological is through its ability to exclude narcissistic characters from key posts."

Lubit compared healthily narcissistic managers versus destructively narcissistic managers for their long-term impact on organizations.





</doc>
<doc id="24573437" url="https://en.wikipedia.org/wiki?curid=24573437" title="Narcissistic parent">
Narcissistic parent

A narcissistic parent is a parent affected by narcissism or narcissistic personality disorder. Typically, narcissistic parents are exclusively and possessively close to their children and are particularly threatened by their children's growing independence. This results in a pattern of narcissistic attachment, with the child considered to exist solely to fulfill the parent's wishes and needs. A common tactic of narcissistic parent is to control their children with threats and emotional abuse. Relative to developmental psychology, narcissistic parenting will adversely affect children in the areas of reasoning, emotional, ethical, and societal behaviors and attitudes as they mature. Within the realm of narcissistic parenting, personal boundaries are often disregarded with the goal of molding and manipulating the child to satisfy the parents’ expectations.

Narcissistic people with low self esteem feel the need to control how others regard them, fearing that otherwise they will be blamed or rejected and their personal inadequacies will be exposed. Narcissistic parents are self-absorbed, often to the point of grandiosity They also tend to be inflexible, and they therefore lack the empathy necessary for child raising.

The term “narcissism,” as used in Sigmund Freud’s clinical study, noted behavioral observations such as self-aggrandizement, self-esteem, vulnerability, fear of losing the affection of people and of failure, reliance on defense mechanisms, perfectionism and interpersonal conflict.

Narcissism tends to play out inter-generationally, with narcissistic parents producing either narcissistic or codependent children in turn. Whereas a self-confident parent, the good-enough parent, can allow a child its autonomous development, the narcissistic parent may instead use the child as a means to promote their own image. The father or mother concerned with self-enhancement, with being mirrored and admired by a son or daughter, may leave the latter feeling a puppet to his parent's emotional/intellectual demands.

To maintain their self-esteem, and protect their vulnerable true selves, narcissists need to control others' behavior, particularly that of their children seen as extensions of themselves. Thus narcissistic parents may speak of carrying the torch, maintaining the family image, or making mother or father proud and may reproach their children for exhibiting weakness, being too dramatic, or not meeting the standard of what is expected. As a result, children of narcissists learn to play their part and to perform their special skill, especially in public or for others; but typically do not have many memories of having felt loved or appreciated for being themselves, rather associating their experience of love and appreciation with conforming to the demands of the narcissistic parent.

Destructive narcissistic parents have a pattern of consistently being the focus of attention, exaggerating, seeking compliments and putting their children down. Punishment in the form of blame, criticism or emotional blackmail, and attempts to induce guilt, may be used to ensure compliance with the parents' wishes and their need for narcissistic supply.

Children of a resistant, more stubborn temperamental parent may defend against being supportive of others in the house. They could possibly observe how the selfish parents get their needs met by others, and learn how manipulation and using guilt gets the parent what he or she wants. They may also develop a false self and use aggression and intimidation to get their way. It is important to note that the opposite may also be true. Not all children of narcissists become aggressive, fake, manipulative adults. Instead, they may invest in the opposite behaviors if they've observed them in friends and other families. When the child of a narcissistic parent experiences safe, real love or sees the example played out in other families, they can very readily learn the difference between their life and that of a healthy family bond. For example, the lack of empathy and volatility at home may increase the child's own empathy and desire to be respectful. Intense emotional control and disrespect for boundaries at home may increase the child's value for emotional expression and their desire to extend respect to others. Although the child does see the parents behavior, it is important to remember they are often on the receiving end of the same behavior. When an alternative to the pain and distress caused at home presents itself, it is possible and maybe even likely the child will focus on the more comforting, social safety-inducing behaviors.

Some of the most common issues in narcissistic parenting are due to the lack of appropriate, responsible nurturing which ultimately contributes to the possibility of a child feeling empty, insecure in loving relationships, developing imagined fears, mistrusting others, experiencing identity conflict and an inability to develop a distinct existence from that of the parent.

Sensitive, guilt-ridden children in the family may learn to meet the parent's needs for gratification and try to get love by accommodating the whims and wishes of the parent. The child's normal feelings are ignored, denied and eventually repressed in attempts to gain the parent's “love”. Guilt and shame keep the child locked into this developmental arrest. Their aggressive impulses and rage could become split off and are not integrated with normal development. These children develop a false self as a defense mechanism and become codependent in relationships. The child's unconscious denial of their true self, presuming this defense mechanism is relevant in the victims life, perpetuates a cycle of self-hatred, fearing any reminder of their authentic self.

Narcissistic parenting may also lead to children being either victimized or bullying themselves, hypersexual in nature (media driven), having a poor or overly inflated body image, tendency to use and/or abuse drugs or alcohol, acting out (in a potentially harmful manner) for attention.

All of the aforementioned research, however, must be considered as preliminary within the growing field of mental health. One must acknowledge the many layers of psychological reaction in children who have been emotionally abused by their parent(s). Specifically in cases of narcissistic abuse, it is as likely for a child to observe manipulation and aggression and engage in it later in life as it is for a child to experience the pain caused by such behavior, and instead focus on developing the opposite behavior.

The only thing that is absolutely certain is that the child of the narcissistic parent is a victim of emotional abuse. How that affects each individual will have general trends, but it will always vary from person to person. With the right support and resources, children of narcissists are capable of having and maintaining healthy, fulfilling relationships. If you are the child of a narcissistic parent and you believe it is impacting your quality of life, you are not alone and there are many resources available, such as ASCA (Adult Survivors of Child Abuse) and professional therapy.

Having a narcissistic parent has many effects on a child - most especially a growing child. Due to their vulnerability, the effects of a narcissistic parent is most evident in their children. People that show traits of narcissism often feel the need to control others. Because a parent's role often involves guiding their children and often being the primary decision maker in the child's life, especially at a young age, a narcissistic parent will often abuse this power and become overly possessive and controlling. This possessiveness and excessive control dis-empowers the child; the parent sees the child simply as an extension of themselves, some would say, as "their puppet". With time, this affects the child's imagination and level of curiosity, and they will often develop an extrinsic style of motivation. This heightened level of control is attributed to the narcissistic parent not allowing for the "process of separate development."

Narcissistic parents are quick to anger. This trait makes their children more at risk of physical and emotional abuse. In order to prevent this anger and further punishment, children of narcissistic parents will often resort to complying to their parent's every demand, no matter the extent. To please a narcissist, you must conform to all their beliefs and essentially cater to their varying needs. Without doing this, one will often find themselves in conflict with a narcissist. This not only affects the child's well-being, but also affects the child's ability to make logical decisions on their own. The child will often lack self confidence and the ability to gain total control over their lives as adults. Identity crisis, loneliness, and struggles with self expression are also common effects seen in children that have been raised by a narcissistic parent. The struggle to discover one's self as an adult stems from the great amount of projective identification that the now adult experienced as a child. Projective identification can be explained as having someone else's identity and traits projected onto another, usually due to a close relationship. As a child, one may never get the opportunity to experience their own identity as a result of projective identification.

Though each individual differs based on other life experiences, children of narcissistic parents often go on to have character traits such as: introversion, kindness, agreeableness, and a keen interest and empathy for people who are mentally ill.

Studies have found that children of narcissistic parents have significantly higher rates of depression and lower self-esteem during adulthood in comparison to those who did not perceive their caregiver to be narcissistic. These outcomes are a result of the narcissistic parents using their children to "promote their own grandiose self-images" and a way for them to "regulate their own emotional experiences. The parent's lack of empathy towards their child is also a contributing factor, as the child's desires are often denied, their feelings restrained, and their overall emotional well-being often ignored.

Children of narcissistic parents are eventually taught to submit and comform, causing them to lose touch of themselves as individuals. This can lead to the child possessing very few memories of feeling appreciated or loved by their parents for being themselves, as they instead associate the love and appreciation to conformity. Children may receive good treatment by distancing themselves from the narcissistic parent, however this may not be an option for children, particularly during their developing years. Some children of narcissistic parents resort to this option during adolescence if they grow to view the relationship as toxic.





</doc>
<doc id="31711387" url="https://en.wikipedia.org/wiki?curid=31711387" title="History of evolutionary psychology">
History of evolutionary psychology

The history of evolutionary psychology began with Charles Darwin, who said that humans have social instincts that evolved by natural selection. Darwin's work inspired later psychologists such as William James and Sigmund Freud but for most of the 20th century psychologists focused more on behaviorism and proximate explanations for human behavior. E. O. Wilson's landmark 1975 book, "", synthesized recent theoretical advances in evolutionary theory to explain social behavior in animals, including humans. Jerome Barkow, Leda Cosmides and John Tooby popularized the term "evolutionary psychology" in their 1992 book "The Adapted Mind: Evolutionary Psychology and The Generation of Culture". Like sociobiology before it, evolutionary psychology has been embroiled in controversy, but evolutionary psychologists see their field as gaining increased acceptance overall.

After his seminal work in developing theories of natural selection, Charles Darwin devoted much of his final years to the study of animal emotions and psychology. He wrote two books;"The Descent of Man, and Selection in Relation to Sex" in 1871 and "The Expression of the Emotions in Man and Animals" in 1872 that dealt with topics related to evolutionary psychology. He introduced the concepts of sexual selection to explain the presence of animal structures that seemed unrelated to survival, such as the peacock's tail. He also introduced theories concerning group selection and kin selection to explain altruism. Darwin pondered why humans and animals were often generous to their group members. Darwin felt that acts of generosity decreased the fitness of generous individuals. This fact contradicted natural selection which favored the fittest individual. Darwin concluded that while generosity decreased the fitness of individuals, generosity would increase the fitness of a group. In this case, altruism arose due to competition between groups. The following quote, from Darwin's "Origin of Species", is often interpreted by evolutionary psychologists as indication of his foreshadowing the emergence of the field:
Darwin's theory inspired William James's functionalist approach to psychology. At the core of his theory was a system of "instincts." James wrote that humans had many instincts, even more than other animals. These instincts, he said, could be overridden by experience and by each other, as many of the instincts were actually in conflict with each other.

In their Evolutionary Psychology Primer Tooby and Cosmides make note of James' perspective, and also quote him:

According to Noam Chomsky, perhaps Anarchist thinker Peter Kropotkin could be credited as having founded evolutionary psychology, when in his 1902 book he argued that the human instinct for cooperation and mutual aid could be seen as stemming from evolutionary adaption.

William McDougall made a reference to "evolutionary psychology" in his 1919 book "An Introduction to Social Psychology": "It is only a comparative and evolutionary psychology that can provide the needed basis (for psychology); and this could not be created before the work of Darwin had convinced men of the continuity of human with animal evolution as regards all bodily characters, and had prepared the way for the quickly following recognition of the similar continuity of man’s mental evolution with that of the animal world." (p. 16)

While Darwin's theories on natural selection gained acceptance in the early part of the 20th century, his theories on evolutionary psychology were largely ignored. Only after the second world war, in the 1950s, did interest increase in the systematic study of animal behavior. It was during this period that the modern field of ethology emerged. Konrad Lorenz and Nikolaas Tinbergen were pioneers in developing the theoretical framework for ethology for which they would receive a Nobel prize in 1973.

Desmond Morris's book "The Naked Ape" attempted to frame human behavior in the context of evolution, but his explanations failed to convince academics because they were based on a teleological (goal-oriented) understanding of evolution. For example, he said that the pair bond evolved so that men who were out hunting could trust that their mates back home were not having sex with other males.

In 1975, E. O. Wilson built upon the works of Lorenz and Tinbergen by combining studies of animal behavior, social behavior and evolutionary theory in his book "." Wilson included a chapter on human behavior. Wilson's application of evolutionary analysis to human behavior caused bitter debate.

With the publication of "Sociobiology", evolutionary thinking for the first time had an identifiable presence in the field of psychology. E. O. Wilson argues that the field of evolutionary psychology is essentially the same as "human sociobiology".

Edward H. Hagen writes in "The Handbook of Evolutionary Psychology" that sociobiology is, despite the public controversy regarding the applications to humans, "one of the scientific triumphs of the twentieth century." "Sociobiology is now part of the core research and curriculum of virtually all biology departments, and it is a foundation of the work of almost all field biologists" Sociobiological research on nonhuman organisms has increased dramatically and appears continuously in the world's top scientific journals such as "Nature" and "Science".The more general term behavioral ecology is commonly used as substitute for the term sociobiology in order to avoid the public controversy.

The term "evolutionary psychology" was used by American biologist Michael Ghiselin in a 1973 article published in the journal "Science". Jerome Barkow, Leda Cosmides and John Tooby popularized the term "evolutionary psychology" in their 1992 book "The Adapted Mind: Evolutionary Psychology and The Generation of Culture".

In contrast to sociobiology and behavioral ecology, evolutionary psychology emphasizes that organisms are "adaptation executors" rather than "fitness maximizers." In other words, organisms have emotional, motivational and cognitive adaptations that generally increased inclusive fitness in the past but may not do so in the present. This distinction may explain some maladaptive behaviors that are the result of "fitness lags" between ancestral and modern environments. For example, our ancestrally developed desires for fat, sugar and salt often lead to health problems in modern environment where these are readily available in large quantities.

Also, in contrast to sociobiology and behavioral ecology (which mostly study non-human animal behavior), rather than focus primarily on overt behavior, EP attempts to identify underlying psychological adaptations (including emotional, motivational and cognitive mechanisms), and how these mechanisms interact with the developmental and current environmental influences to produce behavior.

Before 1990, introductory psychology textbooks scarcely mentioned Darwin. In the 1990s, evolutionary psychology was treated as a fringe theory, and evolutionary psychologists depicted themselves as an embattled minority. Coverage in psychology textbooks was largely hostile. According to evolutionary psychologists, current coverage in psychology textbooks is usually neutral or balanced.

The presence that evolutionary theory holds in psychology has been steadily increasing. According to its proponents, evolutionary psychology now occupies a central place in psychological science.



</doc>
<doc id="32324913" url="https://en.wikipedia.org/wiki?curid=32324913" title="Premastication">
Premastication

Premastication, pre-chewing, or kiss feeding is the act of chewing food for the purpose of physically breaking it down in order to feed another that is incapable of masticating the food by themselves. This is often done by the mother or relatives of a baby to produce baby food capable of being consumed by the child during the weaning process. The chewed food in the form of a bolus is transferred from the mouth of one individual to another, either directly mouth-to-mouth, via utensils, hands, or is further cooked or processed prior to feeding.

The behaviour was common throughout human history and societies and observed in non-human animals. While premastication is less common in present-day Western societies it was commonly practiced, and is still done in more traditional cultures. Although the health benefits of premastication are still being actively studied, the practice appears to confer certain nutritional and immunological benefits to the infant, provided that the caretaker is in good health and not infected by pathogens.

Premastication and mouth-to-mouth feeding in humans is postulated to have evolved from the regurgitation of food from parent to offspring or male to female (courtship feeding) and has been observed in numerous mammals and animals of other species. For instance, food begging behaviour observed in young wolves, wild dogs and certain gulls species, which involves the young approaching the beak or mouth of the adult with their own whereupon gaping their mouths or nuzzling, the adult would regurgitate portions of food to feed the young. However, in the aforementioned animals, this nuzzling behaviour and other types of mouth-to-mouth contact are also used for bonding, socialization, and courtship.

Young orangutans also beg for food by such contact and accordingly their caretakers regurgitate to feed them. Indeed, behaviours of mouth to mouth feeding of premasticated food and ritualized mouth to mouth contact for bonding has been observed in anthropoid great apes such as gorillas, orangutans, and chimpanzees. All of this supports the idea that human behaviours of kissing and feeding of premastication foods, either directly or indirectly from the mouth, have their behavioural roots in higher animals and ancestral great apes.

In all human cultures premastication/kiss-feeding or kissing between mother and infant has been observed in all with kissing believed to be a socially ritualized form of feeding premasticated food. There is high similarity in the execution of kiss-feeding and human kisses (e.g. French kiss); in the former, the tongue is used to push food from mother to child with the child receiving both the mother's food and tongue in sucking movements, and the latter simply forgoes the premasticated food. In fact, observations across various species and cultures confirms that the act of kissing and premastication have most likely evolved from the similar relationship-based feeding behaviours.

Written records of premastication have been found in Ancient Egypt, though the practice likely extends back into prehistoric times to non-human ancestors. For instance, in the Ancient Egyptian Ebers medical papyrus, a mother was instructed to give a medical remedy to a child through premastication. In the fifth century A.D. Roman culture, premastication of infants' food by caretakers was also common, though the lack of sanitation along with the practice contributed to infant mortality. Infants in Medieval Europe were fed an assortment of mashed, premasticated food or bread softened with liquids.

Due to attitudes in Western medicine in the 1940s and 1950s, Native American and Fijian cultures and societies were strongly dissuaded from premastication due to concerns about the hygiene of the practice. However, the lack of knowledge regarding premastication and its prohibition by missionaries and doctors instead caused severe anemia in the infants of the population, or resulted in malnourished infants and children deprived of nourishment.

Although less prevalent in modern post-industrial Western societies, the offering of premasticated foods to infants is found in many traditional cultures and offers their infants numerous benefits. In North America, premastication is still commonly used by Black and Hispanic mothers, and commonly used by women of Inuit and Aleut peoples.

In many human cultures, the act of premastication and direct mouth-to-mouth feeding is linked with the showing of affection, known as "kiss feeding". In the Manus cultures of the Admiralty Islands, the act of premastication has been used by a women to remind children and descendants of their obligations to her. Some human cultures such as the people of Papua New Guinea in fact use mouth to mouth contact primarily for feeding premasticated food, with sexual kissing only observed after the arrival of Europeans. This form of feeding is believed to have evolved into the modern human acts of kissing and French kissing.

Many Western societies have strong aversions toward premastication, which have been compared to their similar criticisms and aversion towards breastfeeding in previous generations for similar rationale, with the same societies finding breastfeeding to be a disagreeable practice performed only by the uneducated lower classes or untempered foreign cultures. For instance, in the late 1800s the medical community of Texas was embroiled in a debate on premastication, with those supporting the practice arguing its benefits and those against it stating that it is "filthy and repulsive and... barbaric".

The act of premastication is commonly found in all human societies and populations, although it is less prevalent in some than others. The evolution and selective advantage of premastication behaviours is that it supplements the infant diet of breast milk by providing access to more macro- and micro-nutrients. Although disease can be transmitted through saliva in the pre-chewed foods, the benefits conferred outweighed any risks of the practice during the evolution of human behaviour. Furthermore, discouraging premastication as prevention to disease transmittion may prove as disastrous an infant public health policy as when infants breastfeeding was discouraged in the late 1980s and early 1990s. In the end, the potential benefits and pitfalls of this practice greatly depend on the dietary and medical circumstances of the provider and child. The true scope of the benefits of premastication and its prevalences in differenct societies is still under active research, though there appears to be some consensus on the nutritional benefits of the practice.



</doc>
<doc id="37123" url="https://en.wikipedia.org/wiki?curid=37123" title="Theories of political behavior">
Theories of political behavior

Theories of political behavior, as an aspect of political science, attempt to quantify and explain the influences that define a person's political views, ideology, and levels of political participation. Broadly speaking, behavior is political whenever individuals or groups try to influence or escape the influence of others. Political behavior is the subset of human behavior that involves politics and powers. Theorists who have had an influence on this field include Karl Deutsch and Theodor Adorno.

Interaction with the political views of parental figures is often thought of as the primary long-term influence on political orientation and willingness to take part in the political system.

Teachers and other educational authority figures are also often thought to have a significant impact on political orientation. During the 2003-2004 school year, In the United States, students spent an average of 180.4 days in primary and secondary education each year, with a school day being defined as approximately 6.7 class hours. This means that on average a student will spend around 1,208.68 hours in class each year. Post-secondary education appears to have an impact on both voting rates and political identification; as a study of 9,784,931 college students found that they voted at a rate of 68.5% in the 2016 Presidential Election compared to the average of 46.1% for citizens aged 18-29 who voted. 

Peers also affect political orientation. Friends often, but not necessarily, have the advantage of being part of the same generation, which collectively develops a unique set of societal issues; Eric L. Dey has argued that "socialisation is the process through which individuals acquire knowledge, habits, and value orientations that will be useful in the future." The ability to relate on this common level is what fuels and enables future ideological growth.

Sociologists and political scientists debate the relationship between age and the formation of political attitudes. The impressionable years hypothesis postulates that political orientation is solidified during early adulthood. By contrast, the "increasing persistence hypothesis" posits that attitudes become less likely to change as individuals become older, while the "life-long openness hypothesis" proposes that the attitudes of individuals remain flexible regardless of age.

Short-term factors also affect voting behavior; the media and the impact of individual election issues are among these factors. These factors differ from the long-term factors as they are often short-lived. However, they can be just as crucial in modifying political orientation. The ways in which these two sources are interpreted often relies on the individuals specific political ideology formed by the long-term factors.

Most political scientists agree that the mass media have a profound impact on voting behavior. One author asserts that "few would argue with the notion that the institutions of the mass media are important to contemporary politics ... in the transition to liberal democratic politics in the Soviet Union and Eastern Europe the media was a key battleground."

Second, there are election issues. These include campaign issues, debates and commercials. Election years and political campaigns can shift certain political behaviors based on the candidates involved, which have different degrees of effectiveness in influencing voters.

Recently, some political scientists have been interested in many studies which aimed to analyze the relation between the behavior of social groups and the political outcomes. Some of the social groups included in their studies have been age demographics, gender, and ethnic groups.

For example, in U.S. politics, the effect of ethnic groups and gender has a great influence on the political outcomes.

Latin Americans have a profound social impact on the political outcome of their vote and are emerging as a strong up-and-coming political force. The most noticeable increase in Latin American voting was in the 2000 presidential election, although the votes did not share a socially common political view at that time. In the 2006 election, the Latin American vote aided tremendously in the election of Florida Senator Mel Martinez, although in the 2004 presidential election, about 44% of Latin Americans voted for Republican President George W. Bush. However, Latin Americans have the lowest voting rate in the United States, with only 47.6% voting in the 2016 Presidential Election in the United States. Currently illegal immigration has been claiming the most attention and Latin Americans, although not completely unanimous, are concerned with the education, employment and deportation of illegal immigrants in the United States. Although the majority of Latin Americans vote for Democratic candidates, Cuban Americans are likely the most conservative of Latinos, with 54% of Cuban American voters casting ballots for Donald Trump in the 2016 Presidential Election, compared to an average of 35% of all Latinos who voted. Although this was represents a net decrease in support for the Republican Party among Cuban Americans, it continues a trend created by the exile of many Cubans after the Cuban Revolution.

African Americans have the second highest voting rates in the United States and even surpassed white voters in the 2008 Presidential Election, although this has declined in the 2016 Presidential Election. In the 2008 Presidential Election and 2012 Presidential election, African Americans voted overwhelmingly for Democratic candidate, Barack Obama. This trend of African Americans voting for candidates of the Democratic Party continued into the 2016 Presidential Election.

Women in the United States have, in the past 30 years, surpassed male voting rates, with the 2016 Presidential Election having a ratio between females and males of 52 to 48. This trend is often referred to as the Gender Gap and when combined with the tendency of women to vote for Democratic candidates, their effect on political outcomes is extremely important. 
Interdisciplinary studies in biology and political science aim to identify correlates of political behavior with biological aspects, for example the linkage of biology and political orientation, but also with other aspects like partisanship and voting behavior. This field of study is typically referred to genopolitics although it is sometimes referred to as biopolitics, although the term has other meanings.

The study of possible genetic bases of political behavior has grown since the 1980s. The term genopolitics was coined by political scientist James Fowler in the early-2000s to describe research into identifying specific transporter/receptor genes responsible for ideological orientation beyond the sociopsychological realm of political socialisation.

Political scientists also aim to understand what drives individuals to participate in the democratic process, either by voting, volunteering for campaigns, signing petitions or protesting. Participation cannot always be explained by rational behavior. The voting paradox, for example, points out that it cannot be in a citizen's self-interest to vote because the effort it takes to vote will almost always outweigh the benefits of voting, particularly considering a single vote is unlikely to change an electoral outcome. Political scientists instead propose that citizens vote for psychological or social reasons. Studies show, for example, that individuals are more likely to vote if they see their friends have voted or if someone in their household has received a nudge to vote.




</doc>
<doc id="33998077" url="https://en.wikipedia.org/wiki?curid=33998077" title="Preferred walking speed">
Preferred walking speed

The preferred walking speed is the speed at which humans or animals choose to walk. Many people tend to walk at about . Although many people are capable of walking at speeds upwards of , especially for short distances, they typically choose not to. Individuals find slower or faster speeds uncomfortable.

Horses have also demonstrated normal, narrow distributions of preferred walking speed within a given gait, which suggests that the process of speed selection may follow similar patterns across species. Preferred walking speed has important clinical applications as an indicator of mobility and independence. For example, elderly people or people suffering from osteoarthritis must walk more slowly. Improving (increasing) people's preferred walking speed is a significant clinical goal in these populations.

People have suggested mechanical, energetic, physiological and psychological factors as contributors to speed selection. Probably, individuals face a trade-off between the numerous costs associated with different walking speeds, and select a speed which minimizes these costs. For example, they may trade off time to destination, which is minimized at fast walking speeds, and metabolic rate, muscle force or joint stress. These are minimized at slower walking speeds. Broadly, increasing value of time, motivation, or metabolic efficiency may cause people to walk more quickly. Conversely, aging, joint pain, instability, incline, metabolic rate and visual decline cause people to walk more slowly.

Commonly, individuals place some value on their time. Economic theory therefore predicts that value-of-time is a key factor influencing preferred walking speed.

Levine and Norenzayan (1999) measured preferred walking speeds of urban pedestrians in 31 countries and found that walking speed is positively correlated with the country's per capita GDP and purchasing power parity, as well as with a measure of individualism in the country's society. It is plausible that affluence correlates with actual value considerations for time spent walking, and this may explain why people in affluent countries tend to walk more quickly.

This idea is broadly consistent with common intuition. Everyday situations often change the value of time. For example, when walking to catch a bus, the value of the one minute immediately before the bus has departed may be worth 30 minutes of time (the time saved not waiting for the next bus). Supporting this idea, Darley and Bateson show that individuals who are hurried under experimental conditions are less likely to stop in response to a distraction, and so they arrive at their destination sooner.

Energy minimization is widely considered a primary goal of the central nervous system. The rate at which a human expends metabolic energy while walking (gross metabolic rate) increases nonlinearly with increasing speed. However, humans also require a continuous basal metabolic rate to maintain normal function. The energetic cost of walking itself is therefore best understood by subtracting basal metabolic rate from gross metabolic rate, yielding net metabolic rate. In human walking, net metabolic rate also increases nonlinearly with speed. These measures of walking energetics are based on how much oxygen people consume per unit time. Many locomotion tasks, however, require walking a fixed distance rather than for a set time. Dividing gross metabolic rate by walking speed results in gross cost of transport. For human walking, gross cost of transport is U-shaped. Similarly, dividing net metabolic rate by walking speed yields a U-shaped net cost of transport. These curves reflect the cost of moving a given distance at a given speed and may better reflect the energetic cost associated with walking.

Ralston (1958) showed that humans tend to walk at or near the speed that minimizes gross cost of transport. He showed that gross cost of transport is minimized at about , which corresponded to the preferred speed of his subjects. Supporting this, Wickler "et al." (2000) showed that the preferred speed of horses both uphill and on the level corresponds closely to the speed that minimizes their gross cost of transport. Among other gait costs that human walkers choose to minimize, this observation has led many to suggest that people minimize cost and maximize efficiency during locomotion. Because gross cost of transport includes velocity, gross cost of transport includes an inherent value of time. Subsequent research suggests that individuals may walk marginally faster than the speed that minimizes gross cost of transport under some experimental setups, although this may be due to how preferred walking speed was measured.

In contrast, other researchers have suggested that gross cost of transport may not represent the metabolic cost of walking. People must continue to expend their basal metabolic rate regardless of whether they are walking, suggesting that the metabolic cost of walking should not include basal metabolic rate. Some researchers have therefore used net metabolic rate instead of gross metabolic rate to characterize the cost of locomotion. Net cost of transport reaches a minimum at about . Healthy pedestrians walk faster than this in many situations.

Gross metabolic rate may also directly limit preferred walking speed. Aging is associated with reduced aerobic capacity (reduced VO max). Malatesta "et al." (2004) suggests that walking speed in elderly individuals is limited by aerobic capacity; elderly individuals are unable to walk faster because they cannot sustain that level of activity. For example, 80-year-old individuals are walking at 60% of their VO2 max even when walking at speeds significantly slower than those observed in younger individuals.

Biomechanical factors such as mechanical work, stability, and joint or muscle forces may also influence human walking speed. Walking faster requires additional external mechanical work per step. Similarly, swinging the legs relative to the center of mass requires some internal mechanical work. As faster walking is accomplished with both longer and faster steps, internal mechanical work also increases with increasing walking speed. Therefore, both internal and external mechanical work per step increases with increasing speed. Individuals may try to reduce either external or internal mechanical work by walking more slowly, or may select a speed at which mechanical energy recovery is at a maximum.

Stability may be another factor influencing speed selection. Hunter "et al." (2010) showed that individuals use energetically suboptimal gaits when walking downhill. He suggests that people may instead be choosing gait parameters that maximize stability while walking downhill. This suggests that under adverse conditions such as down hills, gait patterns may favor stability over speed.

Individual joint and muscle biomechanics also directly affect walking speed. Norris showed that elderly individuals walked faster when their ankle extensors were augmented by an external pneumatic muscle. Muscle force, specifically in the gastrocnemius and/or soleus, may limit walking speed in certain populations and lead to slower preferred speeds. Similarly, patients with ankle osteoarthritis walked faster after a complete ankle replacement than before. This suggests that reducing joint reaction forces or joint pain may factor into speed selection.

The rate at which the environment flows past the eyes seems to be a mechanism for regulating walking speed. In virtual environments, the gain in visual flow can be decoupled from a person’s actual walking speed, much as one might experience when walking on a conveyor belt. There, the environment flows past an individual more quickly than their walking speed would predict (higher than normal visual gain). At higher than normal visual gains, individuals prefer to walk more slowly, while at lower than normal visual gains, individuals prefer to walk more quickly. This behavior is consistent with returning the visually observed speed back toward the preferred speed and suggests that vision is used correctively to maintain walking speed at a value that is perceived to be optimal. Moreover, the dynamics of this visual influence on preferred walking speed are rapid—when visual gains are changed suddenly, individuals adjust their speed within a few seconds. The timing and direction of these responses strongly indicate that a rapid predictive process informed by visual feedback helps select preferred speed, perhaps to complement a slower optimization process that directly senses metabolic rate and iteratively adapts gait to minimize it.

With the wide availability of inexpensive pedometers, medical professionals recommend walking as an exercise for cardiac health and/or weight loss. NIH gives the following guidelines:

The situation becomes slightly more complex when preferred walking speed is introduced. The faster the pace, the more calories burned if weight loss is a goal. Maximum heart rate for exercise (220 minus age), when compared to charts of "fat burning goals" support many of the references that give the average of 1.4 m/s or 3 mph, as within this target range. Pedometers average 100 steps a minute in this range (depending on individual stride), or one and a half to two hours to reach a daily total of 10,000 or more steps (100 minutes at 100 steps per minute would be 10,000 steps).

The typical walking speed of 1.4 m/s is recommended by design guides including the Design Manual for Roads and Bridges. Transport for London recommend 1.33 m/s in the PTAL methodology.



</doc>
<doc id="151604" url="https://en.wikipedia.org/wiki?curid=151604" title="Deception">
Deception

Deception is an act or statement which misleads, hides the truth, or promotes a belief, concept, or idea that is not true. It is often done for personal gain or advantage. Deception can involve dissimulation, propaganda, and sleight of hand, as well as distraction, camouflage, or concealment. There is also self-deception, as in bad faith. It can also be called, with varying subjective implications, beguilement, deceit, bluff, mystification, ruse, or subterfuge.

Deception is a major relational transgression that often leads to feelings of betrayal and distrust between relational partners. Deception violates relational rules and is considered to be a negative violation of expectations. Most people expect friends, relational partners, and even strangers to be truthful most of the time. If people expected most conversations to be untruthful, talking and communicating with others would require distraction and misdirection to acquire reliable information. A significant amount of deception occurs between some romantic and relational partners.

Deceit and dishonesty can also form grounds for civil litigation in tort, or contract law (where it is known as misrepresentation or fraudulent misrepresentation if deliberate), or give rise to criminal prosecution for fraud. It also forms a vital part of psychological warfare in denial and deception.

Deception includes several types of communications or omissions that serve to distort or omit the complete truth. Examples of deception range from false statements to misleading claims in which relevant information is omitted, leading the receiver to infer false conclusions. For example, a claim that 'sunflower oil is beneficial to brain health due to the presence of omega-3 fatty acids' may be misleading, as it leads the receiver to believe sunflower oil will benefit brain health more so than other foods. In fact, sunflower oil is relatively low in omega-3 fatty acids and is not particularly good for brain health, so while this claim is technically true, it leads the receiver to infer false information. Deception itself is intentionally managing verbal or nonverbal messages so that the message receiver will believe in a way that the message sender knows is false. Intent is critical with regard to deception. Intent differentiates between deception and an honest mistake. The Interpersonal Deception Theory explores the interrelation between communicative context and sender and receiver cognitions and behaviors in deceptive exchanges.

Some forms of deception include:

Many people believe that they are good at deception, though this confidence is often misplaced.

Buller and Burgoon (1996) have proposed three taxonomies to distinguish motivations for deception based on their Interpersonal Deception Theory:

Deception detection between relational partners is extremely difficult, unless a partner tells a blatant or obvious lie or contradicts something the other partner knows to be true. While it is difficult to deceive a partner over a long period of time, deception often occurs in day-to-day conversations between relational partners. Detecting deception is difficult because there are no known completely reliable indicators of deception and because people often reply on a truth-default state. Deception, however, places a significant cognitive load on the deceiver. He or she must recall previous statements so that his or her story remains consistent and believable. As a result, deceivers often leak important information both verbally and nonverbally.

Deception and its detection is a complex, fluid, and cognitive process that is based on the context of the message exchange. The interpersonal deception theory posits that interpersonal deception is a dynamic, iterative process of mutual influence between a sender, who manipulates information to depart from the truth, and a receiver, who attempts to establish the validity of the message. A deceiver's actions are interrelated to the message receiver's actions. It is during this exchange that the deceiver will reveal verbal and nonverbal information about deceit. Some research has found that there are some cues that may be correlated with deceptive communication, but scholars frequently disagree about the effectiveness of many of these cues to serve as reliable indicators. Noted deception scholar Aldert Vrij even states that there is no nonverbal behavior that is uniquely associated with deception. As previously stated, a specific behavioral indicator of deception does not exist. There are, however, some nonverbal behaviors that have been found to be correlated with deception. Vrij found that examining a "cluster" of these cues was a significantly more reliable indicator of deception than examining a single cue.

Mark Frank proposes that deception is detected at the cognitive level. Lying requires deliberate conscious behavior, so listening to speech and watching body language are important factors in detecting lies. If a response to a question has a lot disturbances, less talking time, repeated words, and poor logical structure, then the person may be lying. Vocal cues such as frequency height and variation may also provide meaningful clues to deceit.

Fear specifically causes heightened arousal in liars, which manifests in more frequent blinking, pupil dilation, speech disturbances, and a higher pitched voice. The liars that experience guilt have been shown to make attempts at putting distance between themselves and the deceptive communication, producing “nonimmediacy cues” These can be verbal or physical, including speaking in more indirect ways and showing an inability to maintain eye contact with their conversation partners. Another cue for detecting deceptive speech is the tone of the speech itself. Streeter, Krauss, Geller, Olson, and Apple (1977) have assessed that fear and anger, two emotions widely associated with deception, cause greater arousal than grief or indifference, and note that the amount of stress one feels is directly related to the frequency of the voice.

The camouflage of a physical object often works by breaking up the visual boundary of that object. This usually involves colouring the camouflaged object with the same colours as the background against which the object will be hidden. In the realm of deceptive half-truths, camouflage is realized by 'hiding' some of the truths.

Military camouflage as a form of visual deception is a part of military deception.

A disguise is an appearance to create the impression of being somebody or something else; for a well-known person this is also called incognito. Passing involves more than mere dress and can include hiding one's real manner of speech.

Example:

In a more abstract sense, 'disguise' may refer to the act of disguising the nature of a particular proposal in order to hide an unpopular motivation or effect associated with that proposal. This is a form of political spin or propaganda. "See also": rationalisation and transfer within the techniques of propaganda generation.

Example:

Example:

The term "deception" as used by a government is typically frowned upon unless it's in reference to military operations. The terms for the means by which governments employ deception are:


Simulation consists of exhibiting false information. There are three simulation techniques: mimicry (copying another model or example, such as non-poisonous snakes which have the colours and markings of poisonous snakes), fabrication (making up a new model), and distraction (offering an alternative model)

In the biological world, mimicry involves "unconscious" deception by similarity to another organism, or to a natural object. Animals for example may deceive predators or prey by visual, auditory or other means.

To make something that appears to be something that it is not, usually for the purpose of encouraging an adversary to reveal, endanger, or divert that adversary's own resources ("i.e.", as a decoy). For example, in World War II, it was common for the Allies to use hollow tanks made out of wood to fool German reconnaissance planes into thinking a large armor unit was on the move in one area while the real tanks were well hidden and on the move in a location far from the fabricated "dummy" tanks. Mock airplanes and fake airfields have also been created.

To get someone's attention from the truth by offering bait or something else more tempting to divert attention away from the object being concealed. For example, a security company publicly announces that it will ship a large gold shipment down one route, while in reality take a different route. A military unit trying to maneuver out of a dangerous position may make a feint attack or fake retreat, to make the enemy think they are doing one thing, while in fact they have another goal.

A seventeenth-century story collection, Zhang Yingyu's "The Book of Swindles" (ca. 1617), offers multiple examples of the bait-and-switch and fraud techniques involving the stimulation of greed in Ming-dynasty China.

Deception is particularly common within romantic relationships, with more than 90% of individuals admitting to lying or not being completely honest with their partner at one time.

There are three primary motivations for deception in relationships.

Deception impacts the perception of a relationship in a variety of ways, for both the deceiver and the deceived. The deceiver typically perceives less understanding and intimacy from the relationship, in that they see their partner as less empathetic and more distant. The act of deception can also result in feelings of distress for the deceiver, which become worse the longer the deceiver has known the deceived, as well as in longer-term relationships. Once discovered, deception creates feelings of detachment and uneasiness surrounding the relationship for both partners; this can eventually lead to both partners becoming more removed from the relationship or deterioration of the relationship. In general, discovery of deception can result in a decrease in relationship satisfaction and commitment level, however, in instances where a person is successfully deceived, relationship satisfaction can actually be positively impacted for the person deceived, since lies are typically used to make the other partner feel more positive about the relationship.

In general, deception tends to occur less often in relationships with higher satisfaction and commitment levels and in relationships where partners have known each other longer, such as long-term relationships and marriage. In comparison, deception is more likely to occur in casual relationships and in dating where commitment level and length of acquaintanceship is often much lower.

Unique to exclusive romantic relationships is the use of deception in the form of infidelity. When it comes to the occurrence of infidelity, there are many individual difference factors that can impact this behavior. Infidelity is impacted by attachment style, relationship satisfaction, executive function, sociosexual orientation, personality traits, and gender. Attachment style impacts the probability of infidelity and research indicates that people with an insecure attachment style (anxious or avoidant) are more likely to cheat compared to individuals with a secure attachment style, especially for avoidant men and anxious women. Insecure attachment styles are characterized by a lack of comfort within a romantic relationship resulting in a desire to be overly independent (avoidant attachment style) or a desire to be overly dependent on their partner in an unhealthy way (anxious attachment style). Those with an insecure attachment style are characterized by not believing that their romantic partner can/will support and comfort them in an effective way, either stemming from a negative belief regarding themselves (anxious attachment style) or a negative belief regarding romantic others (avoidant attachment style). Women are more likely to commit infidelity when they are emotionally unsatisfied with their relationship whereas men are more likely to commit infidelity if they are sexually unsatisfied with their current relationship. Women are more likely to commit emotional infidelity than men while men are more likely to commit sexual infidelity than women; however, these are not mutually exclusive categories as both men and women can and do engage in emotional or sexual infidelity.

Executive control is a part of executive functions that allows for individuals to monitor and control their behavior through thinking about and managing their actions. The level of executive control that an individual possesses is impacted by development and experience and can be improved through training and practice. Those individuals that show a higher level of executive control can more easily influence/control their thoughts and behaviors in relation to potential threats to an ongoing relationship which can result in paying less attention to threats to the current relationship (other potential romantic mates). Sociosexual orientation is concerned with how freely individuals partake in casual sex outside of a committed relationship and their beliefs regarding how necessary it is to be in love in order to engage in sex with someone. Individuals with a less restrictive sociosexual orientation (more likely to partake in casual sex) are more likely to engage in infidelity. Individuals that have personality traits including (high) neuroticism, (low) agreeableness, and (low) conscientiousness are more likely to commit infidelity. Men are generally speculated to cheat more than women, but it is unclear if this is a result of socialization processes where it is more acceptable for men to cheat compared to women or due to an actual increase in this behavior for men. Research conducted by Conley and colleagues (2011) suggests that the reasoning behind these gender differences stems from the negative stigma associated with women who engage in casual sex and inferences about the sexual capability of the potential sexual partner. In their study, men and women were equally likely to accept a sexual proposal from an individual who was speculated to have a high level of sexual prowess. Additionally, women were just as likely as men to accept a casual sexual proposal when they did not anticipate being subjected to the negative stigma of sexually permissible women as slutty.

Research on the use of deception in online dating has shown that people are generally truthful about themselves with the exception of physical attributes to appear more attractive. According to the Scientific American, “nine out of ten online daters will fib about their height, weight, or age” such that men were more likely to lie about height while women were more likely to lie about weight. In a study conducted by Toma and Hancock, “less attractive people were found to be more likely to have chosen a profile picture in which they were significantly more attractive than they were in everyday life”. Both genders used this strategy in online dating profiles, but women more so than men. Additionally, less attractive people were more likely to have “lied about objective measures of physical attractiveness such as height and weight”. In general, men are more likely to lie on dating profiles the one exception being that women are more likely to lie about weight.

Some methodologies in social research, especially in psychology, involve deception. The researchers purposely mislead or misinform the participants about the true nature of the experiment. In an experiment conducted by Stanley Milgram in 1963 the researchers told participants that they would be participating in a scientific study of memory and learning. In reality the study looked at the participants' willingness to obey commands, even when that involved inflicting pain upon another person. After the study, the subjects were informed of the true nature of the study, and steps were taken in order to ensure that the subjects left in a state of well being. Use of deception raises many problems of research ethics and it is strictly regulated by professional bodies such as the American Psychological Association.

Psychological research often needs to deceive the subjects as to its actual purpose. The rationale for such deception is that humans are sensitive to how they appear to others (and to themselves) and this self-consciousness might interfere with or distort from how they actually behave outside of a research context (where they would not feel they were being scrutinized). For example, if a psychologist is interested in learning the conditions under which students cheat on tests, directly asking them, "how often do you cheat?," might result in a high percent of "socially desirable" answers and the researcher would in any case be unable to verify the accuracy of these responses. In general, then, when it is unfeasible or naive to simply ask people directly why or how often they do what they do, researchers turn to the use of deception to distract their participants from the true behavior of interest. So, for example, in a study of cheating, the participants may be told that the study has to do with how intuitive they are. During the process they might be given the opportunity to look at (secretly, they think) another participant's [presumably highly intuitively correct] answers before handing in their own. At the conclusion of this or any research involving deception, all participants must be told of the true nature of the study and why deception was necessary (this is called debriefing). Moreover, it is customary to offer to provide a summary of the results to all participants at the conclusion of the research.

Though commonly used and allowed by the ethical guidelines of the American Psychological Association, there has been debate about whether or not the use of deception should be permitted in psychological research experiments. Those against deception object to the ethical and methodological issues involved in its use. Dresser (1981) notes that, ethically, researchers are only to use subjects in an experiment after the subject has given informed consent. However, because of its very nature, a researcher conducting a deception experiment cannot reveal its true purpose to the subject, thereby making any consent given by a subject misinformed (p. 3). Baumrind (1964), criticizing the use of deception in the Milgram (1963) obedience experiment, argues that deception experiments inappropriately take advantage of the implicit trust and obedience given by the subject when the subject volunteers to participate (p. 421).

From a practical perspective, there are also methodological objections to deception. Ortmann and Hertwig (1998) note that "deception can strongly affect the reputation of individual labs and the profession, thus contaminating the participant pool" (p. 806). If the subjects in the experiment are suspicious of the researcher, they are unlikely to behave as they normally would, and the researcher's control of the experiment is then compromised (p. 807). Those who do not object to the use of deception note that there is always a constant struggle in balancing "the need for conducting research that may solve social problems and the necessity for preserving the dignity and rights of the research participant" (Christensen, 1988, p. 670). They also note that, in some cases, using deception is the only way to obtain certain kinds of information, and that prohibiting all deception in research would "have the egregious consequence of preventing researchers from carrying out a wide range of important studies" (Kimmel, 1998, p. 805).

Additionally, findings suggest that deception is not harmful to subjects. Christensen's (1988) review of the literature found "that research participants do not perceive that they are harmed and do not seem to mind being misled" (p. 668). Furthermore, those participating in experiments involving deception "reported having enjoyed the experience more and perceived more educational benefit" than those who participated in non-deceptive experiments (p. 668). Lastly, it has also been suggested that an unpleasant treatment used in a deception study or the unpleasant implications of the outcome of a deception study may be the underlying reason that a study using deception is perceived as unethical in nature, rather than the actual deception itself (Broder, 1998, p. 806; Christensen, 1988, p. 671).

Deception is a recurring theme in modern philosophy. In 1641 Descartes published his meditations, in which he introduced the notion of the Deus deceptor, a posited being capable of deceiving the thinking ego about reality. The notion was used as part of his hyperbolic doubt, wherein one decides to doubt everything there is to doubt. The Deus deceptor is a mainstay of so-called skeptical arguments, which purport to put into question our knowledge of reality. The punch of the argument is that all we know might be wrong, since we might be deceived. Stanley Cavell has argued that all skepticism has its root in this fear of deception.

Deception is a common topic in religious discussions. Some sources focus on how religious texts deal with deception. But, other sources focus on the deceptions created by the religions themselves. For example, Ryan McKnight is the founder of an organization called FaithLeaks. He stated that the organizations "goal is to reduce the amount of deception and untruths and unethical behaviors that exist in some facets of religion".

In its purest form, Christianity encourages the pursuit of truth. But, in practice, many Christians are criticized as being deceptive and otherwise problematic. The prominent political speech writer Michael Gerson said that evangelicals were "associating evangelicalism with bigotry, selfishness and deception." His comments were directed specifically towards those evangelicals who support Donald Trump.

In Islam the concept of Taqiyya is often interpreted as legitimized deception. But, many Muslims view Taqiyya as a necessary means of alleviating religious persecution. In the city of Basking Ridge, New Jersey the town's residents used the concept of Taqivya to block a mosque from being built. The dispute went on for years. In a related story, journalist Ian Wilkie of Newsweek asserted that Taquivya provides evidence that Assad has used chemical weapons on the Syrian people.

For legal purposes, deceit is a tort that occurs when a person makes a factual misrepresentation, knowing that it is false (or having no belief in its truth and being reckless as to whether it is true) and intending it to be relied on by the recipient, and the recipient acts to his or her detriment in reliance on it. Deceit may also be grounds for legal action in contract law (known as misrepresentation, or if deliberate, fraudulent misrepresentation), or a criminal prosecution, on the basis of fraud.

Town Porsche of Englewood, New Jersey 07631 - also known as a new Perspective on Human Deceit. 



</doc>
<doc id="16598952" url="https://en.wikipedia.org/wiki?curid=16598952" title="Bad habit">
Bad habit

A bad habit is a negative behaviour pattern. Common examples include: procrastination, overspending, stereotyping,
nail-biting and spending too much time watching television or using a computer.

There is a theory that it takes an average of 66 days to break a habit. The amount of time it takes to break a habit is generally between 18 and 254 days. This should often be repeated once or maybe twice depending on what the habit is, something small like chewing fingernail should only have to be done once. Larger habits like smoking should be repeated at twice but every one is different so it could be less. There are sources that phrase the breaking of bad habits under habit-formation and that an individual acquires a new habit within 66 days. A study found that this process is marked by an asymptomatic increase of the behavior, with the initial acceleration slowing to a plateau after the said time period. There are several variations regarding the period of development. For instance, a source stated that breaking a bad habit or changing an unhealthy behavioral pattern such as smoking takes 90 days while forming a new habit that stick requires 66 days.

A key factor in distinguishing a bad habit from an addiction or mental disease is the element of willpower. If a person still seems to have control over the behavior then it is just a habit. Good intentions are able to override the negative effect of bad habits but their effect seems to be independent and additive — the bad habits remain but are subdued rather than cancelled.

The best time to correct a bad habit is immediately, before it becomes established. So, bad habits are best prevented from developing in childhood.

There are many techniques for removing bad habits once they have become established. One good one is to go for between 21 and 28 days try as hard as possible not to give in to the habit then rewarding yourself at the end of it. Then try to go a week, if the habit remains repeat the process, this method is proven to have a high success rate.



</doc>
<doc id="36076964" url="https://en.wikipedia.org/wiki?curid=36076964" title="Maternal sensitivity">
Maternal sensitivity

Maternal sensitivity is a mother's ability to perceive and infer the meaning behind her infant's behavioural signals, and to respond to them promptly and appropriately. Maternal sensitivity affects child development at all stages through life, from infancy, all the way to adulthood. In general, more sensitive mothers have healthier, more socially and cognitively developed children than those who are not as sensitive. Also, maternal sensitivity has been found to affect the person psychologically even as an adult. Adults who experienced high maternal sensitivity during their childhood were found to be more secure than those who experienced less sensitive mothers. Once the adult becomes a parent themselves, their own understanding of maternal sensitivity will affect their own children's development. Some research suggests that adult mothers display more maternal sensitivity than adolescent mothers who may in turn have children with a lower IQ and reading level than children of adult mothers.

There are different ways of assessing maternal sensitivity, such as through the use of naturalistic observation, the Strange Situation, maternal-synchrony, and maternal mind-mindedness. There are also a number of ways of measuring maternal sensitivity in the scientific world, which include Ainsworth's Maternal Sensitivity Scale (AMSS), the Maternal Behaviour Q-sort (MBQS), and the Pederson and Moran Sensitivity Q-Sort.

Maternal sensitivity was first defined by Mary Ainsworth as "a mother's ability to perceive and interpret accurately her infant's signals and communications and then respond appropriately". It was later revised by Karl and Broom in 1995 as "a mother's ability to recognize infant cues consistently and act on those cues, and the ability to monitor and accurately interpret infant cues, as evidenced by mother–child interactions that are contingent, reciprocal and affectively positive". It can be generally defined as a broad concept combining a variety of behavioral care giving attributes.

The research on maternal sensitivity follows earlier work in psychoanalytics and is especially rooted in attachment theory. As the focus of psychoanalytics shifted from individuals (particularly adults) to children, research studies on mother–infant dyads, on the effects of early childhood on development, and on pregnancy became wider. A psychologist named John Bowlby eventually developed the attachment theory in 1969. Mary Ainsworth, who worked with Bowlby, along with her colleagues created the concept of maternal sensitivity in 1978 in order to describe early mother–infant interaction observed in her empirical studies.

There are four important aspects of maternal sensitivity: dynamic process involving maternal abilities, reciprocal give-and-take with the infant, contingency on the infant's behavior, and quality of maternal behaviors.

Maternal sensitivity is dynamic, elastic and can change over time. A sensitive mother needs to be able to perceive the cues and signals her baby gives her, interpret them correctly and act appropriately. The three most positive affecting factors for the baby are a mother's social support, maternal–fetal attachment and high self-esteem. The three most negative affecting factors are maternal depression, maternal stress, and maternal anxiety. Recent studies have shown that maternal posttraumatic stress disorder (PTSD) can negatively impact a mother's sensitivity during stressful moments with her child that serve as traumatic reminders and that this quite likely has a neural basis in the maternal brain.

Maternal sensitivity is most commonly assessed during naturalistic observation of free play interactions between mother and child. There are several factors surrounding assessment during observation that may cause differences in results, including the setting (home vs laboratory), the context (free play vs structured task), the length of observation and the frequency of observation. While some observational studies focus strictly on the relationship between mother and child during close interaction such as feeding or free play, other studies look into how well the maternal figure divides her attention between the baby and other everyday activities. The latter was demonstrated in an experiment conducted by Atkinson et al. where mothers were given a questionnaire to act as a "distractor task", and were assessed on their ability to effectively divide their attention between the "distractor task" and their child. In regards to length of observation, some studies require no more than a one-time 10-minute assessment, while other studies used a much lengthier time.

The Strange Situation was developed by Mary Ainsworth in the 1970s to assess attachment relationships between caregivers and children between 9 and 18 months old. Because maternal sensitivity is an indicator of attachment relationship, researchers sometimes use the Strange Situation to observe attachment so that they may use the results to predict and infer the level of maternal sensitivity.

In the Strange Situation, the toddler's behavior and stress is observed during a 21-minute free-play session through a one-way glass window as the caregiver and strangers come into and leave the room. The specific sequence of events is as follows:


The children are observed and categorized into one of the four attachment patterns – secure attachment, anxious-ambivalent attachment, anxious-avoidant attachment, or disorganized attachment – based on the infant's separation anxiety, willingness to explore, stranger anxiety, and reunion behavior.

Two related qualitative concepts that are correlated with maternal sensitivity are "mother–infant synchrony" and "maternal mind-mindedness".

In mother–infant synchrony, the mother and infant's ability to change their own behaviour based on the other's response is taken into consideration. Infant affect (vocal and facial) and maternal stimulation (vocal and tactile) are good indicators of mother–infant synchrony. Zentall et al. found that infants' rhythm was stronger and interactions were led better at 5 months than at 3 months. According to the study, an infant's ability to send signals and a mother's ability to perceive them increase with synchrony over time. Studies have shown that mother–infant synchrony will result in the infant's development of self-control and other self-regulating behaviours later on in life.

The related concept of maternal mind-mindedness assesses the mother's ability to understand and verbalize the infant's mind: thoughts, desires, intentions and memories. Maternal mind-mindedness has been found to be related to some developmental results, such as attachment security. A caregiver's comment is deemed an "appropriate" mind-related comment if the comment was deemed to match the infant's behaviour by the independent coder, if the comment associated the infant's current activity to past activities, and/or if the comment encouraged the infant to go on with his or her intentions when the conversation paused. This correlates to high maternal mind-mindedness. If the caregiver assigns the wrong internal state to the baby's behaviour, if the comment about the current activity is not insufficiently associated with a past event, if the comment deters the infant from proceeding with the current activity, and/or if the comment is unclear, it is deemed a "in-appropriate" mind-related comment and correlates to low mind-mindedness.

Infants whose mothers are more sensitive are more likely to display secure attachment relationships. Because the maternal figure is generally accessible and responsive to the infant's needs, the infant is able to form expectations of the mother's behaviour. Once expectations are met and the infant feels a consistency in the mother's sensitivity, the infant is able to find security in the maternal figure. Those infants whose mothers do not respond to the signals from their children or respond inappropriately to their children's cries for attention will form insecure and anxious attachments because the infants are unable to consistently depend on the maternal figures for predictable and safe responses.

In order for the infant to feel that the maternal figure is accessible and responsive, a certain amount of interaction must occur. Though the most research has been done on face-to-face interaction, studies have found that bodily interaction is also important in sensitivity and development. It is not how often the baby is held that reflects attachment, but "how" the baby is held and whether or not the baby desires to be held that matters in attachment development. Another factor that is important is sensitivity to the infant's feeding signals. There lies some controversy in whether infants who form insecure attachment relationships with their mothers do so because the mother is particularly insensitive to her child's needs or because of differences in their personality (i.e. their temperament) and due to life situations.

There is a crucial interplay between parenting and child characteristics such as health, temperament, development and cognition. The children with the most sensitive, consistent mothers are the ones who are generally most healthy, happy and well adapted.

Maternal sensitivity even in the first few months of mother–child relationships are an important factor to health in childhood, especially with obesity. A study using data from the National Institute of Child Health and Human Development's Study of Early Child Care and Youth Development assessed mother–child interactions and categorized them in one of two groups: sensitive or insensitive. Their child's growth (height and weight) was monitored throughout their childhood, from 24 months all the way to grade six, and body mass index was calculated. As the children grew, the percentage of overweight or obese grew too. From 24 months the overall overweight-obese percentage was 15.58% and by grade six, 34.34% of the children were classified as overweight or obese. More interesting is the difference between the maternal sensitive group and the maternal insensitive group. The children with the sensitive mothers started out with an overweight-obese percentage of 14.96% (24 months) and ended the research with 29.54% (grade six). The children classified with insensitive mothers had an overweight-obese percentage of 16.16% at 24 months and 39.28% at grade six. This shows a significant correlation between the mother's sensitivity and the child's risk for overweight-obesity during their elementary years. This is very important for obesity prevention programs for children.

Current studies have shown a correlation between maternal sensitivity or insensitivity, negative discipline and childhood aggression. An experiment sampling 117 mother–child pairs showed a unique relationship between the mother's sensitivity and the use of discipline and the child's temperament level. Observations (of the mother's sensitivity to the child's needs, the child's aggression and temperament level and the relationship between the two) were made when the children on average were 26.71 months old (range of 13.58 to 41.91 months). The data were collected again a year later. Results show a year later that negative discipline is correlated with child aggression, but only when that mother is insensitive.

A study by Jay Belsky and R.M. Pasco Fearon tested the correlation between childhood development and the sensitivity of the mother. The hypotheses were:
The children were tested in five developmental categories: problem behavior, social competence, expressive
language, receptive language and school readiness. Results highly support the hypothesis (i.e. maternal sensitivity and childhood development are positively correlated.) This is an important issue as it shows how influential the early experience of a child affects their future development.

Mothers who were found to display higher sensitivity towards their children from preschool to first grade were found to have higher achieving children than those who displayed lower maternal sensitivity. The children of maternally sensitive mothers scored higher in math and phoneme knowledge than those who had a history of lower maternal sensitivity.

Maternal sensitivity has been shown to teach infants attentional skills, which are necessary later in life for emotional control, and other more complex cognitive processes.

In families with more than one child (twins or triplets), it has been found that maternal sensitivity is lower, as there are more needs to be taken care of by the mother and less time to form a unique bond, which in turn results in decreased cognitive development in the infants (relative to if the child were raised alone). Furthermore, in the newborn period, women who displayed high maternal sensitivity had children who were able to regulate their emotions and who had higher symbolic and cognitive skills. In the case of the triplets, the child that received the least maternal sensitivity was the one that showed the poorest outcomes cognitively and had the most medical problems.

Maternal sensitivity has been shown to have an effect on children's socialization skills. In particular, some research suggests that children of more sensitive caregivers have high levels of effortful (i.e. emotional and behavioural) control. Such control is proposed to have been fostered from the infancy stage when the a sensitive mother's quick and appropriate responses to the baby's distress teaches the baby to adjust his/her arousal. This speedy regulation of arousal is then adapted into childhood resulting in the ability to regulate emotion and behaviour well.

Caregiver sensitivity has also been found to have a connection with empathy in children. Generally, securely attached children have been found to be more empathetic compared with insecurely attached children. The reasoning suggested for this result is that because securely attached children receive more empathy from caregivers during times that they themselves are distressed, they are more likely to show empathy in a situation where someone else is distressed.

Adults' own understanding of maternal sensitivity affects their sensitivity towards their own children. Adults who had insensitive mothers during infancy were found to not be able to remember specific childhood events or their importance. They were not able to present an accurate description of their parents by use of memories, they were found to idealize experiences and are more likely to remember situations in which they were rejected. Adults who experienced higher maternal sensitivity during both infancy and adulthood were found to be less dismissive and more secure than those who did not. Adults who are preoccupied were found to also try to please their parents as they were young, and have a sense of anger towards them. About half of the adults who were found more preoccupied than others were found to have experienced divorce between their parents earlier in life, as well as other negative life events such as death of a parent or sexual abuse. These life events cause the security of attachment between mother and child to decrease as the mother's availability, as well as responsiveness may decrease, no matter the maternal sensitivity experienced prior to these events. Male adults were found to have experienced less maternal sensitivity earlier in life than females and were more likely to be classified as dismissive than females were.

Maternal sensitivity has been found to be greater for adult mothers than for adolescent mothers. The level and quality of mind-mindedness, which refers to how prone the mother is to comment about the infant's mental activity during interaction, is higher in adult mothers, and has been related to greater maternal sensitivity. The comments made by adult mothers were found to be more positive than those made by adolescent mothers. Adolescent mothers used almost no positive comments, but instead negative comments. This causes the adolescent mother to be more insensitive to their baby's needs, possibly because of lack of need understanding, and therefore have lower maternal sensitivity and a less secure attachment to their infants.

Maternal sensitivity in adolescent mothers can be predicted prenatally. Mothers who talked lively and positively about their future relationship with the child were found to display higher maternal sensitivity than those who did not (classified as autonomous mothers). Autonomous mothers were also found to have infants with a more secure attachment. Adolescent mothers who were not classified as autonomous were found to have anxiously attached infants. Furthermore, adolescent mothers were found to have children four–eight years old with lower IQs and a below-average reading level, than did adult mothers.

Although adolescent mothers have been found to display lower maternal sensitivity, there is no evidence that maternal age itself has a negative effect on child development, as other factors at that age such as education and financial status may play a role in the insensitivity of the mother towards the child as well.

Mary Ainsworth developed Ainsworth's Maternal Sensitivity Scale (AMSS) to use as a measure in her Baltimore longitudinal study (1963). The scale is based on naturalistic observations completed by Ainsworth over a period of several hours and thus has no short procedure outline. Her method uses a nine-point scale (nine being very high and one being very low) in a number of important maternal traits. In order for this measurement to be accurate, it is essential that the researcher has developed good observations and insight into the behaviour of the caregiver.


Maternal Behaviour Q-sort (MBQS) was developed by David Pederson, Greg Moran and Sandi Bento to measure maternal sensitivity. It has been used to measure a variety of studies including home based and video-recorded observations. The measures are defined using q-factor analyses. The standard version of the Q-sort consists of 90 items that measure maternal sensitivity with regards to accessibility, responsiveness and promptness to the child's needs and there are many variations. In order to measure sensitivity, observers sort the items into nine piles of ten based on correspondence between the observed behaviour and the item. The maternal sensitivity score is calculated by comparing the descriptive sort and the criterion sort (prototypical sensitive mother). Pederson and Moran based their Q-sort on the Waters Attachment Q-Set, which is an assessment of the behavior of children.

The Pederson and Moran Sensitivity Q-Sort was developed by Pederson D.R., Moran G., Sitko C., Campbell K., and Ghesquire K. in 1990. Similar to Ainsworth's Maternal Sensitivity Scales, the Pederson and Moran Sensitivity Q-Sort was designed to detect changes in maternal sensitivity with relation to infant behaviour.

The Atypical Maternal Behavior Instrument for Assessment and Classification (AMBIANCE) scale was developed by Elisa Bronfman, Elizabeth Parsons and Karlen Lyons-Ruth. It was developed to measure the extent to which the parent failed to follow into the intentional or affective direction of the baby’s communications by engaging in contradictory responses to infant cues or failing to respond to infant cues altogether. AMBIANCE has the following five dimensions:


</doc>
<doc id="26977104" url="https://en.wikipedia.org/wiki?curid=26977104" title="Expressions of dominance">
Expressions of dominance

Power and dominance-submission are two key dimensions of relationships, especially close relationships in which parties rely on one another to achieve their goals and as such it is important to be able to identify indicators of dominance.

Power is the ability to influence behavior and may not be fully assessable until it is challenged with equal force. Unlike power, which can be latent, dominance is a manifest condition characterized by individual, situational and relationship patterns in which attempts to control another party or parties may or may not be accepted. Moskowitz, Suh, and Desaulniers (1994) describe two similar ways that people can relate to society as parties to interpersonal relationships: agency and communion. Agency includes status and is on a continuum from assertiveness-dominance to passive-submissiveness; it can be measured by subtracting submissiveness from dominance. Communion includes love and falls on a continuum from warm-agreeable to cold-hostile-quarrelsome. Those with the greatest and least power typically do not assert dominance while those with more equal relationships make more control attempts.

Power and dominance are closely related concepts that greatly impact relationships. In order to understand how dominance captures relationships one must understand the influence of gender and social roles while watching for verbal and nonverbal indicators of dominance.

Verbal indicators influence perceptions of dominance. To date, dominance has been linked to vocal control (Lamb, 1981, as cited in Dunbar and Burgoon, 2005), loudness as measured by amplitude (Burgoon and Hoobler, 2002, as cited in Dunbar and Burgoon, 2005; and Dillard, 2000), pitch as measured by frequency (Burgoon and Hoobler, 2002, as cited in Dunbar and Burgoon, 2005; Dillard, 2000), interruptions (Karawosky "et al."., as cited in Youngquist, 2009; Karakowsky, McBey, and Miller, as cited in Youngquist, 2009), disfluencies (Dunbar and Burgoon, 2005), amount of talk time (Burgoon and Hoobler, 2002, as cited in Dunbar and Burgoon, 2005) speech rate or the number of words used in an encounter, and message length (Dunbar and Burgoon, 2005; and Dillard, 2000). An important factor for humans and animals to detect in order to survive is the idea of involvement which can be indicated through change and intensity (Tusing and Dillard, 2000). Vocal characteristics such as amplitude and frequency variation indicate change while speech rate can indicate intensity (Tusing and Dillard, 2000). Those with a high speech rate talk faster and as such are usually perceived as more dominant (Aronvitch; Buller and Aune; Buller and Burgoon; Harrigan "et al.".; Scherer "et al."., as cited in Tusing and Dillard, 2000). Interruptions, vocal control, loudness, pitch, verbosity, speech rate and message length were found to predict perceptions of dominance.

In general, interruptions and perceptions of dominance follows a curvilinear relationship (Dunbar and Burgoon, 2005; Youngquist, 2009). Also, when asked to think of typical behaviors of powerful individuals, Carney, Hall, and LeBeau (2005) found that those that were thought to hold more power were also perceived to have more successful interruptions as well as fewer disfluencies. As promised earlier, gender differences exist within interruptions too. Youngquist (2009) chose to look at how dominance as indicated by intrusive interruptions is perceived differently depending on the gender composition of dyads. This was done by asking 378 individuals to listen to one of 4 recordings with 3 subsections, each subsection contained 2 interruptions. The recording was paused after each subsection and assessments were made about dominance. In addition, the conversations varied by gender composition, male/male, female/male, etc. with the same actor making a total of six interruptions within one recording. His findings show that overall, female interrupters in the same sex dyad are perceived as most dominant while male interrupters in a cross sex dyad are perceived as least dominant. This is in contrast to Dunbar and Burgoon's (2005) finding that men overall are perceived as the most dominant with increased interruptions. Youngquist (2009) additionally finds that females in the same sex groups, who interrupt, are perceived as more dominant than males in the same sex group. Though an interrupter in the same sex group was seen more dominant than the male in the cross sex group, it was only for the first two interruptions. Also, for the first section of the survey (with two interruptions, in the same conversation) but not the second or third sections it was found that the female interrupter, compared to a male interrupter, was seen as more dominant in a cross sex dyad. 
Vocal control, loudness, and pitch also have been found to be associated with dominance. Dunbar and Burgoon (2005) had partners and third party observers rate dominance after participating or observing an interaction. Couples were initially separated and asked to write a list of items they would like to spend a hypothetical gift of money on. The couple was then reunited and then asked to jointly decide the top things their money would be spent on. Participants rated their partners' dominance after the interaction while third party observers rated their perceptions during the interaction. They found that observers rated males and females more dominant when they expressed higher vocal control (.76, .70) respectively and only male partners perceived their partner to be more dominant when she had greater vocal control (.23). In an additional experiment, Tusling (2000) used 760 participants and divided them into three groups. Group one watched and listened to a video across various influence goals, while group two was given a transcript of the messages and group three watched the video without sounds. Each group gauged dominance levels using a Likert scale from 1-5. He found that amplitude, a measure of loudness, and amplitude variation, an indicator of change dictated perceptions of dominance. It was also found that frequency, a measure of pitch, and frequency variations were reliable predictors of dominance. Verbosity, speech rate, and message length were all found to be reliable predictors of dominance. The increased amount of words used in an interaction or verbosity was linked to more dominant perceptions by observers for males (.53) and for females (.46) by observers, though only females perceived their partner as more dominant with increased verbosity (.21) (Dunbar and Burgoon, 2005). Tussling and Dillard (2009) found that slower speech rates were found to predict increased dominance perceptions. Shorter messages were also found to predict dominance.

Nonverbal communication indicators are most readily located on the face such as Visual Dominance Ratio and indicators expressed through the hands such as adaptor and illustrator gestures have been linked to dominance. An individual's body can indicate dominance as well through posture, elevation, relaxation and body lean.

Nonverbal behavioral indicators can be seen in the face through factors like expressiveness, visual dominance ratio, gaze, and emotions, and through the body through body control, posture, lean, openness and gestures. Facial indicators such as expressiveness, visual dominance ratio, and gaze, and as well were all found to relate to dominance. In terms of expressiveness, males, but not females, were rated as more dominant when they were facially expressive (.26, -.36) respectively (Dunbar and Burgoon, 2005). In addition, Carney, Hall, and LeBeau (2005) found that more facial expressiveness was appropriate for those with more power and that these individuals were also more likely to have self-assured expressions. The eyes also have something to offer in terms of dominance. Dunbar and Burgoon (2005) found that higher visual dominance ratios were correlated with higher perceived dominance for males and females (.37, .28) respectively as rated by observers. Also, Carney, Hall, and LeBeau (2005) found that more glaring, more mutual gaze, longer gazing, and more looking while speaking would be more appropriate coming from an individual with more power.

Emotions are readily expressed by individuals making it easier to identify expressions of dominance. Hareli and Shomrat (2009) looked at various approach, neutral, and avoidance emotions. They ran two studies in order to understand perceptions of emotions as they related to dominance. Both studies asked participants to gauge levels of dominance. In study one, 208 individuals rated pictures of men and women with different emotions expressed. In study two, 96 individuals watched a male technician fail at his job and then explain himself showing a neutral, angry or shameful expression. Emotions surveyed included approach emotions such as anger and happiness, neutral emotions and inhibitive or avoidance emotions such as shame, fear or sadness. Approach emotions are rated as the most dominant when compared to inhibitory emotions (Carney, Hall, and LeBeau, 2005; Hareli and Shomrat, 2009; Montepare and Dobish, 2003.) In contrast, Montepare and Dobish (2003) found that happiness was perceived as more dominant than anger, while Hareli and Shomrat (2009) found the opposite. Females were perceived as more dominant than males when expressing happiness and males were perceived as slightly more dominant than females when expressing anger (Hareli and Shomrat, 2009). Hareli and Shomrat (2009) also found interesting results as it relates to neutral expressions. For instance, males were seen as significantly more dominant than females when expressing neutral expressions and neutral expressions were seen about as dominant as angry expressions for men, which is more dominant than inhibitory emotions. Inhibitory or avoidance emotions were seen as the least dominant (Carney, Hall, and LeBeau. 2005; Hareli and Shomrat, 2009; Montepare and Dobish, 2003). Sadness as opposed to fear was seen as the least dominant (Hareli and Shomrat, 2009; Montepare and Dobish, 2003). Females expressing fear or sadness were seen as less dominant than males expressing the same emotion (Carney, Hall, and LeBeau. 2005; Hareli, Shomrat, 2009). Sadness and fear were also seen as more fitting for an individual with lower power (Carney, Hall, and LeBeau, 2005) Hareli and Shomrat (2009) found that shame tended to decrease perceptions of dominance more so than anger increases perceptions of dominance for males. For females anger was perceived as the most dominant emotion followed by happiness, then a neutral expression, then fear and least dominant of all sadness. In comparison, anger was perceived as the most dominant expression for males, closely followed by a neutral expression, then happiness, then fear and least dominant of all sadness.

Furthermore, body control, posture, lean, and openness all were found to relate to dominance. For instance, Dunbar and Burgoon (2005) found that the more body control a woman had the more observers perceived her as dominant (.27) and that in general the most powerful are also the most facially expressive and the least controlled in their body. Carney, Hall, and LeBeau (2005) found high power individuals were perceived to lean forward, have open body positions, orient towards the other, and have an erect body posture more so than those of less power.

In addition, gestures also relate to dominance perceptions. Carney, Hall, and LeBeau (2005) found that high power individuals were more likely to use gestures, initiate more hand shaking and engage in a higher frequency of invasive touch. Dunbar and Burgoon (2005) found that observers rated only males as more dominant with increased use of illustrator gestures. The researchers also found that males perceived their partner to be less dominant when she used more adapter gestures.

In conclusion, one can see how dominance is a complex topic. Dominance relates to both power, status, and affiliation. Dominance is seen through manifest behaviors as indicated through the nonverbal and verbal indicators outlined above. Gender differences also exist within dominance perceptions though it depends on if one's work role or ones gender role is more salient.

Russel (as cited in Dunbar and Burgoon, 2005) stated that "the fundamental concept in social science is power, in the same way that energy is the fundamental concept in physics". It is true power and dominance are essential components in all of the world from cells to plants to reptiles, and humans that all have to fight for resources. As humans it is essential to use one's knowledge to make the world a more harmonious place using tools of assessment in order to understand individual and group behavior. This can be done through contemplating gender, social roles, and looking to verbal and non verbal indicators of dominance and submission to see how we as individuals relate to the world and each other. One can use this knowledge to one's advantage, for instance, if a boss is deciding between two individuals who are of relatively equal credentials. An individual could appear more competent by displaying dominant behaviors in reason which would could indicate confidence and the ability for leadership. A knowledge of dominant and submissive indicators could be used to help others in distress feel more equal in a relationship by monitoring one's own dominance displays and possibly by strategically using submissive displays. Overall, it is essential to understand how dominance is manifested in relationships in order to understand how power and dominance influence us.

Gender variations exist because of differences in our expectations about what is appropriate for a particular gender (sex differences in psychology), what is appropriate depending on the composition of two or more people and whether gender or role norms are most salient. For instance, women who display dominance can be judged differently than men exhibiting the same behavior (Burgoon "et al.".; Carli and Winn, as cited by Youngquist, 2009). This is because women are perceived as less competitive and dominant than men and are thought to be less likely to display dominance (Burgoon "et al.", as cited by Youngquist, 2009); a woman who displays dominance might potentially be perceived as more dominant than a man displaying the same behavior because her behavior will be seen as unusual. Gender composition can influence dominant behaviors differently. For instance, individuals in a same-sex group can be perceived to be of equal status and are expected per norms to play fairly (Orcutt and Harvey, as cited by Youngquist, 2009). Gender differences in behavior are often found in mixed sex groups, though some have found that women can become more assertive with men in mixed group settings (Maccoby, as cited in Moskowitz, Suh, and Desaulniers, 1994).Therefore, dominance is more readily perceived when an individual displays a control act in a same sex group as opposed to a mixed sex group. 

Mixed findings have occurred when one attempts to explain dominance displays by gender or role salience. Moskowitz, Suh, and Desaulniers (1994) believe this is because an individual in a lab has less role salience and more gender salience and therefore is inclined to use more gender stereotypical behaviors in the lab while an individual at work has more role salience and gender differences are thought to be reduced (Johnson, as cited in Moskowitz, Suh, and Desaulniers, 1994). Moskowitz, Suh and Desaulniers (1994) had individuals complete survey forms for 20 days over interactions with individuals at work that lasted over five minutes. Individuals completed an average of four forms a day. The forms were divided out across behavioral indicators to keep participants from selecting the same set of behaviors. The forms had equal amounts of behaviors assessing dominance, submission, agreeableness and combativeness. The researchers found that social roles determined agentic behavior at work, not gender roles. When looking at gender composition and communal behavior it was found that gender role, and not social role influenced communal behaviors. Men were indeed more quarrelsome than women in same sex groups, whereas women were more communal with one another. In addition to gender differences it is important to be able to identify and understand how verbal indicators relate to dominance.




</doc>
<doc id="4845933" url="https://en.wikipedia.org/wiki?curid=4845933" title="Cutting in line">
Cutting in line

Cutting in line, also known as line/queue jumping, butting, barging, budding, skipping, breaking, shorting, pushing in, or cutsies, is the act of entering a queue or line at any position other than the end. The act, which may be taboo in some instances, stands in stark contrast to the normal policy of first come, first served that governs most queue areas.

A negative response from the rear of the line is expected when someone has cut in line up ahead. According to one study, a person cutting in line has a 54% chance that others in the line will object. With two people cutting in line, there is a 91.3% chance that someone will object. The proportion of people objecting from anywhere behind the cutter is 73.3%, with the person immediately behind the point of intrusion objecting most frequently.

Nevertheless, physical altercation resulting from cutting is rare. It was reported that an 18-year-old National Serviceman in Malaysia was bludgeoned to death after he attempted to jump the queue at a food counter. Another notable incident occurred in New York City at The Halal Guys food cart, resulting in the death of the man who cut in line. The man who killed him was found not guilty by reason of self-defense.

Cutting is also present on roadways, especially restricted access highways, where traffic queues build up at merge locations. Drivers who bypass traffic by waiting until the last possible moment before merging are sometimes considered to be "cutters," and are frequent instigators of road rage. This behavior is not usually illegal in the US, unless the driver crosses a solid white line or uses dangerous merging techniques. In Germany, Belgium and Austria, using the merging lane until the last moment is required by law as doing so is safer because of more uniform speeds of merging cars and reduces the length of backed up cars. (However, it does not affect the speed through the bottleneck.)

In some instances cutting in line is sanctioned by the authority overseeing the queue. For example amusement park operators such as Cedar Fair (Fast Lane), Six Flags (Flash Pass), and Walt Disney (FastPass) have Virtual queue programs whereby a limited number of patrons cut the line for an attraction by arriving at a pre-designated time (sometimes, but not always, associated with a payment for the privilege). Common penalties for cutting the line without this privilege range from being forced to the back of the line to ejection from the park without a refund. 

At airports, it is customary – for the sake of efficiency – to allow pregnant women, adults accompanying small children, the elderly and the physically disabled to board an airplane first, regardless of their seat, class or assignment. However, the priority afforded wheelchair-using passengers has reportedly given rise to a practice in the United States, whereby some passengers who do not normally use a wheelchair request one, to pass through security checks quickly and to be among the first to board an aircraft. At the conclusion of the flight, these passengers walk off the aircraft, instead of waiting for a wheelchair and thus being among the last to disembark. The neologism "miracle flight" has been coined to describe this behavior, as passengers apparently needing a wheelchair before boarding the aircraft are "miraculously" able to walk afterwards.

In former Communist countries, where waiting in long queues was a near-daily occurrence for some, especially at times of rationing, the act of waiting in line and the code of conduct associated with it is much more institutionalized and regimented to this day ("See" Consumer supply in the Soviet Union in the 1980s). In Russia, for example, the art of queuing is finely-honed: it is acceptable for a person to leave the queue to use the bathroom (or similar brief diversion) and then return to their original place without having to ask permission. It is also common for a person to be allowed to jump to the front of the queue in special cases, like the need to purchase a ticket for an imminently departing train. This can also be seen in Cuba, including notably at the Coppelia ice cream stores, and in Spain where an arriving patron asks "¿Quién es el último?" (Who is last?) and is then behind that person in the queue, which is not always a physical line, but may be merely a jumble of people with the same objective.

Legislators in the US state of Washington passed a bill that makes cutting in line to catch a ferry illegal. Cutters can be fined $101 and forced to return to the end of the line.




</doc>
<doc id="691626" url="https://en.wikipedia.org/wiki?curid=691626" title="Irrationality">
Irrationality

Irrationality is cognition, thinking, talking, or acting without inclusion of rationality. It is more specifically described as an action or opinion given through inadequate use of reason, or through emotional distress or cognitive deficiency. The term is used, usually pejoratively, to describe thinking and actions that are, or appear to be, less useful, or more illogical than other more rational alternatives.

Irrational behaviors of individuals include taking offense or becoming angry about a situation that has not yet occurred, expressing emotions exaggeratedly (such as crying hysterically), maintaining unrealistic expectations, engaging in irresponsible conduct such as problem intoxication, disorganization, and falling victim to confidence tricks. People with a mental illness like schizophrenia may exhibit irrational paranoia.

These more contemporary normative conceptions of what constitutes a manifestation of irrationality are difficult to demonstrate empirically because it is not clear by whose standards we are to judge the behavior rational or irrational.

The study of irrational behavior is of interest in fields such as psychology, cognitive science, economics, game theory, and evolutionary psychology, as well as of practical interest to the practitioners of advertising and propaganda.

Theories of irrational behavior include:

Factors that affect rational behavior include:

Irrationality is not always viewed as a negative. Dada Surrealist art movements embraced irrationality as a means to "reject reason and logic". André Breton, for example, argued for a rejection of pure logic and reason which are seen as responsible for many contemporary social problems.

In science fiction literature, the progress of pure rationality is viewed as a quality which may lead civilization ultimately toward a scientific future dependent on technology. Irrationality in this case, is a positive factor which helps to balance excessive reason.

In psychology, excessive rationality without creativity may be viewed as a form of self-control and protection. Certain problems, such as death and loss, may have no rational solution when they are being experienced. We may seek logical explanations for such events, when in fact the proper emotional response is grief. Irrationality is thus a means of freeing the mind toward purely imaginative solutions, to break out of historic patterns of dependence into new patterns that allow one to move on.

"Irrationalist" is a wide term. It may be applied to mean "one without rationality", for their beliefs or ideas. Or, more precisely, it may mean someone who openly rejects some aspect of rationalism, variously defined. It can be seen as either a negative quality, used pejoratively, or a positive quality: For example, religious faith may variably be seen by some as a virtue which doesn't need to be rational (see fideism), while others (even of the same religious tradition) may view their faiths as being rational, favoring rationalism.

Also, it might be considered irrationalist to gamble or buy a lottery ticket, on the basis that the expected value is negative.

Irrational thought was seen in Europe as part of the reaction against Continental rationalism. For example, Johann Georg Hamann is sometimes classified as an irrationalist.

Ancient Greek philosophy established a fundamental differentiation between logical "true" assumptions of the universe and irrational "false" statements or mere opinions based on emotion or sensorial experience. The German cultural historian Silvio Vietta has shown that Greek philosophy thus founded a dual cultural system based on rationality as the domain of philosophy and science versus "irrational" emotion and sensuality as domains of literature and art. Since the irrational emotions as stirred up in literature threaten the rationality of human beings, in "The Republic" Plato expelled poets from the state.

In the later history of philosophy this opposition of rationality and the irrational was renewed as a methodological differentiation by Descartes, but reversed by Pascal in his statement: “Le coeur a ses raisons, que la raison ne connait point” (“The heart has its reasons which reason does not know”). Pascal thus asserted a specific rationality of the "irrational" emotions. The philosophy of sensualism (John Locke, among others) underlined the importance of the senses as the source of human perception and cognition.

The 19th-century German philosopher Julius Bahnsen asserted that all thought processes, desires and actions ultimately led to irresolvable contradictions which stem from the inherent irrationality of being. Years earlier, Friedrich Wilhelm Joseph Schelling had theorized that despite some traces of rationality in the world, the "dark ground" of being itself rested in an irrational will that could not be explained, only described in an apophatic manner. Arthur Schopenhauer picked up on this idea and completely fleshed out the concept of an irrational will as a cause of existence, by founding his entire metaphysics and explaining the variety of physical phenomena precisely with this underlying, unconscious and dynamic notion of will.

Søren Kierkegaard gave some remit to irrationality in his "Concluding Scientific Postscript to the Philosophical Fragments", where he claimed that 'Subjectivity is Truth'. Rather than allowing reason to do our choosing for us, Kierkegaard argued that irrational leaps of faith could be more useful, as they were more authentic (although, he never used the word 'authentic'), and thus gave more meaning to life. Objectivity, like reason, was opposed to subjectivity, and thus could not be said to give any meaning to anyone's life. Although he never dismissed rationality in its entirety, Kierkegaard argued that we could not allow rationality to make our decisions for us. In this, and to some degree, he offers a vindication of irrationality.

Much subject matter in literature can be seen as an expression of human longing for the irrational. The Romantics valued irrationality over what they perceived as the sterile, calculating and emotionless philosophy which they thought to have been brought about by the Age of Enlightenment and the Industrial Revolution.
The Dadaists and Surrealists later used irrationality as a basis for their art. The disregard of reason and preference for dream states in Surrealism was an exaltation of the irrational and the rejection of logic.

Mythology nearly always incorporates elements of fantasy and the supernatural; however myths are largely accepted by the societies that create them, and only come to be seen as irrational through the spyglass of time and by other cultures. But though mythology serves as a way to rationalize the universe in symbolic and often anthropomorphic ways, a pre-rational and irrational way of thinking can be seen as tacitly valued in mythology's supremacy of the imagination, where rationality as a philosophical method has not been developed.

On the other side the irrational is often depicted from a rational point of view in all types of literature, provoking amusement, contempt, disgust, hatred, awe, and many other reactions.

The term "irrational" is often used in psychotherapy and the concept of irrationality is especially known in rational emotive behavior therapy originated and developed by American psychologist Albert Ellis. In this approach, the term "irrational" is used in a slightly different way than in general. Here irrationality is defined as the tendency and leaning that humans have to act, emote and think in ways that are inflexible, unrealistic, absolutist and most importantly self-defeating and socially defeating and destructive.

One psychotherapist describes the overlapping of irrationality and psychotherapy: I didn't understand enough about them [patients] or how they thought even to begin to reach them. Listening to their stories, I wanted to offer advice. Why don't you escape from such a relationship? Leave your home, don't submit! Seek out others, expect more for yourself, I wanted to say. But I came to realize that they could not really hear me. They heard my words, perhaps even agreed with my recommendations. They had brain compartments to which new information, my suggestions for example, had easy access. But habits, learned emotional responses, and remembered expectations were buried deep in their brains that dictated the course of their lives. These patients, like victims of encephalitis, could not be awakened.





</doc>
<doc id="1277702" url="https://en.wikipedia.org/wiki?curid=1277702" title="Herd mentality">
Herd mentality

Herd mentality, mob mentality and pack mentality, also lesser known as gang mentality, describes how people can be influenced by their peers to adopt certain behaviors on a largely emotional, rather than rational, basis. When individuals are affected by mob mentality, they may make different decisions than they would have individually.

Social psychologists study the related topics of group intelligence, crowd wisdom, groupthink, deindividuation, and decentralized decision making.

The idea of a "group mind" or "mob behavior" was first put forward by 19th-century French social psychologists Gabriel Tarde and Gustave Le Bon. Herd behavior in human societies has also been studied by Sigmund Freud and Wilfred Trotter, whose book "Instincts of the Herd in Peace and War" is a classic in the field of social psychology. Sociologist and economist Thorstein Veblen's "The Theory of the Leisure Class" illustrates how individuals imitate other group members of higher social status in their consumer behavior. More recently, Malcolm Gladwell in "The Tipping Point", examines how cultural, social, and economic factors converge to create trends in consumer behavior. In 2004, the "New Yorker" financial columnist James Suroweicki published "The Wisdom of Crowds".
Twenty-first-century academic fields such as marketing and behavioral finance attempt to identify and predict the rational and irrational behavior of investors. (See the work of Daniel Kahneman, Robert Shiller, Vernon L. Smith, and Amos Tversky.) Driven by emotional reactions such as greed and fear, investors can be seen to join in frantic purchasing and sales of stocks, creating bubbles and crashes. As a result, herd behavior is closely studied by behavioral finance experts in order to help predict future economic crises.

The Asch conformity experiments (1951) involved a series of studies directed by American Psychologist Solomon Asch that measured the effects of majority group belief and opinion on individuals. 50 male students from Swarthmore College participated in a vision test with a line judgement task. A naive participant was put in a room with seven confederates (i.e. actors) who had agreed in advance to match their responses. The participant was not aware of this, and was told the actors were also naive participants. There was one control condition with no confederates. Confederates purposefully gave the wrong answer on 12 trials. Through 18 trials total, Asch (1951) found that one-third (32%) of naive participants conformed with the clearly incorrect majority, with 75% of participants over the 12 trials. Less than 1% of participants gave the wrong answer when there were no confederates.

Researchers at Leeds University performed a group experiment where volunteers were told to randomly walk around a large hall without talking to each other. A select few were then given more detailed instructions on where to walk. The scientists discovered that people end up blindly following one or two instructed people who appear to know where they’re going. The results of this experiments showed that it only takes 5% of confident looking and instructed people to influence the direction of the 95% of people in the crowd and the 200 volunteers did this without even realizing it.

Researchers from Hebrew University, NYU, and MIT explored herd mentality in online spaces, specifically in the context of "digitized, aggregated opinions." Online comments were given an initial positive or negative vote (up or down) on an undisclosed website over five months. The control group comments were left alone. The researchers found that "the first person reading the comment was 32 percent more likely to give it an up vote if it had been already given a fake positive score." Over the five months, comments artificially rated positively showed a 25% higher average score than the control group, with the initial negative vote ending up with no statistical significance in comparison to the control group. The researchers found that "prior ratings created significant bias in individual rating behavior, and positive and negative social influences created asymmetric herding effects." 

“That is a significant change,” Dr. Aral, one of the researchers involved in the experiment, stated. “We saw how these very small signals of social influence snowballed into behaviors like herding.”




</doc>
<doc id="16592741" url="https://en.wikipedia.org/wiki?curid=16592741" title="Stonewalling">
Stonewalling

Stonewalling is a refusal to communicate or cooperate. Such behaviour occurs in situations such as marriage guidance counseling, diplomatic negotiations, politics and legal cases. Body language may indicate and reinforce this by avoiding contact and engagement with the other party. People use deflection in a conversation in order to render a conversation pointless and insignificant. Tactics in stonewalling include giving sparse, vague responses, refusing to answer questions, or responding to questions with additional questions. In most cases, stonewalling is used to create a delay, rather than to put the conversation off forever.

In politics, stonewalling is used to refuse to answer or comment on certain questions about policy and issues, especially if the committee or politician in question is under investigation. Stonewalling in politics and in the world of business can sometimes create a critical advantage. William Safire wrote that "stonewalling" was originally used in Australian cricket, but its use during president Richard Nixon's Watergate affair brought it into usage in American politics as a "refusal to comment".
Stonewalling can also be seen as filibustering, or delaying or stalling the passage of bills until they become outdated or changed when engaging in parliamentary procedures.

When one or both members of a couple refuse to communicate, this can mark the final step in the breakdown of their relationship. John Gottman characterised this stage as the fourth horseman of the Apocalypse in his cascade model of divorce prediction. In his studies, "stonewalling" was overwhelmingly done by men, with women overwhelmingly using "criticism". In his studies, men's physiology reached a state of arousal prior to them doing "stonewalling", while the female partner showed a physiological reaction of increased heart rate after her partner had "stonewalled" her. 

As stonewalling perpetuates in a relationship and becomes a continuous cycle or the negative effects of stonewalling outweigh the positive effects, stonewalling becomes the greatest predictor of divorce in a marriage. When one or both partners in a relationship stonewall, their ability to hear each other or listen to each other's disagreement, concern, side or argument, reduces their ability to engage and help address the situation. Stonewalling can be detrimental to relationships because there is often no chance for resolution of conflict.

When stonewalling occurs, it has both a physiological and psychological effect on the person who is stonewalling. Physiologically, the person who is stonewalling can shut completely down, particularly when people stonewall as a self-soothing mechanism. The person doing stonewalling may be aware or unaware that this is taking place, because of an increase in adrenaline due to an increase in stress, where the person can either engage or flee the situation. Because stonewalling is a physiological reaction, the stonewalling can be thought of as a fight or flight response. Psychologically, stonewalling is a defense mechanism used to preserve one's self and emotions. 

Other signs of stonewalling are silence, mumbling monotone utterances, changing the subject and physically removing oneself from the situation (e.g., leaving the room).

Witnesses in court or other legal actions may refuse to cooperate with a counsel by not volunteering information and refusing to testify. Prosecutors may try to break their united front by offering incentives such as immunity from prosecution. Another tactic of stonewalling is providing the jurors with misleading information or purposefully withholding certain pieces of information that can be self incriminating. When witnesses practice the stonewalling practice they are usually in an agreement with other witnesses to do the same in order for the tactic to be effective.


</doc>
<doc id="229719" url="https://en.wikipedia.org/wiki?curid=229719" title="Popularity">
Popularity

In sociology, the popularity of a person, idea, place, item or other concept can be defined in terms of liking, attraction, dominance and superiority. With respect to interpersonal popularity, there are two primary divisions: perceived and sociometric.

According to psychologist Tessa Lansu at the Radboud University Nijmegen, "Popularity [has] to do with being the middle point of a group and having influence on it."

The term "Popularity" is borrowed from the Latin term "popularis", which originally meant "common." The current definition of the word popular, the "fact or condition of being well liked by the people", was first seen in 1601.

While popularity is a trait often ascribed to an individual, it is an inherently social phenomenon and thus can only be understood in the context of groups of people. Popularity is a collective perception, and individuals report the consensus of a group's feelings towards an individual or object when rating popularity. It takes a group of people to like something, so the more that people advocate for something or claim that someone is best liked, the more attention it will get, and the more popular it will be deemed.

Notwithstanding the above, popularity as a concept can be applied, assigned, or directed towards objects such as songs, movies, websites, activities, soaps, foods etc. Together, these objects collectively make up popular culture, or the consensus of mainstream preferences in society. In essence, anything, human or non-human, can be deemed popular.

For many years, popularity research focused on a definition of popularity that was based on being "well liked." Eventually, it was discovered that those who are "perceived" as popular are not necessarily the most well liked as originally assumed. When students are given the opportunity to freely elect those they like most and those they perceive as popular, a discrepancy often emerges. This is evidence that there are two main forms of personal popularity that social psychology recognizes, sociometric popularity and perceived popularity.

Sociometric popularity can be defined by how liked an individual is. This "liking" is correlated with prosocial behaviours. Those who act in prosocial ways are likely to be deemed sociometrically popular. Often they are known for their interpersonal abilities, their empathy for others, and their willingness to cooperate non-aggressively. This is a more private judgement, characterized by likability, that will not generally be shared in a group setting. Often, it is impossible to know whom individuals find popular on this scale unless confidentiality is ensured.

Perceived popularity is used to describe those individuals who are known among their peers as being popular. Unlike sociometric popularity, perceived popularity is often associated with aggression and dominance and is not dependent on prosocial behaviors. This form of popularity is often explored by the popular media. Notable works dealing with perceived popularity include "Mean Girls", "Odd Girl Out", and "Ferris Bueller's Day Off". Individuals who have perceived popularity are often highly socially visible and frequently emulated but rarely liked. Since perceived popularity is a measure of visible reputation and emulation, this form of popularity is most openly discussed, agreed upon within a group, and what most people refer to when they call someone popular.

To date, only one comprehensive theory of interpersonal popularity has been proposed: that of A. L. Freedman in the book "Popularity Explained". The 3 Factor Model proposed attempts to reconcile the two concepts of sociometric and perceived popularity by combining them orthogonally and providing distinct definitions for each. In doing so, it reconciles the counter intuitive fact that liking does not guarantee perceived popularity nor does perceived popularity guarantee being well liked.

"Popularity Explained" was first published as a blog before being converted to a book and various versions have been available online since 2013.

There are four primary concepts that "Popularity Explained" relies on.


According to Freedman, an individual's place in the social landscape is determined by a combination of three factors: "what" they are; "who" they are; and the situation.


One of the most widely agreed upon theories about what leads to an increased level of popularity for an individual is the perceived value which that individual brings to the group. This seems to be true for members of all groups, but is especially demonstrable in groups that exist for a specific purpose. For example, sports teams exist with the goal of being successful in competitions against other sports teams. Study groups exist so that the members of the group can mutually benefit from one another's academic knowledge. In these situations, leaders often emerge because other members of the group perceive them as adding a lot of value to the group as a whole. On a sports team, this means that the best players are usually elected captain and in study groups people might be more inclined to like an individual who has a lot of knowledge to share. It has been argued that this may be a result of our evolutionary tendencies to favor individuals who are most likely to aid in our own survival.

It is also of note that the actual value which an individual brings to a group is not of consequence in determining his or her popularity; the only thing that is important is his or her value as perceived by the other members of the group. While perceived value and actual value may often overlap, this is not a requisite and it has been shown that there are instances in which an individual's actual value is relatively low, but they are perceived as highly valuable nevertheless.

Attractiveness, specifically physical attractiveness, has been shown to have very profound effects on popularity. People who are physically attractive are more likely to be thought of as possessing positive traits. People who are attractive are expected to perform better on tasks and are more likely to be trusted. Additionally, they are judged to possess many other positive traits such as mental health, intelligence, social awareness, and dominance.

Additionally, people who are of above average attractiveness are assumed to also be of above average value to the group. Research shows that attractive people are often perceived to have many positive traits based on nothing other than their looks, regardless of how accurate these perceptions are. This phenomenon is known as the Halo effect This means that, in addition to being more well-liked, attractive people are more likely to be seen as bringing actual value to the group, even when they may be of little or no value at all. In essence, physically attractive people are given the benefit of the doubt while less attractive individuals must prove that they are bringing value to the group. It has been shown empirically that being physically attractive is correlated with both sociometric and perceived popularity. Some possible explanations for this include increased social visibility and an increased level of tolerance for aggressive, social interactions that may increase perceived popularity.

The degree to which an individual is perceived as popular is often highly correlated with the level of aggression with which that individual interacts with his or her peers. There are two main categories of aggression, relational and overt, both of which have varying consequences for popularity depending on several factors, such as the gender and attractiveness of the aggressor.

Relational aggression is nonviolent aggression that is emotionally damaging to another individual. Examples of relationally aggressive activities include ignoring or excluding an individual from a group, delivering personal insults to another person, and the spreading of rumors. Relational aggression is more frequently used by females than males.

It has been found that relational aggression almost always has a strongly negative relationship with sociometric popularity but can have a positive relationship with perceived popularity depending on the perceived level of attractiveness of the aggressor. For an aggressor who is perceived as unattractive, relational aggression, by both males and females, leads to less perceived popularity. For an attractive aggressor however, relational aggression has been found to actually have a positive relationship with perceived popularity.

The relationship between attractiveness and aggression is further intertwined by the finding that increased levels of physical attractiveness actually further decreased the sociometric popularity of relationally aggressive individuals.

In short, the more physically attractive an individual is, the more likely they are to experience decreased levels of sociometric popularity but increased levels of perceived popularity for engaging in relationally aggressive activities.

Overt aggression is aggression that involves individuals physically interacting with each other in acts such as pushing, hitting, kicking or otherwise causing physical harm or submission in the other person. This includes threats of violence and physical intimidation as well.

It has been shown that overt aggression directly leads to perceived popularity when the aggressor is attractive. Experiments that are controlled for levels of physical attractiveness show that individuals who are attractive and overtly aggressive have a higher degree of perceived popularity than attractive non-overtly aggressive individuals. This was found to be true to a small degree for females and a large degree for males.

Attractive individuals who are overtly aggressive barely suffer any consequences in terms of sociometric popularity. This is a key difference between overt and relational aggression because relational aggression has a strongly negative relationship on sociometric popularity, especially for attractive individuals. For unattractive individuals, there is again a strongly negative relationship between overt aggression and sociometric popularity. This means that attractive individuals stand to gain a lot of perceived popularity at the cost of very little sociometric popularity by being overtly aggressive while unattractive individuals stand to gain very little perceived popularity from acts of overt aggression but will be heavily penalized with regards to sociometric popularity.

According to Talcott Parsons, as rewritten by Fons Trompenaars, there are four main types of culture: marked by love/hate (Middle East, Mediterranean, Latin America); approval/criticism (United Kingdom, Canada, Scandinavia, Germanic countries); esteem/contempt (Japan, Eastern Asia); and responsiveness/rejection (the United States).

There is no effort for popularity in Northern or Southern Europe, Latin America or Asia. This emotional bonding is specific for the high schools of the United States. In the love/hate cultures, the family and close friends are more important than popularity. In the approval/criticism cultures, actions are more important than persons, no strong links develop during school.

Popularity is gauged primarily through social status. Because of the importance of social status, peers play the primary role in social decision making so that individuals can increase the chances that others like them. However, as children, individuals tend to do this through friendship, academics, and interpersonal conduct. By adulthood, work and romantic relationships become much more important. This peer functioning and gaining popularity is a key player in increasing interest in social networks and groups in the workplace. To succeed in such a work environment, adults then place popularity as a higher priority than any other goal, even romance.

These two types of popularity, perceived popularity and sociometric popularity, are more correlated for girls than they are for boys. However, it is said that men can possess these qualities to a larger extent, making them more likely to be a leader, more powerful, and more central in a group, but also more likely than women to be socially excluded. Boys tend to become popular based on athletic ability, coolness, toughness, and interpersonal skills; however, the more popular a boy gets, the worse he tends to do on his academic work. On the other hand, this negative view of academics is not seen at all in popular girls, who gain popularity based on family background (primarily socioeconomic status), physical appearance, and social ability. Boys are also known to be more competitive and rule focused, whereas girls have more emotional intimacy.

In some instances, it has been found that in predominantly white high schools, attractive non-white students are on average significantly more sociometrically popular than equally attractive white students. One theory that has been put forth to explain this phenomenon is a high degree of group cohesiveness among minority students compared with the relative lack of cohesion amongst members of the majority. Since there is more cohesion, there is more availability for one person to be liked by many since they are all in contact. This acts like Zipf's Law, where the cohesion is a confounding factor that forces the greater links in the smaller minority, causing them to be more noticed and thus more popular. When considering race as a predictor for perceived popularity by asking a class how popular and important each other person is, African American students were rated most popular by their peers. Popularity in race was found to be correlated with athleticism, and because African Americans have a stereotype of being better at sports than individuals of other races, they are viewed as more popular. Additionally, White and Hispanic children were rated as more popular the better they succeeded in school and came from a higher socioeconomic background. No single factor can explain popularity, but instead the interaction between many factors such as race and athleticism vs. academics.

More tasks in the workplace are being done in teams, leading to a greater need of people to seek and feel social approval. In academic settings, a high social standing among peers is associated with positive academic outcomes. Popularity also leads to students in academic environments to receive more help, have more positive relationships and stereotypes, and be more approached by peers. While this is the research found in schools, it is likely to be generalized to a workplace.

Popularity is positively linked to job satisfaction, individual job performance, and group performance. The popular worker, besides just feeling more satisfied with his job, feels more secure, believes he has better working conditions, trusts his supervisor, and possesses more positive opportunities for communication with both management and co-workers, causing a greater feeling of responsibility and belongingness at work. Others prefer to work with popular individuals, most notably in manual labor jobs because, although they might not be the most knowledgeable for the job, they are approachable, willing to help, cooperative in group work, and are more likely to treat their coworkers as an equal. If an employee feels good-natured, genial, but not overly independent, more people will say that they most prefer to work with that employee.

According to the mere-exposure effect, employees in more central positions that must relate to many others throughout the day, such as a manager, are more likely to be considered popular.
There are many characteristics that contribute to popularity:

With a greater focus on groups in the workplace, it is essential that leaders effectively deal with and mediate groups to avoid clashing. Sometimes a leader does not need to be popular to be effective, but there are a few characteristics that can help a leader be more accepted and better liked by his group. Without group or team cohesiveness, there is no correlation between leadership and popularity; however, when a group is cohesive, the higher up someone is in the leadership hierarchy, the more popular they are for two reasons. First, a cohesive group feels more personal responsibility for their work, thus placing more value on better performance. Cohesive members see leaders as taking a bulk of the work and investing a lot of personal time, so when they see a job's value they can ascribe its success to the leader. This greatest contribution principle is perceived as a great asset to the team, and members view the leader more favorably and he gains popularity. Secondly, cohesive groups have well established group values. Leaders can become more popular in these groups by realizing and acting on dominant group values. Supporting group morals and standards leads to high positive valuation from the group, leading to popularity.

Popularity is a term widely applicable to the modern era thanks primarily to social networking technology. Being "liked" has been taken to a completely different level on ubiquitous sites such as Facebook.

Popularity is a social phenomenon but it can also be ascribed to objects that people interact with. Collective attention is the only way to make something popular, and information cascades play a large role in rapid rises in something's popularity. Rankings for things in popular culture, like movies and music, often do not reflect the public's taste, but rather the taste of the first few buyers because social influence plays a large role in determining what is popular and what is not through an information cascade.

Information cascades have strong influence causing individuals to imitate the actions of others, whether or not they are in agreement. For example, when downloading music, people don't decide 100% independently which songs to buy. Often they are influenced by charts depicting which songs are already trending. Since people rely on what those before them do, one can manipulate what becomes popular among the public by manipulating a website's download rankings. Experts paid to predict sales often fail but not because they are bad at their jobs; instead, it is because they cannot control the information cascade that ensues after first exposure by consumers. Music is again, an excellent example. Good songs rarely perform poorly on the charts and poor songs rarely perform very well, but there is tremendous variance that still makes predicting the popularity of any one song very difficult.

Experts can determine if a product will sell in the top 50% of related products or not, but it is difficult to be more specific than that. Due to the strong impact that influence plays, this evidence emphasizes the need for marketers. They have a significant opportunity to show their products in the best light, with the most famous people, or being in the media most often. Such constant exposure is a way of gaining more product followers. Marketers can often make the difference between an average product and a popular product. However, since popularity is primarily constructed as a general consensus of a group's attitude towards something, word-of-mouth is a more effective way to attract new attention. Websites and blogs start by recommendations from one friend to another, as they move through social networking services. Eventually, when the fad is large enough, the media catches on to the craze. This spreading by word-of-mouth is the social information cascade that allows something to grow in usage and attention throughout a social group until everyone is telling everyone else about it, at which point it is deemed popular.

Individuals also rely on what others say when they know that the information they are given could be completely incorrect. This is known as groupthink. Relying on others to influence one's own decisions is a very powerful social influence, but can have negative impacts.

The popularity of many different things can be described by Zipf's powerlaw, which posits that there is a low frequency of very large quantities and a high frequency of low quantities. This illustrates popularity of many different objects.

For example, there are few very popular websites, but many websites have small followings. This is the result of interest; as many people use e-mail, it is common for sites like Yahoo! to be accessed by large numbers of people; however, a small subset of people would be interested in a blog on a particular video game. In this situation, only Yahoo! would be deemed a popular site by the public. This can additionally be seen in social networking services, such as Facebook. The majority of people have about 130 friends, while very few people have larger social networks. However, some individuals do have more than 5,000 friends. This reflects that very few people can be extremely well-connected, but many people are somewhat connected. The number of friends a person has, has been a way to determine how popular an individual is, so the small number of people who have an extremely high number of friends is a way of using social networking services, like Facebook, to illustrate how only a few people are deemed popular.
Popular people may not be those who are best liked interpersonally by their peers, but they do receive most of the positive behavior from coworkers when compared to nonpopular workers. This is a result of the differences between sociometric and perceived popularity. When asked who is most popular, employees typically respond based on perceived popularity; however, they really prefer the social interactions with those who are more sociometrically popular. For each individual to ensure that they are consistent with the group's popularity consensus, those who are high in perceived popularity are treated with the same positive behaviors as those who are more interpersonally, but privately, liked by specific individuals. Well-liked workers are most likely to get salary increases and promotions, while disliked (unpopular) workers are the first to get their salary cut back or laid off during recessions.

During interactions with others in the work environment, more popular individuals receive more organizational citizenship behavior (helping and courteousness from others) and less counter productive work behavior (rude reactions and withheld information) than those who are considered less popular in the workplace. Coworkers agree with each other on who is and who is not popular and, as a group, treat popular coworkers more favorably. While popularity has proven to be a big determiner of getting more positive feedback and interactions from coworkers, such a quality matters less in organizations where workloads and interdependence is high, such as the medical field.

In many instances, physical appearance has been used as one indicator of popularity. Attractiveness plays a large role in the workplace and physical appearance influences hiring, whether or not the job might benefit from it. For example, some jobs, such as salesperson, benefit from attractiveness when it comes down to the bottom line, but there have been many studies which have shown that, in general, attractiveness is not at all a valid predictor of on-the-job performance. Many individuals have previously thought this was only a phenomenon in the more individualistic cultures of the Western world, but research has shown that attractiveness also plays a role in hiring in collectivist cultures as well. Because of the prevalence of this problem during the hiring process in all cultures, researchers have recommended training a group to ignore such influencers, just like legislation has worked to control for differences in sex, race, and disabilities.




</doc>
<doc id="88369" url="https://en.wikipedia.org/wiki?curid=88369" title="Obfuscation">
Obfuscation

Obfuscation is the obscuring of the intended meaning of communication by making the message difficult to understand, usually with confusing and ambiguous language. The obfuscation might be either unintentional or intentional (although intent usually is connoted), and is accomplished with circumlocution (talking around the subject), the use of jargon (technical language of a profession), and the use of an argot (ingroup language) of limited communicative value to outsiders.

In expository writing, unintentional obfuscation usually occurs in draft documents, at the beginning of composition; such obfuscation is illuminated with critical thinking and editorial revision, either by the writer or by an editor. Etymologically, the word "obfuscation" derives from the Latin "obfuscatio", from "obfuscāre" (to darken); synonyms include the words beclouding and abstrusity.

Doctors are faulted for using jargon to conceal unpleasant facts from a patient; the American author and physician Michael Crichton said that medical writing is a "highly skilled, calculated attempt to confuse the reader". The psychologist B. F. Skinner said that medical notation is a form of multiple audience control, which allows the doctor to communicate to the pharmacist things which the patient might oppose if they could understand medical jargon.

"Eschew obfuscation", also stated as "eschew obfuscation, espouse elucidation", is a humorous fumblerule used by English teachers and professors when lecturing about proper writing techniques. Literally, the phrase means "avoid being unclear" or "avoid being unclear, support being clear", but the use of relatively uncommon words causes confusion in much of the audience (those lacking the vocabulary), making the statement an example of irony, and more precisely a heterological phrase. The phrase has appeared in print at least as early as 1959, when it was used as a section heading in a NASA document.

An earlier similar phrase appears in Mark Twain's "Fenimore Cooper's Literary Offenses", where he lists rule fourteen of good writing as "eschew surplusage".

In white-box cryptography, obfuscation refers to the protection of cryptographic keys from extraction when they are under the control of the adversary, e.g., as part of a DRM scheme.

In network security, obfuscation refers to methods used to obscure an attack payload from inspection by network protection systems.



</doc>
<doc id="14091987" url="https://en.wikipedia.org/wiki?curid=14091987" title="Cold feet">
Cold feet

Cold feet is a phrase which refers to a person not going through with an action, particularly one which requires long term commitment, due to fear, uncertainty, and doubt. A person is said to be "getting cold feet" when, after previously committing to a plan, they ultimately do not carry out the planned course of action.

The origin of the term itself has been largely attributed to American author Stephen Crane, who added the phrase, in 1896, to the second edition of his short novel, "". Crane writes, "I knew this was the way it would be. They got cold feet." The term is present in "Seed Time and Harvest" by Fritz Reuter published in 1862. Kenneth McKenzie, a former professor of Italian at Princeton University attributed the first use of the phrase to the play "Volpone" produced by Ben Jonson in 1605. The true origin and first usage of the phrase remains debated and unconfirmed as exemplified above.

A common use of the phrase is when people fear the commitment of marriage and get "cold feet" before a wedding ceremony. This pre-marital doubt or fear may manifest for a variety of reasons and sometimes cause the bride or groom to back out of a planned marriage. Original research on the "cold feet" phenomenon is very limited, but a four-year study conducted by UCLA researchers found feelings of pre-marital doubt or uncertainty about an impending marriage were associated with future marital problems and a viable predictor of divorce.



</doc>
<doc id="39260154" url="https://en.wikipedia.org/wiki?curid=39260154" title="Calculus of concepts">
Calculus of concepts

The calculus of concepts is an abstract language and theory, which was developed to simplify the reasons behind effective messaging when delivered to a specific target or set of targets. The theory aims to maximize the likelihood of desired outcomes, by using messaging elements and techniques while analyzing the delivery mechanisms in certain scenarios. The reduction of uncertainty, but not its elimination, is often cost effective and practical.

Empowered by the internet of things (IoT) the framework looks at numerous device such as smart phones, tablets, laptops, hand held gaming devices, GPS devices, automobile Event Data Recorders and other electronic devices as remote sensors capable of providing data channels.

By using elements, the theory discovers underlying key concepts and their relations to better understand how messages can by used to elicit desired behaviors through mental model heuristics and biases. The framework does not serve up spam to potential consumers; it is a new paradigm for effective messaging. The nature of the data produced and consumed by devices in the IoT naturally lends itself to location-based awareness.

Just-in-time and real-time broadcasting of key messages gives the framework an extra dimension, putting it at the forefront of behavioral methodologies. Broadcasting can take place across a number of platforms, text, photographic, video, audio or even direct human contact.

The use of anchoring-and-adjustment, framing and representativeness heuristics provides fertile grounds for “re-wiring” the decision making processes to include either positive or mitigating mental models of a given concept or set of related concepts. The “re-wiring” will often produce results that have a significant impact on later decisions and behaviors on the target audience. The framework analyses key factors that influence the effectiveness of messaging mechanisms and how differing approaches can lead to entirely different results.

The calculus of concepts framework has been practically implemented utilizing a combination of Naive Bayes classification and Support Vector Machines (SVM) algorithms to actively identify the key components of a messaging campaign and its effectiveness. The effectiveness of a communications campaign is often measured by numerous results including reach, frequency and duration.

The training data set for the model implementation utilized the potential messages and delivery mechanisms with Actors, Actions, Objects, Contexts and Indicia as a few examples.

Each concept within the framework is treated by the practical implementation as either an independent or dependent variable (as applicable) and therefore may have a meaningful effect on the outcome of any communication. As with any machine-learning tool the Calculus of Concepts model implementation inputs can be either nominal or ordinal and depending on the particular case.

Between 2005 and 2012 one of the largest oil companies in China attempted to buy the twelfth biggest oil company in Canada. Initial proposals and takeover plans were rejected due to a number of issues surrounding political tensions.

Stakeholders identified Environment, Context, Domain, Event, Condition, State, Decision, Relation, Actor, Action and Object concepts that needed to be in place to have the key decision makers utilizing the mental models needed to secure the takeover.

Over the next 7 years messaging activities were fielded by the Chinese company and its authorized agents, specifically designed to elicit Ideations and Decisions that would result in the takeover going through. In 2012, the takeover was completed after a coordinated and concerted field messaging activity.



</doc>
<doc id="30046841" url="https://en.wikipedia.org/wiki?curid=30046841" title="Nagging">
Nagging

Nagging, in interpersonal communication, is repetitious behaviour in the form of pestering, hectoring, or otherwise continuously urging an individual to complete previously discussed requests or act on advice.

Reporter Elizabeth Bernstein defined, in a "Wall Street Journal" article, nagging as "the interaction in which one person repeatedly makes a request, the other person repeatedly ignores it and both become increasingly annoyed". Thus, nagging is a form of persistent persuasion that is more repetitive than aggressive and it is an interaction to which each party contributes. Nagging is a very common form of persuasion used in all aspects of life including domestic and professional. It is also a common practice in order to avoid more aggressive persuasive moves like threats. The word is derived from the Scandinavian "nagga", which means "to gnaw".

Kari P. Soule describes nagging as an "interpersonal ritual" but states that the term "seldom appears in interpersonal communication or conflict textbooks. It appears that 'nagging' is commonly used in everyday conversation but it rarely makes it to academic print".

Nagging as a form of interpersonal communication is considered to be a repetitious form of persuasion that can be employed as an alternative to resorting to more aggressive tactics in order to gain compliance. Martin Kozloff, Ph.D., Professor of Education at the University of North Carolina at Wilmington, identifies four main steps of nagging :

Kozloff argues that this interaction cycle continues until either the one who is being nagged complies to the nagger’s request or the nagger gives up the attempt to persuade. Kozloff identifies other important aspects of nagging; for instance, non-compliance is necessary for the persuader to be persistent. In addition, the persuader will often change the initial requests words and paralinguistic cues as a strategic tactic to entice the target into complying with the request.

Regarding compliance, behavioural noncompliance describes the situation that occurs when the person being nagged remains silent or agrees to complete the request, but later does not follow through. This strategy is employed to end the confrontation or interaction quickly without conflict, which is why it is common among spouses or partners. As the nagging interaction that starts out in a calm and polite manner continues and the persuader becomes more repetitive, the interaction is more likely to become aggressive in nature. Verbal noncompliance, on the other hand, describes the situation that occurs when the target tells the persuader through words that he will not comply, and is a more direct tactic than behavioural noncompliance. An example of verbal noncompliance could be a simple no, or I am too busy right now, or an even more elaborate response. This tactic does end the nagging interaction more rapidly; however, it can cause a more aggressive response from the persuader, who may escalate persistent persuasion into a threat or another aggressive form of persuasion.

Psychotherapists such as Edward S. Dean, M.D. have reported that individuals who nag are often "weak, insecure, and fearful ... their nagging disguises a basic feeling of weakness and provides an illusion of power and superiority". Nagging is sometimes used by spouses of alcoholics as one of several "drinking control efforts", but it is often unproductive. Psychologically, nagging can act to reinforce behavior. A study by the University of Florida found the main factors that lead a person to nag are differences in "gender, social distance, and social status and power".

Kari P. Soule found that nagging is viewed to be more of a feminine form of interpersonal communication than masculine. An equal number of men and women nag; however, studies have shown that women are more likely to nag both men and women, while men are more likely to nag only men. Meaning women nag all people, which can be attributed to the reason why women are stereotyped as nagging people all the time.

Nagging by spouses is a common marital complaint. Nagging can be found between both male and female spouses. An 1897 article in "Good Housekeeping" magazine stated that at that time, topics differed by gender; husbands' nagging usually involved finding "fault with their dinner, with the household bills [and] with the children", along with "carry[ing] home the worries of business."

A study done at Washington State University and published in 1959 described parental nagging of children as being a "symptom of the rejection of the child" in circumstances when children's requirements regarding "time and energy" are perceived to interfere with the mother's "individual needs and aspirations." According to James U. McNeal, there are seven classifications of juvenile nagging, wherein children nag their parents to obtain something they desire.

During the Middle Ages, a scold's bridle, also called a brank, was an instrument of punishment used primarily on women. The device was an iron muzzle in an iron framework that enclosed the head. A bridle-bit (or curb-plate), about 2 inches long and 1 inch broad, projected into the mouth and pressed down on top of the tongue. The curb-plate was frequently studded with spikes, so that if the offender moved her tongue, it inflicted pain and made speaking impossible. Wives who were seen as witches, shrews and scolds, were forced to wear the branks, locked onto their head.




</doc>
<doc id="26615845" url="https://en.wikipedia.org/wiki?curid=26615845" title="Role-playing">
Role-playing

Role-playing is the changing of one's behaviour to assume a role, either unconsciously to fill a social role, or consciously to act out an adopted role. While the "Oxford English Dictionary" offers a definition of role-playing as "the changing of one's behaviour to fulfill a social role", in the field of psychology, the term is used more loosely in four senses:

Many children participate in a form of role-playing known as make believe, wherein they adopt certain roles such as doctor and act out those roles in character. Sometimes make believe adopts an oppositional nature, resulting in games such as cops and robbers.

Historical re-enactment has been practiced by adults for millennia. The ancient Romans, Han Chinese, and medieval Europeans all enjoyed occasionally organizing events in which everyone pretended to be from an earlier age, and entertainment appears to have been the primary purpose of these activities. Within the 20th century historical re-enactment has often been pursued as a hobby.

Improvisational theatre dates back to the Commedia dell'Arte tradition of the 16th century. Modern improvisational theatre began in the classroom with the "theatre games" of Viola Spolin and Keith Johnstone in the 1950s. Viola Spolin, who was one of the founders the famous comedy troupe Second City, insisted that her exercises were games, and that they involved role-playing as early as 1946. She accurately judged role-playing in the theatre as rehearsal and actor training, or the playing of the role of actor versus theatre roles, but many now use her games for fun in their own right.

A role-playing game is a game in which the participants assume the roles of characters and collaboratively create stories. Participants determine the actions of their characters based on their characterisation, and the actions succeed or fail according to a formal system of rules and guidelines. Within the rules, they may improvise freely; their choices shape the direction and outcome of the games.

Role-playing can also be done online in the form of group story creation, involving anywhere from two to several hundred people, utilizing public forums, private message boards, mailing lists, chatrooms, and instant-messaging chat clients (e.g., MSN, Yahoo!, ICQ) to build worlds and characters that may last a few hours, or several years. Often on forum-based roleplays, rules, and standards are set up, such as a minimum word count, character applications, and "plotting" boards to increase complexity and depth of story. 

There are different genres of which one can choose while role-playing, including, but not limited to, fantasy, modern, medieval, steam punk, and historical. Books, movies, or games can be, and often are, used as a basis for role-plays (which in such cases may be deemed "collaborative fan-fiction"), with players either assuming the roles of established canon characters or using those the players themselves create ("Original Characters") to replace—or exist alongside—characters from the book, movie, or game, playing through well-trodden plots as alternative characters, or expanding upon the setting and story outside of its established canon.

In psychology, an individual's personality can be conceptualized as a set of expectations about oneself and others and that these add up to role-playing or role-taking. Here, the role is fiction because it is not real but it has a degree of consistency. Role-playing is also an important part of a child's psychological development. For example, the instance when a child starts to define "I" and separate him or herself from an adult is the initial condition for and the result of role play. There are also experiments that found role-playing resulted in behavioral change such as the case of smokers who reported negative attitude towards smoking after being asked to pretend to be a person diagnosed with lung cancer. 

Role-playing may also refer to role training where people rehearse situations in preparation for a future performance and to improve their abilities within a role. The most common examples are occupational training role-plays, educational role-play exercises, and certain military wargames.

One of the first uses of computers was to simulate real-world conditions for participants role-playing the flying of aircraft. Flight simulators used computers to solve the equations of flight and train future pilots. The army began full-time role-playing simulations with soldiers using computers both within full scale training exercises and for training in numerous specific tasks under wartime conditions. Examples include weapon firing, vehicle simulators, and control station mock-ups.

Role playing may also refer to the technique commonly used by researchers studying interpersonal behavior by assigning research participants to particular roles and instructing the participants to act as if a specific set of conditions were true. This technique of assigning and taking roles in psychological research has a long history. It has been used in the early classic social psychological experiments by Kurt Lewin (1939/1997), Stanley Milgram (1963), and Phillip Zimbardo (1971). Herbert Kelman suggested that role-playing might be "the most promising source" of research methods alternative to methods using deception (Kelman 1965).



</doc>
<doc id="675275" url="https://en.wikipedia.org/wiki?curid=675275" title="Distraction">
Distraction

Distraction is the process of diverting the attention of an individual or group from a desired area of focus and thereby blocking or diminishing the reception of desired information. Distraction is caused by: the lack of ability to pay attention; lack of interest in the object of attention; or the great intensity, novelty or attractiveness of something other than the object of attention. Distractions come from both external sources, and internal sources. External distractions include factors such as visual triggers, social interactions, music, text messages, and phone calls. There are also internal distractions such as hunger, fatigue, illness, worrying, and daydreaming. Both external and internal distractions contribute to the interference of focus.

Distracted driving is a dangerous threat to road safety across the world. While drunk driving rates have been on the decline since 1983, distracted driving has been increasing in recent years. Many feel this incline is due to the widespread prevalence of cell phones. While distracted driving can be attributed to anything that diverts attention away from the road, it is often the cell phone that receives the blame for distracted driving incidents. Most of the recent Studies have shown that cell phone usage while driving has striking similarities to the effects of drinking while driving; Cell phones tend to take the driver's attention away from the road and onto itself. With drunk driving, drivers often experience the "looking but not seeing" phenomena. While their eyes do indeed view objects on the road, their brains do not comprehend the meaning behind the image. All levels of distraction while driving are dangerous, and potential drivers are cautioned to keep awareness of their surroundings.

Many psychological studies show that switching between tasks, use of technology, and overstimulation has increased levels of distraction in the school setting. At school, distraction is often viewed as a source of poor performance and misbehavior. Distraction makes focusing on singular, assigned tasks more difficult. Digital components of learning are an emerging component to classroom distraction. Parents, teachers, students, and scholars all have opinions about how technology either benefits or harms a students’ focus in an academic setting. Research studies show that neuron circuits indicate a decrease in ability to be attentive to goal relative stimulus with the addition of distracting stimuli interference. School-aged students, with developing brains, are more apt to conflicting stimuli while trying to focus. Large classroom sizes, technology use in and outside the classroom, and less natural stimuli have been seen as contributing factors to deflating test scores and classroom participation.

Multitasking could also be considered as distraction in situations requiring full attention on a single object (e.g., sports, academic tests, performance). The issue of distraction in the workplace is studied in interruption science. According to Gloria Mark, a leader in interruption science, the average knowledge worker switches tasks every three minutes, and, once distracted, a worker takes nearly a half-hour to resume the original task.

In works of fiction, distraction is often used as a source of comedy, whether the amusement comes from the gullibility of those distracted or the strangeness of whatever is utilized to create the distraction. Examples of comedic distraction, also called comic relief, can oftentimes be found in Shakespearean plays. In "Hamlet", Shakespeare includes a scene in which two gravediggers joke around about Ophelia's death. While her death is by no means meant to be funny, a small break from the sadness helped to appease the groundlings in Shakespeare's time, as well as allow the rest of the audience to take a break from the constant "doom and gloom" of his tragedies.

Rabbi Allen Lew in his book, "This is Real and You are Completely Unprepared", writes, "The thoughts that carry our attention away [during prayer or meditation] are never insignificant thoughts and they never arise at random. We lose our focus precisely because these thoughts need our attention and we refuse to give it to them. This is why they keep sneaking up on our attention and stealing it away. This is how it is that we come to know ourselves as we settle deeply into the act of prayer [or meditation]". According to philosopher Damon Young, distraction is chiefly an inability to identify, attend to or attain what is valuable, even when we are hard-working or content.

Distraction was a key battle strategy in tales from the Trojan War. According to the legend, the Greeks seemed to have retreated by pretending to sail away. In their stead, they left a large wooden horse, which the Trojans then chose to bring back within their walls in order to celebrate their supposed victory. The Greeks used the Trojans' pride as a distraction, as they actually hid men within the Trojan Horse in order to let the rest of the army in during the cover of night. The Greeks then entered and destroyed the city of Troy, effectively ending the 10-year standoff that was the Trojan War.

Distraction is useful in the management of pain and anxiety. Dentists, for example may intentionally hum an annoying tune or engage in small talk just to create a diversion from the dental surgery process. Topical ointments containing capsaicin, provide a superficial burning sensation that can momentarily distract a patient's attention away from the more serious pain of arthritis or muscle strain. A similar effect is made by oil of cloves, which produces a burning sensation on the gums, and distracts from toothache.

Distraction is often used as a coping mechanism for short-term emotion regulation. When presented with an unpleasant reality, humans often choose to occupy their attention with some other reality in order to remain in a positive mental state. This is referred to as ‘procrastination’ when the unpleasant reality is in the form of work. The natural human inclination to distract oneself was put to the test when the Department of Psychology at Humboldt-Universität zu Berlin (Humboldt University of Berlin) held an experiment to study distraction. The goal of the experiment was to examine whether the effects of distraction on where subjects held their attention during repeated picture processing is changed by regular emotional functions. Furthermore, they hypothesized that while distraction assists in short-term emotional regulation, it is actually harmful in the long term. In order to do so, the experimenters had subjects view 15 unpleasant pictures (Set A) and “attend” to them (meaning the subjects were asked to pay full attention to the pictures). Next, the subjects were shown 15 unpleasant pictures (Set B) and were asked to distract themselves from the pictures (meaning they were to think about anything other than the picture on the screen; their example was to think about “the way to the supermarket”). Finally, the subjects were shown 15 neutral pictures (Set C) and were asked to attend to them. After 10 minutes of rest, the subjects entered the “re-exposure phase”, which repeated the experiment- this time requiring the subjects to pay attention to all of the sets, including Set B. This experiment was performed on 3 separate blocks of participants. To examine the state of the subjects’ brain, the subject was to wear “Ag/AgCl-electrodes from 61 head sites using an EasyCap electrode system with an equidistant electrode montage. Additional external electrodes were placed below the left (IO1) and right eye (IO2), below T1 (ground), on the nasion, and on the neck.” The subjects were also asked to rate the unpleasantness of the picture on the screen on a scale of 1-9. To test whether distraction in the first phase resulted in increased responsiveness during the re-exposure phase, experimenters “compared mean unpleasantness ratings between unpleasant pictures that were previously presented in the attend (previous attention) versus distract (previous distraction) condition using a paired t-test”. The end results of the experiment were as such:
Essentially, when exposed to an unpleasant image, the subject feels initial discomfort. However, after being exposed to it once with their full attention, the subject feels much less discomfort the second time they are exposed. When the subject distracts themselves from the initial unpleasant image, the subject feels more discomfort the second time when they are required to attend to the image. The experimenters’ conclusion is thus: “the obtained results suggest that distraction inhibits elaborate processing of the stimulus' meaning and adapting to it.”

Con artists and shoplifters sometimes create a distraction to facilitate their crimes. Armed robbers may create a distraction after their robbery, such as pulling a fire alarm, to create confusion and aid in their getaway. In a more serious case of crime, Norwegian Anders Behring Breivik exploded a car bomb in Oslo city. It was a reportedly distraction that directed police resources to Oslo city, allowing him to carry out his shooting spree unopposed on Utøya island.

Magicians use distraction techniques to draw the audience's attention away from whichever hand is engaged in sleight of hand. Magicians can accomplish this by encouraging the audience to look elsewhere or by having an assistant do or say something to draw the audience's attention away.
Sleight of hand is often used in close-up magic, performed with the audience close to the magician, usually within three or four meters, possibly in physical contact. It often makes use of everyday items as props, such as cards and coins. The guiding principle of sleight-of-hand, articulated by legendary close-up magician Dai Vernon, is "be natural". A well-performed sleight looks like an ordinary, natural and completely innocent gesture, change in hand-position or body posture.

It is commonly believed that sleight of hand works because “the hand is quicker than the eye” but this is usually not the case. In addition to manual dexterity, sleight of hand depends on the use of psychology, timing, misdirection, and natural choreography in accomplishing a magical effect. Misdirection is perhaps the most important component of the art of sleight of hand. The magician choreographs his actions so that all spectators are likely to look where he or she wants them to. More importantly, they do not look where the performer does not wish them to look. Two types of misdirection are timing and movement. Timing is simple: by allowing a small amount of time to pass after an action, events are skewed in the viewer's mind. Movement is a little more complicated. A phrase often used is "A larger action covers a smaller action". Care must be taken however to not make the larger action so big that it becomes suspicious.

Propagandizing techniques of distraction are used in media manipulation. The idea is to encourage the public to focus on a topic or idea that the compliance professional feels is supportive of their cause. By focusing attention, a particular ideology can be made to seem the only reasonable choice. Oftentimes, media competition is the driving force for media bias, manipulation, and distraction. If a media company can find an audience with a united ideology, it then develops a loyal consumer base, as its consumers will be happy with the way media is presented. A so-called "conservative" media outlet would not hire a "liberal" reporter, as they would run the risk of alienating its viewership. 

Distraction is also important in studies of media multitasking, or the simultaneous use of multiple media at once. This behavior has emerged as increasingly common since the 1990's, especially among younger media users. Studies show that while humans are predisposed to the desire to multitask, most people struggle to have legitimate productivity while multitasking. Instead of giving a task full attention, the split attention that multitasking necessitates can cause one task to be a distraction to another. On the other hand, some studies show that multitasking has the potential for a high-risk high-reward situation, leading to the idea that success can arise from multitasking if one is good at the activity.



</doc>
<doc id="27475941" url="https://en.wikipedia.org/wiki?curid=27475941" title="Queen bee syndrome">
Queen bee syndrome

Queen bee syndrome was first defined by G.L. Staines, T.E. Jayaratne, and C. Tavris in 1973. It describes a woman in a position of authority who views or treats subordinates more critically if they are female. 

This phenomenon has been documented by several studies. In another study, scientists from the University of Toronto speculated that the queen bee syndrome may be the reason that women find it more stressful to work for women managers; no difference was found in stress levels for male workers. 

An alternate, though closely related, definition describes a queen bee as one who has succeeded in her career, but refuses to help other women do the same.

In recent years, research has shown that adolescent girls form (often small) groups called cliques, which are often created based on a shared characteristic or quality of the members such as attractiveness or popularity. Association with such a group is often wanted by those who are part of the larger, all encompassing group, such as a class or school. It is the association with these groups that brings an individual similar treatment.

Middle school and high school seems to be the place in which the queen bee syndrome is born. Vicious bullying of teen girls can sometimes be found in the interactions of adolescent girls, often with the operations spearheaded by one individual, who has of late been dubbed the "queen bee." 

A popular example of a movie based on girls with queen bee syndrome is the 2004 film, "Mean Girls". The authorized biographer of Margaret Thatcher, Charles Moore, stated in an interview his belief that the former British prime minister had herself suffered from the syndrome. However, it is important to bear in mind that cases are not representative and the existence of cases does not suggest that all or most women in power act like queen bees.

Recent research has postulated that queen bee syndrome may be a product of certain cultural influences, especially those related to the modern workplace. 

Researchers have hypothesized that queen bee behavior may be developed by women who have achieved high workplace positions within their respective fields as a way to defend against any gender bias found in their cultures. By opposing attempts of subordinates of their own sex to advance in career paths, women displaying queen bee behavior try to fit in with their male counterparts by adhering to the cultural stigmas placed on gender in the workplace. Distancing themselves from female subordinates can allow for the opportunity to show more masculine qualities, stereotypically seen as more culturally valuable and professional. By showing these supposedly important masculine qualities, women displaying queen bee behavior seek to further legitimize their right to be in important professional positions as well as attaining job security by showing commitment to their professional roles.

Recent research, that uses a robust causal identification mechanism (i.e., regression discontinuity design), strongly contests the existence of the queen bee phenomenon; the results of this study suggest that previous research was biased either by eliciting confirming cases (as is often done in qualitative research) or that observational data based on questionnaire measures was biased because of endogeneity issues. 




</doc>
<doc id="9330907" url="https://en.wikipedia.org/wiki?curid=9330907" title="Critical-Creative Thinking and Behavioral Research Laboratory">
Critical-Creative Thinking and Behavioral Research Laboratory

Critical-Creative Thinking and Behavioral Research Laboratory (ELYADAL) was founded in March 2002 as a branch in the Faculty of Economics and Administrative Sciences in Baskent University, Ankara, Turkey. Academicians and students in different disciplines like psychology, social science, philosophy of science, scientific method, political science, business management and economics have convened to conduct scientific research and produce applications of science pertaining to human behavior as it is manifested in varying contexts. ELYADAL's perspective regarding scientific inquiry can be summarized as "disseminating the scientific knowledge accumulated by following curiosity through asking questions and answering them by means of scientific research."

ELYADAL has two branches to organize interests in two broad topics. The first branch is related to make research on, develop programs to train on, and to provide information about critical thinking and creative thinking for students, faculty members, and organizations. The second branch is organized around behavior, its antecedents and consequences, and research conducted in an interdisciplinary fashion is welcomed.

The process of establishment have started in March 2002. A core group of academicians have called for colleagues and students to participate in the new-founding laboratory. Following a number of meetings with the initial respondents, ELYADAL Research Group has started its activities on April 13, 2002. In order to test and form the interaction dynamics of the group, three subgroups have been established to work independently on three topics of interest. These topics were chosen so that no group member had previous in-depth knowledge or formal education about them, enabling equidistant amateurship in the beginning. The topics were the number Pi in mathematics, the microscopic organism Volvox colony in biology, and the black hole in physics. The studies of the groups were disseminated by a then-established popular scientific magazine named PiVOLKA (an anagram name including syllables from the three topics). Mini-group studies on natural scientific issues then become a one-step hurdle for the selection of prospective adjoiners and PiVOLKA became a source of dissemination and open forum to students and academicians in other universities as well.

The ongoing studies by members of ELYADAL include critical thinking, political and economical psychology research and publications, convention papers, the PiVOLKA peer-reviewed quarterly scholarly publication, a booklet about research methods-academic writing, a booklet about statistics, seminars and panels on various topics.

ELYADAL has 28 papers that are published in various journals, 33 papers that are presented in various seminars and conferences, 16 talks that are broadcast in media about psychology, social psychology, critical thinking, creative thinking, science against pseudoscience, etc.






</doc>
<doc id="5091728" url="https://en.wikipedia.org/wiki?curid=5091728" title="Accismus">
Accismus

Accismus is a feigned refusal of something earnestly desired.

The 1823 "Encyclopædia Britannica" writes that accismus may sometimes be considered a virtue, sometimes a vice.

The Latin term comes from the Greek word is "ἀκκισμός", which, according to "Britannica", was "supposed to be formed from Acco (Greek: Akko), the name of a foolish old woman, famous in antiquity for an affectation of this kind." (An 1806 "Lexicon manuale Graeco-Latinum et Latino-Graecum" agrees with this derivation. However an 1820 "Lexicon Graeco-Latinum" associates Acco with idle occupation, e.g., chatting with other women or looking into a mirror, hence the Greek coinages Ακκιζειν / Ακκους).

More particularly, in rhetorics, accismus is a figure of speech, a figure of refutation, is a type of irony.




</doc>
<doc id="40512089" url="https://en.wikipedia.org/wiki?curid=40512089" title="Reasoned action approach">
Reasoned action approach

The reasoned-action approach (RAA) is an integrative framework for the prediction (and change) of human social behavior. The reasoned-action approach states that attitudes towards the behavior, perceived norms, and perceived behavioral control determine people's intentions, while people's intentions predict their behaviors.

The reasoned-action approach is the latest version of the theoretical ideas of Martin Fishbein and Icek Ajzen, following the earlier theory of reasoned action and the theory of planned behavior. Those theoretical ideas have since 1975 resulted in over a thousand empirical studies in behavioral science journals.

Behavior is determined by the intention and moderated by actual control. Intention is determined by attitude, perceived norm, and perceived behavioral control. Perceived behavioral control influences behavior directly and indirectly through intention. Actual control feeds back to perceived control. Performing the behavior feeds back to the beliefs underlying the three determinants of intention. All possible influences on behavior that are not in the model are treated as background variables and are supposed to be mediated by the determinants in the model.

The reasoned-action approach uses a number of concepts, each of which is briefly defined here:

Attitude, perceived norm, and perceived behavioral control are all based on beliefs: behavioral beliefs, normative beliefs, and control beliefs. Attitude is the result of the strength of behavioral beliefs reflecting positive and negative outcomes (and experiences) of the behavior, each multiplied by outcome evaluations in terms of good – bad. Perceived norm is the result of the strength of injunctive beliefs reflecting the expectations of various relevant others in the environment, each multiplied by the motivation to comply with these expectations, and of descriptive beliefs reflecting the behaviors of various relevant others, each multiplied by the degree of identification with these others. Perceived behavioral control is the result of the strength of control beliefs reflecting perceived skills, barriers and facilitators, each multiplied by the degree of power of control over these factors. These underlying beliefs have to be identified through a careful elicitation procedure, combining qualitative and quantitative research methods.

Concepts in the reasoned-action approach can be measured directly, and indirectly through the underlying beliefs.

These are a number of examples of the ways in which measurement items are constructed to measure the variables specified in the RAA.

In their 2010 book, Fishbein & Ajzen provide detailed examples of indirect measures in the Appendix, pp. 449–463.

The reasoned-action approach has been criticized for being too rational. Fishbein & Ajzen argue that to be a misunderstanding of the theory. There is nothing in their theory to suggest that people are rational; the theory only assumes that people have behavioral, normative and control beliefs which may be completely irrational but will determine behavior.

Another critical comment implies that most behavior is not intentional. Fishbein & Ajzen argue that beliefs and intention can be activated automatically. They also suggest that alternative concepts, such as willingness, are in fact measures of intentions. Implicit associations are often different from explicit attitude measures, but there is little evidence to suggest that they predict behavior more adequately.

A further criticism on the reasoned-action approach concerns the sufficiency assumption, which suggests that the theory captures all relevant determinants of intention. Ajzen stated that the theory is open to the inclusion of additional predictors if it can be shown that they capture a significant proportion of the variance in intention or behavior after the theory's current variables have been taken into account. Several researchers have indeed offered possible extensions, for example self-identity, next to the three current variables claiming these contribute significant additional explained variance in intention and behavior. In the reasoned-action approach, Fishbein and Ajzen have indeed included new variables, but within the current three determinants (p. 282). They formulate strict criteria for a so-called 'fourth' variable and argue that none of the variables proposed fulfill these criteria.

With respect to social-cognitive theories in general, authors have criticized the 'Western' character of theories and argued that theories are not culture-free. However, finding, in a specific cultural setting, specific beliefs that are not part of a general theory does not in itself invalidate the usefulness of the theory. Fishbein & Ajzen have repeatedly stressed the importance of an open elicitation procedure to identify all relevant underlying beliefs. The theory of reasoned action and the theory of planned behavior have successfully been applied in many different cultural settings.

In the reasoned-action approach change is seen as a planned process in three phases: elicitation of the relevant beliefs, changing intentions by changing salient beliefs, and changing behavior by changing intentions and increasing skills or decreasing environmental barriers. The basic idea behind selecting any potential change method is that the salient beliefs are to be changed. Fishbein & Ajzen recognize methods such as persuasive communication, use of arguments, framing, active participation, modeling, and group discussion, but indicate that these methods will only have effect when salient behavioral, normative, or control beliefs are changed. Obviously, it is important that the salient beliefs are identified and measured correctly. Witte suggests to first organize the results of the beliefs elicitation in a list of relevant categories (for example, behavioral beliefs, normative beliefs, self-efficacy beliefs, values) and then to decide which beliefs need to be changed, which need to be reinforced, and which need to be introduced.

The reasoned-action approach, mostly as the theory of planned behavior, is applied in many different settings and with many different behaviors, such as: health-related behaviors, sustainable behaviors, traffic behaviors, organizational behaviors, political behaviors, and discriminatory behaviors. A number of meta-analyses support the claims of the theory.




</doc>
<doc id="40603620" url="https://en.wikipedia.org/wiki?curid=40603620" title="Gender psychology">
Gender psychology

Gender is generally conceived as a set of characteristics or traits that are associated with a certain biological sex (male or female). In non-western countries, gender is not always conceived as binary, or strictly linked to biological sex. As a result, in some cultures there are third, fourth, fifth or "some" genders. The characteristics that generally define gender are referred to as "masculine" or "feminine."

The formation of gender is controversial in many scientific fields, including psychology. Specifically, researchers and theorists take different perspectives on how much of gender is due to biological, neurochemical, and evolutionary factors (nature), or is the result of culture and socialization (nurture). This is known as the nature versus nurture debate. The subfields of psychology note specific differences in the traits of each gender, based on their perspective of the issue on the nature versus nurture debate.

The study of gender took off in the 1970s. During this time period, academic works were published reflecting the changing views of researchers towards gender studies. Some of these works included textbooks, as they were an important way that information was compiled and made sense of the new field. In 1978 "Women and sex roles: A social psychological perspective" was published, one of the first textbooks on the psychology behind women and sex roles. Another textbook to be published, "Gender and Communication", was the first textbook to discuss the topic of its subject.

Other influential academic works focused on the development of gender. In 1966, "The Development of Sex Differences" was published. This book went into what factors influence a child’s gender development, with contributors proposing the effects of hormones, social learning, and cognitive development in respective chapters. "Man and Woman, Boy and Girl", by John Money was published in 1972, reporting findings of research done with intersex subjects. The book proposed that the social environment a child grows up in is more important in determining gender than the genetic factors he or she inherits. In recent years, the majority of Dr. Money's theories regarding the importance of socialization in the determination of gender have come under intense criticism, especially in connection with the false reporting of success in the 'John/Joan' experiment (see David Reimer).

In 1974, "The Psychology of Sex Differences" was published. It said that men and women behave more similarly than had been previously supposed. They also proposed that children have much power over what gender role they grow into, whether by choosing which parent to imitate, or doing activities such as playing with action figures or dolls. These works added new knowledge to the field of gender psychology.

Biological differentiation is fundamental in determining differences in males and females. Males have two different sex chromosomes, an X and a Y. Females have two X chromosomes. The Y chromosome is what determines sexual differentiation. If the Y chromosome is present, growth is along male lines. The SRY is a specific part of the Y chromosome which is the sex-determining gene region of the chromosome. This is what is responsible for the differentiation between male and females. 
Testosterone helps differentiate gender by increasing the likelihood of male patterns of behavior. It has effects on the central nervous system that trigger these behaviors. Parts of the SRY and specific parts of the Y chromosome could also possibly influence different gender behaviors.
The biological approach states that the distinction between men and women are due to inherent and hormonal differences. Some critique this approach because it leaves little room for sexual expression and gender because it claims both are dependent on biological makeup. Biological explanations of gender and sexual differences have been correlated to the work done by Charles Darwin regarding evolution. He suggested that just as wild animals and plants had physiological differences between sexes, humans did as well.
Biological perspectives on psychological differentiation often place parallels to the physical nature of sexual differentiation. These parallels include genetic and hormonal factors that create different individuals, with the main difference being the reproductive function. The brain controls behavior by individuals, but it is influenced by genes, hormones and evolution. Evidence has shown that the ways boys and girls become men and women is different, and that there are variations between the individuals of each sex.
There have been studies conducted to try and associate hormones with the gender identity of males and females. Okayama University in Japan did a study investigating the biological nature of gender identity disorder. The researchers looked at five different sex related hormones and whether or not they increased the chances of an individual being a transsexual. They examined male to female (MTF) and female to male (FTM) transsexuals, using control males and females for comparison. Their research did not find a significant difference in the distribution of the examined genes. The results currently can not provide evidence that the different genetic variants of sex hormone genes influence an individual to MTF or FTM transsexualism.

Sex-related differences of cognitive functioning is questioned in research done on the areas of perception, attention, reasoning, thinking, problem solving, memory, learning, language and emotion. Cognitive testing on the sexes involves written tests that typically have a time limit, the most common form being a standardized test such as the SAT or ACT. These test basic individual abilities rather than complex combination of abilities needed to solve real life problems. Analysis of the research has found a lack of credibility when relying on published studies about cognition because most contain findings of cognitive differences between the males and females, but they overlook those that do not show any differences, creating a pool of biased information. Those differences found are attributed to both social and biological factors. The introduction of cultural factors are in congruence to necessary biological elements.
An article published in the Review of Educational Research summarizes the history of the controversy around sex differences in variability of intelligence. Through modern research, the main idea has held that males have a much wider range in test performance in IQ tests. The study also analyzes data concerning differences in central tendencies through environmental and biological theories. Males were found to have much wider variation than females in areas of quantitative reasoning, spatial visualization, spelling, and general knowledge than females. In the studies conclusion, to form an accurate summary, both the variability in sex differences and in the central tendencies must be examined to generalize the cognitive variances of males and females.
Doreen Kimura, a psychobiologist, has published books and articles specifically on the subject of sex and cognition. Since studying gender differences in cognition, Kimura has further proved generalizations made from research data collected in the field of cognitive psychology. Males are found to be better at the motor skill of aiming, while females excel at the coordination of fine motor skills. Male chimpanzees, for example, are much more likely to throw stones or clumps of earth than their corresponding gender. In spatial tasks, males found it easier to visualize geometrically and rotationally while females used references to objects when guiding through a route. Females test higher on object location memory and verbal memory, approximately over a half of a standard deviation. These tests have not been comprehensively studied over an adequate amount of time to make a full and accurate conclusion. Standardized spatial tests, like the Vandenberg mental rotations test, have consistently shown sex differences in this area over the last thirty years. The differences for such tests average to a full standard deviation. These scientific findings have not been generalized cross culturally. Females have shown to have a higher ability in reading facial and body cues than their male counterparts. Though studies have found females to have more advanced verbal skills, men and women in adulthood do not have varied vocabularies. Women tend to have better spelling capabilities and verbal memory.
Kimura refers to an example of a study done in east Africa, which correlated children, mostly males, who travelled the farthest from their tribe with excelled performance in spatial tasks. She offers three possible explanations for the correlation. First, those with the more trained spatial ability tend to take part in experiences that require those skills. The unconscious positive reinforcement a person receives from doing something well, and the praise that follows, may make certain activities more attractive and more likely to occur. This phenomenon, where people end up taking part in activities or occupations in a self-directed way, is called self-selection. Second, the experience trained the person to develop spatial abilities. The type of experience determines which traits are developed. Third, an outside factor, such as early exposure to androgens, could influence both the biological and environmental components.

Although there is a large collection of information about how men and women differ in cognitive functioning and the physical differences of each gender's brain, both sets of information have not been definitely related to each other in research. The biggest difference between the genders are the sizes of the brains. Men's brains are larger and heavier than women's by 10-15%, though the ratio of brain to body size in both males and females varies based on body size. Researchers propose the extra brain weight in males is the reason for the large sex difference in spatial ability. Women appear to have larger areas of connective fibers between the two hemispheres, called hemispheric asymmetry. It is suggested that there are more clearly defined roles of the hemispheres in males than there are in females because of this asymmetry. A report by Simon LeVay in 1991, disclosed information about the sex related difference of the interstitial nuclei of the anterior hypothalamus, or INAH. Females were found to have smaller areas of the INAH in postmortem analysis by Dutch and American researchers. The same differences were found between homosexual and heterosexual men. An inference has been made that the size of this region is somehow related to the preference of an individual's sex partner. LeVay notes that these findings are correlational.

In most cultures, humans are subject from infancy to gender socialization, for example infant girls typically wear pink and infant boys typically wear blue. Gender schemas, or gendered cultural ideals which determine a person’s preferences, are also installed into our behaviors beginning at infancy. Studies show that toddler children are more likely to interact with children of the same sex than they are to interact with children of the opposite sex or even a mixed group.

As people get older, gender stereotypes become more applied. The social role theory primarily deals with such stereotypes, more specifically the division of labor and a gender hierarchy. When this theory is applied in social settings, such as the workplace, it can often lead to sexism. This theory also applies to certain personality trails, such as men are more typically more assertive and women more passive. According to this theory, ideally, in most cultures, the woman is to stay and tend to the house and home while the man works to both better the house itself and increase finances.

In the midst of so many idealistic concepts on the roles of a specific gender in society, there are also individuals who choose to not to conform to the ideas of their culture. Throughout history, gender roles have been altered and are much more flexible than they were in recent centuries. Such alterations include equal political rights as well as employment and education opportunities solely available to females. Homosexual people are also subject to go against gender conformities. The term "congenital gender invert" is used to define homosexuals who possess a trait of the opposite sex. Such individuals tend to have the most social difficultly in regards to cultural norms.




</doc>
<doc id="203510" url="https://en.wikipedia.org/wiki?curid=203510" title="Social constructionism">
Social constructionism

Social constructionism is a theory of knowledge in sociology and communication theory that examines the development of jointly constructed understandings of the world that form the basis for shared assumptions about reality. The theory centers on the notion that meanings are developed in coordination with others rather than separately within each individual.

Social constructionism questions what is defined by humans and society to be reality. Therefore, social constructs can be different based on the society and the events surrounding the time period in which they exist. An example of a social construct is money or the concept of currency, as people in society have agreed to give it importance/ value. Another example of a social construction is the concept of self/ self-identity. Charles Cooley stated based on his Looking-Glass-Self theory: "I am not who you think I am; I am not who I think I am; I am who I think you think I am." This demonstrates how people in society construct ideas or concepts that may not exist without the existence of people or language to validate those concepts.

There are weak and strong social constructs. Weak social constructs rely on brute facts (which are fundamental facts that are difficult to explain or understand, such as quarks) or institutional facts (which are formed from social conventions). Strong social constructs rely on the human perspective and knowledge that does not just exist, but is rather constructed by society.

A social construct or construction concerns the meaning, notion, or connotation placed on an object or event by a society, and adopted by the inhabitants of that society with respect to how they view or deal with the object or event. In that respect, a social construct as an idea would be widely accepted as natural by the society.

A major focus of social constructionism is to uncover the ways in which individuals and groups participate in the construction of their perceived social reality. It involves looking at the ways social phenomena are developed, institutionalized, known, and made into tradition by humans.

In terms of background, social constructionism is rooted in "symbolic interactionism" and "phenomenology." With Berger and Luckmann's "The Social Construction of Reality" published in 1966, this concept found its hold. More than four decades later, a sizable number of theory and research pledged to the basic tenet that people "make their social and cultural worlds at the same time these worlds make them." It is a viewpoint that uproots social processes "simultaneously playful and serious, by which reality is both revealed and concealed, created and destroyed by our activities." It provides a substitute to the "Western intellectual tradition" where the researcher "earnestly seeks certainty in a representation of reality by means of propositions."

In social constructionist terms, "taken-for-granted realities" are cultivated from "interactions between and among social agents;" furthermore, reality is not some objective truth "waiting to be uncovered through positivist scientific inquiry." Rather, there can be "multiple realities that compete for truth and legitimacy." Social constructionism understands the "fundamental role of language and communication" and this understanding has "contributed to the linguistic turn" and more recently the "turn to discourse theory." The majority of social constructionists abide by the belief that "language does not mirror reality; rather, it constitutes [creates] it."

A broad definition of social constructionism has its supporters and critics in the organizational sciences. A constructionist approach to various organizational and managerial phenomena appear to be more commonplace and on the rise.

Andy Lock and Tomj Strong trace some of the fundamental tenets of social constructionism back to the work of the 18th-century Italian political philosopher, rhetorician, historian, and jurist Giambattista Vico.

Berger and Luckmann give credit to Max Scheler as a large influence as he created the idea of Sociology of knowledge which influenced social construction theory.

According to Lock and Strong, other influential thinkers whose work has affected the development of social constructionism are: Edmund Husserl, Alfred Schutz, Maurice Merleau-Ponty, Martin Heidegger, Hans-Georg Gadamer, Paul Ricoeur, Jürgen Habermas, Emmanuel Levinas, Mikhail Bakhtin, Valentin Volosinov, Lev Vygotsky, George Herbert Mead, Ludwig Wittgenstein, Gregory Bateson, Harold Garfinkel, Erving Goffman, Anthony Giddens, Michel Foucault, Ken Gergen, Mary Gergen, Rom Harre, and John Shotter..

Since its appearance in the 1950s, personal construct psychology (PCP) has mainly developed as a constructivist theory of personality and a system of transforming individual meaning-making processes, largely in therapeutic contexts. It was based around the notion of persons as scientists who form and test theories about their worlds. Therefore, it represented one of the first attempts to appreciate the constructive nature of experience and the meaning persons give to their experience. Social constructionism (SC), on the other hand, mainly developed as a form of a critique, aimed to transform the oppressing effects of the social meaning-making processes. Over the years, it has grown into a cluster of different approaches, with no single SC position. However, different approaches under the generic term of SC are loosely linked by some shared assumptions about language, knowledge, and reality.

A usual way of thinking about the relationship between PCP and SC is treating them as two separate entities that are similar in some aspects, but also very different in others. This way of conceptualizing this relationship is a logical result of the circumstantial differences of their emergence. In subsequent analyses these differences between PCP and SC were framed around several points of tension, formulated as binary oppositions: personal/social; individualist/relational; agency/structure; constructivist/constructionist. Although some of the most important issues in contemporary psychology are elaborated in these contributions, the polarized positioning also sustained the idea of a separation between PCP and SC, paving the way for only limited opportunities for dialogue between them.

Reframing the relationship between PCP and SC may be of use in both the PCP and the SC communities. On one hand, it extends and enriches SC theory and points to benefits of applying the PCP “toolkit” in constructionist therapy and research. On the other hand, the reframing contributes to PCP theory and points to new ways of addressing social construction in therapeutic conversations.

Like social constructionism, social constructivism states that people work together to construct artifacts. While social constructionism focuses on the artifacts that are created through the social interactions of a group, social constructivism focuses on an individual's learning that takes place because of his or her interactions in a group.

Social constructivism has been studied by many educational psychologists, who are concerned with its implications for teaching and learning. For more on the psychological dimensions of social constructivism, see the work of Ernst von Glasersfeld and A. Sullivan Palincsar.

Systemic therapy is a form of psychotherapy which seeks to address people as people in relationship, dealing with the interactions of groups and their interactional patterns and dynamics.

A bibliographic review of social constructionism as used within communication studies was published in 2016. It features a good overview of resources from that disciplinary perspective.

The concepts of "weak" and strong as applied to opposing philosophical positions, "isms", inform a teleologythe goal-oriented, meaningful or "final end" of an interpretation of reality. "Isms" are not personal opinions, but the extreme, modal, formulations that actual persons, individuals, can then consider, and take a position between. There are opposing philosophical positions concerning the feasibility of co-creating a common, shared, social reality, called "weak" and strong.

John R. Searle does not elucidate the terms strong and "weak" in his book "The Construction of Social Reality", but he clearly uses them in his Chinese room argument, where he debates the feasibility of creating a computing machine with a sharable understanding of reality, and he adds "We are precisely such machines." Strong artificial intelligence (Strong AI) is the bet that computer programmers will somehow eventually achieve a computing machine with a mind of its own, and that it will eventually be more powerful than a human mind. Weak AI bets they won't.

David Deutsch in his book "The Fabric of Reality" uses a form of strong Turing principle to share Frank Tipler's view of the final state of the universe as an omnipotent (but not omniscient), Omega point, computer. "But this computer is a society of creative thinkers, or people" (albeit posthuman transhuman persons), having debates in order to generate information, in the never-ending attempt to attain omniscience of this physicsits evolutionary forms, its computational abilities, and the methods of its epistemologyhaving an eternity to do so. (p. 356)

Because both the Chinese room argument and the construction of social reality deal with Searle and his debates, and because they both use "weak" and strong to denote a philosophical position, and because both debate the programmability of "the other", it is worth noting the correspondence that "strong AI" is strong social constructionism, and "weak AI" is weak social constructivism.

Strong social constructiv"ism" says "none are able to "communicate" either a full reality or an accurate ontology, therefore my position must impose, by a sort of divine right, my observer-relative epistemology", whereas weak social constructiv"ism" says "none are able to "know" a full reality, therefore "we must" cooperate, informing and conveying an objective ontology as best we can."

"Weak social constructionism" sees the underlying, objective, "brute fact" elements of the class of languages and functional assignments of human, metaphysical, reality. Brute facts are all facts that are not institutional facts (e.g., metaphysical, social agreement). The skeptic portrays the "weak" aspect of social constructivism, and wants to spend effort debating the institutional realities.

Harvard psychologist Steven Pinker writes that "some categories really are social constructions: they exist only because people tacitly agree to act as if they exist. Examples include money, tenure, citizenship, decorations for bravery, and the presidency of the United States."

In a similar vein, Stanley Fish has suggested that baseball's "balls and strikes" are social constructions.

Both Fish and Pinker agree that the sorts of objects indicated here can be described as part of what John Searle calls "social reality." In particular, they are, in Searle's terms, ontologically subjective but epistemologically objective. "Social facts" are temporally, ontologically, and logically dependent on "brute facts." For example, "money" in the form of its raw materials (rag, pulp, ink) as constituted socially for barter (for example by a banking system) is a social fact of "money" by virtue of (i) collectively willing and intending (ii) to impose some particular function (purpose for which), (iii) by constitutive rules atop the "brute facts." "Social facts have the remarkable feature of having no analogue among physical brute facts" (34). The existence of language is itself constitutive of the social fact (37), which natural or brute facts do not require. Natural or "brute" facts exist independently of language; thus a "mountain" is a mountain in every language and in no language; it simply is what it is.

Searle illustrates the evolution of social facts from brute facts by the constitutive rule: X counts as Y in C. "The Y terms has to assign a new "status" that the object does not already have just in virtue of satisfying the Y term; and there has to be collective agreement, or at least acceptance, both in the imposition of that status on the stuff referred to by the X term and about the function that goes with that status. Furthermore, because the physical features brute facts specified by the X term are insufficient by themselves to guarantee the fulfillment of the assigned function specified by the Y term, the new status and its attendant functions have to be the sort of things that can be constituted by collective agreement or acceptance."

It is true or false that language is not a "brute fact," that it is an institutional fact, a human convention, a metaphysical reality (that happens to be physically uttered), but Searle points out that there are language-independent thoughts "noninstitutional, primitive, biological inclinations and cognitions not requiring any linguistic devices," and that there are many "brute facts" amongst both humans and animals that are truths that should not be altered in the social constructs because language does not truly constitute them, despite the attempt to institute them for any group's gain: money and property are language dependent, but desires (thirst, hunger) and emotions (fear, rage) are not. (In "Meditations on First Philosophy" Rene Descartes describes the difference between imagination as a sort of vision, or image, and intellect as conceptualizing things by symbolic manipulation.) Therefore, there is doubt that society or a computer can be completely programmed by language and images, (because there is a programmable, emotive effect of images that derives from the language of judgment towards images).

Finally, against the strong theory and for the weak theory, Searle insists, "it could not be the case, as some have maintained, that all facts are institutional [i.e., social] facts, that there are no brute facts, because the structure of institutional facts reveals that they are logically dependent on brute facts. To suppose that all facts are institutional [i.e., social] would produce an infinite regress or circularity in the account of institutional facts. In order that some facts are institutional, there must be other facts that are brute [i.e., physical, biological, natural]. This is the consequence of the logical structure of institutional facts.".

Ian Hacking, Canadian philosopher of science, insists, "the notion that everything is socially constructed has been going the rounds. John Searle [1995] argues vehemently (and in my opinion cogently) against universal constructionism." "Universal social constructionism is descended from the doctrine that I once named linguistic idealism and attributed, only half in jest, to Richard Nixon [Hacking, 1975, p. 182]. Linguistic idealism is the doctrine that only what is talked about exists, nothing has reality until it is spoken of, or written about. This extravagant notion is descended from Berkeley's idea-ism, which we call idealism: the doctrine that all that exists is mental." "They are a part of what John Searle [1995] calls social reality. His book is titled the "Construction of Social Reality," and as I explained elsewhere [Hacking, 1996], that is not a "social" construction book at all."

Hacking observes, "the label 'social constructionism' is more code than description" of every Leftist, Marxist, Freudian, and Feminist PostModernist to call into question every moral, sex, gender, power, and deviant claim as just another essentialist claim—including the claim that members of the male and female sex are inherently different, rather than historically and socially constructed. Hacking observes that his 1995 simplistic dismissal of the concept actually revealed to many readers the outrageous implications of the theorists: Is child abuse a real evil, or a social construct, asked Hacking? His dismissive attitude, "gave some readers a way to see that there need be no clash between construction and reality," inasmuch as "the metaphor of social construction once had excellent shock value, but now it has become tired."

Informally, they require human practices to sustain their existence, but they have an effect that is (basically) universally agreed upon. The disagreement lies in whether this category should be called "socially constructed." argues that it should not. Furthermore, it is not clear that authors who write "social construction" analyses ever mean "social construction" in Pinker's sense. If they never do, then Pinker (probably among others) has misunderstood the point of a social constructionist argument.

To understand how weak social constructionism can conclude that metaphysics (a human affair) is not the entire "reality," see the arguments against the study of metaphysics. This inability to accurately share the "full" reality, even given time for a rational conversation, is similarly proclaimed by "weak artificial intelligence".

Constructionism became prominent in the U.S. with Peter L. Berger and Thomas Luckmann's 1966 book, "The Social Construction of Reality". Berger and Luckmann argue that all knowledge, including the most basic, taken-for-granted common sense knowledge of everyday reality, is derived from and maintained by social interactions. When people interact, they do so with the understanding that their respective perceptions of reality are related, and as they act upon this understanding their common knowledge of reality becomes reinforced. Since this common sense knowledge is negotiated by people, human typifications, significations and institutions come to be presented as part of an objective reality, particularly for future generations who were not involved in the original process of negotiation. For example, as parents negotiate rules for their children to follow, those rules confront the children as externally produced "givens" that they cannot change. Berger and Luckmann's social constructionism has its roots in phenomenology. It links to Heidegger and Edmund Husserl through the teaching of Alfred Schutz, who was also Berger's PhD adviser.

During the 1970s and 1980s, social constructionist theory underwent a transformation as constructionist sociologists engaged with the work of Michel Foucault and others as a narrative turn in the social sciences was worked out in practice. This particularly affected the emergent sociology of science and the growing field of science and technology studies. In particular, Karin Knorr-Cetina, Bruno Latour, Barry Barnes, Steve Woolgar, and others used social constructionism to relate what science has typically characterized as objective facts to the processes of social construction, with the goal of showing that human subjectivity imposes itself on those facts we take to be objective, not solely the other way around. A particularly provocative title in this line of thought is Andrew Pickering's "Constructing Quarks: A Sociological History of Particle Physics". At the same time, Social Constructionism shaped studies of technology – the Sofield, especially on the Social construction of technology, or SCOT, and authors as Wiebe Bijker, Trevor Pinch, Maarten van Wesel, etc. Despite its common perception as objective, mathematics is not immune to social constructionist accounts. Sociologists such as Sal Restivo and Randall Collins, mathematicians including Reuben Hersh and Philip J. Davis, and philosophers including Paul Ernest have published social constructionist treatments of mathematics.

Social constructionism can be seen as a source of the postmodern movement, and has been influential in the field of cultural studies. Some have gone so far as to attribute the rise of cultural studies (the cultural turn) to social constructionism. Within the social constructionist strand of postmodernism, the concept of socially constructed reality stresses the ongoing mass-building of worldviews by individuals in dialectical interaction with society at a time. The numerous realities so formed comprise, according to this view, the imagined worlds of human social existence and activity, gradually crystallized by habit into institutions propped up by language conventions, given ongoing legitimacy by mythology, religion and philosophy, maintained by therapies and socialization, and subjectively internalized by upbringing and education to become part of the identity of social citizens.

In the book "The Reality of Social Construction", the British sociologist Dave Elder-Vass places the development of social constructionism as one outcome of the legacy of postmodernism. He writes "Perhaps the most widespread and influential product of this process [coming to terms with the legacy of postmodernism] is social constructionism, which has been booming [within the domain of social theory] since the 1980s."

Social constructionism falls toward the nurture end of the spectrum of the larger nature and nurture debate. Consequently, critics have argued that it generally ignores the contribution made by physical and biological sciences. It particularly denies the influences of biology on behaviour and culture, or suggests that they are unimportant to achieve an understanding of human behaviour. The view of most psychologists and social scientists is that behaviour is a complex outcome of both biological and cultural influences. Other disciplines, such as evolutionary psychology, behaviour genetics, behavioural neuroscience, epigenetics, etc., take a nature–nurture interactionism approach to understand behaviour or cultural phenomena.

In 1996, to illustrate what he believed to be the intellectual weaknesses of social constructionism and postmodernism, physics professor Alan Sokal submitted an article to the academic journal "Social Text" deliberately written to be incomprehensible but including phrases and jargon typical of the articles published by the journal. The submission, which was published, was an experiment to see if the journal would "publish an article liberally salted with nonsense if (a) it sounded good and (b) it flattered the editors' ideological preconceptions." The Postmodernism Generator is a computer program that is designed to produce similarly incomprehensible text. In 1999, Sokal, with coauthor Jean Bricmont published the book "Fashionable Nonsense", which criticized postmodernism and social constructionism.

Philosopher Paul Boghossian has also written against social constructionism. He follows Ian Hacking's argument that many adopt social constructionism because of its potentially liberating stance: if things are the way that they are only because of our social conventions, as opposed to being so naturally, then it should be possible to change them into how we would rather have them be. He then states that social constructionists argue that we should refrain from making absolute judgements about what is true and instead state that something is true in the light of this or that theory. Countering this, he states:
Later in the same work, Boghossian severely constrains the requirements of relativism. He states that instead of believing that any world view is just as true as any other (cultural relativism), we should believe that:
Woolgar and Pawluch argue that constructionists tend to 'ontological gerrymander' social conditions in and out of their analysis.

Social constructionism has been criticized by psychologists such as University of Toronto Professor Jordan Peterson and evolutionary psychologists, including Steven Pinker in his book "The Blank Slate". John Tooby and Leda Cosmides used the term "standard social science model" to refer to social-science philosophies that they argue fail to take into account the evolved properties of the brain.





</doc>
<doc id="40913941" url="https://en.wikipedia.org/wiki?curid=40913941" title="Intrusiveness">
Intrusiveness

Intrusiveness can refer to a behavior, act, state or disposition towards being intrusive, interrupting and disturbing to others. Intrusiveness is typically unwelcome and recipients of intrusive behavior may feel like the intruder is coming without welcome or invitation, invading their personal space, or interfering in their private life. People who are introverted may be more likely to experience the feeling of being intruded upon.

There are many interjections, idioms and phrases which are related to intrusiveness, such as "mind your own business" or "being nosey". Nouns for people who are associated with intrusive behavior include snooper, interferer, interrupter, intruder, interposer, invader, intervener, intervenist, interventionist, pryer, stickybeak, gatecrasher, interloper, peeping tom, persona non grata, encroacher, backseat driver, kibitzer, meddler, nosy parker, marplot, gossipmonger and yenta. There are also some more derisive terms such as buttinsky or busybody. Intrusiveness can some at the hands of a political administration where it may be described as a nanny state or mass surveillance, but can also be derived from oneself or by other individuals such as family members, friends, associates or strangers. Such an occurrence may culminate into feelings of embarrassment.


</doc>
<doc id="47172" url="https://en.wikipedia.org/wiki?curid=47172" title="Misanthropy">
Misanthropy

Misanthropy is the general hatred, dislike, distrust or contempt of the human species or human nature. A misanthrope or misanthropist is someone who holds such views or feelings. The word's origin is from the Greek words μῖσος ("misos", "hatred") and ἄνθρωπος ("anthrōpos", "man, human"). The condition is often confused with asociality.

Gustave Flaubert once declared that he would "die of suppressed rage at the folly of [his] fellow men." Misanthropy has also been ascribed to a number of writers of satire, such as William S. Gilbert ("I hate my fellow-man") and William Shakespeare ("Timon of Athens"). Jonathan Swift is widely believed to have been misanthropic (see "A Tale of a Tub" and, most especially, Book IV of "Gulliver's Travels"). Poet Philip Larkin has been described as a misanthrope.

Molière's play "The Misanthrope" is one of the more famous French plays on this topic. Less famous, but more contemporary is the 1971 play by Françoise Dorin, "Un sale égoïste" (A Filthy Egoist) which takes the point of view of the misanthrope and entices the viewer to understand his motives.

Michelangelo has been called a misanthrope. Don Van Vliet (commonly known as Captain Beefheart) has been described as a misanthrope, with close friend Kristine McKenna stating that he "thought human beings were the worst species that was ever dreamed up". Morrissey, a songwriter, has been dubbed "pop's most famous misanthrope".

Fernando Pessoa's "factless autobiography" "The Book of Disquiet" has been described as misanthropic. 

The pre-Socratic philosopher Heraclitus was by various accounts a misanthrope and a loner who had little patience for human society. In a fragment, the philosopher complained that "people [were] forever without understanding" of what was, in his view, the nature of reality.

In Western philosophy, misanthropy has been connected to isolation from human society. In Plato's "Phaedo", Socrates describes a misanthrope in relation to his fellow man: "Misanthropy develops when without art one puts complete trust in somebody thinking the man absolutely true and sound and reliable and then a little later "discovers" him to be bad and unreliable ... and when it happens to someone often ... he ends up ... hating everyone." Misanthropy, then, is presented as a potential result of thwarted expectations or even excessively naïve optimism, since Plato argues that "art" would have allowed the potential misanthrope to recognize that the majority of men are to be found in between good and evil. Aristotle follows a more ontological route: the misanthrope, as an essentially solitary man, is not a man at all: he must be a beast or a god, a view reflected in the Renaissance view of misanthropy as a "beast-like state".

There is a difference between philosophical pessimism and misanthropy. Immanuel Kant said that "Of the crooked timber of humanity, no straight thing can ever be made", and yet this was not an expression of the uselessness of mankind itself. Kant further stated that hatred of mankind can take two distinctive forms: aversion from men (Anthropophobia) and enmity towards them. The condition can arise partly from dislike and partly from ill-will.

Martin Heidegger has also been said to show misanthropy in his concern of the "they"—the tendency of people to conform to one view, which no one has really thought through, but is just followed because, "they say so". This might be thought of as more a criticism of conformity than of people in general. Unlike Schopenhauer, Heidegger was opposed to any systematic ethics; however, in some of his later thought, he does see the possibility of harmony between people, as part of the four-fold, mortals, gods, earth, and sky.

In early Islamic philosophy, certain freethinkers such as Ibn al-Rawandi and Muhammad ibn Zakariya ar-Razi often expressed misanthropic views. In the Judeo-Islamic philosophies (800 - 1400), the Jewish philosopher, Saadia Gaon, uses the Platonic idea that the self-isolated man is dehumanized by friendlessness to argue against the misanthropy of anchorite asceticism and reclusiveness.


</doc>
<doc id="21766531" url="https://en.wikipedia.org/wiki?curid=21766531" title="Spotlight effect">
Spotlight effect

The spotlight effect is the phenomenon in which people tend to believe they are being noticed more than they really are. Being that one is constantly in the center of one's own world, an "accurate "evaluation of how much one is noticed by others is uncommon. The reason behind the spotlight effect comes from the innate tendency to forget that although one is the center of one's own world, one is not the center of everyone else's. This tendency is especially prominent when one does something atypical.

Research has empirically shown that such drastic over-estimation of one's effect on others is widely common. Many professionals in social psychology encourage people to be conscious of the spotlight effect and to allow this phenomenon to moderate the extent to which one believes one is in a social spotlight.

The term "spotlight effect" was coined by Thomas Gilovich and Kenneth Savitsky. The phenomenon made its first appearance in the world of psychology in the journal "Current Directions in Psychological Science" in 1999. Although this was the first time the effect was termed, it was not the first time it had been described. There were other studies done before 1999 that had looked at phenomena similar to the spotlight effect that Gilovich and Savitsky described. Thomas Gilovich had been studying this phenomenon for many years and wrote other research papers in the years leading up to his work with Savitsky. In his study with Savitsky, he combined the different effects he had observed previously to describe the spotlight.Gilovich was not the only one who had noticed this occurrence of the spotlight effect. David Kenny and Bella DePaulo conducted a study that looked at whether or not people knew how others view them. Kenny and DePaulo thought that individuals would base what others thought of them using their own self-perceptions rather than other feedback given to them. The study found that individuals' views of what others think of them is variable compared to what is actually thought of them.

The spotlight effect is an extension of several psychological phenomena. Among these is the phenomenon known as "anchoring and adjustment," which suggests that individuals will use their own internal feelings of anxiety and the accompanying self-representation as an anchor, then insufficiently correct for the fact that others are less privy to those feelings than they are themselves. Consequently, they overestimate the extent to which their anxiety is obvious to onlookers. In fact, Clark and Wells (1995) suggest that socially phobic people enter social situations in a heightened self-focused state, namely, from a raised emotional anchor. This self-focused state makes it difficult for individuals to set aside public and private self-knowledge to focus on the task.

Another related phenomenon is called the "false-consensus effect". The false-consensus effect occurs when individuals overestimate the extent to which other people share their opinions, attitudes, and behavior. This leads to a false conclusion which will increase someone's self-esteem. The false-consensus effect is the opposing theory to the "false uniqueness effect", which is the tendency of one to underestimate the extent to which others share the same positive attitudes and behavior. Either of these effects can be applied to the spotlight effect.

The "self-as-target bias" is another closely linked phenomenon with the spotlight effect. This concept describes when someone believes that events are disproportionately directed towards him or herself. For example, if a student had an assignment due in class and did not prepare as well as they should have, the student may start to panic and think that simply because they did not prepare well, the teacher will know and call on them for answers.

Also relevant to the spotlight effect is the "illusion of transparency" (sometimes called the "observer's illusion of transparency)", which is people's tendency to overestimate the degree to which their personal mental state is known by others. Another manifestation of the illusion of transparency is a tendency for people to overestimate how well they understand others' personal mental states. This cognitive bias is similar to the "illusion of asymmetric insight", in which people perceive their knowledge of others to surpass other people's knowledge of themselves.

Other related concepts are egocentric bias, self-referential encoding, self-reference effect and Ideas of reference and delusions of reference.

The spotlight effect plays a significant role in many different aspects of psychology and society. Primarily, research on this phenomenon has been pioneered by four individuals: Thomas Gilovich, Kenneth Savitsky, Victoria Medvec, and Thomas Kruger. The main focuses of their research center around social judgments, salience of individual contributions, actions of individuals, and how individuals believe others perceive them.

In social judgment, embarrassment plays a considerable role in the degree to which the spotlight effect is manifested. Research by Gilovich, Kruger, and Medvec indicated that certain situations in which perceivably embarrassing items are factors, such as an embarrassing t-shirt, increase the extent to which the spotlight effect is experienced by an individual. The timing of the exposure during a perceivably embarrassing situation also plays a role in the severity of the spotlight effect. If the exposure is immediate, the spotlight effect significantly increases in decision making scenarios. Delayed exposure, however, decreases spotlight effect intensity.

Salience of ideas and important contributions within a group are additional aspects of social judgment that are affected by the spotlight effect. Individuals tend to overestimate the extent to which their contributions make an impact on those around them. In a group setting, those contributions are thought of by the individual as being more significant than the contributions of their group members and that the other members believe the same about that individual's contributions.

Actions of individuals and how they believe others perceive their performance also plays an important part of spotlight effect research. Gilovich, Medvec, and Savitsky further explored this idea. In situations that involve large, interacting groups, a common detail identifies the reason attention of others is not solely focused on the individual. In these settings, like a class lecture or athletic competition, attention is divided between focusing on the individual and on the actions of the group. The inability to identify the split attention leads individuals to overestimate the likelihood that their peers will perceive them poorly.

Similarly, Gilovich, Medvec, and Savitsky further elaborated upon their research and concluded that in situations involving an audience member whose sole purpose is to observe, the severity of the spotlight effect is not overestimated because the focus of an audience's attention is centered upon the individual performing.


</doc>
<doc id="231353" url="https://en.wikipedia.org/wiki?curid=231353" title="Sycophancy">
Sycophancy

Sycophancy is flattery that is very obedient, or an indication of deference to another, to an excessive or servile degree. A user of sycophancy is referred to as a sycophant or a “yes-man.”

Alternative phrases are often used such as:


</doc>
<doc id="41894950" url="https://en.wikipedia.org/wiki?curid=41894950" title="National Procrastination Week">
National Procrastination Week

National Procrastination Week is a national holiday devoted to procrastination and putting-off important tasks. It is an annual event that takes place during the first two weeks of March, but, in spirit of the holiday, the specific dates change annually.

There are several expressed goals for the week. The first is to celebrate the act of procrastinating by leaving necessary tasks to be done at a later time. There are, however, other purposes for the holiday. One claim is that the week of putting-off provides a mental and emotional break causing a decrease in stress and anxiety. However the holiday does not advocate sloth and inaction. Instead it places emphasis on accomplishing tasks and leisurely activities that could not be accomplished while one had other responsibilities. These may include reading, cooking, cleaning, and exercising.

There is a significant amount of opposition to and disagreement with the holiday. Mostly, those against the holiday state that the week promotes negative and self-destructive behavior. One article expressed "we could even add a week for problem drinkers as well. In fact, I'm sure we could find something for at least one week a month to celebrate all the flavors of our self-regulation failure". Many claim that even in a week, procrastination can develop into a difficult-to-change habit. The opposition claims that through procrastination, one harms all aspects of their lives, at work, at home, and in health.



</doc>
<doc id="34534118" url="https://en.wikipedia.org/wiki?curid=34534118" title="Behavioral urbanism">
Behavioral urbanism

Behavioral urbanism and its related area of study, behavioral architecture, is an interdisciplinary field focused on the interaction between humans and the built environment, studying the effects of social, cognitive, and emotional factors in understanding the spatial behavior of individuals.



</doc>
<doc id="24499728" url="https://en.wikipedia.org/wiki?curid=24499728" title="Psychological manipulation">
Psychological manipulation

Psychological manipulation is a type of social influence that aims to change the behavior or perception of others through indirect, deceptive, or underhanded tactics. By advancing the interests of the manipulator, often at another's expense, such methods could be considered exploitative and devious.

Social influence is not necessarily negative. For example, people such as friends, family and doctors, can try to persuade to change clearly unhelpful habits and behaviors. Social influence is generally perceived to be harmless when it respects the right of the influenced to accept or reject it, and is not unduly coercive. Depending on the context and motivations, social influence may constitute underhanded manipulation.

According to psychology author George K. Simon, successful psychological manipulation primarily involves the manipulator:

Consequently, the manipulation is likely to be accomplished through covert aggressive means. 

Harriet B. Braiker (2004) identified the following ways that manipulators control their victims:

Simon identified the following manipulative techniques:


According to Braiker's self-help book, manipulators exploit the following vulnerabilities (buttons) that may exist in victims:

According to Simon, manipulators exploit the following vulnerabilities that may exist in victims:

Manipulators generally take the time to scope out the characteristics and vulnerabilities of their victims.

Kantor advises in his book "The Psychopathology of Everyday Life: How Antisocial Personality Disorder Affects All of Us" that vulnerability to psychopathic manipulators involves being too:

Manipulators can have various possible motivations, including but not limited to:

Being manipulative is in Factor 1 of the Hare Psychopathy Checklist (PCL).

The workplace psychopath may often rapidly shift between emotions – used to manipulate people or cause high anxiety.

The authors of the book "" describe a five phase model of how a typical workplace psychopath climbs to and maintains power. In phase three (manipulation) - the psychopath will create a scenario of "psychopathic fiction" where positive information about themselves and negative disinformation about others will be created, where your role as a part of a network of pawns or patrons will be utilised and you will be groomed into accepting the psychopath's agenda.

Corporate jargon, variously known as corporate speak, corporate lingo, business speak, business jargon, management speak, workplace jargon, or commercialese, is the jargon often used in large corporations, bureaucracies, and similar workplaces.[1][2] The use of corporate jargon, also known as "corporatese", is criticised for its lack of clarity as well as for its tedium, making meaning and intention opaque and understanding difficult.
Sophism In modern usage sophist and sophistry are redefined and used disparagingly. A sophism is a specious argument for displaying ingenuity in reasoning or for deceiving someone. A sophist is a person who reasons with clever but fallacious and deceptive arguments.

Impression Management (unethical). In business, "managing impressions" normally "involves someone trying to control the image that a significant stakeholder has of them". The ethics of impression management has been hotly debated on whether we should see it as an effective self-revelation or as cynical manipulation.

According to Kernberg, antisocial, borderline, and narcissistic personality disorders are all organized at a borderline level of personality organization, and the three share some common characterological deficits and overlapping personality traits, with deceitfulness and exceptional manipulative abilities being the most common traits among antisocial and narcissism. Borderline is emphasized by unintentional and dysfunctional manipulation, but stigma towards borderlines being deceitful still wrongfully persists. Antisocials, borderlines, and narcissists are often pathological liars. Other shared traits may include pathological narcissism, consistent irresponsibility, machiavellianism, lack of empathy, cruelty, meanness, impulsivity, proneness to self-harm and addictions, interpersonal exploitation, hostility, anger and rage, vanity, emotional instability, rejection sensitivity, perfectionism, and the use of primitive defence mechanisms that are pathological and narcissistic. Common narcissistic defences include splitting, denial, projection, projective identification, primitive idealization and devaluation, distortion (including exaggeration, minimization and lies), and omnipotence.

Psychologist Marsha M. Linehan has stated that people with borderline personality disorder often exhibit behaviors which are not truly manipulative, but are erroneously interpreted as such. According to her, these behaviors often appear as unthinking manifestations of intense pain, and are often not deliberate as to be considered truly manipulative. In the DSM-V, manipulation was removed as a defining characteristic of borderline personality disorder.

Manipulative behavior is intrinsic to narcissists, who use manipulation to obtain power and narcissistic supply. Those with antisocial personalities will manipulate for material items, power, revenge, and a wide variety of other reasons.

People with histrionic personality disorder are usually high-functioning, both socially and professionally. They usually have good social skills, despite tending to use them to manipulate others into making them the center of attention.

Machiavellianism is a term that some social and personality psychologists use to describe a person's tendency to be unemotional, uninfluenced by conventional morality and more prone to deceive and manipulate others. In the 1960s, Richard Christie and Florence L. Geis developed a test for measuring a person's level of Machiavellianism (sometimes referred to as the "Machiavelli test").


</doc>
<doc id="24747438" url="https://en.wikipedia.org/wiki?curid=24747438" title="Avoidance coping">
Avoidance coping

In psychology, avoidance/avoidant coping or escape coping is a maladaptive coping mechanism characterized by the effort to avoid dealing with a stressor. "Coping" refers to behaviors that attempt to protect oneself from psychological damage. Alternatives to avoidance coping include modifying or eliminating the conditions that gave rise to the problem and changing the perception of an experience in a way that neutralizes the problem.

Avoidance coping, including social withdrawal, is an aspect of avoidant personality disorder, but not everyone who displays such behaviors meets the definition of having a personality disorder.

Post-traumatic stress disorder may include avoidance coping behavior: PTSD sufferers may draw into themselves, avoiding the trauma and partaking in cognitive or behavioral avoidance coping.

Cognitive behavioral and psychoanalytic therapy are used to help those coping by avoidance to acknowledge, comprehend, and express their emotions. Acceptance and commitment therapy, a behavioral therapy that focuses on breaking down avoidance coping and showing it to be an unhealthy method for dealing with traumatic experiences, is also sometimes used.

Both active-cognitive and active-behavioral coping are used as replacement techniques for avoidance coping. Active-cognitive coping includes changing one's attitude towards a stressful event and looking for any positive impacts. Active-behavioral coping refers taking positive actions after finding out more about the situation.

Literature on coping often classifies coping strategies into two broad categories: approach/active coping and avoidance/passive coping. Approach coping includes behaviors that attempt to reduce stress by alleviating the problem directly, and avoidance coping includes behaviors that reduce stress by distancing oneself from the problem. Traditionally, approach coping has been seen as the healthiest and most beneficial way to reduce stress, while avoidance coping has been associated with negative personality traits, potentially harmful activities, and generally poorer outcomes. However, research has shown that some types of avoidance coping have beneficial outcomes. A study by Long and Haney found that both jogging and relaxation techniques were equally successful at reducing anxiety and increasing feelings of self-efficacy. Therefore, it seems that positive forms of passive coping such as exercise and meditation have qualitatively different outcomes from negative forms such as binge eating and drug use. These positive forms of passive coping may be particularly beneficial for alleviating stress when the individual does not currently have the resources to eliminate the problem directly, indicating the advantage of flexibility when engaging in coping behaviors.



</doc>
<doc id="26789" url="https://en.wikipedia.org/wiki?curid=26789" title="Sexual selection">
Sexual selection

Sexual selection is a mode of natural selection where members of one biological sex choose mates of the other sex to mate with (intersexual selection), and compete with members of the same sex for access to members of the opposite sex (intrasexual selection). These two forms of selection mean that some individuals have better reproductive success than others within a population, either from being more attractive or preferring more attractive partners to produce offspring. For instance, in the breeding season, sexual selection in frogs occurs with the males first gathering at the water's edge and making their mating calls: croaking. The females then arrive and choose the males with the deepest croaks and best territories. In general, males benefit from frequent mating and monopolizing access to a group of fertile females. Females can have a limited number of offspring and maximize the return on the energy they invest in reproduction.

The concept was first articulated by Charles Darwin and Alfred Russel Wallace who described it as driving species adaptations and that many organisms had evolved features whose function was deleterious to their individual survival, and then developed by Ronald Fisher in the early 20th century. Sexual selection can, typically, lead males to extreme efforts to demonstrate their fitness to be chosen by females, producing sexual dimorphism in secondary sexual characteristics, such as the ornate plumage of birds such as birds of paradise and peafowl, or the antlers of deer, or the manes of lions, caused by a positive feedback mechanism known as a Fisherian runaway, where the passing-on of the desire for a trait in one sex is as important as having the trait in the other sex in producing the runaway effect. Although the sexy son hypothesis indicates that females would prefer male offspring, Fisher's principle explains why the sex ratio is 1:1 almost without exception. Sexual selection is also found in plants and fungi.

The maintenance of sexual reproduction in a highly competitive world is one of the major puzzles in biology given that asexual reproduction can reproduce much more quickly as 50% of offspring are not males, unable to produce offspring themselves. Many non-exclusive hypotheses have been proposed, including the positive impact of an additional form of selection, sexual selection, on the probability of persistence of a species.

Sexual selection was first proposed by Charles Darwin in "The Origin of Species" (1859) and developed in "The Descent of Man and Selection in Relation to Sex" (1871), as he felt that natural selection alone was unable to account for certain types of non-survival adaptations. He once wrote to a colleague that "The sight of a feather in a peacock's tail, whenever I gaze at it, makes me sick!" His work divided sexual selection into male-male competition and female choice.

These views were to some extent opposed by Alfred Russel Wallace, mostly after Darwin's death. He accepted that sexual selection could occur, but argued that it was a relatively weak form of selection. He argued that male-male competitions were forms of natural selection, but that the "drab" peahen's coloration is itself adaptive as camouflage. In his opinion, ascribing mate choice to females was attributing the ability to judge standards of beauty to animals (such as beetles) far too cognitively undeveloped to be capable of aesthetic feeling.

Ronald Fisher, the English statistician and evolutionary biologist developed a number of ideas about sexual selection in his 1930 book "The Genetical Theory of Natural Selection" including the sexy son hypothesis and Fisher's principle. The Fisherian runaway describes how sexual selection accelerates the preference for a specific ornament, causing the preferred trait and female preference for it to increase together in a positive feedback runaway cycle. In a remark that was not widely understood for another 50 years he said:

This causes a dramatic increase in both the male's conspicuous feature and in female preference for it, resulting in marked sexual dimorphism, until practical physical constraints halt further exaggeration. A positive feedback loop is created, producing extravagant physical structures in the non-limiting sex. A classic example of female choice and potential runaway selection is the long-tailed widowbird. While males have long tails that are selected for by female choice, female tastes in tail length are still more extreme with females being attracted to tails longer than those that naturally occur. Fisher understood that female preference for long tails may be passed on genetically, in conjunction with genes for the long tail itself. Long-tailed widowbird offspring of both sexes inherit both sets of genes, with females expressing their genetic preference for long tails, and males showing off the coveted long tail itself.

Richard Dawkins presents a non-mathematical explanation of the runaway sexual selection process in his book "The Blind Watchmaker". Females that prefer long tailed males tend to have mothers that chose long-tailed fathers. As a result, they carry both sets of genes in their bodies. That is, genes for long tails and for preferring long tails become linked. The taste for long tails and tail length itself may therefore become correlated, tending to increase together. The more tails lengthen, the more long tails are desired. Any slight initial imbalance between taste and tails may set off an explosion in tail lengths. Fisher wrote that:

The female widow bird chooses to mate with the most attractive long-tailed male so that her progeny, if male, will themselves be attractive to females of the next generation - thereby fathering many offspring that carry the female's genes. Since the rate of change in preference is proportional to the average taste amongst females, and as females desire to secure the services of the most sexually attractive males, an additive effect is created that, if unchecked, can yield exponential increases in a given taste and in the corresponding desired sexual attribute.

Since Fisher's initial conceptual model of the 'runaway' process, Russell Lande and Peter O'Donald have provided detailed mathematical proofs that define the circumstances under which runaway sexual selection can take place.

The reproductive success of an organism is measured by the number of offspring left behind, and their quality or probable fitness.

Sexual preference creates a tendency towards assortative mating or homogamy. The general conditions of sexual discrimination appear to be (1) the acceptance of one mate precludes the effective acceptance of alternative mates, and (2) the rejection of an offer is followed by other offers, either certainly, or at such high chance that the risk of non-occurrence is smaller than the chance advantage to be gained by selecting a mate.

The conditions determining which sex becomes the more limited resource in intersexual have been hypothesized with the Bateman's principle, which states that the sex which invests the most in producing offspring becomes a limiting resource over which the other sex competes, illustrated by the greater nutritional investment of an egg in a zygote, and the limited capacity of females to reproduce; for example, in humans, a woman can only give birth every ten months, whereas a male can become a father numerous times.

The sciences of evolutionary psychology, human behavioural ecology, and sociobiology study the influence of sexual selection in humans.

Darwin's ideas on sexual selection were met with scepticism by his contemporaries and not considered of great importance in the early 20th century, until in the 1930s biologists decided to include sexual selection as a mode of natural selection. Only in the 21st century have they become more important in biology. The theory however is generally applicable and analogous to natural selection.

Research in 2015 indicates that sexual selection, including mate choice, "improves population health and protects against extinction, even in the face of genetic stress from high levels of inbreeding" and "ultimately dictates who gets to reproduce their genes into the next generation - so it's a widespread and very powerful evolutionary force." The study involved the flour beetle over a ten-year period where the only changes were in the intensity of sexual selection.

Another theory, the handicap principle of Amotz Zahavi, Russell Lande and W. D. Hamilton, holds that the fact that the male is able to survive until and through the age of reproduction with such a seemingly maladaptive trait is taken by the female to be a testament to his overall fitness. Such handicaps might prove he is either free of or resistant to disease, or that he possesses more speed or a greater physical strength that is used to combat the troubles brought on by the exaggerated trait. Zahavi's work spurred a re-examination of the field, which has produced an ever-accelerating number of theories. In 1984, Hamilton and Marlene Zuk introduced the "Bright Male" hypothesis, suggesting that male elaborations might serve as a marker of health, by exaggerating the effects of disease and deficiency. In 1990, Michael Ryan and A.S. Rand, working with the tungara frog, proposed the hypothesis of "Sensory Exploitation", where exaggerated male traits may provide a sensory stimulation that females find hard to resist. Subsequently, the theories of the "Gravity Hypothesis" by Jordi Moya-Larano et al. (2002), invoking a simple biomechanical model to account for the adaptive value for smaller male spiders of speed in clmbing vertical surfaces, and "Chase Away" by Brett Holland and William R. Rice have been added. In the late 1970s, Janzen and Mary Willson, noting that male flowers are often larger than female flowers, expanded the field of sexual selection into plants.

In the past few years, the field has exploded to include other areas of study, not all of which fit Darwin's definition of sexual selection. These include cuckoldry, nuptial gifts, sperm competition, infanticide (especially in primates), physical beauty, mating by subterfuge, species isolation mechanisms, male parental care, ambiparental care, mate location, polygamy, and homosexual rape in certain male animals.

Focusing on the effect of sexual conflict, as hypothesized by William Rice, Locke Rowe et Göran Arnvist, Thierry Lodé argues that divergence of interest constitutes a key for evolutionary process. Sexual conflict leads to an antagonistic co-evolution in which one sex tends to control the other, resulting in a tug of war. Besides, "the sexual propaganda theory" only argued that mates were opportunistically lead, on the basis of various factors determining the choice such as phenotypic characteristics, apparent vigour of individuals, strength of mate signals, trophic resources, territoriality etc. and could explain the maintenance of genetic diversity within populations.

Several workers have brought attention to the fact that elaborated characters that ought to be costly in one way or another for their bearers (e.g., the tails of some species of Xiphophorus fish) do not always appear to have a cost in terms of energetics, performance or even survival. One possible explanation for the apparent lack of costs is that "compensatory traits" have evolved in concert with the sexually selected traits.

Sexual selection may explain how certain characteristics (such as feathers) had distinct survival value at an early stage in their evolution. Geoffrey Miller proposes that sexual selection might have contributed by creating evolutionary modules such as "Archaeopteryx" feathers as sexual ornaments, at first. The earliest proto-birds such as China's "Protarchaeopteryx", discovered in the early 1990s, had well-developed feathers but no sign of the top/bottom asymmetry that gives wings lift. Some have suggested that the feathers served as insulation, helping females incubate their eggs. But perhaps the feathers served as the kinds of sexual ornaments still common in most bird species, and especially in birds such as peacocks and birds-of-paradise today. If proto-bird courtship displays combined displays of forelimb feathers with energetic jumps, then the transition from display to aerodynamic functions could have been relatively smooth.

Sexual selection sometimes generates features that may help cause a species' extinction, as has been suggested for the giant antlers of the Irish elk ("Megaloceros giganteus") that became extinct in Pleistocene Europe. However, sexual selection can also do the opposite, driving species divergence - sometimes through elaborate changes in genitalia - such that new species emerge.

Sex differences directly related to reproduction and serving no direct purpose in courtship are called primary sexual characteristics. Traits amenable to sexual selection, which give an organism an advantage over its rivals (such as in courtship) without being directly involved in reproduction, are called secondary sex characteristics.

In most sexual species the males and females have different equilibrium strategies, due to a difference in relative investment in producing offspring. As formulated in Bateman's principle, females have a greater initial investment in producing offspring (pregnancy in mammals or the production of the egg in birds and reptiles), and this difference in initial investment creates differences in variance in expected reproductive success and bootstraps the sexual selection processes. Classic examples of reversed sex-role species include the pipefish, and Wilson's phalarope. Also, unlike a female, a male (except in monogamous species) has some uncertainty about whether or not he is the true parent of a child, and so is less interested in spending his energy helping to raise offspring that may or may not be related to him. As a result of these factors, males are typically more willing to mate than females, and so females are typically the ones doing the choosing (except in cases of forced copulations, which can occur in certain species of primates, ducks, and others). The effects of sexual selection are thus held to typically be more pronounced in males than in females.

Differences in secondary sexual characteristics between males and females of a species are referred to as sexual dimorphisms. These can be as subtle as a size difference (sexual size dimorphism, often abbreviated as SSD) or as extreme as horns and colour patterns. Sexual dimorphisms abound in nature. Examples include the possession of antlers by only male deer, the brighter coloration of many male birds in comparison with females of the same species, or even more distinct differences in basic morphology, such as the drastically increased eye-span of the male stalk-eyed fly. The peacock, with its elaborate and colourful tail feathers, which the peahen lacks, is often referred to as perhaps the most extraordinary example of a dimorphism. Male and female black-throated blue warblers and Guianan cock-of-the-rocks also differ radically in their plumage. Early naturalists even believed the females to be a separate species. The largest sexual size dimorphism in vertebrates is the shell dwelling cichlid fish "Neolamprologus callipterus" in which males are up to 30 times the size of females. Many other fish such as guppies also exhibit sexual dimorphism. Extreme sexual size dimorphism, with females larger than males, is quite common in spiders and birds of prey.

Male-male competition occurs when two males of the same species compete for the opportunity to mate with a female. Sexually dimorphic traits, size, sex ratio and the social situation may all play a role in the effects male-male competition has on the reproductive success of a male and the mate choice of a female.

There are multiple types of male-male competition that may occur in a population at different times depending on the conditions. Competition variation occurs based on the frequency of various mating behaviours present in the population. One factor that can influence the type of competition observed is the population density of males. When there is a high density of males present in the population, competition tends to be less aggressive and therefore sneak tactics and disruptions techniques are more often employed. These techniques often indicate a type of competition referred to as scramble competition. In Japanese medaka, "Oryzias latipes", sneaking behaviours refer to when a male interrupts a mating pair during copulation by grasping on to either the male or the female and releasing their own sperm in the hopes of being the one to fertilize the female. Disruption is a technique which involves one male bumping the male that is copulating with the female away just before his sperm is released and the eggs are fertilized.

However, all techniques are not equally successful when in competition for reproductive success. Disruption results in a shorter copulation period and can therefore disrupt the fertilization of the eggs by the sperm, which frequently results in lower rates of fertilization and smaller clutch size.

Another factor that can influence male-male competition is the value of the resource to competitors. Male-male competition can pose many risks to a male's fitness, such as high energy expenditure, physical injury, lower sperm quality and lost paternity. The risk of competition must therefore be worth the value of the resource. A male is more likely to engage in competition for a resource that improves their reproductive success if the resource value is higher. While male-male competition can occur in the presence or absence of a female, competition occurs more frequently in the presence of a female. The presence of a female directly increases the resource value of a territory or shelter and so the males are more likely to accept the risk of competition when a female is present. The smaller males of a species are also more likely to engage in competition with larger males in the presence of a female. Due to the higher level of risk for subordinate males, they tend to engage in competition less frequently than larger, more dominant males and therefore breed less frequently than dominant males. This is seen in many species, such as the Omei treefrog, "Rhacophorus omeimontis," where larger males obtained more mating opportunities and successfully mated with larger mates. 

A third factor that can impact the success of a male in competition is winner-loser effects. Burrowing crickets, "velarifictorous aspersus," compete for burrows to attract females using their large mandibles for fighting. Female burrowing crickets, are more likely to choose winner of a competition in the 2 hours after the fight. The presence winning male suppresses mating behaviours of the losing males because the winning male tends to produce more frequent and enhanced mating calls in this period of time.

Male-male competition can both positively and negatively affect female fitness. When there is a high density of males in a population and a large number of males attempting to mate with the female, she is more likely to resist mating attempts, resulting in lower fertilization rates. High levels of male-male competition can also result in a reduction in female investment in mating. Many forms of competition can also cause significant distress for the female negatively impacting her ability to reproduce. An increase in male-male competition can affect a females ability to select the best mates, and therefore decrease the likelihood of successful reproduction.

However, group mating in Japanese medaka has been shown to positively affect the fitness of females due to an increase in genetic variation, a higher likelihood of paternal care and a higher likelihood of successful fertilization.

In Japanese medaka, females mate daily during mating season. Males compete for the opportunity to mate with the female by displaying themselves aggressively and chasing each other. To obtain the selection of the females, they court the females prior to copulation by performing a courting behaviour referred to as "quick circles".

In leaf-footed cactus bugs, "Narnia femorata", males compete for territories where females can lay their eggs. Males compete more intensely for cacti territories with fruit to attract females. In this species, the males possess sexually dimorphic traits, such as their larger size and hind legs, which are used to gain the most advantage over competitors when females are present, but are not used in the absence of females.

Anuran (such as frogs) select habitats (pools) free of conspecifics in order to minimize male-male competition for both themselves and their offspring. Displays are used in attempt to keep competitors out of their territory and deter sneaking behaviours, while fighting is only used when necessary due to the high costs and risks associated with fighting.


Sexual selection has been observed to occur in plants, animals and fungi. In certain hermaphroditic snail and slug species of molluscs the throwing of love darts is a form of sexual selection. Certain male insects of the lepidoptera order of insects cement the vaginal pores of their females.

Today, biologists say that certain evolutionary traits can be explained by intraspecific competition - competition between members of the same species - distinguishing between competition before or after sexual intercourse.

Before copulation, "intrasexual selection" - usually between males - may take the form of "male-to-male combat". Also, "intersexual selection", or "mate choice", occurs when females choose between male mates. Traits selected by male combat are called secondary sexual characteristics (including horns, antlers, etc.), which Darwin described as "weapons", while traits selected by mate (usually female) choice are called "ornaments". Due to their sometimes greatly exaggerated nature, secondary sexual characteristics can prove to be a hindrance to an animal, thereby lowering its chances of survival. For example, the large antlers of a moose are bulky and heavy and slow the creature's flight from predators; they also can become entangled in low-hanging tree branches and shrubs, and undoubtedly have led to the demise of many individuals. Bright colourations and showy ornamenations, such as those seen in many male birds, in addition to capturing the eyes of females, also attract the attention of predators. Some of these traits also represent energetically costly investments for the animals that bear them. Because traits held to be due to sexual selection often conflict with the survival fitness of the individual, the question then arises as to why, in nature, in which survival of the fittest is considered the rule of thumb, such apparent liabilities are allowed to persist. However, one must also consider that intersexual selection can occur with an emphasis on resources that one sex possesses rather than morphological and physiological differences. For example, males of "Euglossa imperialis", a non-social bee species, form aggregations of territories considered to be leks, to defend fragrant-rich primary territories. The purpose of these aggregations is only facultative, since the more suitable fragrant-rich sites there are, the more habitable territories there are to inhabit, giving females of this species a large selection of males with whom to potentially mate.

After copulation, male–male competition distinct from conventional aggression may take the form of sperm competition, as described by Parker in 1970. More recently, interest has arisen in "cryptic" female choice, a phenomenon of internally fertilised animals such as mammals and birds, where a female can get rid of a male's sperm without his knowledge.

Finally, sexual conflict is said to occur between breeding partners, sometimes leading to an evolutionary arms race between males and females. Sexual selection can also occur as a product of pheromone release, such as with the stingless bee, "Trigona corvina".

Female mating preferences are widely recognized as being responsible for the rapid and divergent evolution of male secondary sexual traits. Females of many animal species prefer to mate with males with external ornaments - exaggerated features of morphology such as elaborate sex organs. These preferences may arise when an arbitrary female preference for some aspect of male morphology — initially, perhaps, a result of genetic drift — creates, in due course, selection for males with the appropriate ornament. One interpretation of this is known as the sexy son hypothesis. Alternatively, genes that enable males to develop impressive ornaments or fighting ability may simply show off greater disease resistance or a more efficient metabolism, features that also benefit females. This idea is known as the good genes hypothesis.

Bright colors that develop in animals during mating season function to attract partners. It has been suggested that there is a causal link between strength of display of ornaments involved in sexual selection and free radical biology. To test this idea, experiments were performed on male painted dragon lizards. Male lizards are brightly conspicuous in their breeding coloration, but their color declines with aging. Experiments involving administration of antioxidants to these males led to the conclusion that breeding coloration is a reflection of innate anti-oxidation capacity that protects against oxidative damage, including oxidative DNA damage. Thus color could act as a “health certificate” that allows females to visualize the underlying oxidative stress induced damage in potential mates.

Darwin conjectured that heritable traits such as beards and hairlessness in different human populations are results of sexual selection in humans. Geoffrey Miller has hypothesized that many human behaviours not clearly tied to survival benefits, such as humour, music, visual art, verbal creativity, and some forms of altruism, are courtship adaptations that have been favoured through sexual selection. In that view, many human artefacts could be considered subject to sexual selection as part of the extended phenotype, for instance clothing that enhances sexually selected traits. Some argue that the evolution of human intelligence is a sexually selected trait, as it would not confer enough fitness in itself relative to its high maintenance costs.




</doc>
<doc id="3753857" url="https://en.wikipedia.org/wiki?curid=3753857" title="Work behavior">
Work behavior

Work behavior is the behavior one uses in employment and is normally more formal than other types of human behavior. This varies from profession to profession, as some are far more casual than others. For example, a computer programmer would usually have far more leeway in their work behavior than a lawyer.

People are usually more careful than outside work in how they behave around their colleagues, as many actions intended to be in jest can be perceived as inappropriate or even harassment in the work environment. In some cases, men may take considerably more care so as not to be perceived as being sexually harassing than they would ordinarily.

Counterproductive work behavior is also a type of work behavior. Most of the people do not know what counterproductive work behavior is. Counterproductive work behavior is the acts that employees have against the organizations that do harm or violate the work production. Some Counterproductive work behavior would include passive actions such as not working to meet date line or faking incompetence. Even people do not recognize this behavior, it seems normal to them. Some examples of counterproductive behavior are:

Intimate partner violence: Intimate partner violence is occurring more often in the workplace. About 36% to 75% of employed women who experience Intimate partner violence have come out reporting that they have been harassed by a significant other while working. A variety of abusive behaviors is being demonstrated against victims to hinder their ability to get to work, get their work done, and stay in their current employment. The interference that the perpetrators employ are: Stocking them at their work site, harassment, and interfering with the victim's work; for example, sabotaging the victim, so they can not get to work.

Boredom: Jobs that require individuals to do the same task on a daily basis can lead to counterproductive behaviors. Boredom on the job could result in unfavorable work practices such as frequently missing work, lack of concentration or withdrawal from the task that the person was hired to do, and thus, leading to a decrease in work efficiency.


These are the examples of counterproductive behavior that people confront in their daily life.

A way to counteract this unproductive behavior is to address the principle that work behavior is function of contingent consequences. By addressing what employees value most in their workplace, boredom on the job can be avoided. Competitive compensation, bonuses and merit-based rewards, retirement plans, supplemental training program and flexible work locations are the top five values that employees value most at their workplace. Recognizing positive and productive behavior at a workplace can be quite simple by using job analysis. This method gives others a better understanding and evaluation of a typical duty they are looking for (see also Industrial & Organizational Assessment).

Sexual harassment occurs when one individual (whether it's a male or female) takes a sexual interest in the other person while at work and try to exploit them. The act of objectifying the target could lead to the feeling of insecurities, and pressures to leave the company. A researched showed that out of 134,200 people in a studied, 65% of men and 93% of women were harassed sexually in the place of work and that efficiency of work was affected due to job turnover and people calling out sick. The study also showed that sexual harassment could lead to people feeling depressed, result in high level of anxiety, and mental and physical stress.

Verbal abuse is a concept that indicates some form of mistreatment via oral expression. Verbal abuse can impact the productivity in the workplace, both for the employee and employer. This type of behavior could lead to the resignation of the employee, poor quality of work, turnovers, and illness. Additionally, there is another type of verbal abuse called mobbing. This is when a group of individuals engages in non-physical abusive behavior at work. This could be expressed in aggressive and unprincipled forms of verbal abuse towards one person. If this behavior continues, the person will eventually feel pressured to quit his/her job due to poor performance.

It is important to resolve any issues that arise at work among team members. Conflict resolution plays a huge role in this. Handling this issues appropriately helps decrease harmful influences of all types of conflicts by bringing back integrity, building success in the work place and restoring efficiency. Working together to resolve conflict resolution lets conflict of different types to be fixed in a way that is beneficial to the group.


</doc>
<doc id="42306483" url="https://en.wikipedia.org/wiki?curid=42306483" title="Denunciation">
Denunciation

Denunciation (from Latin "denuntiare", to denounce) is the act of publicly assigning blame of a perceived wrongdoing to a person with the hopes of bringing attention to it.



</doc>
<doc id="26317569" url="https://en.wikipedia.org/wiki?curid=26317569" title="Maturity (psychological)">
Maturity (psychological)

In psychology, maturity is the ability to respond to the environment 
aware of the correct time and location to behave and knowing when to act, according to the circumstances and the culture of the society one lives in. Adult development and maturity theories include the purpose in life concept, in which maturity emphasizes a clear comprehension of life's purpose, directedness, and intentionality, which contributes to the feeling that life is meaningful. 

The status of maturity is distinguished by the shift away from reliance on guardianship and the oversight of an adult in decision-making acts. Maturity has different definitions across legal, social, religious, political, sexual, emotional, and intellectual contexts. The age or qualities assigned for each of these contexts are tied to culturally-significant indicators of independence that often vary as a result of social sentiments. The concept of psychological maturity has implications across both legal and social contexts, while a combination of political activism and scientific evidence continue to reshape and qualify its definition. Because of these factors, the notion and definition of maturity and immaturity is somewhat subjective.

American psychologist Jerome Bruner proposed the purpose of the period of immaturity as being a time for experimental play without serious consequences, where a young animal can spend a great deal of time observing the actions of skilled others in coordination with oversight by and activity with its mother. The key to human innovation through the use of symbols and tools, therefore, is re-interpretive imitation that is "practiced, perfected, and varied in play" through extensive exploration of the limits on one's ability to interact with the world. Evolutionary psychologists have also hypothesized that cognitive immaturity may serve an adaptive purpose as a protective barrier for children against their own under-developed meta-cognition and judgment, a vulnerability that may put them in harm's way.
For youth today, the steadily extending period of 'play' and schooling going into the 21st century comes as a result of the increasing complexity of our world and its technologies, which too demand an increasing intricacy of skill as well as a more exhaustive set of pre-requisite abilities. Many of the behavioral and emotional problems associated with adolescence may arise as children cope with the increased demands placed on them, demands which have become increasingly abstracted from the work and expectations of adulthood.

Although psychological maturity is specifically grounded in the autonomy of one's decision-making ability, these outcomes are deeply embedded in not only cognition, but also in lifelong processes of emotional, social and moral development. Various theorists have provided frameworks for recognizing the indicators of maturity. Erikson's stages of psychosocial development describe progression into adult maturity, with each maturational stage characterized by a certain kind of psychsocial conflict. The "Identity" stage is characterized as being mainly concerned with issues of role exploration and role confusion, and also the exploration of sexual and other identities. Adolescents navigate a web of conflicting values and selves in order to emerge as 'the person one has come to be' and 'the person society expects one to become'. Erikson did not insist that stages begin and end at globally pre-defined points, but that particular stages such as "Identity" could extend into adulthood for as long as it took to resolve the conflict. Piaget's theory of cognitive development defines the formal operational stage as a plateau reached once an individual can think logically using symbols and is marked by a shift away from "concrete" thought, or thought bound to immediacy and facts, and toward "abstract" thought, or thought employing reflection and deduction. These theories have shaped the investigation of adolescent development and reflect the limitations of cognition prior to adulthood.

While maturity is often termed as a label awarded to a child, research has revealed that children themselves hold a clear sense of their own autonomy and personal jurisdiction. For instance, American elementary-aged school children demonstrated an acknowledgement of the limits of their parents' authority over their choice of dress, hairstyle, friends, hobbies, and media choices. But this constrained earlier concept of personal autonomy later develops into a broader understanding of individual freedoms, with an understanding of freedom of speech as a universal right emerging by elementary school age. However, younger children do have difficulty with maintaining a consistent view on universal rights, with 75% of first-grade children expressing uncertainty about prohibiting freedom of speech in Canada. But this same study also found that 6- to 11-year-old Canadian children rejected nondemocratic systems on the basis of violating principles of majority vote, equal representation, and right to a voice, which provides evidence for an emerging knowledge of political decision-making skills from a young age.

Where maturity is an earned status that often carries responsibilities, immaturity is then defined in contrast by the absence of serious responsibility and in its place is the freedom for unmitigated growth. This period of growth is particularly important for humans, who undergo a unique four-stage pattern of development (infancy, childhood, juvenility, adolescence) that has been theorized to confer a number of evolutionarily competitive benefits (Locke & Bogin, 2006). In infancy, motor development stretches long into the early years of life, necessitating that young infants rely on their mothers almost entirely. This state of helplessness provides for an intensely close bond between infant and mother, where separation is infrequent and babies are rarely out of a caregiver's arms. For non-human primates and all non-human mammalian species the growth of the first permanent molar marks the end of lactation and the beginning of foraging, setting an early requirement for independence. Human children, on the other hand, do not have an advanced motor control capable of foraging and also lack the digestive capacity for unprepared food, and so have always relied on the active involvement of their mother and other caregivers in their care into childhood.

The pre-frontal cortex, which is responsible for higher cognitive functions such as planning, decision-making, judgment and reasoning, develops and matures most rapidly during early adolescence and into the early 20s. Accompanying the growth of the pre-frontal cortex is continued synaptic pruning (the trimming of rarely used synapses) as well as increased myelination of nerve fibers in the brain, which serves to insulate and speed up signal transmission between neurons. The incomplete development of this process contributes to the finding that adolescents use their brain less broadly than do adults when asked to inhibit a response and show less cross-talk (communication across diverse regions of the brain). The brain's "cross-talk" may be related to decision-making concerning risk-taking, with one study of American adolescents finding delayed reaction time and decreased spread across brain regions in a task asking them to determine whether a dangerous action is a good idea or not. Steinberg observes that there is close overlap in the activated brain regions for socioemotional and reward information, which may pose a challenge when making decisions in the most high-risk peer contexts. One study found that preference for small immediate rewards over larger long-term rewards was associated with increased activation with regions primarily responsible for socioemotional decision-making.

The definition and determination of maturity has been applied to the issue of criminal responsibility of juvenile offenders and to a number of legal ages. The age of majority, the most broadly applied legal threshold of adulthood, is typically characterized by recognition of control over oneself and one's actions and decisions. The most common age threshold is 18 years of age, with thresholds ranging from 14 to 21 across nations and between provinces. Although age of majority is referred to as a jurisdiction's legal age, the legal ages of various other issues of legal maturity like sexual consent or drinking and smoking ages are often different from the age of majority.
Aside from age-based thresholds of maturity, restrictions based in a perceived intellectual immaturity also extend to those with a variety of mental impairments (generally defined as anyone with a mental disability that requires guardianship), with laws in place in most regions limiting the voting rights of the mentally disabled and often requiring the judgment of a court to declare fitness. Similar to those restrictions placed on children, persons with mental disabilities also have freedoms restricted and have their rights assigned to parental guardians.

One reason cited for why children and the mentally disabled are not permitted to vote in elections is that they are too intellectually immature to understand voting issues. This view is echoed in concerns about the adult voting population, with observers citing concern for a decrease in 'civic virtue' and 'social capital,' reflecting a generalized panic over the political intelligence of the voting population. Although critics have cited 'youth culture' as contributing to the malaise of modern mass media's shallow treatment of political issues, interviews with youth themselves about their political views have revealed a widespread sense of frustration in their political powerlessness as well as a strongly cynical view of the actions of politicians. Several researchers have attempted to explain this sense of cynicism as a way of rationalizing the sense of alienation and legal exclusion of youth in political decision-making.

Another reason cited against child voting rights is that children would be unduly biased by media and other societal pressures. On the whole, this view is unsubstantiated, with interviews with youth revealing that they often have a great deal of knowledge about news programming, media bias, the importance of evidence, evaluation of arguments on the merits of their evidence, as well as a preparedness for forming arguments of one's own using available evidence. In cognitive research, some studies conducted in the 1970s offered a skeptical view of adolescent understanding of democratic principles like freedom of speech. However, this research is now recognized to have used challenging and contradictory vignettes that placed a high demand on still-developing verbal and metacognitive skills which are not recognized as requisite to an understanding of individual political rights. More recent research has unveiled that even elementary school age children have a concept of freedom of speech and that by ages 8–9 this concept expands beyond a concern for personal autonomy and onto awareness for its social implications and the importance of the right to a political voice.

Maturity has also been taken into account when determining the fairness of the death penalty in cases involving mentally retarded or underage perpetrators. In Atkins v. Virginia, the U.S. Supreme Court decision banning the execution of mentally retarded persons, was decided on the grounds that "diminished capacities to understand and process mistakes and learn from experience, to engage in logical reasoning, to control impulses, and to understand the reactions of others" was cited as the evidence supporting a reduced view of criminal culpability.

In Jewish religion, the "becoming a Bar or "Bat Mitzvah"" (literally "an [agent] who is subject to the law") refers to the ceremony declaring that a Jewish child is morally and ethically responsible for their actions, is eligible to be called to read from the Torah, as well as responsibility to abide by the 613 laws written in the Torah. Traditionally, this ceremony awarded adult legal rights as well as the right to marry. Similarly, Christian churches hold Confirmation as a rite of passage in early adolescence. The rite holds fewer practical responsibilities than the Bar/Bat Mitzavah, but carries ethical and moral consequences. In all churches, of age Christians are responsible for going to church on Sundays and for confessing their sins periodically; within certain denominations it is also a common practice to warn children that it would be a mortal sin (an act punishable by banishment to hell) to lapse in these responsibilities.

Prom is celebrated throughout many countries of the world following or prior to final coursework for the year or after graduation. Various parties, ceremonies, or gatherings are held, ranging in their focus on academics, bonding, or as a farewell. In some Western European countries a post-degree party consists of burning notebooks and final projects. In certain countries, such as Colombia and the United States, the prom has come to take on a dual role of celebrating both academic achievement as well as sexual maturity. Quinceañera, in parts of Latin America, Début in the Philippines, Ji Li in China, and Sweet Sixteen in the United States coincide closely with graduation, which highlights the importance and broad recognition of the transition; however, these celebrations have been most prominently celebrated only by girls up until recently.

A number of traditions are associated with the earlier critical maturation point of menarche. A girl's menarche is commemorated in varying ways, with some traditional Jewish customs defining it as a contamination, with the customs shaped around cleaning it away and ensuring it does not make anything or one unclean. This served a historical purpose of blocking women from taking part in economic or political events. The Maori of New Zealand, the Tinne Indians of the Yukon, the Chichimilia of Mexico, and the Eskimos, among other groups, all hold varyingly negative beliefs about the time of menarche and what dangers it brings.

For boys and young men, practices such as scarification and hazing act as a rite of passage into a group. These practices test and assert the expectations for pain tolerance and allegiance for men in those groups. Various branches of the military hold similar formal proving rituals, such as boot camp, that, aside from serving to train entrants, also demarcate an initial recognition of maturity in the organization, with successive experiences building upon that. Many occupations and social groups recognize similar tiers of maturity within the group across many cultures, which emphasise maturity as a form of status.

While older persons are generally perceived as more mature and to possess greater credibility, psychological maturity is not determined by one's age. However, for legal purposes, people are not considered psychologically mature enough to perform certain tasks (such as driving, consenting to sex, signing a binding contract or making medical decisions) until they have reached a certain age. In fact, judge Julian Mack, who helped create the juvenile court system in the United States, said that juvenile justice was based on the belief that young people do not always make good decisions because they are not mature, but this means that they can be reformed more easily than adults. However, the relationship between psychological maturity and age is a difficult one, and there has been much debate over methods of determining maturity, considering its subjective nature, relativity to the current environment and/or other factors, and especially regarding social issues such as religion, politics, culture, laws and etc. 



</doc>
<doc id="27087481" url="https://en.wikipedia.org/wiki?curid=27087481" title="Jeitinho">
Jeitinho

Jeitinho (, literally "little way") is finding a way to accomplish something by circumventing or bending the rules or social conventions. Most times it is harmless, made for basic ordinary opportunistic advantages, as gatecrashing a party just to get free food and beverage. But sometimes it is used for questionable, serious violations, where an individual can use emotional resources, blackmail, family ties, promises, rewards or money to obtain (sometimes illegal) favors or to get advantage. Some people see it as a typically Brazilian method of social navigation, but people forget to mention that this expression also comes from the necessity associated to a lack of resources and help. Most Brazilians have to be creative and invent new simpler ways to do things they need, as living.

The word "jeitinho" is the diminutive form of "jeito", which comes from the expression "dar um jeito", meaning "to find a way". It implies the use of resources at hand, as well as personal connections, and creativity. "Como é que ele conseguiu os bilhetes?" How did he get the tickets? "Ele deu um jeito." He found a way.

One way to understand jeitinho is as a "recurso de esperteza", which means a resource used by "espertos" — savvy, cunning, or sly individuals who use common sense and prior knowledge, as well as naturally gifted intelligence in their thought processes. It implies that a person is "street-smart", but not necessarily "book-smart." It typically also connotes opportunism, pragmatism, and using one's networks, with little regard for the law, the state or for persons outside of one's own circle or family.

Brazilian scholar and historian Sérgio Buarque de Hollanda connects the concept of jeitinho to Brazil's mixed heritage and Iberian ancestry in his book "Roots of Brazil" (Raízes do Brasil). In this work, jeitinho is tied to the idea that a typical Brazilian is a friendly, cordial man, prone to making initial decisions based on his emotions instead of his reason, and that this feature can be found everywhere in the country, from the highest offices of government to the most common situations of everyday life. Jeitinho is also observed in Rio de Janeiro's carnival industry by the scholar Roberto DaMatta in his book "Carnavais, Malandros e Heróis" (Carnival, Rogues and Heroes. Notre Dame Press). Da Matta sees jeitinho in the creative culture of carnival.

The terms "malandro" and "malandragem", which can be roughly translated as "rogue" and "roguishness", are very similar to the "jeitinho", but these terms imply a greater degree of breaking the rules, as opposed to bending the rules.

Elsewhere in Latin America, similar concepts include "viveza criolla" in Argentina and Uruguay, "juega vivo" in Panama and "malicia indígena" in Colombia.




</doc>
<doc id="1573182" url="https://en.wikipedia.org/wiki?curid=1573182" title="Behavior change (public health)">
Behavior change (public health)

Behavior change, in the context of public health, refers to efforts to change people's personal habits to prevent disease. Behavior change in public health is also known as social and behavior change communication (SBCC). More and more, efforts focus on prevention of disease to save healthcare care costs. This is particularly important in low and middle income countries, where health interventions have come under increased scrutiny because of the cost.

The 3-4-50 concept outlines that there are 3 behaviors (poor diet, little to no physical activity, and smoking), that lead to four diseases (heart disease/stroke, diabetes, cancer, pulmonary disease), that account for 50% of deaths worldwide. This is why so much emphasis in public health interventions have been on changing behaviors or intervening early on to decrease the negative impacts that come with these behaviors. With successful intervention, there is the possibility of decreasing healthcare costs by a drastic amount, as well as general costs to society (morbidity and mortality). A good public health intervention is not only defined by the results they create, but also the number of levels it hits on the socioecological model (individual, interpersonal, community and/or environment). The challenge that public health interventions face is generalizability: what may work in one community may not work in others. However, there is the development of HealthyPeople 2020 that has national objectives aimed to accomplish in 10 years to improve the health of all Americans.

Health conditions and infections are associated with risky behaviors. Tobacco use, alcoholism, multiple sex partners, substance use, reckless driving, obesity, or unprotected sexual intercourse are some examples. Human beings have, in principle, control over their conduct. Behavior modification can contribute to the success of self-control, and health-enhancing behaviors. Risky behaviors can be eliminated including physical exercise, weight control, preventive nutrition, dental hygiene, condom use, or accident prevention. Health behavior change refers to the motivational, volitional, and action based processes of abandoning such health-compromising behaviors in favor of adopting and maintaining health-enhancing behaviors. Addiction that is associated with risky behavior may have a genetic component.

One emerging concept in the American health system is that of small, manageable changes. It is not necessary to make sweeping, drastic alterations to one's whole lifestyle in order to see benefit. Dietary and exercise contexts in particular show the benefit of moderate, slow changes. For example, behavior change steps to include more physical activity can improve one's life expectancy, control weight, and boost mental health. It is also known to reduce the chance of some diseases such as type 2 diabetes, cardiovascular disease, and some cancers. Healthy behaviors and practices during youth, particularly in school settings, is far more cost-effective than waiting until unhealthy behaviors are entrenched. A study of the Toward No Tobacco program, which was designed to prevent cigarette use among middle and high school students, found that for every dollar invested in school tobacco prevention programs, almost $20 in future medical care costs would be saved.

Behavior change programs tend to focus on a few behavioral change theories which gained ground in the 1980s. These theories share a major commonality in defining individual actions as the locus of change. Behavior change programs that are usually focused on activities that help a person or a community to reflect upon their risk behaviors and change them to reduce their risk and vulnerability are known as interventions. Examples include: "Transtheoretical (Stages of Change) Model of Behavior Change", "theory of reasoned action", "health belief model", "theory of planned behavior", diffusion of innovation", and the health action process approach. Developments in health behavior change theories since the late 1990s have focused on incorporating disparate theories of health behavior change into a single unified theory.














Behavior change communication, or BCC, is an approach to behavior change focused on communication. It is also known as social and behavior change communication, or SBCC. The assumptions is that through communication of some kind, individuals and communities can somehow be persuaded to behave in ways that will make their lives safer and healthier. BCC was first employed in HIV and TB prevention projects. More recently, its ambit has grown to encompass any communication activity whose goal is to help individuals and communities select and practice behavior that will positively impact their health, such as immunization, cervical cancer check up, employing single-use syringes, etc.






</doc>
<doc id="27922567" url="https://en.wikipedia.org/wiki?curid=27922567" title="Good Behavior Game">
Good Behavior Game

The Good Behavior Game (GBG) is a scientifically proven strategy used to increase self-regulation, group regulation and stimulate prosocial behavior among students while reducing problematic behavior. The Good Behavior Game has more than 60 published studies at the National Library of Medicine. Major research at Johns Hopkins Center for Prevention and Early Intervention has studied three cohorts of thousands of student, some of whom have been followed from first grade into their 20s. In multiple scientific studies, the Good Behavior Game dramatically reduces problematic behavior within days and weeks.

The first study of GBG was published in 1969, in a 4th grade classroom. The study was the first application of applied behavior analysis to a whole classroom. In the original study, the classroom was divided into two teams. The students were to engage in the math or reading activities as teams. Paying attention, engaging in the lessons or activity, was the "good behavior". If students engaged in actions the interfered with the lesson (e.g., getting out their seat, interrupting), that was a penalty point against the team—much like playing a sport. Each team could make up a fixed number of mistakes, and still win the game. That is much like professional sports, except both teams could win. If a team won the game, they earned an activity reward normally not allowed, which was based on the Premack Principle. Since the original 1969 study, the Good Behavior Game has become one of the most effective and proven strategies to prevent mental, emotional, and behavioral disorders cited by the U.S. Institute of Medicine in 2009

The Good Behavior Game was first used in 1967 in Baldwin City, Kansas by Muriel Saunders, who was then a new teacher in a fourth-grade classroom. Muriel Saunders, Harriet Barrish (a graduate student at the University of Kansas), and the professor and co-founder of applied-behavior analysis, the late Montrose Wolfe, co-created the Good Behavior Game in 1969.

The Game works by positive peer pressure of 2-to-5 classroom teams, who work together reduce inattentive, disturbing, disruptive, and destructive behaviors that interfere with learning and success. When the teams succeed, all the "winners" earn brief intrinsic activity rewards based on Premack's principle. While the teacher can define the behaviors to be reduced, the game can be just as effective when students define the behaviors to be reduced to make a better learning environment. A scientific major proponent of the benefits of GBG, Dennis Embry argues that the game is more likely to be acceptable, adopted, and sustained by teachers and students, when students actively participate in setting up the "rules" of the game.

Students teams win the game by having very low rates of disturbing, disruptive, destructive, or inattentive behaviors. The teacher must respond to such problematic behaviors neutrally and unemotionally, and the person who committed the breach is not called out or given "consequences." Rather, the team has a point against it, not the individual. Teams who have less than a criterion of low points, win—typically less than 4 per team.]

The first study was by Barrish, Saunders, and Wolf (1969) who evaluated the effectiveness of the Good Behavior Game on reducing out-of-seat behavior and talking-out behavior with 24 fourth-grade students. Seven of the students had been referred by the teacher to the principal for a number of disruptive behaviors. It was also noted that the classroom was lacking a general behavior management plan. Prior to implementation, the teacher presented a short overview of the game to explain the instructional times when the game would be in effect (i.e., math period), rules, and rewards. When the experimental conditions were altered, the teacher provided a new explanation of the change(s). The researchers used a reversal and multiple baseline experimental design with four phases: (1) math baseline and reading baseline, (2) math game and reading baseline, (3) math reversal and reading game, and (4) math game and reading game. The results indicate a significant decrease in disruptive behavior during both math and reading class when the Good Behavior Game was in effect. Talking-out behavior decreased from 96% during baseline to 19% when the game was applied during the math period. Similarly, the students' out-of-seat behavior was reduced from 82% of the scored intervals to 9% during the intervention. Both teams won on all but three occasions. Evidence of generalization and maintenance was not reported.

A later study by Harris and Sherman (1973) replicated the Barrish, Saunders, and Wolf (1969) study and examined the effectiveness of the Good Behavior Game with one fifth-grade classroom and one sixth-grade classroom. Each teacher attended a 15-minute meeting prior to implementing the procedures in his or her classroom. During the meeting, the researchers defined disruptive behavior, explained the recording system, and reviewed the procedures of the Good Behavior Game. After baseline data was collected, the teachers divided the students into two teams, discussed the rules of the game, and outlined the contingencies. The reward was a 10-minute early dismissal at the end of the school day. Again, the researchers recorded talking and out-of-seat behavior during 30-minute observation sessions. Each session was divided into 30 one-minute intervals. If one or more of the children exhibited the disruptive behavior, the interval was scored as containing disruptive behavior. In addition to collecting data on disruptive behavior, the researchers evaluated the students' academic performance during two math periods in the fifth-grade classroom. In the sixth-grade classroom, several experimental manipulations (i.e., eliminating the consequences, changing the maximum number of marks needed to win, eliminating feedback, and keeping the class intact) were performed to identify which components of the game were the most effective in reducing disruptive behavior. The findings show that implementing the Good Behavior Game successfully reduced disruptive out-of-seat and talking behavior. Each of the following procedural components contributed to its effectiveness: permission to leave school early, the number of marks chosen as a criterion, and the division of students into teams. Furthermore, a reduction in problem behavior resulted in slightly higher accuracy rates on the independent math tasks.

More than 30 "single subject" or behavior analyses studies have been conducted on the Good Behavior Game, such as Lannie and McCurdy who extended the research on the Good Behavior Game by evaluating its impact in an urban classroom serving a population of students characterized by a high level of poverty and also evaluating the effects of the game on teacher behavior, especially teacher praise. The study was conducted in a first-grade classroom with 22 students and examined the effects of the Good Behavior Game on on-task and disruptive behaviors, as well as teacher response statements. The study used similar procedures as detailed in the prior studies but employed an ABAB withdrawal design. Each observation session was a 30-minute math period. Results show that students’ on-task behavior increased while disruptive behavior decreased. The number of teacher praise statements remained at near zero levels across conditions.

Since the original 1969 study of the Good Behavior Game, there have been multiple randomized control trials conducted by Johns Hopkins University Center for Prevention and Early Intervention.




</doc>
<doc id="27135557" url="https://en.wikipedia.org/wiki?curid=27135557" title="Meaningful life">
Meaningful life

In positive psychology, a meaningful life is a construct having to do with the purpose, significance, fulfillment, and satisfaction of life. While specific theories vary, there are two common aspects: a global schema to understand one's life and the belief that life itself is meaningful. Meaning can be defined as the connection linking two presumably independent entities together; a meaningful life links the biological reality of life to a symbolic interpretation or meaning. Those possessing a sense of meaning are generally found to be happier, to have lower levels of negative emotions, and to have lower risk of mental illness.

Logotherapy emphasizes finding values and purpose in an individual's life, and building relationships with others in order to reach fulfilment and attain meaningfulness. "Value" can be further subcategorized into three main areas: creative, experiential, and attitudinal. Creative values are reached through acts of creating or producing something. Experiential values are actualized when a person experiences something through sight, touch, smell, or hearing. Finally, attitudinal values are reserved for individuals who cannot, for one reason or another, have new experiences or create new things. Thus they find meaning through adopting a new attitude that allows "suffering with dignity". For all of these classes of values, it is because of one's sense of responsibility that one pursues these values and consequently experiences a meaningful life. It is through the realization that one is the sole being responsible for rendering life meaningful that values are actualized and life becomes meaningful.

Terror management theory studies meaningfulness and its relationship to culture. A human's consciousness makes them aware of their own mortality. In order to deal with their inevitable death, humans attempt to leave their mark in some symbolic act of immortality within the structured society. The structure created through society and culture provides humans with a sense of order. Through the structured society we are able to create a symbolic immortality which can take various forms, e.g., monuments, theatrical productions, children, etc. Culture's order reduces death anxiety as it allows the individual to live up to the societal standards and in living up to such ideals; one is given self-esteem which counterbalances the mortal anxiety.

Hope theory operationalizes meaningfulness as having more to do with self-control that leads to higher self-esteem. As one lives by societal standards of living, one exercises self-control and it is through this self-control that higher self-esteem is achieved. Meaning is found when one realizes that one is capable and able to effectively achieve their goals through successful management. Control is "a cognitive model whereby people strive to comprehend the contingencies in their lives so as to attain desired outcomes and avoid undesirable ones". From this feeling of control, meaningfulness is achieved when one feels able to effectively live his/her life and achieve goals.

Narrative psychology proposes that people construct life stories as a way to understand life events and impose meaning on life, thus connecting [via explanation] the individual to the event. Meaningfulness is a subjective evaluation of how well these stories connect to the person. Furthermore, meaningfulness is actualized through positive functioning, satisfaction with life, the enjoyment of work, happiness, positive affect and hope. Meaningfulness can also be translated into physical health and a generalized well-being. Baumeister posits that meaningfulness is divided into four needs: sense of purpose, efficacy, value, and a sense of positive self-worth.

Social exclusion results in a perceived loss of meaningfulness in life. Furthermore, the four needs for meaning (sense of purpose, efficacy, value and sense of positive self-worth) were found to be mediators in the perception of meaningfulness of life. When an individual thinks themself to be socially excluded, one's sense of purpose, efficacy, value, and self-worth are all indirectly diminished.

Recent systematic reviews addressing meaning in life found that higher meaning in life is associated to better physical health in general, lower distress among cancer patients, and higher subjective well-being in China. On the other hand, in another systematic review a more specific type of meaning, a purpose in life, was associated to reduced mortality and cardiovascular events. Another meta-analysis found that purpose in life was in average slightly lower in older age-groups compared to younger ones.

A study found an association between the discovery of meaning and a lower rate of AIDS-related mortality. This was the first study in which the findings appear to not be mediated by health behaviors or other potential confounds. The study looked at HIV-seropositive men who had recently witnessed the death of a close friend from AIDS-related death. When confronted with the stress of such a death those men, who were able to find meaning in the loss, were subject to less rapid declines in CD4 T cell levels. Furthermore, the subjects who went through cognitive processing in response to the bereavement were more likely to find meaning in the death of the close friend. Thus in experiencing a stressful life event if one is able to engage successfully in finding meaning there is a potential link to positive immunological benefits and health outcomes.

A happy life and a meaningful life are strongly correlated attitudes. However, happiness may be distinguished as relating more to biological needs and desires, such as the absence of pain or unpleasant experiences, while meaning is more cultural and abstract, relating to overall life satisfaction or eudaimonia. According to a research, living a meaningful life is one of the several enduring pathways to happiness. Another study found that difficulty, health, purchasing power, and a focus on the present corresponded more to happiness than meaning, while thinking about the past or the future, struggle, stress, worry, argument, anxiety, generosity, and viewing daily activities such as raising children as reflective of oneself corresponded more with finding life meaningful. Feeling more connected to others improved both happiness and meaning, according to the study. Yet, the role a person adopts in the relationships makes an important difference. Those who agreed with the statement, “I am a giver,” reported less happiness than those who were more likely to agree with, “I am a taker.” However, the “givers” reported higher levels of meaning in their lives compared to the “takers.”

A meaningful life is associated with positive functioning: life satisfaction, enjoyment of work, happiness, general positive affect, hope and in general a higher level of well-being.

Psychological adjustment in the event of a stressor has been linked with meanings finding whether in the form of benefit seeking or making sense of the loss. In terms of how meaning is manifested, making sense of the loss seems to be more important earlier on in the adjustment process after the loss whereas perceiving the benefit may be a more long term process that occurs over time with the greatest benefit usually experienced later on (Davis, Nolen-Hoeksema & Larson, 1998).

Based on systematic reviews, there are various promising therapies and interventions that focus on increasing meaning or purpose in life. Many of these interventions been created for patients with advanced disease.

While there are benefits to making meaning out of life, there is still not one definitive way in which one can establish such a meaning. Those who were successful in creating a meaningful life enjoyed benefits such as higher levels of positive affect, life satisfaction, etc. When faced with a stressful life situation, finding meaning is shown to help adjustment. Meaningfulness in life is intrinsically related to positive psychology's goal to expand the good life for the normal non-disordered person. It is with a meaningful life that one is able to find connections to people, places, things and leave a mark on society; it renders a good life a meaningful one.



</doc>
<doc id="232495" url="https://en.wikipedia.org/wiki?curid=232495" title="Motivation">
Motivation

Motivation is the reason for people's actions, willingness and goals. Motivation is derived from the word "motive" in the English language which is defined as a need that requires satisfaction. These needs could also be wants or desires that are acquired through influence of culture, society, lifestyle, etc. or generally innate. Motivation is one's direction to behaviour, or what causes a person to want to repeat a behaviour, a set of force that acts behind the motives. An individual's motivation may be inspired by others or events (extrinsic motivation) or it may come from within the individual (intrinsic motivation). Motivation has been considered as one of the most important reasons that inspires a person to move forward in life. Motivation results from the interaction of both conscious and unconscious factors. Mastering motivation to allow sustained and deliberate practice is central to high levels of achievement e.g. in the worlds of elite sport, medicine or music.

Motivation as a desire to perform an action is usually defined as having two parts, directional such as directed towards a positive stimulus or away from a negative one, as well as the activated "seeking phase" and consummatory "liking phase". This type of motivation has neurobiological roots in the basal ganglia, and mesolimbic, dopaminergic pathways. Activated "seeking" behaviour, such as loco-motor activity, is influenced by dopaminergic drugs, and microdialysis experiments reveal that dopamine is released during the anticipation of a reward. The "wanting behaviour" associated with a rewarding stimulus can be increased by microinjections of dopamine and dopaminergic drugs in the dorsorostral nucleus accumbens and posterior ventral palladum. Opioid injections in this area produce pleasure, however outside of these hedonic hotspots they create an increased desire. Furthermore, depletion or inhibition of dopamine in neurons of the nucleus accumbens decreases appetitive but not consummatory behaviour. Dopamine is further implicated in motivation as administration of amphetamine increased the break point in a progressive ratio self-reinforcement schedule. That is, subjects were willing to go to greater lengths (e.g. press a lever more times) to obtain a reward.

Motivation can be conceived of as a cycle in which thoughts influence behaviours, drive performance affects thoughts, and the cycle begins again. Each stage of the cycle is composed of many dimensions including attitudes, beliefs, intentions, effort, and withdrawal which can all affect the motivation that an individual experiences. Most psychological theories hold that motivation exists purely within the individual, but socio-cultural theories express motivation as an outcome of participation in actions and activities within the cultural context of social groups.

The natural system assumes that people have higher order needs, which contrasts with the rational theory that suggests people dislike work and only respond to rewards and punishment.<ref name="Lecture 10/1">Dobbin, Frank. “From Incentives to Teamwork: Rational and Natural Management Systems.” Lecture. Harvard University. Cambridge, Massachusetts. 1 October 2012.</ref> According to McGregor's Theory Y, human behaviour is based on satisfying a hierarchy of needs: physiological, safety, social, ego, and self-fulfillment.

Physiological needs are the lowest and most important level. These fundamental requirements include food, rest, shelter, and exercise. After physiological needs are satisfied, employees can focus on safety needs, which include “protection against danger, threat, deprivation.” However, if management makes arbitrary or biased employment decisions, then an employee's safety needs are unfulfilled.

The next set of needs is social, which refers to the desire for acceptance, affiliation, reciprocal friendships and love. As such, the natural system of management assumes that close-knit work teams are productive. Accordingly, if an employee's social needs are unmet, then he will act disobediently.

There are two types of egoistic needs, the second-highest order of needs. The first type refers to one's self-esteem, which encompasses self-confidence, independence, achievement, competence, and knowledge. The second type of needs deals with reputation, status, recognition, and respect from colleagues. Egoistic needs are much more difficult to satisfy.

The highest order of needs is for self-fulfillment, including recognition of one's full potential, areas for self-improvement, and the opportunity for creativity. This differs from the rational system, which assumes that people prefer routine and security to creativity. Unlike the rational management system, which assumes that humans don’t care about these higher order needs, the natural system is based on these needs as a means for motivation.

The author of the reductionist motivation model is Sigmund Freud. According to the model, physiological needs raise tension, thereby forcing an individual to seek an outlet by satisfying those needs 

To successfully manage and motivate employees, the natural system posits that being part of a group is necessary. Because of structural changes in social order, the workplace is more fluid and adaptive according to Mayo. As a result, individual employees have lost their sense of stability and security, which can be provided by a membership in a group. However, if teams continuously change within jobs, then employees feel anxious, empty, and irrational and become harder to work with. The innate desire for lasting human association and management “is not related to single workers, but always to working groups.” In groups, employees will self-manage and form relevant customs, duties, and traditions.

Humans are motivated by additional factors besides wage incentives. Unlike the rational theory of motivation, people are not driven toward economic interests per the natural system. For instance, the straight piecework system pays employees based on each unit of their output. Based on studies such as the Bank Wiring Observation Room, using a piece rate incentive system does not lead to higher production. Employees actually set upper limits on each person's daily output. These actions stand “in direct opposition to the ideas underlying their system of financial incentive, which countenanced no upper limit to performance other than physical capacity.” Therefore, as opposed to the rational system that depends on economic rewards and punishments, the natural system of management assumes that humans are also motivated by non-economic factors.

Employees seek autonomy and responsibility in their work, contrary to assumptions of the rational theory of management. Because supervisors have direct authority over employees, they must ensure that the employee's actions are in line with the standards of efficient conduct. This creates a sense of restriction on the employee and these constraints are viewed as “annoying and seemingly functioned only as subordinating or differentiating mechanisms." Accordingly, the natural management system assumes that employees prefer autonomy and responsibility on the job and dislike arbitrary rules and overwhelming supervision. An individual's motivation to complete a task is increased when this task is autonomous. When the motivation to complete a task comes from an "external pressure" that pressure then "undermines" a person's motivation, and as a result decreases a persons desire to complete the task.

The idea that human beings are rational and human behaviour is guided by reason is an old one. However, recent research (on satisfying for example) has significantly undermined the idea of homo economicus or of perfect rationality in favour of a more bounded rationality. The field of behavioural economics is particularly concerned with the limits of rationality in economic agents.

Motivation can be divided into two different theories known as "intrinsic" (internal or inherent) motivation and "extrinsic" (external) motivation.

Intrinsic motivation has been studied since the early 1970s. Intrinsic motivation is the self-desire to seek out new things and new challenges, to analyse one's capacity, or to observe and to achieve a goal - for example, developing one's own understanding or giving up smoking. It is driven by an interest or enjoyment in the task itself, and exists within the individual rather than relying on external pressures or a desire for consideration. Deci (1971) explained that some activities provide their own inherent reward, meaning certain activities are not dependent on external rewards. The phenomenon of intrinsic motivation was first acknowledged within experimental studies of animal behaviour. In these studies, it was evident that the organisms would engage in playful and curiosity-driven behaviours in the absence of reward. Intrinsic motivation is a natural motivational tendency and is a critical element in cognitive, social, and physical development. The two necessary elements for intrinsic motivation are self-determination and an increase in perceived competence. In short, the cause of the behaviour must be internal, known as internal local of causality, and the individual who engages in the behaviour must perceive that the task increases their competence. According to various research reported by Deci's published findings in 1971, and 1972, tangible rewards could actually undermine the intrinsic motivation of college students. However, these studies didn't just effect college students, Kruglanski, Friedman and Zeevi (1971) repeated this study and found that symbolic and material rewards can undermine not just high school students, but preschool students as well.

Students who are intrinsically motivated are more likely to engage in the task willingly as well as work to improve their skills, which will increase their capabilities. Students are likely to be intrinsically motivated if they...

An example of intrinsic motivation is when an employee becomes an IT professional because he or she wants to learn about how computer users interact with computer networks. The employee has the intrinsic motivation to gain more knowledge. Art for art's sake is an example of intrinsic motivation in the domain of art.

Traditionally, researchers thought of motivations to use computer systems to be primarily driven by extrinsic purposes; however, many modern systems have their use driven primarily by intrinsic motivations. Examples of such systems used primarily to fulfil users' intrinsic motivations, include on-line gaming, virtual worlds, online shopping, learning/education, online dating, digital music repositories, social networking, online pornography, gamified systems, and general gamification. Even traditional management information systems (e.g., ERP, CRM) are being 'gamified' such that both extrinsic and intrinsic motivations must increasingly be considered. Deci's findings didn't come without controversy. Articles stretching over the span of 25 years from the perspective of behavioral theory argue there isn't enough evidence to explain intrinsic motivation and this theory would inhibit "scientific progress." As stated above, we now can see technology such as various forms of computer systems are highly intrinsic.

Not only can intrinsic motivation be used in a personal setting, but it can also be implemented and utilized in a social environment. Instead of attaining mature desires, such as those presented above via internet which can be attained on one's own, intrinsic motivation can be used to assist extrinsic motivation to attain a goal. For example, Eli, a 4-year-old with autism, wants to achieve the goal of playing with a toy train. To get the toy, he must first communicate to his therapist that he wants it. His desire to play is strong enough to be considered intrinsic motivation because it is a natural feeling, and his desire to communicate with his therapist to get the train can be considered extrinsic motivation because the outside object is a reward (see incentive theory). Communicating with the therapist is the first, slightly more challenging goal that stands in the way of achieving his larger goal of playing with the train. Achieving these goals in attainable pieces is also known as the goal-setting theory.

Intrinsic motivation comes from within one’s self. Pursuing challenges and goals come easier and more enjoyable when one is intrinsically motivated to complete a certain objective. Edward Deci and Richard Ryan’s theory of intrinsic motivation is essentially examining the conditions that “elicit and sustain” this phenomenon. Deci and Ryan coin the term “cognitive evaluation theory which concentrates on the needs of competence and autonomy. The CET essentially states that social-contextual events like feedback and reinforcement can cause feelings of competence and therefore increase intrinsic motivation. However, feelings of competence will not increase intrinsic motivation if there is no sense of autonomy. In situations where choices, feelings, and opportunities are present, intrinsic motivation is increased because people feel a greater sense of autonomy.

Intrinsic motivation can be long-lasting and self-sustaining. Efforts to build this kind of motivation are also typically efforts at promoting student learning. Such efforts often focus on the subject rather than rewards or punishments. Focusing on the subject can enhance interest, performance, and creativity. Individuals who are rewarded thrive on a heightened level, even though there could be others with the same competency level. Setting a positive reward experience will support tendencies of well-being. In contrast, choice, acknowledgment of feelings, and opportunities for self-direction were found to enhance intrinsic motivation because they allow people a greater feeling of autonomy (Deci & Ryan, 1985).

Efforts at fostering intrinsic motivation can be slow to affect behaviour and can require special and lengthy preparation. Students are individuals, so a variety of approaches may be needed to motivate different students. It is often helpful to know what interests one's students in order to connect these interests with the subject matter. This requires getting to know one's students. Also, it helps if the instructor is interested in the subject.

Extrinsic motivation comes from influences outside of the individual. In extrinsic motivation, the harder question to answer is where do people get the motivation to carry out and continue to push with persistence. Usually extrinsic motivation is used to attain outcomes that a person wouldn't get from intrinsic motivation. Common extrinsic motivations are rewards (for example money or grades) for showing the desired behaviour, and the threat of punishment following misbehaviour. Competition is an extrinsic motivator because it encourages the performer to win and to beat others, not simply to enjoy the intrinsic rewards of the activity. A cheering crowd and the desire to win a trophy are also extrinsic incentives.

The most simple distinction between extrinsic and intrinsic motivation is the type of reasons or goals that lead to an action. While intrinsic motivation refers to doing something because it is inherently interesting or enjoyable, extrinsic motivation, refers to doing something because it leads to a separable outcome. Extrinsic motivation thus contrasts with intrinsic motivation, which is doing an activity simply for the enjoyment of the activity itself, instead of for its instrumental value.

Social psychological research has indicated that extrinsic rewards can lead to overjustification and a subsequent reduction in intrinsic motivation. In one study demonstrating this effect, children who expected to be (and were) rewarded with a ribbon and a gold star for drawing pictures spent less time playing with the drawing materials in subsequent observations than children who were assigned to an unexpected reward condition. However, another study showed that third graders who were rewarded with a book showed more reading behaviour in the future, implying that some rewards do not undermine intrinsic motivation. While the provision of extrinsic rewards might reduce the desirability of an activity, the use of extrinsic constraints, such as the threat of punishment, against performing an activity has actually been found to increase one's intrinsic interest in that activity. In one study, when children were given mild threats against playing with an attractive toy, it was found that the threat actually served to increase the child's interest in the toy, which was previously undesirable to the child in the absence of threat.

Allows individuals to become easily motivated and work towards a goal.

Motivation will only last as long as the external rewards are satisfying.

Flow theory refers to desirable subjective state a person experiences when completely involved in some challenging activity that matches the individual skill.

Mihali Csikszentmihaly described Flow theory as "A state in which people are so involved in an activity that nothing else seems to matter; the experience is so enjoyable that people will continue to do it even at great cost, for the sheer sake of doing it."

The idea of flow theory as first conceptualized by Csikszentmihalyi. Flow in the context of motivation can be seen as an activity that is not too hard, frustrating or madding, or too easy boring and done too fast. If one has achieved perfect flow, then the activity has reached maximum potential.

Flow is part of something called positive psychology of the psychology of happiness. Positive psychology looks into what makes a person happy. Flow can be considered as achieving happiness or at the least positive feelings. A study that was published in the journal "Emotion" looked at flow experienced in college students playing Tetris. The students that they were being evaluated on looks then told to wait and play Tetris. There were three categories; Easy, normal, and hard. The students that played Tetris on normal level experienced flow and were less stressed about the evaluation.

Csikszentmihalyi describes 8 characteristics of flow as a complete concentration on the task, clarity of goals and reward in mind and immediate feedback, transformation of time (speeding up/slowing down of time), the experience is intrinsically rewarding, effortlessness and ease, there is a balance between challenge and skills, actions and awareness are merged, losing self-conscious rumination, there is a feeling of control over the task.

The activity no longer becomes something seen as a means to an end and it becomes something an individual wants to do. This can be seen as someone who likes to run for the sheer joy of running and not because they need to do it for exercise or because they want to brag about it. Peak flow can be different for each person. It could take an individual years to reach flow or only moments. If an individual becomes too good at an activity they can become bored. If the challenge becomes too hard then the individual could become discouraged and want to quit.

While many theories on motivation have a mentalistic perspective, behaviorists focus only on observable behaviour and theories founded on experimental evidence. In the view of behaviorism, motivation is understood as a question about what factors cause, prevent, or withhold various behaviours, while the question of, for instance, conscious motives would be ignored. Where others would speculate about such things as values, drives, or needs, that may not be observed directly, behaviorists are interested in the observable variables that affect the type, intensity, frequency and duration of observable behaviour. Through the basic research of such scientists as Pavlov, Watson and Skinner, several basic mechanisms that govern behaviour have been identified. The most important of these are classical conditioning and operand conditioning.

In classical (or respondent) conditioning, behaviour is understood as responses triggered by certain environmental or physical stimuli. They can be "unconditioned", such as in-born reflexes, or learned through the pairing of an unconditioned stimulus with a different stimulus, which then becomes a conditioned stimulus. In relation to motivation, classical conditioning might be seen as one explanation as to why an individual performs certain responses and behaviors in certain situations. For instance, a dentist might wonder why a patient does not seem motivated to show up for an appointment, with the explanation being that the patient has associated the dentist (conditioned stimulus) with the pain (unconditioned stimulus) that elicits a fear response (conditioned response), leading to the patient being reluctant to visit the dentist.

In operant conditioning, the type and frequency of behaviour is determined mainly by its consequences. If a certain behaviour, in the presence of a certain stimulus, is followed by a desirable consequence (a reinforcer), the emitted behaviour will increase in frequency in the future, in the presence of the stimulus that preceded the behaviour (or a similar one). Conversely, if the behaviour is followed by something undesirable (a punisher), the behaviour is less likely to occur in the presence of the stimulus. In a similar manner, removal of a stimulus directly following the behaviour might either increase or decrease the frequency of that behaviour in the future (negative reinforcement or punishment). For instance, a student that gained praise and a good grade after turning in a paper, might seem more motivated in writing papers in the future (positive reinforcement); if the same student put in a lot of work on a task without getting any praise for it, he or she might seem less motivated to do school work in the future (negative punishment). If a student starts to cause trouble in class gets punished with something he or she dislikes, such as detention (positive punishment), that behaviour would decrease in the future. The student might seem more motivated to behave in class, presumably in order to avoid further detention (negative reinforcement).

The strength of reinforcement or punishment is dependent on schedule and timing. A reinforcer or punisher affects the future frequency of a behaviour most strongly if it occurs within seconds of the behaviour. A behaviour that is reinforced intermittently, at unpredictable intervals, will be more robust and persistent, compared to one that is reinforced every time the behaviour is performed. For example, if the misbehaving student in the above example was punished a week after the troublesome behaviour, that might not affect future behaviour.

In addition to these basic principles, environmental stimuli also affect behavior. Behaviour is punished or reinforced in the context of whatever stimuli were present just before the behaviour was performed, which means that a particular behaviour might not be affected in every environmental context, or situation, after it is punished or reinforced in one specific context. A lack of praise for school-related behaviour might, for instance, not decrease after-school sports-related behaviour that is usually reinforced by praise.

The various mechanisms of operant conditioning may be used to understand the motivation for various behaviours by examining what happens just after the behaviour (the consequence), in what context the behaviour is performed or not performed (the antecedent), and under what circumstances (motivating operators).

Incentive theory is a specific theory of motivation, derived partly from behaviorist principles of reinforcement, which concerns an incentive or motive to do something. The most common incentive would be a compensation. Compensation can be tangible or intangible, It helps in motivating the employees in their corporate life, students in academics and inspire to do more and more to achieve profitability in every field. Studies show that if the person receives the reward immediately, the effect is greater, and decreases as delay lengthens. Repetitive action-reward combination can cause the action to become a habit.

"Reinforcers and reinforcement principles of behaviour differ from the hypothetical construct of reward." A reinforcer is anything that follows an action, with the intentions that the action will now occur more frequently. From this perspective, the concept of distinguishing between intrinsic and extrinsic forces is irrelevant.

Incentive theory in psychology treats motivation and behaviour of the individual as they are influenced by beliefs, such as engaging in activities that are expected to be profitable. Incentive theory is promoted by behavioral psychologists, such as B.F. Skinner. Incentive theory is especially supported by Skinner in his philosophy of Radical behaviorism, meaning that a person's actions always have social ramifications: and if actions are positively received people are more likely to act in this manner, or if negatively received people are less likely to act in this manner.

Incentive theory distinguishes itself from other motivation theories, such as drive theory, in the direction of the motivation. In incentive theory, stimuli "attract" a person towards them, and push them towards the stimulus. In terms of behaviorism, incentive theory involves positive reinforcement: the reinforcing stimulus has been conditioned to make the person happier. As opposed to in drive theory, which involves negative reinforcement: a stimulus has been associated with the removal of the punishment—the lack of homeostasis in the body. For example, a person has come to know that if they eat when hungry, it will eliminate that negative feeling of hunger, or if they drink when thirsty, it will eliminate that negative feeling of thirst.

Motivating operations, MOs, relate to the field of motivation in that they help improve understanding aspects of behaviour that are not covered by operant conditioning. In operant conditioning, the function of the reinforcer is to influence "future behavior". The presence of a stimulus believed to function as a reinforcer does not according to this terminology explain the current behaviour of an organism – only previous instances of reinforcement of that behavior (in the same or similar situations) do. Through the behavior-altering effect of MOs, it is possible to affect current behaviour of an individual, giving another piece of the puzzle of motivation.

Motivating operations are factors that affect learned behaviour in a certain context. MOs have two effects: a value-altering effect, which increases or decreases the efficiency of a reinforcer, and a behavior-altering effect, which modifies learned behaviour that has previously been punished or reinforced by a particular stimulus.

When a motivating operation causes an increase in the effectiveness of a reinforcer, or amplifies a learned behaviour in some way (such as increasing frequency, intensity, duration or speed of the behaviour), it functions as an establishing operation, EO. A common example of this would be food deprivation, which functions as an EO in relation to food: the food-deprived organism will perform behaviours previously related to the acquisition of food more intensely, frequently, longer, or faster in the presence of food, and those behaviours would be especially strongly reinforced. For instance, a fast-food worker earning minimal wage, forced to work more than one job to make ends meet, would be highly motivated by a pay raise, because of the current deprivation of money (a conditioned establishing operation). The worker would work hard to try to achieve the raise, and getting the raise would function as an especially strong reinforcer of work behaviour.

Conversely, a motivating operation that causes a decrease in the effectiveness of a reinforcer, or diminishes a learned behaviour related to the reinforcer, functions as an abolishing operation, AO. Again using the example of food, satiation of food prior to the presentation of a food stimulus would produce a decrease on food-related behaviours, and diminish or completely abolish the reinforcing effect of acquiring and ingesting the food. Consider the board of a large investment bank, concerned with a too small profit margin, deciding to give the CEO a new incentive package in order to motivate him to increase firm profits. If the CEO already has a lot of money, the incentive package might not be a very good way to motivate him, because he would be satiated on money. Getting even more money wouldn't be a strong reinforcer for profit-increasing behaviour, and wouldn't elicit increased intensity, frequency or duration of profit-increasing behaviour.

Motivation lies at the core of many behaviorist approaches to psychological treatment. A person with autism-spectrum disorder is seen as lacking motivation to perform socially relevant behaviours – social stimuli are not as reinforcing for people with autism compared to other people. Depression is understood as a lack of reinforcement (especially positive reinforcement) leading to extinction of behavior in the depressed individual. A patient with specific phobia is not motivated to seek out the phobic stimulus because it acts as a punisher, and is over-motivated to avoid it (negative reinforcement). In accordance, therapies have been designed to address these problems, such as EIBI and CBT for major depression and specific phobia.

Sociocultural theory (also known as Social Motivation) emphasizes impact of activity and actions mediated through social interaction, and within social contexts. Sociocultural theory represents a shift from traditional theories of motivation, which view the individual's innate drives or mechanistic operand learning as primary determinants of motivation. Critical elements to socio-cultural theory applied to motivation include, but are not limited to, the role of social interactions and the contributions from culturally-based knowledge and practice. Sociocultural theory extends the social aspects of Cognitive Evaluation Theory, which espouses the important role of positive feedback from others during action, but requires the individual as the internal locus of causality. Sociocultural theory predicts that motivation has an external locus of causality, and is socially distributed among the social group.

Motivation can develop through an individuals involvement within their cultural group. Personal motivation often comes from activities a person believes to be central to the everyday occurrences in their community. An example of socio-cultural theory would be social settings where people work together to solve collective problems. Although individuals will have internalized goals, they will also develop internalized goals of others, as well as new interests and goals collectively with those that they feel socially connected to. Oftentimes, it is believed that all cultural groups are motivated in the same way. However, motivation can come from different child-rearing practices and cultural behaviors that greatly vary between cultural groups.

In some indigenous cultures, collaboration between children and adults in community and household tasks is seen as very important A child from an indigenous community may spend a great deal of their time alongside family and community members doing different tasks and chores that benefit the community. After having seen the benefits of collaboration and work, and also having the opportunity to be included, the child will be intrinsically motivated to participate in similar tasks. In this example, because the adults in the community do not impose the tasks upon the children, the children therefore feel self-motivated and a desire to participate and learn through the task. As a result of the community values that surround the child, their source of motivation may vary from a different community with different values.

In more Westernized communities, where segregation between adults and children participating in work related task is a common practice. As a result of this, these adolescents demonstrate less internalized motivation to do things within their environment than their parents. However, when the motivation to participate in activities is a prominent belief within the family, the adolescents autonomy is significantly higher. This therefore demonstrating that when collaboration and non-segregative tasks are norms within a child's upbringing, their internal motivation to participate in community tasks increases. When given opportunities to work collaboratively with adults on shared tasks during childhood, children will therefore become more intrinsically motivated through adulthood.

Social motivation is tied to one's activity in a group. It cannot form from a single mind alone. For example, bowling alone is naught but the dull act of throwing a ball into pins, and so people are much less likely to smile during the activity alone, even upon getting a strike because their satisfaction or dissatisfaction does not need to be communicated, and so it is internalized. However, when with a group, people are more inclined to smile regardless of their results because it acts as a positive communication that is beneficial for pleasurable interaction and teamwork. Thus the act of bowling becomes a social activity as opposed to a dull action because it becomes an exercise in interaction, competition, team building, and sportsmanship. It is because of this phenomenon that studies have shown that people are more intrigued in performing mundane activities so long as there is company because it provides the opportunity to interact in one way or another, be it for bonding, amusement, collaboration, or alternative perspectives. Examples of activities that may one may not be motivated to do alone but could be done with others for social benefit are things such as throwing and catching a baseball with a friend, making funny faces with children, building a treehouse, and performing a debate.

Push motivations are those where people push themselves towards their goals or to achieve something, such as the desire for escape, rest and relaxation, prestige, health and fitness, adventure, and social interaction.

However, with push motivation it's also easy to get discouraged when there are obstacles present in the path of achievement. Push motivation acts as a willpower and people's willpower is only as strong as the desire behind the willpower.

Additionally, a study has been conducted on social networking and its push and pull effects. One thing that is mentioned is "Regret and dissatisfaction correspond to push factors because regret and dissatisfaction are the negative factors that compel users to leave their current service provider." So from reading this, we now know that Push motivations can also be a negative force. In this case, that negative force is regret and dissatisfaction.

Pull motivation is the opposite of push. It is a type of motivation that is much stronger. "Some of the factors are those that emerge as a result of the attractiveness of a destination as it is perceived by those with the propensity to travel. They include both tangible resources, such as beaches, recreation facilities, and cultural attractions, and traveler's perceptions and expectation, such as novelty, benefit expectation, and marketing image." Pull motivation can be seen as the desire to achieve a goal so badly that it seems that the goal is pulling us toward it. That is why pull motivation is stronger than push motivation. It is easier to be drawn to something rather than to push yourself for something you desire.
It can also be an alternative force when compared to negative force. From the same study as previously mentioned, "Regret and dissatisfaction with an existing SNS service provider may trigger a heightened interest toward switching service providers, but such a motive will likely translate into reality in the presence of a good alternative. Therefore, alternative attractiveness can moderate the effects of regret and dissatisfaction with switching intention" And so, pull motivation can be an attracting desire when negative influences come into the picture.

The self-control aspect of motivation is increasingly considered to be a subset of emotional intelligence; it is suggested that although a person may be classed as highly intelligent (as measured by many traditional intelligence tests), they may remain unmotivated to pursue intellectual endeavours. Vroom's "expectancy theory" provides an account of when people may decide to exert self-control in pursuit of a particular goal.

A drive or desire can be described as a deficiency or need that activates behavior that is aimed at a goal or an incentive.<ref name="Drive/Desire"></ref> These drives are thought to originate within the individual and may not require external stimuli to encourage the behavior. Basic drives could be sparked by deficiencies such as hunger, which motivates a person to seek food whereas more subtle drives might be the desire for praise and approval, which motivates a person to behave in a manner pleasing to others.

Another basic drive is the sexual drive which like food motivates us because it is essential to our survival. The desire for sex is wired deep into the brain of all human beings as glands secrete hormones that travel through the blood to the brain and stimulates the onset of sexual desire. The hormone involved in the initial onset of sexual desire is called Dehydroepiandrosterone (DHEA). The hormonal basis of both men and women's sex drives is testosterone. Men naturally have more testosterone than women do and so are more likely than women to think about sex. 

Drive theory grows out of the concept that people have certain biological drives, such as hunger and thirst. As time passes the strength of the drive increases if it is not satisfied (in this case by eating). Upon satisfying a drive the drive's strength is reduced. Created by Clark Hull and further developed by Kenneth Spence, the theory became well known in the 1940s and 1950s. Many of the motivational theories that arose during the 1950s and 1960s were either based on Hull's original theory or were focused on providing alternatives to the drive-reduction theory, including Abraham Maslow's hierarchy of needs, which emerged as an alternative to Hull's approach.

Drive theory has some intuitive or folk validity. For instance when preparing food, the drive model appears to be compatible with sensations of rising hunger as the food is prepared, and, after the food has been consumed, a decrease in subjective hunger. There are several problems, however, that leave the validity of drive reduction open for debate.

Suggested by Leon Festinger, cognitive dissonance occurs when an individual experiences some degree of discomfort resulting from an inconsistency between two cognitions: their views on the world around them, and their own personal feelings and actions. For example, a consumer may seek to reassure themselves regarding a purchase, feeling that another decision may have been preferable. Their feeling that another purchase would have been preferable is inconsistent with their action of purchasing the item. The difference between their feelings and beliefs causes dissonance, so they seek to reassure themselves.

While not a theory of motivation, per se, the theory of cognitive dissonance proposes that people have a motivational drive to reduce dissonance. The cognitive miser perspective makes people want to justify things in a simple way in order to reduce the effort they put into cognition. They do this by changing their attitudes, beliefs, or actions, rather than facing the inconsistencies, because dissonance is a mental strain. Dissonance is also reduced by justifying, blaming, and denying. It is one of the most influential and extensively studied theories in social psychology.

The content theory was one of the earliest theories of motivation. Content theories can also be referred to needs theories, because the theory focuses on the importance of what motivates people (needs). In other words, they try to identify what are the "needs" and how they relate to motivation to fulfill those needs. Another definition could be defined by Pritchard and Ashwood, is the process used to allocate energy to maximize the satisfaction of needs.

Content theory of human motivation includes both Abraham Maslow's hierarchy of needs and Herzberg's two-factor theory. Maslow's theory is one of the most widely discussed theories of motivation. Abraham Maslow believed that man is inherently good and argued that individuals possess a constantly growing inner drive that has great potential. The needs hierarchy system, devised by Maslow (1954), is a commonly used scheme for classifying human motives.

The American motivation psychologist Abraham H. Maslow developed the hierarchy of needs consisting of five hierarchic classes. According to Maslow, people are motivated by unsatisfied needs.
The needs, listed from basic (lowest-earliest) to most complex (highest-latest) are as follows:
The basic requirements build upon the first step in the pyramid: physiology. If there are deficits on this level, all behavior will be oriented to satisfy this deficit. Essentially, if you have not slept or eaten adequately, you won't be interested in your self-esteem desires. Subsequently, we have the second level, which awakens a need for security. After securing those two levels, the motives shift to the social sphere, the third level. Psychological requirements comprise the fourth level, while the top of the hierarchy consists of self-realization and self-actualization.

Maslow's hierarchy of needs theory can be summarized as follows:

One of the first influential figures to discuss the topic of Hedonism was Socrates, and he did so around 470- 399 BC in ancient Greece. Hedonism, as Socrates described it, is the motivation wherein a person will behave in a manner that will maximize pleasure and minimize pain. The only instance in which a person will behave in a manner that results in more pain than pleasure is when the knowledge of the effects of the behavior is lacking. Sex is one of the pleasures people pursue.

Sex is on the first level of Maslow's Hierarchy of needs. It is a necessary physiological need, like air, warmth, or sleep, and if the body lacks it will not function optimally. Without the orgasm that comes with sex, a person will experience “pain,” and as Hedonism would predict, a person will minimize this pain by pursuing sex. That being said, sex as a basic need is different from the need for sexual intimacy, which is located on the third level in Maslow's Hierarchy.

There are multiple theories for why sex is a strong motivation, and many fall under the Theory of Evolution. On an evolutionary level, the motivation for sex likely has to do with a species’ ability to reproduce. Species that reproduce more, survive and pass on their genes. Therefore, species have sexual desire that leads to sexual intercourse as a means to create more offspring. Without this innate motivation, a species may determine that attaining intercourse is too costly in terms of effort, energy, and danger.

In addition to sexual desire, the motivation for romantic love runs parallel in having an evolutionary function for the survival of a species. On an emotional level, romantic love satiates a psychological need for belonging. Therefore, this is another hedonistic pursuit of pleasure. From the evolutionary perspective, romantic love creates bonds with the parents of offspring. This bond will make it so that the parents will stay together and take care and protect the offspring until it is independent. By rearing the child together, it increases the chances that the offspring will survive and pass on its genes itself, therefore continuing the survival of the species. Without the romantic love bond, the male will pursue satiation of his sexual desire with as many mates as possible, leaving behind the female to rear the offspring by herself. Child rearing with one parent is more difficult and provides less assurance that the offspring survives than with two parents. Romantic love therefore solves the commitment problem of parents needing to be together; individuals that are loyal and faithful to one another will have mutual survival benefits.

Additionally, under the umbrella of evolution, is Darwin's term sexual selection. This refers to how the female selects the male for reproduction. The male is motivated to attain sex because of all the aforementioned reasons, but how he attains it can vary based on his qualities. For some females, they are motivated by the will to survive mostly, and will prefer a mate that can physically defend her, or financially provide for her (among humans). Some females are more attracted to charm, as it is an indicator of being a good loyal lover that will in turn make for a dependable child rearing partner. Altogether, sex is a hedonistic pleasure seeking behavior that satiates physical and psychological needs and is instinctively guided by principles of evolution.

Frederick Herzberg's two-factor theory concludes that certain factors in the workplace result in job satisfaction (motivators), while others (hygiene factors), if absent, lead to dissatisfaction but are not related to satisfaction. The factors that motivate people can change over their lifetime, but "respect for me as a person" is one of the top motivating factors at any stage of life.

He distinguished between:

Herzberg concluded that job satisfaction and dissatisfaction were the products of two separate factors: motivating factors (satisfiers) and hygiene factors (dissatisfiers).

Some motivating factors (satisfiers) were: Achievement, recognition, work itself, responsibility, advancement, and growth.

Some hygiene factors (dissatisfiers) were: company policy, supervision, working conditions, interpersonal relations, salary, status, job security, and personal life.

The name hygiene factors is used because, like hygiene, the presence will not improve health, but absence can cause health deterioration.

Herzberg's theory has found application in such occupational fields as information systems and in studies of user satisfaction such as computer user satisfaction.

Alderfer, expanding on Maslow's hierarchy of needs, created the "ERG theory". This theory posits that there are three groups of core needs — existence, relatedness, and growth, hence the label: ERG theory. The existence group is concerned with providing our basic material existence requirements. They include the items that Maslow considered to be physiological and safety needs. The second group of needs are those of relatedness- the desire we have for maintaining important personal relationships. These social and status desires require interaction with others if they are to be satisfied, and they align with Maslow's social need and the external component of Maslow's esteem classification. Finally, Alderfer isolates growth needs as an intrinsic desire for personal development. Maslow's categories are broken down into many different parts and there are a lot of needs. The ERG categories are more broad and covers more than just certain areas. As a person grows, the existence, relatedness, and growth for all desires continue to grow. All these needs should be fulfilled to greater wholeness as a human being. These include the intrinsic component from Maslow's esteem category and the characteristics included under self-actualization.

Since the early 1970s Edward L. Deci and Richard M. Ryan have conducted research that eventually led to the proposition of the self-determination theory (SDT). This theory focuses on the degree to which an individual's behaviour is self-motivated and self-determined. SDT identifies three innate needs that, if satisfied, allow optimal function and growth: competence, relatedness, and autonomy.

These three psychological needs motivate the self to initiate specific behaviour and mental nutriments that are essential for psychological health and well-being. When these needs are satisfied, there are positive consequences, such as well-being and growth, leading people to be motivated, productive and happy. When they are thwarted, people's motivation, productivity and happiness plummet.

There are three essential elements to the theory:

Within Self-Determination Theory, Deci & Ryan distinguish between four different types of extrinsic motivation, differing in their levels of perceived autonomy:

A recent approach in developing a broad, integrative theory of motivation is temporal motivation theory. Introduced in a 2006 "Academy of Management Review" article, it synthesizes into a single formulation the primary aspects of several other major motivational theories, including Incentive Theory, Drive Theory, Need Theory, Self-Efficacy and Goal Setting. It simplifies the field of motivation and allows findings from one theory to be translated into terms of another. Another journal article that helped to develop the Temporal Motivation Theory, "The Nature of Procrastination, " received American Psychological Association's George A. Miller award for outstanding contribution to general science.

where Motivation is the desire for a particular outcome, Expectancy or self-efficacy is the probability of success, Value is the reward associated with the outcome, Impulsiveness is the individual's sensitivity to delay and Delay is the time to realization.

Achievement motivation is an integrative perspective based on the premise that performance motivation results from the way broad components of personality are directed towards performance. As a result, it includes a range of dimensions that are relevant to success at work but which are not conventionally regarded as being part of performance motivation. The emphasis on performance seeks to integrate formerly separate approaches as need for achievement with, for example, social motives like dominance. Personality is intimately tied to performance and achievement motivation, including such characteristics as tolerance for risk, fear of failure, and others.

Achievement motivation can be measured by The Achievement Motivation Inventory, which is based on this theory and assesses three factors (in 17 separated scales) relevant to vocational and professional success. This motivation has repeatedly been linked with adaptive motivational patterns, including working hard, a willingness to pick learning tasks with much difficulty, and attributing success to effort.

Achievement motivation was studied intensively by David C. McClelland, John W. Atkinson and their colleagues since the early 1950s. This type of motivation is a drive that is developed from an emotional state. One may feel the drive to achieve by striving for success and avoiding failure. In achievement motivation, one would hope that they excel in what they do and not think much about the failures or the negatives. Their research showed that business managers who were successful demonstrated a high need to achieve no matter the culture. There are three major characteristics of people who have a great need to achieve according to McClelland's research.

Cognitive theories define motivation in terms of how people think about situations. Cognitive theories of motivation include goal-setting theory and expectancy theory.

Goal-setting theory is based on the notion that individuals sometimes have a drive to reach a clearly defined end state. Often, this end state is a reward in itself. A goal's efficiency is affected by three features: proximity, difficulty and specificity. One common goal setting methodology incorporates the SMART criteria, in which goals are: specific, measurable, attainable/achievable, relevant, and time-bound. Time management is an important aspect to consider, when regarding time as a factor contributing to goal achievement. Having too much time allows area for distraction and procrastination, which simultaneously distracts the subject by steering his or her attention away from the original goal. An ideal goal should present a situation where the time between the initiation of behavior and the end state is close. With an overly restricting time restraint, the subject could potentially feel overwhelmed, which could deter the subject from achieving the goal because the amount of time provided is not sufficient or rational. This explains why some children are more motivated to learn how to ride a bike than to master algebra. A goal should be moderate, not too hard or too easy to complete.

Most people are not optimally motivated, as many want a challenge (which assumes some kind of insecurity of success). At the same time people want to feel that there is a substantial probability that they will succeed. Specificity concerns the description of the goal in their class. The goal should be objectively defined and intelligible for the individual. Similarly to Maslow's Hierarchy of Needs, a larger end goal is easier to achieve if the subject has smaller, more attainable yet still challenging goals to achieve first in order to advance over a period of time. A classic example of a poorly specified goal is trying to motivate oneself to run a marathon when s/he has not had proper training. A smaller, more attainable goal is to first motivate oneself to take the stairs instead of an elevator or to replace a stagnant activity, like watching television, with a mobile one, like spending time walking and eventually working up to a jog.

Expectancy theory was proposed by Victor H. Vroom in 1964. Expectancy theory explains the behavior process in which an individual selects a behavior option over another, and why/how this decision is made in relation to their goal.

There's also an equation for this theory which goes as follows:

Procrastination is the act to voluntarily postpone or delay an intended course of action despite anticipating that you will be worse off because of that delay. While procrastination was once seen as a harmless habit, recent studies indicate otherwise. In a 1997 study conducted by Dianne Tice and William James Fellow Roy Baumeister at Case Western University, college students were given ratings on an established scale of procrastination, and tracked their academic performance, stress, and health throughout the semester. While procrastinators experienced some initial benefit in the form of lower stress levels (presumably by putting off their work at first), they ultimately earned lower grades and reported higher levels of stress and illness.

Procrastination can be seen as a defense mechanism. Because it is less demanding to simply avoid a task instead of dealing with the possibility of failure, procrastinators choose the short-term gratification of delaying a task over the long-term uncertainty of undertaking it. Procrastination can also be a justification for when the user ultimately has no choice but to undertake a task and performs below their standard. For example, a term paper could be seem as a daunting task. If the user puts it off until the night before, they can justify their poor score by telling themselves that they would have done better with more time. This kind of justification is extremely harmful and only helps to perpetuate the cycle of procrastination.

Over the years, scientists have determined that not all procrastination is the same. The first type are chronic procrastinators whom exhibit a combination of qualities from the other, more specialized types of procrastinators. "Arousal" types are usually self-proclaimed "pressure performers" and relish the exhilaration of completing tasks close to the deadline. "Avoider" types procrastinate to avoid the outcome of whatever task they are pushing back - whether it be a potential failure or success. "Avoider" types are usually very self-conscious and care deeply about other people's opinions. Lastly, "Decisional" procrastinators avoid making decisions in order to protect themselves from the responsibility that follows the outcome of events.

Social-cognitive models of behavior change include the constructs of motivation and volition. Motivation is seen as a process that leads to the forming of behavioral intentions. Volition is seen as a process that leads from intention to actual behavior. In other words, motivation and volition refer to goal setting and goal pursuit, respectively. Both processes require self-regulatory efforts. Several self-regulatory constructs are needed to operate in orchestration to attain goals. An example of such a motivational and volitional construct is perceived self-efficacy. Self-efficacy is supposed to facilitate the forming of behavioral intentions, the development of action plans, and the initiation of action. It can support the translation of intentions into action.

John W. Atkinson, David Birch and their colleagues developed the theory of "Dynamics of Action" to mathematically model change in behavior as a consequence of the interaction of motivation and associated tendencies toward specific actions. The theory posits that change in behavior occurs when the tendency for a new, unexpressed behavior becomes dominant over the tendency currently motivating action. In the theory, the strength of tendencies rises and falls as a consequence of internal and external stimuli (sources of instigation), inhibitory factors, and consummatory in factors such as performing an action. In this theory, there are three causes responsible for behavior and change in behavior:

Thematic Apperception Test (TAT) was developed by American psychologists Henry A. Murray and Christina D. Morgan at Harvard during the early 1930's. Their underlying goal was to test and discover the dynamics of personality such as internal conflict, dominant drives, and motives. Testing is derived of asking the individual to tell a story, given 31 pictures that they must choose ten to describe. To complete the assessment, each story created by the test subject must be carefully recorded and monitored to uncover underlying needs and patterns of reactions each subject perceives. After evaluation, two common methods of research, Defense Mechanisms Manual (DMM) and Social Cognition and Object Relations (SCOR), are used to score each test subject on different dimensions of object and relational identification. From this, the underlying dynamics of each specific personality and specific motives and drives can be determined.

Starting from studies involving more than 6,000 people, Professor Steven Reiss has proposed a theory that found 16 basic desires that guide nearly all human behavior. The 16 basic desires that motivate our actions and define our personalities are:

Attribution theory is a theory developed by psychologist, Fritz Heider that describes the processes by which individuals explain the causes of their behavior and events. A form of attribution theory developed by psychologist, Bernard Weiner describes an individual's beliefs about how the causes of success or failure affect their emotions and motivations. Bernard Weiner's theory can be defined into two perspectives: intrapersonal or interpersonal. The intrapersonal perspective includes self-directed thoughts and emotions that are attributed to the self. The interpersonal perspective includes beliefs about the responsibility of others and other directed affects of emotions; the individual would place the blame on another individual.

Individuals formulate explanatory attributions to understand the events they experience and to seek reasons for their failures. When individuals seek positive feedback from their failures, they use the feedback as motivation to show improved performances. For example, using the intrapersonal perspective, a student who failed a test may attribute their failure for not studying enough and would use their emotion of shame or embarrassment as motivation to study harder for the next test. A student who blames their test failure on the teacher would be using the interpersonal perspective, and would use their feeling of disappointment as motivation to rely on a different study source other than the teacher for the next test.

Approach motivation (i.e., incentive salience) can be defined as when a certain behavior or reaction to a situation/environment is rewarded or results in a positive or desirable outcome. In contrast, avoidance motivation (i.e., aversive salience) can be defined as when a certain behavior or reaction to a situation/environment is punished or results in a negative or undesirable outcome. Research suggests that, all else being equal, avoidance motivations tend to be more powerful than approach motivations. Because people expect losses to have more powerful emotional consequences than equal-size gains, they will take more risks to avoid a loss than to achieve a gain.

Conditioned taste aversion.

“A strong dislike (nausea reaction) for food because of prior Association with of that food with nausea or upset stomach.” 

Conditioned taste aversion is the only type of conditioning that only needs one exposure. It does not need to be the specific food or drinks that cause the taste. Conditioned taste aversion can also be attributed to extenuating circumstances. An example of this can be eating a rotten apple. Eating the apple then immediately throwing up. Now it is hard to even near an apple without feeling sick. Conditioned taste aversion can also come about by the mere associations of two stimuli. Eating a peanut butter and jelly sandwich, but also have the flu. Eating the sandwich makes one feel nauseous, so one throws up, now one cannot smell peanut butter without feeling queasy. Though eating the sandwich does not cause one to through up, they are still linked.

In his book "A General Introduction to Psychoanalysis", Sigmund Freud explained his theory on the conscious-unconscious distinction. To explain this relationship, he used a two-room metaphor. The smaller of the two rooms is filled with a person's preconscious, which is the thoughts, emotions, and memories that are available to a person's consciousness. This room also houses a person's consciousness, which is the part of the preconscious that is the focus at that given time. Connected to the small room is a much larger room that houses a person's unconscious. This part of the mind is unavailable to a person's consciousness and consists of impulses and repressed thoughts. The door between these two rooms acts as the person's mental censor. It's job is to keep anxiety inducing thoughts and socially unacceptable behaviors or desires out of the preconscious. Freud describes the event of a thought or impulse being denied at the door as repression, one of the many defense mechanisms. This process is supposed to protect the individual from any embarrassment that could come from acting on these impulses or thoughts that exist in the unconscious.

In terms of motivation, Freud argues that unconscious instinctual impulses can still have great influence on behavior even though the person is not aware of the source. When these instincts serve as a motive, the person is only aware of the goal of the motive, and not its actual source. He divides these instincts into sexual instincts, death instincts, and ego or self-preservation instincts. Sexual instincts are those that motivate humans to stay alive and ensure the continuation of the mankind. On the other hand, Freud also maintains that humans have an inherent drive for self-destruction, or the death instinct. Similar to the devil and angel that everyone has on there should, the sexual instinct and death instinct are constantly battling each other to both be satisfied. The death instinct can be closely related to Freud's other concept, the id, which is our need to experience pleasure immediately, regardless of the consequences. The last type of instinct that contributes to motivation is the ego or self-preservation instinct. This instinct is geared towards assuring that a person feels validated in whatever behavior or thought they have. The mental censor, or door between the unconscious and preconscious, helps satisfy this instinct. For example, one may be sexually attracted to a person, due to their sexual instinct, but the self-preservation instinct prevents them to act on this urge until that person finds that it is socially acceptable to do so. Quite similarly to his psychic theory that deals with the id, ego, and superego, Freud's theory of instincts highlights the interdependence of these three instincts. All three instincts serve as a checks and balances system to control what instincts are acted on and what behaviors are used to satisfy as many of them at once.

Priming is a phenomenon, often used as an experimental technique, whereby a specific stimulus sensitizes the subject to later presentation of a similar stimulus.

“Priming refers to an increased sensitivity to certain stimuli, resulting from prior exposure to related visual or audio messages. When an individual is exposed to the word “cancer,” for example, and then offered the choice to smoke a cigarette, we expect that there is a greater probability that they will choose not to smoke as a result of the earlier exposure.”

Priming can affect motivation, in the way that we can be motived to do things by an outside source.

Priming can be linked with the mere exposer theory. People tend to like thing that they have exposed to before. Mere exposer theory is used by advertising companies to get people to buy their products. An example of this is seeing a picture of the product on a sign and then buying that product later. If an individual is in a room with two strangers they are more likely to gravitate towards the person that they occasionally pass on the street, then the person that they have never seen before. An example of the use of mere exposure theory can be seen in product placements in movies and TV shows. We see a product that our is in our favorite movie, and we are more inclined to buy that product when we see it again.

Priming can fit into these categories; Semantic Priming, Visual Priming, Response Priming, Perceptual and Conceptual Priming, Positive and Negative Priming, Associative and Context Priming, and Olfactory Priming. Visual and Semantic priming is the most used in motivation. Most priming is linked with emotion, the stronger the emotion, the stronger the connection between memory and the stimuli.

Priming also has an effect on drug users. In this case, it can be defined at, the reinstatement or increase in drug craving by a small dose of the drug or by stimuli associated with the drug. If a former drug user is in a place where they formerly did drugs, then they are tempted to do that same thing again even if they have been clean for years.

Mental fatigue is being tired, exhausted, or not functioning effectively. Not wanting to proceed further with the current mental course of action, this in contrast with physical fatigue, because in most cases no physical activity is done. This is best seen in the workplace or schools. A perfect example of mental fatigue is seen in college students just before finals approach. One will notice that students start eating more than they usually do and care less about interactions with friends and classmates. Mental fatigue arises when an individual becomes involved in a complex task but does no physical activity and is still worn out, the reason for this is because the brain uses about 20 percent of the human body’s metabolic heart rate. The brain consumes about 10.8 calories every hour. Meaning that a typical human adult brain runs on about twelve watts of electricity or a fifth of the power need to power a standard light bulb. These numbers represent an individual’s brain working on routine tasks, things that are not challenging. One study suggests that when presented with a complex task, an individual would need to consume about two hundred more calories than if they were resting or relaxing.  The symptoms of mental fatigue can range from low motivation and loss of concentration to the more severe symptoms of headaches, dizziness, and impaired decision making and judgment. Mental fatigue can affect an individual’s life by causing a lack of motivation, avoidance of friends and family members and changes in one’s mood. To treat mental fatigue, one must figure out what is causing the fatigue. Once the cause of the stress has been identified the individual must determine what they can do about it. Most of the time mental fatigue can be fixed by a simple life change like being more organized or learning to say no. According to the study: Mental fatigue caused by prolonged cognitive load associated with sympathetic hyperactivity, “there is evidence that decreased parasympathetic activity and increased relative sympathetic activity are associated with mental fatigue induced by prolonged cognitive load in healthy adults.” this means that though no physical activity was done, the sympathetic nervous system was triggered. An individual who is experiencing mental fatigue will not feel relaxed but feel the physical symptoms of stress.

Learned industriousness theory is the theory about an acquired ability to sustain the physical or mental effort. It can also be described as being persistent despite the building up subjective fatigue. This is the ability to push through to the end for a greater or bigger reward. The more significant or more rewarding the incentive, the more the individual is willing to do to get to the end of a task. This is one of the reasons that college students will go on to graduate school. The students may be worn out, but they are willing to go through more school for the reward of getting a higher paying job when they are out of school.

The control of motivation is only understood to a limited extent. There are many different approaches of "motivation training", but many of these are considered pseudoscientific by critics. To understand how to control motivation it is first necessary to understand why many people lack motivation.

Natural theories of motivation such as Theory Y argue that individuals are naturally willing to work and prefer jobs with high responsibility, creativity and ingenuity. Holistically, the implementation in the workplace based on natural theories of motivation requires creating a comfortable and open work environment because it is through this climate that the individuals’ goals are most likely to be aligned with the organization's goals.
Based on the assumptions of natural theorists, individuals are motivated to work for an organization when they feel fulfillment from the work and organization. Therefore, hiring should focus on matching the goals of the individual with the goals of the organization rather than solely on the candidate's proficiency at completing a task, as rational theorists would argue. Logistically, there are several ways that firms can implement the assumptions of natural theories of motivation, including delegation of responsibilities, participation in management by employees, job enlargement, and membership within the firm.

McGregor's Theory Y makes the assumption that the average person not only accepts, but also seeks out responsibility. Thus, as a firm gives individuals’ greater responsibilities, they will feel a greater sense of satisfaction and, subsequently, more commitment to the organization. Additionally, Malone argues that the delegation of responsibility encourages motivation because employees have creative control over their work and increases productivity as many people can work collaboratively to solve a problem rather than just one manager tackling it alone.

Participative management styles involve consulting employees through the decision making process. Markowitz argues that this boosts employees’ morale and commitment to the organization, subsequently increasing productivity. Furthermore, Denison provides empirical evidence demonstrating that employee participation is correlated with better organizational performance. It is important to note that this stands in contrast to Graham's rationalist view that kaizen, a participative management style used in Japan, does not engage employees’ minds in the decision making process. Graham, however, only examines one specific and flawed participative management style that only allows limited input from employees. With a properly implemented process that actively engages employees, participative management will create a welcoming and productive environment.

Job enlargement refers to increasing the responsibilities of a job by adding to the scope of the tasks. This provides more variety and prevents a job from getting boring. Additionally, this prevents the problem of alienation brought on by the rational theorists of Fordism. In assembly lines, the employee feels disconnected from the final product because he or she only performs one task repeatedly. Job enlargement instead keeps employees engaged in the organization and creates a more welcoming environment. It stems on the assumption that employees enjoy doing work and, therefore, are more satisfied when they have a wider range of work to do.

As Mayo details, based on observations of the Hawthorn Western Electric Company, an additional facet of motivation stems from creating a culture of teams and membership within the firm. For employees, a large part of job satisfaction is feeling as though one is a member of a larger team. For example, Mayo writes about a young girl worker who refused a transfer to a higher paid position in order to stay with a group that she felt a connection to. This example demonstrates that workers are not necessarily rational and only working for higher monetary compensation; instead, the social aspects of a firm can provide incentives to work. It is important, therefore, to create an inclusive environment that welcomes each worker or employee as a member of that organization.

“If one wishes to create a highly valid theory, which is also constructed with the purpose of enhanced usefulness in practice in mind, it would be best to look to motivation theories ... for an appropriate model” (Miner, 2003, p. 29).

The Job characteristics Model (JCM), as designed by Hackman and Oldham attempts to use job design to improve employee motivation. They show that any job can be described in terms of five key job characteristics:

The JCM links the core job dimensions listed above to critical psychological states which results in desired personal and work outcomes. This forms the basis of this 'employee growth-need strength." The core dimensions listed above can be combined into a single predictive index, called the "motivating potential score".

The motivating potential score (MPS) can be calculated, using the core dimensions discussed above, as follows:

Jobs high in motivating potential must be high on both Autonomy and Feedback, and also must be high on at least one of the three factors that lead to experienced meaningfulness. If a job has a high MPS, the job characteristics model predicts motivation, performance and job satisfaction will be positively affected and the likelihood of negative outcomes, such as absenteeism and turnover, will be reduced.

Employee recognition is not only about gifts and points. It's about changing the corporate culture in order to meet goals and initiatives and most importantly to connect employees to the company's core values and beliefs. Strategic employee recognition is seen as the most important program not only to improve employee retention and motivation but also to positively influence the financial situation. The difference between the traditional approach (gifts and points) and strategic recognition is the ability to serve as a serious business influencer that can advance a company's strategic objectives in a measurable way. "The vast majority of companies want to be innovative, coming up with new products, business models and better ways of doing things. However, innovation is not so easy to achieve. A CEO cannot just order it, and so it will be. You have to carefully manage an organization so that, over time, innovations will emerge."

Motivation is of particular interest to educational psychologists because of the crucial role it plays in student learning. However, the specific kind of motivation that is studied in the specialized setting of education differs qualitatively from the more general forms of motivation studied by psychologists in other fields.

Motivation in education can have several effects on how students learn and how they behave towards subject matter. It can:

Because students are not always internally motivated, they sometimes need "situated motivation", which is found in environmental conditions that the teacher creates.

If teachers decided to extrinsically reward productive student behaviors, they may find it difficult to extricate themselves from that path. Consequently, student dependency on extrinsic rewards represents one of the greatest detractors from their use in the classroom.

The majority of new student orientation leaders at colleges and universities recognize that distinctive needs of students should be considered in regard to orientation information provided at the beginning of the higher education experience. Research done by Whyte in 1986 raised the awareness of counselors and educators in this regard. In 2007, the National Orientation Directors Association reprinted Cassandra B. Whyte's research report allowing readers to ascertain improvements made in addressing specific needs of students over a quarter of a century later to help with academic success.

Generally, motivation is conceptualized as either "intrinsic" or "extrinsic". Classically, these categories are regarded as distinct. Today, these concepts are less likely to be used as distinct categories, but instead as two ideal types that define a continuum:

Cassandra B. Whyte researched and reported about the importance of locus of control and academic achievement. Students tending toward a more internal locus of control are more academically successful, thus encouraging curriculum and activity development with consideration of motivation theories.

Academic motivation orientation may also be tied with one's ability to detect and process errors. Fisher, Nanayakkara, and Marshall conducted neuroscience research on children's motivation orientation, neurological indicators of error monitoring (the process of detecting an error), and academic achievement. Their research suggests that students with high intrinsic motivation attribute performance to personal control and that their error-monitoring system is more strongly engaged by performance errors. They also found that motivation orientation and academic achievement were related to the strength in which their error-monitoring system was engaged.

Motivation has been found to be an important element in the concept of andragogy (what motivates the adult learner), and in treating Autism Spectrum Disorders, as in pivotal response treatment.
Motivation has also been found critical in adolescents compliance to health suggestions, since "commitment requires belief in potentially negative and serious consequences of not acting".

Doyle and Moeyn have noted that traditional methods tended to use anxiety as negative motivation (e.g. use of bad grades by teachers) as a method of getting students to work. However, they have found that progressive approaches with focus on positive motivation over punishment has produced greater effectiveness with learning, since anxiety interferes with performance of complex tasks.

Symer et al. attempted to better define those in medical training programs who may have a ”surgical personality”. They evaluated a group of eight hundred and one first-year surgical interns to compare motivational traits amongst those who did and did not complete surgical training. There was no difference noted between the 80.5% who completed training when comparing their responses to the 19.5% who did not complete training using the validated Behavior Inhibitory System/Behavior Approach System. They concluded based on this that resident physician motivation is not associated with completion of a surgical training program.

It may appear that the reason some students are more engaged and perform better in class activities relative to other students is because some are more motivated than others. However, current research suggests that motivation is not concrete or quantifiable; it is “dynamic, context sensitive, and changeable.” Thus, students have the flexibility to, intrinsically, motivate themselves to engage in an activity or learn something new even if they were not intrinsically motivated in the first place. While having this type of flexibility is important, research reveals that a teacher's teaching style and the school environment also play a factor in student motivation.;

According to Sansone and Morgan, when students are already motivated to engage in an activity for their own personal pleasure and then a teacher provides the student with feedback, the type of feedback given can change the way that student views the activity and can even undermine their intrinsic motivation. Maclellan also looked at the relationship between tutors and students and in particular, and the type of feedback the tutor would give to the student. Maclellan's results showed that praise or criticism directed towards the student generated a feeling of “fixed intelligence” while praise and criticism directed towards the effort and strategy used by the student generated a feeling of “malleable intelligence”. In other words, feedback concerning effort and strategy leaves students knowing that there is room for growth. This is important because when students believe their intelligence is “fixed”, their mindset can prevent skill development because students will believe that they only have a “certain amount” of understanding on a particular subject matter and might not even try.Therefore, it's crucial that a teacher is aware of how the feedback they give to their students can both positively and negatively impact the student's engagement and motivation.

In a correlational study, Katz and Shahar used a series of questionnaires and Likert-style scales and gave them to 100 teachers to see what makes a motivating teacher. Their results indicate that teachers who are intrinsically motivated to teach and believe that students should be taught in an autonomous style are the types of teachers that promote intrinsic motivation in the classroom. Deci, Sheinman, and Nezlek also found that when teachers adapted to an autonomous teaching style, students were positively affected and became more intrinsically motivated to achieve in the classroom. However, while the students were quick to adapt to the new teaching style the impact was short-lived. Thus, teachers are limited in the way they teach because they’ll feel a pressure to act, teach, and provide feedback in a certain way from the school district, administration, and guardians. Furthermore, even if students do have a teacher that promotes an autonomous teaching style, their overall school environment is also a factor because it can be extrinsically motivating. Examples of this would be posters around school promoting pizza parties for highest grade point average or longer recess times for the classroom that brings more canned food donations.

In conclusion, it is not a matter whether a student is motivated, unmotivated, or more motivated than other students- it's a matter of understanding what motivates students before providing a certain type of feedback. Furthermore, it is also important to note that despite the classroom environment and the teacher's teaching style, the overall school environment plays a role in students’ intrinsic motivation.

For many indigenous students (such as Native American children), motivation may be derived from social organization; an important factor educators should account for in addition to variations in sociolinguistics and cognition. While poor academic performance among Native American students is often attributed to low levels of motivation, Top-down classroom organization is often found to be ineffective for children of many cultures who depend on a sense of community, purpose, and competence in order to engage. Horizontally structured, community-based learning strategies often provide a more structurally supportive environment for motivating indigenous children, who tend to be driven by "social/affective emphasis, harmony, holistic perspectives, expressive creativity, and nonverbal communication." This drive is also traceable to a cultural tradition of community-wide expectations of participation in the activities and goals of the greater group, rather than individualized aspirations of success or triumph.

Also, in some indigenous communities, young children can often portray a sense of community-based motivation through their parent-like interactions with siblings. Furthermore, it is commonplace for children to assist and demonstrate for their younger counterparts without being prompted by authority figures. Observation techniques and integration methods are demonstrated in such examples as weaving in Chiapas, Mexico, where it is commonplace for children to learn from "a more skilled other" within the community. The child's real responsibility within the Mayan community can be seen in, for example, weaving apprenticeships; often, when the "more skilled other" is tasked with multiple obligations, an older child will step in and guide the learner. Sibling guidance is supported from early youth, where learning through play encourages horizontally structured environments through alternative educational models such as "Intent Community Participation." Research also suggests that formal Westernized schooling can actually reshape the traditionally collaborative nature of social life in indigenous communities. This research is supported cross-culturally, with variations in motivation and learning often reported higher between indigenous groups and their national Westernized counterparts than between indigenous groups across international continental divides.

Also, in some Indigenous communities in the Americas, motivation is a driving force for learning. Children are incorporated and welcomed to participate in daily activities and thus feel motivated to participate due to them seeking a sense of belonging in their families and communities.

Children's participation is encouraged and their learning is supported by their community and family, furthering their motivation. Children are also trusted to be active contributors. Their active participation allows them to learn and gain skills that are valuable and useful in their communities.

As children transition from early childhood to middle childhood, their motivation to participate changes. In both the Indigenous communities of Quechua people and Rioja in Peru, children often experience a transition in which they become more included into their family's and community's endeavors. This changes their position and role in their families to more responsible ones and leads to an increase in their eagerness to participate and belong. As children go through this transition, they often develop a sense of identity within their family and community.

The transition from childhood to adolescence can be seen in the amount of work children partake in as this changes over time. For example, Yucatec Mayan children's play time decreases from childhood to adolescence and as the child gets older, is replaced for time spent working. In childhood the work is initiated by others whereas in adolescence it is self-initiated. The shift in initiation and the change in time spent working versus playing shows the children's motivation to participate in order to learn.

This transition between childhood and adolescence increases motivation because children gain social responsibility within their families. In some Mexican communities of Indigenous-heritage, the contributions that children make within their community is essential to being social beings, establishes their developing roles, and also helps with developing their relationship with their family and community.

As children gain more roles and responsibilities within their families, their eagerness to participate also increases. For example, Young Mayan children of San Pedro, Guatemala learn to work in the fields and family run businesses because they are motivated to contribute to their family. Many San Pedro women learned to weave by watching their mothers sew when they were children, sometimes earning their own wool through doing small tasks such as watching young children of busy mothers. Eager to learn and contribute, these young girls helped other members of their community in order to help their mothers with their weaving businesses or through other tasks such as helping carry water while young boys helped with tasks such as carrying firewood alongside their fathers.

Children's motivation to learn is not solely influenced on their desire to belong but also their eagerness to see their community succeed. Children from Navajo communities were shown to have higher levels of social concern than Anglo American children in their schools. By having high levels of social concern the indigenous children are showing concern for not only their learning but also their peers', which serves as an example of their instilled sense of responsibility for their community. They wish to succeed as a united group rather than just themselves.

In order to be knowledgeable contributors, children must be aware of their surroundings and community's goals. Children's learning in Indigenous-heritage communities is mainly based upon observing and helping out others in their community. Through this type of participation within their community, they gain purpose and motivation for the activity that they are doing within their community and become active participants because they know they are doing it for their community.

Self-determination is the ability to make choices and exercise a high degree of control, such as what the student does and how they do it (Deci et al., 1991; Reeve, Hamm, & Nix, 2003; Ryan & Deci, 2002). Self-determination can be supported by providing opportunities for students to be challenged, such as leadership opportunities, providing appropriate feedback and fostering, establishing and maintaining good relationships between teachers and students. These strategies can increase students' interest, competence, creativity and desire to be challenged and ensure that students are intrinsically motivated to study. On the other hand, students who lack self-determination are more likely to feel their success is out of their control. Such students lose motivation to study, which causes a state of "learned helplessness". Students who feel helpless readily believe they will fail and therefore cease to try. Over time, a vicious circle of low achievement develops.

Physical activity is body movement that works your muscles and requires more energy than resting. According to a blog by the American Intercontinental University, college students should make time for exercise to maintain and increase motivation. AIU states that regular exercise has impeccable effects on the brain. With consistent running routines, there are more complex connections between neurons, meaning the brain is able to access its brain cells more flexibly. By performing well physically, motivation will be present in education because of how well the brain is performing. After exercising, the brain can have more desire to obtain knowledge and better retain the information. In addition, exercise can relieve stress. Exercising can ease anxiety and relieve negative effects of stress on the body. Without stress factors, individuals can perform better and more efficiently, since their minds will have a more positive outlook. This positive mood will help keep students motivated and more open and willing to succeed academically. Lastly, exercise increases focus and concentration that could also help students maintain their motivation and focus on their studies. AIU claims that exercise may have improved the students' ability to participate and retain information during the class after they had exercised. Being able to retain information and being willing to participate keeps students motivated and performing well academically.

Within Maslow's hierarchy of needs (first proposed in 1943), at lower levels (such as physiological needs) money functions as a motivator; however, it tends to have a motivating effect on staff that lasts only for a short period (in accordance with Herzberg's two-factor model of motivation of 1959). At higher levels of the hierarchy, praise, respect, recognition, empowerment and a sense of belonging are far more powerful motivators than money, as both Abraham Maslow's theory of motivation and Douglas McGregor's theory X and theory Y (originating in the 1950s and pertaining to the theory of leadership) suggest.

According to Maslow, people are motivated by unsatisfied needs. The lower-level needs (such as Physiological and Safety needs) must be satisfied before addressing higher-level needs. One can relate Maslow's Hierarchy of Needs theory with employee motivation. For example, if managers attempt to motivate their employees by satisfying their needs; according to Maslow, they should try to satisfy the lower-level needs before trying to satisfy the upper-level needs - otherwise the employees will not become motivated. Managers should also remember that not everyone will be satisfied by the same needs. A good manager will try to figure out which levels of needs are relevant to a given individual or employee.

Maslow places money at the lowest level of the hierarchy and postulates other needs as better motivators to staff. McGregor places money in his Theory X category and regards it as a poor motivator. Praise and recognition (placed in the Theory Y category) are considered stronger motivators than money.

The average workplace lies about midway between the extremes of high threat and high opportunity. Motivation by threat is a dead-end strategy, and naturally staff are more attracted to the opportunity side of the motivation curve than the threat side. Lawrence Steinmetz (1983) sees motivation as a powerful tool in the work environment that can lead to employees working at their most efficient levels of production.

Nonetheless, Steinmetz also discusses three common character-types of subordinates: ascendant, indifferent, and ambivalent - who all react and interact uniquely, and must be treated, managed, and motivated accordingly. An effective leader must understand how to manage all characters, and more importantly the manager must utilize avenues that allow room for employees to work, grow, and find answers independently.

A classic study at Vauxhall Motors' UK manufacturing plant challenged the assumptions of Maslow and Herzberg were by. Goldthorpe "et al." (1968) introduced the concept of orientation to work and distinguished three main orientations:


Other theories expanded and extended those of Maslow and Herzberg. These included the 1930s force-field analysis of Kurt Lewin, Edwin A. Locke's goal-setting theory (mid-1960s onwards) and Victor Vroom's expectancy theory of 1964. These tend to stress cultural differences and the fact that different factors tend to motivate individuals at different times.

According to the system of scientific management developed by Frederick Winslow Taylor (1856-1915), pay alone determines a worker's motivation, and therefore management need not consider psychological or social aspects of work. In essence, scientific management bases human motivation wholly on extrinsic rewards and discards the idea of intrinsic rewards.

In contrast, David McClelland (1917-1998) believed that workers could not be motivated by the mere need for money—in fact, extrinsic motivation (e.g., money) could extinguish intrinsic motivation such as achievement motivation, though money could be used as an indicator of success for various motives, e.g., keeping score. In keeping with this view, his consulting firm, McBer & Company (1965-1989), had as its first motto "To make everyone productive, happy, and free". For McClelland, satisfaction lay in aligning peoples' lives with their fundamental motivations.

Elton Mayo (1880-1949) discovered the importance of the social contacts a worker has at the workplace and found that boredom and repetitiveness of tasks lead to reduced motivation. Mayo believed that workers could be motivated by acknowledging their social needs and making them feel important. As a result, employees were given freedom to make decisions on-the-job and greater attention was paid to informal work-groups.

Mayo named his model the Hawthorne effect. His model has been judged as placing undue reliance on social contacts within work situations for motivating employees.

In 1981 William Ouchi introduced Theory Z, a hybrid management approach consisting of both Japanese and American philosophies and cultures. Its Japanese segment is much like the clan culture where organizations focus on a standardized structure with heavy emphasis on socialization of its members. All underlying goals are consistent across the organization. Its American segment retains formality and authority amongst members and the organization. Ultimately, Theory Z promotes common structure and commitment to the organization, as well as constant improvement of work efficacy.

In "Essentials of Organizational Behavior" (2007), Robbins and Judge examine recognition programs as motivators, and identify five principles that contribute to the success of an employee-incentive program:
Modern organizations adopt non-monetary employee motivation methods rather than tying it with tangible rewards.

This method makes employees feel they're a part of the organization and their reward is seeing it grow through their efforts.

Motivational models are central to game design, because without motivation, a player will not be interested in progressing further within a game. Several models for gameplay motivations have been proposed, including Richard Bartle's. Jon Radoff has proposed a four-quadrant model of gameplay motivation that includes cooperation, competition, immersion and achievement. The motivational structure of games is central to the gamification trend, which seeks to apply game-based motivation to business applications. In the end, game designers must know the needs and desires of their customers for their companies to flourish.

There have been various studies on the connection between motivation and games. One particular study was on Taiwanese adolescents and their drive of addiction to games. Two studies by the same people were conducted. The first study revealed that addicted players showed higher intrinsic than extrinsic motivation and more intrinsic motivation than the non-addicted players. It can then be said that addicted players, according to the studies findings, are more internally motivated to play games. They enjoy the reward of playing. There are studies that also show that motivation gives these players more to look for in the future such as long-lasting experience that they may keep later on in life.



</doc>
<doc id="13149599" url="https://en.wikipedia.org/wiki?curid=13149599" title="Habit">
Habit

A habit (or wont as a humorous and formal term) is a routine of behavior that is repeated regularly and tends to occur subconsciously.

The "American Journal of Psychology" (1903) defines a "habit, from the standpoint of psychology, [as] a more or less fixed way of thinking, willing, or feeling acquired through previous repetition of a mental experience." Habitual behavior often goes unnoticed in persons exhibiting it, because a person does not need to engage in self-analysis when undertaking routine tasks. Habits are sometimes compulsory. A 2002 daily experience study by habit researcher Wendy Wood (social psychologist) and her colleagues found that approximately 43% of daily behaviors are performed out of habit. New behaviours can become automatic through the process of habit formation. Old habits are hard to break and new habits are hard to form because the behavioural patterns which humans repeat become imprinted in neural pathways, but it is possible to form new habits through repetition.

A 2007 study by Wood and Neal found that when behaviors are repeated in a consistent context, there is an incremental increase in the link between the context and the action. This increases the automaticity of the behavior in that context. Features of an automatic behavior are all or some of:


Habit formation is the process by which a behavior, through regular repetition, becomes automatic or habitual. This is modelled as an increase in automaticity with number of repetitions up to an asymptote. This process of habit formation can be slow. Lally "et al." (2010) found the average time for participants to reach the asymptote of automaticity was 66 days with a range of 18–254 days.

There are three main components to habit formation: the context cue, behavioral repetition, and the reward.The context cue can be a prior action, time of day, location, or any thing that triggers the habitual behavior. This could be anything that one's mind associates with that habit and one will automatically let a habit come to the surface. The behavior is the actual habit that one exhibits, and the reward, such as a positive feeling, therefore continues the "habit loop". A habit may initially be triggered by a goal, but over time that goal becomes less necessary and the habit becomes more automatic. Intermittent or uncertain rewards have been found to be particularly effective in promoting habit learning. 

A variety of digital tools, online or mobile apps, have been introduced that are designed to support habit formation. For example, Habitica is a system that uses gamification, implementing strategies found in video games to real life tasks by adding rewards such as experience and gold. A review of such tools, however, suggests most are poorly designed with respect to theory and fail to support the development of automaticity. 

Shopping habits are particularly vulnerable to change at "major life moments" like graduation, marriage, birth of first child, moving to a new home, and divorce. Some stores use purchase data to try to detect these events and take advantage of the marketing opportunity.

Some habits are known as "keystone habits", and these influence the formation of other habits. For example, identifying as the type of person who takes care of their body and is in the habit of exercising regularly, can also influence eating better and using credit cards less. In business, safety can be a keystone habit that influences other habits that result in greater productivity.

A recent study by Adriaanse et al. (2014) found that habits mediate the relationship between self-control and unhealthy snack consumption. The results of the study empirically demonstrate that high-self control may influence the formation of habits and in turn affect behavior.

The habit–goal interface or interaction is constrained by the particular manner in which habits are learned and represented in memory. Specifically, the associative learning underlying habits is characterized by the slow, incremental accrual of information over time in procedural memory. Habits can either benefit or hurt the goals a person sets for themselves.

Goals guide habits by providing the initial outcome-oriented motivation for response repetition. In this sense, habits are often a trace of past goal pursuit. Although, when a habit forces one action, but a conscious goal pushes for another action, an oppositional context occurs. When the habit prevails over the conscious goal, a capture error has taken place.

Behavior prediction is also derived from goals. Behavior prediction is to acknowledge the likelihood that a habit will form, but in order to form that habit, a goal must have been initially present. The influence of goals on habits is what makes a habit different from other automatic processes in the mind.

The following is a description of a classic goal devaluation experiment (from a Scientific American MIND guest blog post called Should Habits or Goals Direct Your Life? It Depends) which demonstrates the difference between goal-directed and habitual behavior: A series of elegant experiments conducted by Anthony Dickinson and colleagues in the early 1980s at the University of Cambridge in England clearly exposes the behavioral differences between goal-directed and habitual processes. Basically, in the training phase, a rat was trained to press a lever in order to receive some food. Then, in a second phase, the rat was placed in a different cage without a lever and was given the food, but it was made ill whenever it ate the food. This caused the rat to "devalue" the food, because it associated the food with being ill, without directly associating the action of pressing the lever with being ill. Finally, in the test phase, the rat was placed in the original cage with the lever. (To prevent additional learning, no food was delivered in the test phase.) Rats that had undergone an extensive training phase continued to press the lever in the test phase even though the food was devalued; their behavior was called habitual. Rats that had undergone a moderate training phase did not, and their behavior was called goal-directed. … [G]oal-directed behavior is explained by the rat using an explicit prediction of the consequence, or outcome, of an action to select that action. If the rat wants the food, it presses the lever, because it predicts that pressing the lever will deliver the food. If the food has been devalued, the rat will not press the lever. Habitual behavior is explained by a strong association between an action and the situation from which the action was executed. The rat presses the lever when it sees the lever, not because of the predicted outcome.

There are a number of habits possessed by individuals that can be classified as nervous habits. These include nail-biting, stammering, sniffling, and banging the head. They are known as symptoms of an emotional state and are generally based upon conditions of anxiety, insecurity, inferiority and tension. These habits are often formed at a young age and may be because of a need for attention. When trying to overcome a nervous habit it is important to resolve the cause of the nervous feeling rather than the symptom which is a habit itself or a mountain as a result one could experience anxiety. Anxiety is a disorder known for excessive and unexpected worry that negatively impacts an individuals daily life, and routines.

A bad habit is an undesirable behavior pattern. Common examples include: procrastination, fidgeting, overspending, and nail-biting. The sooner one recognizes these bad habits, the easier it is to fix them. Rather than merely attempting to eliminate a bad habit, it may be more productive to seek to replace it with a healthier coping mechanism.

A key factor in distinguishing a bad habit from an addiction or mental disease is willpower. If a person can easily control over the behavior, then it is a habit. Good intentions can override the negative effect of bad habits, but their effect seems to be independent and additive—the bad habits remain, but are subdued rather than cancelled.

Many techniques exist for removing established bad habits, e.g., "withdrawal of reinforcers"—identifying and removing factors that trigger and reinforce the habit. The basal ganglia appears to remember the context that triggers a habit, so habits can be revived if triggers reappear. Recognizing and eliminating bad habits as soon as possible is advised. Habit elimination becomes more difficult with age because repetitions reinforce habits cumulatively over the lifespan. According to Charles Duhigg, there is a loop that includes a cue, routine and reward for every habit. An example of a habit loop is TV program ends (cue), go to the fridge (routine), eat a snack (reward). The key to changing habits is to identify your cue and modify your routine and reward.








</doc>
<doc id="569092" url="https://en.wikipedia.org/wiki?curid=569092" title="Behavioral modernity">
Behavioral modernity

Behavioral modernity is a suite of behavioral and cognitive traits that distinguishes current "Homo sapiens" from other anatomically modern humans, hominins, and primates. Although often debated, most scholars agree that modern human behavior can be characterized by abstract thinking, planning depth, symbolic behavior (e.g., art, ornamentation), music and dance, exploitation of large game, and blade technology, among others. Underlying these behaviors and technological innovations are cognitive and cultural foundations that have been documented experimentally and ethnographically. Some of these human universal patterns are cumulative cultural adaptation, social norms, language, and extensive help and cooperation beyond close kin. It has been argued that the development of these modern behavioral traits, in combination with the climatic conditions of the Last Glacial Maximum causing genetic bottlenecks, was largely responsible for the human replacement of Neanderthals, Denisovans, and the other species of humans of the rest of the world.

Arising from differences in the archaeological record, a debate continues as to whether anatomically modern humans were behaviorally modern as well. There are many theories on the evolution of behavioral modernity. These generally fall into two camps: gradualist and cognitive approaches. The Later Upper Paleolithic Model refers to the theory that modern human behavior arose through cognitive, genetic changes abruptly around 40,000–50,000 years ago. Other models focus on how modern human behavior may have arisen through gradual steps; the archaeological signatures of such behavior only appearing through demographic or subsistence-based changes.

To classify what traits should be included in modern human behavior, it is necessary to define behaviors that are universal among living human groups. Some examples of these human universals are abstract thought, planning, trade, cooperative labor, body decoration, control and use of fire. Along with these traits, humans possess much reliance on social learning. This cumulative cultural change or cultural "ratchet" separates human culture from social learning in animals. As well, a reliance on social learning may be responsible in part for humans' rapid adaptation to many environments outside of Africa.
Since cultural universals are found in all cultures including some of the most isolated indigenous groups, these traits must have evolved or have been invented in Africa prior to the exodus.

Archaeologically, a number of empirical traits have been used as indicators of modern human behavior. While these are often debated a few are generally agreed upon. Archaeological evidence of behavioral modernity includes:


Several critiques have been placed against the traditional concept of behavioral modernity, both methodologically and philosophically. Shea (2011) outlines a variety of problems with this concept, arguing instead for "behavioral variability", which, according to the author, better describes the archaeological record. The use of trait lists, according to Shea (2011), runs the risk of taphonomic bias, where some sites may yield more artifacts than others despite similar populations; as well, trait lists can be ambiguous in how behaviors may be empirically recognized in the archaeological record. Shea (2011) in particular cautions that population pressure, cultural change, or optimality models, like those in human behavioral ecology, might better predict changes in tool types or subsistence strategies than a change from "archaic" to "modern" behavior. Some researchers argue that a greater emphasis should be placed on identifying only those artifacts which are unquestionably, or purely, symbolic as a metric for modern human behavior.

The Late Upper Paleolithic Model, or Upper Paleolithic Revolution, refers to the idea that, though anatomically modern humans first appear around 150,000 years ago, they were not cognitively or behaviorally "modern" until around 50,000 years ago, leading to their expansion into Europe and Asia. These authors note that traits used as a metric for behavioral modernity do not appear as a package until around 40–50,000 years ago. Klein (1995) specifically describes evidence of fishing, bone shaped as a tool, hearths, significant artifact diversity, and elaborate graves are all absent before this point. Although assemblages before 50,000 years ago show some diversity the only distinctly modern tool assemblages appear in Europe at 48,000. According to these authors, art only becomes common beyond this switching point, signifying a change from archaic to modern humans. Most researchers argue that a neurological or genetic change, perhaps one enabling complex language, such as FOXP2, caused this revolutionary change in our species.

Contrasted with this view of a spontaneous leap in cognition among ancient humans, some authors like Alison S. Brooks, primarily working in African archaeology, point to the gradual accumulation of "modern" behaviors, starting well before the 50,000 year benchmark of the Upper Paleolithic Revolution models. Howiesons Poort, Blombos, and other South African archaeological sites, for example, show evidence of marine resource acquisition, trade, the making of bone tools, blade and microlith technology, and abstract ornamentation at least by 80,000 years ago. Given evidence from Africa and the Middle East, a variety of hypotheses have been put forth to describe an earlier, gradual transition from simple to more complex human behavior. Some authors have pushed back the appearance of fully modern behavior to around 80,000 years ago in order to incorporate the South African data.

Others focus on the slow accumulation of different technologies and behaviors across time. These researchers describe how anatomically modern humans could have been cognitively the same and what we define as behavioral modernity is just the result of thousands of years of cultural adaptation and learning. D'Errico and others have looked at Neanderthal culture, rather than early human behavior exclusively, for clues into behavioral modernity. Noting that Neanderthal assemblages often portray traits similar to those listed for modern human behavior, researchers stress that the foundations for behavioral modernity may in fact lie deeper in our hominin ancestors. If both modern humans and Neanderthals express abstract art and complex tools then "modern human behavior" cannot be a derived trait for our species. They argue that the original "human revolution" theory reflects a profound Eurocentric bias. Recent archaeological evidence, they argue, proves that humans evolving in Africa some 300,000 or even 400,000 years ago were already becoming cognitively and behaviourally "modern". These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the "human revolution" model, but at sites that are widely separated in space and time. This suggests a gradual assembling of the package of modern human behaviours in Africa, and its later export to other regions of the Old World.

Between these extremes is the view – currently supported by archaeologists Chris Henshilwood, Curtis Marean, Ian Watts and others – that there was indeed some kind of 'human revolution' but that it occurred in Africa and spanned tens of thousands of years. The term "revolution" in this context would mean not a sudden mutation but a historical development along the lines of "the industrial revolution" or "the Neolithic revolution". In other words, it was a relatively accelerated process, too rapid for ordinary Darwinian "descent with modification" yet too gradual to be attributed to a single genetic or other sudden event. These archaeologists point in particular to the relatively explosive emergence of ochre crayons and shell necklaces apparently used for cosmetic purposes. These archaeologists see symbolic organisation of human social life as the key transition in modern human evolution. Recently discovered at sites such as Blombos Cave and Pinnacle Point, South Africa, pierced shells, pigments and other striking signs of personal ornamentation have been dated within a time-window of 70,000–160,000 years ago in the African Middle Stone Age, suggesting that the emergence of "Homo sapiens" coincided, after all, with the transition to modern cognition and behaviour. While viewing the emergence of language as a 'revolutionary' development, this school of thought generally attributes it to cumulative social, cognitive and cultural evolutionary processes as opposed to a single genetic mutation.

A further view, taken by archaeologists such as Francesco D'Errico and João Zilhão, is a multi-species perspective arguing that evidence for symbolic culture in the form of utilised pigments and pierced shells are also found in Neanderthal sites, independently of any "modern" human influence.

Cultural evolutionary models may also shed light on why although evidence of behavioral modernity exists before 50,000 years ago it is not expressed consistently until that point. With small population sizes, human groups would have been affected by demographic and cultural evolutionary forces that may not have allowed for complex cultural traits. According to some authors until population density became significantly high, complex traits could not have been maintained effectively. It is worth noting that some genetic evidence supports a dramatic increase in population size before human migration out of Africa. High local extinction rates within a population also can significantly decrease the amount of diversity in neutral cultural traits, regardless of cognitive ability.

Highly speculatively, bicameral mind theory argues for an additional, and cultural rather than genetic, shift from selfless to self-perceiving forms of human cognition and behavior very late in human history, in the Bronze Age. This is based on a literary analysis of Bronze Age texts which claims to show the first appearances of the concept of self around this time, replacing the voices of gods as the primary form of recorded human cognition. This non-mainstream theory is not widely accepted but does receive serious academic interest from time to time.

Before the Out of Africa theory was generally accepted, there was no consensus on where the human species evolved and, consequently, where modern human behavior arose. Now, however, African archaeology has become extremely important in discovering the origins of humanity. Since the first Cro-Magnon expansion into Europe around 48,000 years ago is generally accepted as already "modern", the question becomes whether behavioral modernity appeared in Africa well before 50,000 years ago, as a late Upper Paleolithic "revolution" which prompted migration out of Africa, or arose outside Africa and diffused back.

A variety of evidence of abstract imagery, widened subsistence strategies, and other "modern" behaviors have been discovered in Africa, especially South and North Africa. The Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was confirmed to be around 77,000 years old. Beads and other personal ornamentation have been found from Morocco which might be as much as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads dating from significantly prior to 50,000 years ago. Specialized projectile weapons as well have been found at various sites in Middle Stone Age Africa, including bone and stone arrowheads at South African sites such as Sibudu Cave (along with an early bone needle found at Sibudu) dating approximately 60,000-70,000 years ago, and bone harpoons at the Central African site of Katanda dating ca. 90,000 years ago. Evidence also exists for the systematic heat treating of silcrete stone to increased its flake-ability for the purpose of toolmaking, beginning approximately 164,000 years ago at the South African site of Pinnacle Point and becoming common there for the creation of microlithic tools at ca. 72,000 years ago. 
In 2008, an ochre processing workshop likely for the production of paints was uncovered dating to ca. 100,000 bc at Blombos Cave, South Africa. Analysis shows that a liquefied pigment-rich mixture was produced and stored in the two abalone shells, and that ochre, bone, charcoal, grindstones and hammer-stones also formed a composite part of the toolkits. Evidence for the complexity of the task includes procuring and combining raw materials from various sources (implying they had a mental template of the process they would follow), possibly using pyrotechnology to facilitate fat extraction from bone, using a probable recipe to produce the compound, and the use of shell containers for mixing and storage for later use.
Modern behaviors, such as the making of shell beads, bone tools and arrows, and the use of ochre pigment, are evident at a Kenyan site by 78,000-67,000 years ago. The oldest known stone-tipped projectile weapons (a characteristic tool of homo sapiens), the stone tips of javelins or throwing spears, are known from the Ethiopian site of Gademotta, and date to ca. 279,000 years ago.

Expanding subsistence strategies beyond big-game hunting and the consequential diversity in tool types has been noted as signs of behavioral modernity. A number of South African sites have shown an early reliance on aquatic resources from fish to shellfish. Pinnacle Point, in particular, shows exploitation of marine resources as early as 120,000 years ago, perhaps in response to more arid conditions inland. Establishing a reliance on predictable shellfish deposits, for example, could reduce mobility and facilitate complex social systems and symbolic behavior. Blombos Cave and Site 440 in Sudan both show evidence of fishing as well. Taphonomic change in fish skeletons from Blombos Cave have been interpreted as capture of live fish, clearly an intentional human behavior.

Humans in North Africa (Nazlet Sabaha, Egypt) are known to have dabbled in chert mining, as early as ≈100,000 years ago, for the construction of stone tools.

While traditionally described as evidence for the later Upper Paleolithic Model, European archaeology has shown that the issue is more complex. A variety of stone tool technologies are present at the time of human expansion into Europe and show evidence of modern behavior. Despite the problems of conflating specific tools with cultural groups, the Aurignacian tool complex, for example, is generally taken as a purely modern human signature. The discovery of "transitional" complexes, like "proto-Aurignacian", have been taken as evidence of human groups progressing through "steps of innovation". If, as this might suggest, human groups were already migrating into eastern Europe around 40,000 years and only afterward show evidence of behavioral modernity, then either the cognitive change must have diffused back into Africa or was already present before migration.

In light of a growing body of evidence of Neanderthal culture and tool complexes some researchers have put forth a "multiple species model" for behavioral modernity. Neanderthals were often cited as being an evolutionary dead-end, apish cousins who were less advanced than their human contemporaries. Personal ornaments were relegated as trinkets or poor imitations compared to the cave art produced by "H. sapiens". Despite this, European evidence has shown a variety of personal ornaments and artistic artifacts produced by Neanderthals; for example, the Neanderthal site of Grotte du Renne has produced grooved bear, wolf, and fox incisors, ochre and other symbolic artifacts. Although burials are few and controversial, there has been circumstantial evidence of Neanderthal ritual burials. There are two options to describe this symbolic behavior among Neanderthals: they copied cultural traits from arriving modern humans or they had their own cultural traditions comparative with behavioral modernity. If they just copied cultural traditions, which is debated by several authors, they still possessed the capacity for complex culture described by behavioral modernity. As discussed above, if Neanderthals also were "behaviorally modern" then it cannot be a species-specific derived trait.

Most debates surrounding behavioral modernity have been focused on Africa or Europe but an increasing amount of focus has been placed on East Asia. This region offers a unique opportunity to test hypotheses of multi-regionalism, replacement, and demographic effects. Unlike Europe, where initial migration occurred around 50,000 years ago, human remains have been dated in China to around 100,000 years ago. This early evidence of human expansion calls into question behavioral modernity as an impetus for migration.

Stone tool technology is particularly of interest in East Asia. Following Homo erectus migrations out of Africa, Acheulean technology never seems to appear beyond present-day India and into China. Analogously, Mode 3, or Levallois technology, is not apparent in China following later hominin dispersals. This lack of more advanced technology has been explained by serial founder effects and low population densities out of Africa. Although tool complexes comparative to Europe are missing or fragmentary, other archaeological evidence shows behavioral modernity. For example, the peopling of the Japanese archipelago offers an opportunity to investigate the early use of watercraft. Although one site, Kanedori in Honshu, does suggest the use of watercraft as early as 84,000 years ago, there is no other evidence of hominins in Japan until 50,000 years ago.

The Zhoukoudian cave system near Beijing has been excavated since the 1930s and has yielded precious data on early human behavior in East Asia. Although disputed, there is evidence of possible human burials and interred remains in the cave dated to around 34-20,000 years ago. These remains have associated personal ornaments in the form of beads and worked shell, suggesting symbolic behavior. Along with possible burials, numerous other symbolic objects like punctured animal teeth and beads, some dyed in red ochre, have all been found at Zhoukoudian. Although fragmentary, the archaeological record of eastern Asia shows evidence of behavioral modernity before 50,000 years ago but, like the African record, it is not fully apparent until that time.



</doc>
<doc id="1947070" url="https://en.wikipedia.org/wiki?curid=1947070" title="Demonstration effect">
Demonstration effect

Demonstration effects are effects on the behavior of individuals caused by observation of the actions of others and their consequences. The term is particularly used in political science and sociology to describe the fact that developments in one place will often act as a catalyst in another place.

Parents may take care of their parents to create a demonstration effect by which their children later care for them.

Countries and local governments may adopt laws and economic policies similar to those that appear to demonstrate success elsewhere. The proven success of the policies provides a demonstration effect that leads other governments to attempt to emulate that success.

The demonstration effect ye ye has been observed as a natural consequence of tourism. One study argues that the demonstration effect can be broken down into four forms: exact imitation, deliberately inexact imitation, accidental inexact imitation, and social learning.

In economics, demonstration effects may help explain the spread of financial or economic crises like the Asian financial crisis. Investors do not know everything about the economic situation of countries where they invest. When investors see a country's economy collapse, however, they may question the safety of investments in other countries with similar economic policies.

Some heterodox economists such as James Duesenberry and Robert H. Frank, following the original insights of Thorstein Veblen (1899), have argued that awareness of the consumption habits of others tends to inspire emulation in of these practices. Duesenberry (1949) gave the name "demonstration effect" to this phenomenon, arguing that it promoted unhappiness with current levels of consumption, which impacted savings rates and consequently opportunities for macroeconomic growth. Similarly, Ragnar Nurkse (1953) argued that the exposure of a society to new goods or ways of living creates unhappiness with what had previously been acceptable consumption practices; he dubbed it the "international demonstration effect." He claimed that in developing nations, pressure to increase access to material goods rapidly increases, primarily because people "come into contact with superior goods or superior patterns of consumption, with new articles or new ways of meeting old wants." As a result, he argued, these people are "apt to feel after a while a certain restlessness and dissatisfaction. Their knowledge is extended, their imagination stimulated; new desires are aroused".

In the late 18th century, the successful American Revolution may have provided a demonstration effect that sparked the subsequent French Revolution. Political movements may be given a boost from the observed success of similar movements in other countries. The domino effect thesis relates to this idea; it argued that communist revolutions in some countries would spread to other countries.



</doc>
<doc id="14594" url="https://en.wikipedia.org/wiki?curid=14594" title="Internet troll">
Internet troll

In Internet slang, a troll is a person who starts quarrels or upsets people on the Internet to distract and sow discord
by posting inflammatory and digressive, extraneous, or off-topic messages in an online community (such as a newsgroup, forum, chat room, or blog) with the intent of provoking readers into displaying emotional responses
and normalizing tangential discussion, whether for the troll's amusement or a specific gain.

Both the noun and the verb forms of "troll" are associated with Internet discourse. However, the word has also been used more widely. Media attention in recent years has equated trolling with online harassment. For example, the mass media have used "troll" to mean "a person who defaces Internet tribute sites with the aim of causing grief to families". In addition, depictions of trolling have been included in popular fictional works, such as the HBO television program "The Newsroom", in which a main character encounters harassing persons online and tries to infiltrate their circles by posting negative sexual comments.

Application of the term "troll" is subjective. Some readers may characterize a post as "trolling", while others may regard the same post as a legitimate contribution to the discussion, even if controversial. Like any pejorative term, it can be used as an "ad hominem" attack, suggesting a negative motivation.

As noted in an "OS News" article titled "Why People Troll and How to Stop Them" (25 January 2012), "The traditional definition of trolling includes intent. That is, trolls purposely disrupt forums. This definition is too narrow. Whether someone intends to disrupt a thread or not, the results are the same if they do." Others have addressed the same issue, e.g., Claire Hardaker, in her Ph.D. thesis "Trolling in asynchronous computer-mediated communication: From user discussions to academic definitions." Popular recognition of the existence (and prevalence) of non-deliberate, "accidental trolls", has been documented widely, in sources as diverse as Nicole Sullivan's keynote speech at the 2012 Fluent Conference, titled "Don't Feed the Trolls" Gizmodo, online opinions on the subject written by Silicon Valley executives and comics.

Regardless of the circumstances, controversial posts may attract a particularly strong response from those unfamiliar with the robust dialogue found in some online, rather than physical, communities. Experienced participants in online forums know that the most effective way to discourage a troll is usually to ignore it, because responding tends to encourage trolls to continue disruptive postshence the often-seen warning: "Please do not feed the trolls". Some believe this to be bad or incomplete advice for effectively dealing with trolls.

The "trollface" is an image occasionally used to indicate trolling in Internet culture.

At times the word is incorrectly used to refer to anyone with controversial, or differing, opinions. Such usage goes against the ordinary meaning of troll in multiple ways. While psychologists have determined that the dark triad traits are common among Internet trolls, some observers claim trolls don't actually believe the controversial views they claim. Farhad Manjoo criticises this view, noting that if the person really is trolling, they are more intelligent than their critics would believe.

There are competing theories of where and when "troll" was first used in Internet slang, with numerous unattested accounts of BBS and UseNet origins in the early 1980s or before.

The English noun "troll" in the standard sense of ugly dwarf or giant dates to 1610 and comes from the Old Norse word "troll" meaning giant or demon. The word evokes the trolls of Scandinavian folklore and children's tales: antisocial, quarrelsome and slow-witted creatures which make life difficult for travellers.

In modern English usage, "trolling" may describe the fishing technique of slowly dragging a lure or baited hook from a moving boat, whereas "trawling" describes the generally commercial act of dragging a fishing net. Early non-Internet slang use of "trolling" can be found in the military: by 1972 the term "trolling for MiGs" was documented in use by US Navy pilots in Vietnam. It referred to use of "...decoys, with the mission of drawing...fire away..."

The contemporary use of the term is said to have appeared on the Internet in the late 1980s, but the earliest known attestation according to the "Oxford English Dictionary" is in 1992.

The context of the quote cited in the "Oxford English Dictionary" sets the origin in Usenet in the early 1990s as in the phrase "trolling for newbies", as used in "alt.folklore.urban" (AFU). Commonly, what is meant is a relatively gentle inside joke by veteran users, presenting questions or topics that had been so overdone that only a new user would respond to them earnestly. For example, a veteran of the group might make a post on the common misconception that glass flows over time. Long-time readers would both recognize the poster's name and know that the topic had been discussed repeatedly, but new subscribers to the group would not realize, and would thus respond. These types of trolls served as a practice to identify group insiders. This definition of trolling, considerably narrower than the modern understanding of the term, was considered a positive contribution. One of the most notorious AFU trollers, David Mikkelson, went on to create the urban folklore website Snopes.com.

By the late 1990s, "alt.folklore.urban" had such heavy traffic and participation that trolling of this sort was frowned upon. Others expanded the term to include the practice of playing a seriously misinformed or deluded user, even in newsgroups where one was not a regular; these were often attempts at humor rather than provocation. The noun "troll" usually referred to an act of trolling – or to the resulting discussion – rather than to the author, though some posts punned on the dual meaning of "troll."

In Chinese, trolling is referred to as "bái mù" (), which can be straightforwardly explained as "eyes without pupils", in the sense that whilst the pupil of the eye is used for vision, the white section of the eye cannot see, and trolling involves blindly talking nonsense over the Internet, having total disregard to sensitivities or being oblivious to the situation at hand, akin to having eyes without pupils. An alternative term is "bái làn" (), which describes a post completely nonsensical and full of folly made to upset others, and derives from a Taiwanese slang term for the male genitalia, where genitalia that is pale white in colour represents that someone is young, and thus foolish. Both terms originate from Taiwan, and are also used in Hong Kong and mainland China. Another term, "xiǎo bái" () is a derogatory term for both "bái mù" and "bái làn" that is used on anonymous posting Internet forums. Another common term for a troll used in mainland China is "pēn zi" ().

In Japanese, means "fishing" and refers to intentionally misleading posts whose only purpose is to get the readers to react, i.e. get trolled. means "laying waste" and can also be used to refer to simple spamming.

In Icelandic, "þurs" (a thurs) or "tröll" (a troll) may refer to trolls, the verbs "þursa" (to troll) or "þursast" (to be trolling, to troll about) may be used.

In Korean, "nak-si" (낚시) means "fishing", refers to Internet trolling attempts, as well as purposefully misleading post titles. A person who recognizes the troll after having responded (or, in case of a post title "nak-si", having read the actual post) would often refer to himself as a caught fish.

In Portuguese, more commonly in its Brazilian variant, (produced in most of Brazil as spelling pronunciation) is the usual term to denote Internet trolls (examples of common derivate terms are "trollismo" or "trollagem", "trolling", and the verb "trollar", "to troll", which entered popular use), but an older expression, used by those which want to avoid anglicisms or slangs, is "" to denote trolling behavior, and "pombos enxadristas" (literally, "chessplayer pigeons") or simply "pombos" are the terms used to name the trolls. The terms are explained by an adage or popular saying: "Arguing with "fulano" (i.e., John Doe) is the same as playing chess with a pigeon: it defecates on the table, drops the pieces and simply flies off, claiming victory."

In Thai, the term "krian" (เกรียน) has been adopted to address Internet trolls. According to the Royal Institute of Thailand, the term, which literally refers to a closely cropped hairstyle worn by schoolboys in Thailand, is from the behaviour of these schoolboys who usually gather to play online games and, during which, make annoying, disruptive, impolite, or unreasonable expressions. The term "top krian" (ตบเกรียน; "slap a cropped head") refers to the act of posting intellectual replies to refute and cause the messages of Internet trolls to be perceived as unintelligent.

Early incidents of trolling were considered to be the same as flaming, but this has changed with modern usage by the news media to refer to the creation of any content that targets another person. The Internet dictionary NetLingo suggests there are four grades of trolling: playtime trolling, tactical trolling, strategic trolling, and domination trolling.
The relationship between trolling and flaming was observed in open-access forums in California, on a series of modem-linked computers. "CommuniTree" was begun in 1978 but was closed in 1982 when accessed by high school teenagers, becoming a ground for trashing and abuse. Some psychologists have suggested that flaming would be caused by deindividuation or decreased self-evaluation: the anonymity of online postings would lead to disinhibition amongst individuals Others have suggested that although flaming and trolling is often unpleasant, it may be a form of normative behavior that expresses the social identity of a certain user group
According to Tom Postmes, a professor of social and organisational psychology at the universities of Exeter, England, and Groningen, The Netherlands, and the author of "Individuality and the Group", who has studied online behavior for 20 years, "Trolls aspire to violence, to the level of trouble they can cause in an environment. They want it to kick off. They want to promote antipathetic emotions of disgust and outrage, which morbidly gives them a sense of pleasure."

The practice of trolling has been documented by a number of academics as early as the 1990s. This included Steven Johnson in 1997 in the book "Interface Culture", and a paper by Judith Donath in 1999. Donath's paper outlines the ambiguity of identity in a disembodied "virtual community" such as Usenet:

Donath provides a concise overview of identity deception games which trade on the confusion between physical and epistemic community:

Trolls can be costly in several ways. A troll can disrupt the discussion on a newsgroup or online forum, disseminate bad advice, and damage the feeling of trust in the online community. Furthermore, in a group that has become sensitized to trollingwhere the rate of deception is highmany honestly naïve questions may be quickly rejected as trolling. This can be quite off-putting to the new user who upon venturing a first posting is immediately bombarded with angry accusations. Even if the accusation is unfounded, being branded a troll may be damaging to one's online reputation.

Susan Herring and colleagues in "Searching for Safety Online: Managing 'Trolling' in a Feminist Forum" point out the difficulty inherent in monitoring trolling and maintaining freedom of speech in online communities: "harassment often arises in spaces known for their freedom, lack of censure, and experimental nature".
Free speech may lead to tolerance of trolling behavior, complicating the members' efforts to maintain an open, yet supportive discussion area, especially for sensitive topics such as race, gender, and sexuality.

In an effort to reduce uncivil behavior by increasing accountability, many web sites (e.g. Reuters, Facebook, and Gizmodo) now require commenters to register their names and e-mail addresses.

Investigative journalist Sharyl Attkisson is one of several in the media who has reported on the trend for organizations to utilize trolls to manipulate public opinion as part and parcel of an astroturfing initiative. Teams of sponsored trolls, sometimes referred to as sockpuppet armies, swarm a site to overwhelm any honest discourse and denigrate any who disagree with them.
A 2012 Pew Center on the States presentation on "effective messaging" included two examples of social media posts by a recently launched "rapid response team" dedicated to promoting fluoridation of community water supplies. That same presentation also emphasized changing the topic of conversation as a winning strategy.

A 2016 study by Harvard political scientist Gary King reported that the Chinese government's 50 Cent Party creates 440 million pro-government social media posts per year. The report said that government employees were paid to create pro-government posts around the time of national holidays to avoid mass political protests. The Chinese Government ran an editorial in the state-funded "Global Times" defending censorship and 50 Cent Party trolls.

A 2016 study for the NATO Strategic Communications Centre of Excellence on hybrid warfare notes that the Ukrainian crisis "demonstrated how fake identities and accounts were used to disseminate narratives through social media, blogs, and web commentaries in order to manipulate, harass, or deceive opponents." The NATO report describes that a "Wikipedia troll" uses a type of message design where a troll does not add "emotional value" to reliable "essentially true" information in re-posts, but presents it "in the wrong context, intending the audience to draw false conclusions." For example, information, without context, from Wikipedia about the military history of the United States "becomes value-laden if it is posted in the comment section of an article criticizing Russia for its military actions and interests in Ukraine. The Wikipedia troll is 'tricky', because in terms of actual text, the information is true, but the way it is expressed gives it a completely different meaning to its readers."

Unlike "classic trolls," Wikipedia trolls "have no emotional input, they just supply misinformation" and are one of "the most dangerous" as well as one of "the most effective trolling message designs." Even among people who are "emotionally immune to aggressive messages" and apolitical, "training in critical thinking" is needed, according to the NATO report, because "they have relatively blind trust in Wikipedia sources and are not able to filter information that comes from platforms they consider authoritative." While Russian-language hybrid trolls use the Wikipedia troll message design to promote anti-Western sentiment in comments, they "mostly attack aggressively to maintain emotional attachment to issues covered in articles." Discussions about topics other than international sanctions during the Ukrainian crisis "attracted very aggressive trolling" and became polarized according to the NATO report, which "suggests that in subjects in which there is little potential for re-educating audiences, emotional harm is considered more effective" for pro-Russian Latvian-language trolls.

"The New York Times" reported in late October 2018 that Saudi Arabia used an online army of Twitter trolls to harass the late Saudi dissident journalist Jamal Khashoggi and other critics of the Saudi government.

In October 2018, "The Daily Telegraph" reported that Facebook "banned hundreds of pages and accounts which it says were fraudulently flooding its site with partisan political content – although they came from the US instead of being associated with Russia." 

Researcher Ben Radford wrote about the phenomenon of clowns in history and modern day in his book "Bad Clowns" and found that bad clowns have evolved into Internet trolls. They do not dress up as traditional clowns but, for their own amusement, they tease and exploit "human foibles" in order to speak the "truth" and gain a reaction. Like clowns in make-up, Internet trolls hide behind "anonymous accounts and fake usernames." In their eyes they are the trickster and are performing for a nameless audience via the Internet.

A "concern troll" is a false flag pseudonym created by a user whose actual point of view is opposed to the one that the troll claims to hold. The concern troll posts in web forums devoted to its declared point of view and attempts to sway the group's actions or opinions while claiming to share their goals, but with professed "concerns". The goal is to sow fear, uncertainty, and doubt within the group often by appealing to outrage culture. This is a particular case of sockpuppeting and safe-baiting.

An example of this occurred in 2006 when Tad Furtado, a staffer for then-Congressman Charles Bass (R-NH), was caught posing as a "concerned" supporter of Bass's opponent, Democrat Paul Hodes, on several liberal New Hampshire blogs, using the pseudonyms "IndieNH" or "IndyNH". "IndyNH" expressed concern that Democrats might just be wasting their time or money on Hodes, because Bass was unbeatable. Hodes eventually won the election.

Although the term "concern troll" originated in discussions of online behavior, it now sees increasing use to describe similar behaviors that take place offline. For example, James Wolcott of "Vanity Fair" accused a conservative "New York Daily News" columnist of "concern troll" behavior in his efforts to downplay the Mark Foley scandal. Wolcott links what he calls concern trolls to what Saul Alinsky calls "Do-Nothings", giving a long quote from Alinsky on the Do-Nothings' method and effects:

"The Hill" published an op-ed piece by Markos Moulitsas of the liberal blog Daily Kos titled "Dems: Ignore 'Concern Trolls. The concern trolls in question were not Internet participants but rather Republicans offering public advice and warnings to the Democrats. The author defines "concern trolling" as "offering a poisoned apple in the form of advice to political opponents that, if taken, would harm the recipient". Concern trolls just use a different type of bait than the more stereotypical troll in their attempts to manipulate participants and disrupt conversations.

While many webmasters and forum administrators consider trolls a scourge on their sites, some websites welcome them. For example, a "The New York Times" article discussed troll activity at 4chan and at Encyclopedia Dramatica, which it described as "an online compendium of troll humor and troll lore". 4chan's /b/ board is recognized as "one of the Internet's most infamous and active trolling hotspots". This site and others are often used as a base to troll against sites that their members can not normally post on. These trolls feed off the reactions of their victims because "their agenda is to take delight in causing trouble".

The French internet group Ligue du LOL has been accused of organized harassment and described as a troll group.

Mainstream media outlets have focused their attention on the willingness of some Internet users to go to extreme lengths to participate in organized psychological harassment.

In February 2010, the Australian government became involved after users defaced the Facebook tribute pages of murdered children Trinity Bates and Elliott Fletcher. Australian communications minister Stephen Conroy decried the attacks, committed mainly by 4chan users, as evidence of the need for greater Internet regulation, stating, "This argument that the Internet is some mystical creation that no laws should apply to, that is a recipe for anarchy and the wild west." Facebook responded by strongly urging administrators to be aware of ways to ban users and remove inappropriate content from Facebook pages. In 2012, the "Daily Telegraph" started a campaign to take action against "Twitter trolls", who abuse and threaten users. Several high-profile Australians including Charlotte Dawson, Robbie Farah, Laura Dundovic, and Ray Hadley have been victims of this phenomenon.

Newslaundry covered the phenomenon of "Twitter trolling" in its "Criticles". It has also been characterising Twitter trolls in its weekly podcasts.

In the United Kingdom, contributions made to the Internet are covered by the Malicious Communications Act 1988 as well as Section 127 of the Communications Act 2003, under which jail sentences were, until 2015, limited to a maximum of six months. In October 2014, the UK's Justice Secretary, Chris Grayling, said that "Internet trolls" would face up to two years in jail, under measures in the Criminal Justice and Courts Bill that extend the maximum sentence and time limits for bringing prosecutions. The House of Lords Select Committee on Communications had earlier recommended against creating a specific offence of trolling. Sending messages which are "grossly offensive or of an indecent, obscene or menacing character" is an offence whether they are received by the intended recipient or not. Several people have been imprisoned in the UK for online harassment.

Trolls of the testimonial page of Georgia Varley faced no prosecution due to misunderstandings of the legal system in the wake of the term trolling being popularized. In October 2012, a twenty-year-old man was jailed for twelve weeks for posting offensive jokes to a support group for friends and family of April Jones.

On 31 March 2010, NBC's "Today" ran a segment detailing the deaths of three separate adolescent girls and trolls' subsequent reactions to their deaths. Shortly after the suicide of high school student Alexis Pilkington, anonymous posters began performing organized psychological harassment across various message boards, referring to Pilkington as a "suicidal slut", and posting graphic images on her Facebook memorial page. The segment also included an exposé of a 2006 accident, in which an eighteen-year-old fatally crashed her father's car into a highway pylon; trolls emailed her grieving family the leaked pictures of her mutilated corpse (see Nikki Catsouras photographs controversy).

In 2007, the media was fooled by trollers into believing that students were consuming a drug called Jenkem, purportedly made of human waste. A user named Pickwick on TOTSE posted pictures implying that he was inhaling this drug. Major news corporations such as Fox News Channel reported the story and urged parents to warn their children about this drug. Pickwick's pictures of Jenkem were fake and the pictures did not actually feature human waste.

In August 2012, the subject of trolling was featured on the HBO television series "The Newsroom". The character of Neal Sampat encounters harassing individuals online, particularly looking at 4chan, and he ends up choosing to post negative comments himself on an economics-related forum. The attempt by the character to infiltrate trolls' inner circles attracted debate from media reviewers critiquing the series.

The publication of the 2015 non-fiction book "The Dark Net: Inside the Digital Underworld" by Jamie Bartlett, a journalist and a representative of the British think tank Demos, attracted some attention for its depiction of misunderstood sections of the Internet, describing interactions on encrypted sites such as those accessible with the software Tor. Detailing trolling-related groups and the harassment created by them, Bartlett advocated for greater awareness of them and monitoring of their activities. Professor Matthew Wisnioski wrote for "The Washington Post" that a "league of trolls, anarchists, perverts and drug dealers is at work building a digital world beyond the Silicon Valley offices where our era's best and brightest have designed a Facebook-friendly" surface and agreed with Bartlett that the activities of trolls go back decades to the Usenet "flame wars" of the 1990s and even earlier.

In February 2019, Glenn Greenwald wrote that a cybersecurity company New Knowledge "was caught just six weeks ago engaging in a massive scam to create fictitious Russian troll accounts on Facebook and Twitter in order to claim that the Kremlin was working to defeat Democratic Senate nominee Doug Jones in Alabama. "The New York Times", when exposing the scam, quoted a New Knowledge report that boasted of its fabrications: “We orchestrated an elaborate ‘false flag’ operation that planted the idea that the [Roy] Moore campaign was amplified on social media by a Russian botnet.'" 

As reported on 8 April 1999, investors became victims of trolling via an online financial discussion regarding PairGain, a telephone equipment company based in California. Trolls operating in the stock's Yahoo Finance chat room posted a fabricated Bloomberg News article stating that an Israeli telecom company could potentially acquire PairGain. As a result, PairGain's stock jumped by 31%. However, the stock promptly crashed after the reports were identified as false.

So-called Gold Membership trolling originated in 2007 on 4chan boards, when users posted fake images claiming to offer upgraded 4chan account privileges; without a "Gold" account, one could not view certain content. This turned out to be a hoax designed to fool board members, especially newcomers. It was copied and became an Internet meme. In some cases, this type of troll has been used as a scam, most notably on Facebook, where fake Facebook Gold Account upgrade ads have proliferated in order to link users to dubious websites and other content.

The case of "Zeran v. America Online, Inc." resulted primarily from trolling. Six days after the Oklahoma City bombing, anonymous users posted advertisements for shirts celebrating the bombing on AOL message boards, claiming that the shirts could be obtained by contacting Mr. Kenneth Zeran. The posts listed Zeran's address and home phone number. Zeran was subsequently harassed.

Anti-Scientology protests by Anonymous, commonly known as Project Chanology, are sometimes labeled as "trolling" by media such as "Wired", and the participants sometimes explicitly self-identify as "trolls".

Neo-Nazi website "The Daily Stormer" orchestrates what it calls a "Troll Army", and has encouraged trolling of Jewish MP Luciana Berger and Muslim activist Mariam Veiszadeh.






</doc>
<doc id="45413769" url="https://en.wikipedia.org/wiki?curid=45413769" title="Safety behaviors (anxiety)">
Safety behaviors (anxiety)

Safety behaviors (also known as safety-seeking behaviors) are coping behaviors used to reduce anxiety and fear when the user feels threatened. An example of a safety behavior in social anxiety is to think of excuses to escape a potentially uncomfortable situation. These safety behaviors, although useful for reducing anxiety in the short term, might become maladaptive over the long term by prolonging anxiety and fear of nonthreatening situations. This problem is commonly experienced in anxiety disorders. Treatments such as exposure and response prevention focus on eliminating safety behaviors due to the detrimental role safety behaviors have in mental disorders. There is a disputed claim that safety behaviors can be beneficial to use during the early stages of treatment.

The concept of safety behaviors was first related to a mental disorder in 1984 when the “safety perspective” hypothesis was proposed to explain how agoraphobia is maintained over time. The “safety perspective” hypothesis states that people with agoraphobia act in ways they believe will increase or maintain their level of safety. In 1991, the use of safety behaviors was observed in people with panic disorders. Later studies observed the use of safety behaviors in people with other disorders such as social phobia, obsessive compulsive disorder, and posttraumatic stress disorder.

Safety behaviors directly amplify fear and anxiety.

Safety behaviors reduce anxiety in feared situations but retain anxiety in the long term.

Safety behaviors can be grouped into two major categories: preventive and restorative safety behaviors.

These behaviors are also known as "emotional avoidance behaviors". These behaviors are aimed to reduce fear or anxiety in future situations. 
Examples include:

These behaviors are aimed to reduce fear or anxiety in a currently threatening situation. 
Examples include:

People may increase their risk for agoraphobia when they use safety behaviors to avoid potentially dangerous environments even though the danger may not be as severe as perceived. A common safety behavior is when a person with agoraphobia attempts to entirely avoid a crowded place such as a mall or a public bus. If the affected person does end up in a crowded area, then the person may tense his or her legs to prevent collapsing in the area. The affected person may also attempt to escape these crowded situations. People with agoraphobia then attribute the lack of feared symptoms to the safety behaviors instead of to the lack of danger itself. This incorrect attribution may lead to persisting fears and symptoms.

People with generalized anxiety disorder (GAD) view the world as a highly threatening environment. These people continuously search for safety and use safety behaviors. A common safety behavior used by GAD sufferers is seeking reassurance from a loved one to reduce the excessive worry. The affected person may also attempt to avoid all possible risks of danger and protect others from that danger. However, these behaviors are unlikely to significantly reduce anxiety because the affected person often has multiple fears that are not clearly defined.

People with insomnia tend to excessively worry about getting enough sleep and the consequences of not getting enough sleep. These people use safety behaviors in an attempt to reduce their excessive anxiety. However, the use of safety behaviors serves to increase anxiety and reduce the chances that the affected person will disconfirm these anxiety-provoking thoughts. A common safety behavior used by affected people is attempting to control the anxiety-provoking thoughts by distracting themselves with other thoughts. The affected person may also cancel appointments and decide not to work because the person believes that he or she will not function properly. The affected person may take naps to compensate for the lack of sleep.

People with obsessive-compulsive disorder (OCD) use safety behaviors to reduce their anxiety when obsessions arise. Common safety behaviors include washing hands more times than needed and avoiding potential contaminants by not shaking hands. However, when people with OCD use safety behaviors to reduce the chance of contamination, their awareness of potential contamination increases. This heightened awareness then leads to an increased fear of being contaminated.

Checking rituals, such as checking several times to determine if all of the doors to a house are locked, are also common safety behaviors. People with OCD often believe that if they do not perform their checking rituals, others will be in danger. Consequentially, people with OCD often perceive themselves as more responsible for the wellbeing of others than people without the disorder. Therefore, people with OCD use safety behaviors when they believe that other people will be in danger if these behaviors are not used. Continuous checking reduces the certainty and vividness of memories related to checking. Exposure and response prevention therapy is effective in treating OCD.

People with posttraumatic stress disorder (PTSD) believe that their general safety has been compromised after a trauma has occurred. These people use safety behaviors to restore their general sense of safety and to prevent the trauma from happening again. A common safety behavior used by affected people is staying awake for long periods of time to make sure that potential intruders do not attempt to break into their homes. The person may also attempt to avoid potential reminders of the trauma such as moving away from the place where the trauma occurred. These behaviors may lead to persistent fears because the behaviors prevent the affected person from disconfirming the threatening beliefs.

People with schizophrenia may have persecutory delusions. These people use safety behaviors to prevent the potential threats that arise from their persecutory delusions. Common safety behaviors include avoiding locations where perceived persecutors can be found and physically escaping from the perceived persecutors. These behaviors may increase the amount of persecutory delusions the person experiences because the safety behaviors prevent the affected person from disconfirming the threatening beliefs.

Generally, people use social behaviors to either seek approval or avoid disapproval from others. People without social anxiety tend to use behaviors that are designed to gain approval from others, while people with social anxiety prefer to use behaviors that help to avoid disapproval from others.

Safety behaviors seem to reduce the chances of obtaining criticism by drawing less attention to the affected person. Common safety behaviors include avoiding eye contact with other people, focusing on saying the proper words, and other self-controlling behaviors.

Exposure therapy alone is mildly effective in treating social anxiety. There are larger decreases in anxiety and fear when people are also told to stop themselves from using safety behaviors during therapy than when people are encouraged to use safety behaviors. These decreases are largest when people are told to stop using safety behaviors and disconfirm the thoughts that the threatening situation will most likely not happen even if the safety behaviors are stopped. This combination of techniques is used in exposure and response prevention therapy for social anxiety.

Several assessments have been developed to measure the amount of safety behaviors used by people with specific psychological conditions. Two examples of assessments developed to measure safety behaviors performed by people with social anxiety are the Social Behavior Questionnaire and the Subtle Avoidance Frequency Examination. An assessment developed to measure safety behaviors performed by people with panic disorder is the Texas Safety Maneuver Scale.

The Social Behavior Questionnaire (SBQ) is an assessment of safety behaviors in social anxiety that was developed in 1994. The frequency at which a behavior is performed is rated from “never” to “always.” Examples of safety behaviors recorded in this assessment include “avoiding asking questions” and “controlling shaking.” The SBQ has been shown to distinguish between people with strong from people with weak fears of being negatively evaluated by others.

The Subtle Avoidance Frequency Examination (SAFE) is an assessment of safety behaviors in social anxiety that was developed in 2009. The frequency at which a behavior is performed and the total number of safety behaviors utilized is rated from “never” to “always.” Examples of safety behaviors recorded in this assessment include “speaking softly” and “avoiding eye contact.” This measure has been shown to distinguish between people with clinical levels of social anxiety and those without. This assessment has also been shown to support other measures of social anxiety such as the Social Phobia Scale.

The Texas Safety Maneuver Scale (TSMS) is an assessment of safety behaviors in panic disorder that was developed in 1998. The frequency at which each behavior is performed is measured on a five-point scale from “never” to “always.” Examples of safety behaviors recorded in this assessment include “checking pulse” and “avoiding stressful encounters.” This assessment has also been shown to correlate with agoraphobia measures such as the Fear Questionnaire.

Some researchers have claimed that safety behaviors can be helpful in therapy but only when the behaviors are used during the early stages of treatment. For example, exposure therapy will appear less threatening if patients are able to use safety behaviors during the treatment. Patients will also feel more in control in the threatening situations if they are able to use their safety behaviors to reduce anxiety. The studies testing this claim have shown mixed results.



</doc>
